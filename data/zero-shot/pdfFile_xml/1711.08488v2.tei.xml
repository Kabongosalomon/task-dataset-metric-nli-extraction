<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frustum PointNets for 3D Object Detection from RGB-D Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nuro, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nuro, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Frustum PointNets for 3D Object Detection from RGB-D Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, great progress has been made on 2D image understanding tasks, such as object detection <ref type="bibr" target="#b12">[13]</ref> and instance segmentation <ref type="bibr" target="#b13">[14]</ref>. However, beyond getting 2D bounding boxes or pixel masks, 3D understanding is eagerly in demand in many applications such as autonomous driving and augmented reality (AR). With the popularity of 3D sensors deployed on mobile devices and autonomous vehicles, more and more 3D data is captured and processed. In this work, we study one of the most important 3D perception tasks -3D object detection, which classifies the object category and estimates oriented 3D bounding boxes of physical objects from 3D sensor data.</p><p>While 3D sensor data is often in the form of point clouds, how to represent point cloud and what deep net architectures to use for 3D object detection remains an open problem. Most existing works convert 3D point clouds to images by projection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b25">26]</ref> or to volumetric grids by quantization <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> and then apply convolutional networks. * Majority of the work done as an intern at Nuro, Inc.  <ref type="figure">Figure 1</ref>. 3D object detection pipeline. Given RGB-D data, we first generate 2D object region proposals in the RGB image using a CNN. Each 2D region is then extruded to a 3D viewing frustum in which we get a point cloud from depth data. Finally, our frustum PointNet predicts a (oriented and amodal) 3D bounding box for the object from the points in frustum.</p><p>This data representation transformation, however, may obscure natural 3D patterns and invariances of the data. Recently, a number of papers have proposed to process point clouds directly without converting them to other formats. For example, <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> proposed new types of deep net architectures, called PointNets, which have shown superior performance and efficiency in several 3D understanding tasks such as object classification and semantic segmentation.</p><p>While PointNets are capable of classifying a whole point cloud or predicting a semantic class for each point in a point cloud, it is unclear how this architecture can be used for instance-level 3D object detection. Towards this goal, we have to address one key challenge: how to efficiently propose possible locations of 3D objects in a 3D space. Imitating the practice in image detection, it is straightforward to enumerate candidate 3D boxes by sliding windows <ref type="bibr" target="#b7">[8]</ref> or by 3D region proposal networks such as <ref type="bibr" target="#b32">[33]</ref>. However, the computational complexity of 3D search typically grows cubically with respect to resolution and becomes too expensive for large scenes or real-time applications such as autonomous driving.</p><p>Instead, in this work, we reduce the search space following the dimension reduction principle: we take the advantage of mature 2D object detectors ( <ref type="figure">Fig. 1</ref>). First, we extract the 3D bounding frustum of an object by extruding 2D bounding boxes from image detectors. Then, within the 3D space trimmed by each of the 3D frustums, we consecutively perform 3D object instance segmentation and amodal 3D bounding box regression using two variants of Point-Net. The segmentation network predicts the 3D mask of the object of interest (i.e. instance segmentation); and the regression network estimates the amodal 3D bounding box (covering the entire object even if only part of it is visible).</p><p>In contrast to previous work that treats RGB-D data as 2D maps for CNNs, our method is more 3D-centric as we lift depth maps to 3D point clouds and process them using 3D tools. This 3D-centric view enables new capabilities for exploring 3D data in a more effective manner. First, in our pipeline, a few transformations are applied successively on 3D coordinates, which align point clouds into a sequence of more constrained and canonical frames. These alignments factor out pose variations in data, and thus make 3D geometry pattern more evident, leading to an easier job of 3D learners. Second, learning in 3D space can better exploits the geometric and topological structure of 3D space. In principle, all objects live in 3D space; therefore, we believe that many geometric structures, such as repetition, planarity, and symmetry, are more naturally parameterized and captured by learners that directly operate in 3D space. The usefulness of this 3D-centric network design philosophy has been supported by much recent experimental evidence.</p><p>Our method achieve leading positions on KITTI 3D object detection <ref type="bibr" target="#b0">[1]</ref> and bird's eye view detection <ref type="bibr" target="#b1">[2]</ref> benchmarks. Compared with the previous state of the art <ref type="bibr" target="#b5">[6]</ref>, our method is 8.04% better on 3D car AP with high efficiency (running at 5 fps). Our method also fits well to indoor RGB-D data where we have achieved 8.9% and 6.4% better 3D mAP than <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b29">[30]</ref> on SUN-RGBD while running one to three orders of magnitude faster.</p><p>The key contributions of our work are as follows:</p><p>? We propose a novel framework for RGB-D data based 3D object detection called Frustum PointNets.</p><p>? We show how we can train 3D object detectors under our framework and achieve state-of-the-art performance on standard 3D object detection benchmarks.</p><p>? We provide extensive quantitative evaluations to validate our design choices as well as rich qualitative results for understanding the strengths and limitations of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Object Detection from RGB-D Data Researchers have approached the 3D detection problem by taking various ways to represent RGB-D data. Front view image based methods: <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref> take monocular RGB images and shape priors or occlusion patterns to infer 3D bounding boxes. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7]</ref> represent depth data as 2D maps and apply CNNs to localize objects in 2D image. In comparison we represent depth as a point cloud and use advanced 3D deep networks (PointNets) that can exploit 3D geometry more effectively.</p><p>Bird's eye view based methods: MV3D <ref type="bibr" target="#b5">[6]</ref> projects Li-DAR point cloud to bird's eye view and trains a region proposal network (RPN <ref type="bibr" target="#b28">[29]</ref>) for 3D bounding box proposal. However, the method lags behind in detecting small objects, such as pedestrians and cyclists and cannot easily adapt to scenes with multiple objects in vertical direction.</p><p>3D based methods: <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34]</ref> train 3D object classifiers by SVMs on hand-designed geometry features extracted from point cloud and then localize objects using slidingwindow search. <ref type="bibr" target="#b7">[8]</ref> extends <ref type="bibr" target="#b37">[38]</ref> by replacing SVM with 3D CNN on voxelized 3D grids. <ref type="bibr" target="#b29">[30]</ref> designs new geometric features for 3D object detection in a point cloud. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b16">17]</ref> convert a point cloud of the entire scene into a volumetric grid and use 3D volumetric CNN for object proposal and classification. Computation cost for those method is usually quite high due to the expensive cost of 3D convolutions and large 3D search space. Recently, <ref type="bibr" target="#b15">[16]</ref> proposes a 2Ddriven 3D object detection method that is similar to ours in spirit. However, they use hand-crafted features (based on histogram of point coordinates) with simple fully connected networks to regress 3D box location and pose, which is sub-optimal in both speed and performance. In contrast, we propose a more flexible and effective solution with deep 3D feature learning (PointNets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learning on Point Clouds</head><p>Most existing works convert point clouds to images or volumetric forms before feature learning. <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> voxelize point clouds into volumetric grids and generalize image CNNs to 3D CNNs. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8]</ref> design more efficient 3D CNN or neural network architectures that exploit sparsity in point cloud. However, these CNN based methods still require quantitization of point clouds with certain voxel resolution. Recently, a few works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> propose a novel type of network architectures (PointNets) that directly consumes raw point clouds without converting them to other formats. While PointNets have been applied to single object classification and semantic segmentation, our work explores how to extend the architecture for the purpose of 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Definition</head><p>Given RGB-D data as input, our goal is to classify and localize objects in 3D space. The depth data, obtained from LiDAR or indoor depth sensors, is represented as a point cloud in RGB camera coordinates. The projection matrix is also known so that we can get a 3D frustum from a 2D image region. Each object is represented by a class (one among k predefined classes) and an amodal 3D bounding box. The amodal box bounds the complete object even if part of the object is occluded or truncated. The 3D box is  parameterized by its size h, w, l, center c x , c y , c z , and orientation ?, ?, ? relative to a predefined canonical pose for each category. In our implementation, we only consider the heading angle ? around the up-axis for orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">3D Detection with Frustum PointNets</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our system for 3D object detection consists of three modules: frustum proposal, 3D instance segmentation, and 3D amodal bounding box estimation. We will introduce each module in the following subsections. We will focus on the pipeline and functionality of each module, and refer readers to supplementary for specific architectures of the deep networks involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Frustum Proposal</head><p>The resolution of data produced by most 3D sensors, especially real-time depth sensors, is still lower than RGB images from commodity cameras. Therefore, we leverage mature 2D object detector to propose 2D object regions in RGB images as well as to classify objects.</p><p>With a known camera projection matrix, a 2D bounding box can be lifted to a frustum (with near and far planes specified by depth sensor range) that defines a 3D search space for the object. We then collect all points within the frustum to form a frustum point cloud. As shown in <ref type="figure">Fig 4 (a)</ref>, frustums may orient towards many different directions, which result in large variation in the placement of point clouds. We therefore normalize the frustums by rotating them toward a center view such that the center axis of the frustum is orthogonal to the image plane. This normalization helps improve the rotation-invariance of the algorithm. We call this entire procedure for extracting frustum point clouds from RGB-D data frustum proposal generation.</p><p>While our 3D detection framework is agnostic to the exact method for 2D region proposal, we adopt a FPN <ref type="bibr" target="#b19">[20]</ref> based model. We pre-train the model weights on ImageNet classification and COCO object detection datasets and further fine-tune it on a KITTI 2D object detection dataset to classify and predict amodal 2D boxes. More details of the 2D detector training are provided in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D Instance Segmentation</head><p>Given a 2D image region (and its corresponding 3D frustum), several methods might be used to obtain 3D location of the object: One straightforward solution is to directly regress 3D object locations (e.g., by 3D bounding box) from a depth map using 2D CNNs. However, this problem is not easy as occluding objects and background clutter is common in natural scenes (as in <ref type="figure" target="#fig_3">Fig. 3</ref>), which may severely distract the 3D localization task. Because objects are naturally separated in physical space, segmentation in 3D point cloud is much more natural and easier than that in images where pixels from distant objects can be near-by to each other. Having observed this fact, we propose to seg-   ment instances in 3D point cloud instead of in 2D image or depth map. Similar to Mask-RCNN <ref type="bibr" target="#b13">[14]</ref>, which achieves instance segmentation by binary classification of pixels in image regions, we realize 3D instance segmentation using a PointNet-based network on point clouds in frustums. Based on 3D instance segmentation, we are able to achieve residual based 3D localization. That is, rather than regressing the absolute 3D location of the object whose offset from the sensor may vary in large ranges (e.g. from 5m to beyond 50m in KITTI data), we predict the 3D bounding box center in a local coordinate system -3D mask coordinates as shown in <ref type="figure">Fig. 4</ref> (c).</p><p>3D Instance Segmentation PointNet. The network takes a point cloud in frustum and predicts a probability score for each point that indicates how likely the point belongs to the object of interest. Note that each frustum contains exactly one object of interest. Here those "other" points could be points of non-relevant areas (such as ground, vegetation) or other instances that occlude or are behind the object of interest. Similar to the case in 2D instance segmentation, depending on the position of the frustum, object points in one frustum may become cluttered or occlude points in another. Therefore, our segmentation PointNet is learning the occlusion and clutter patterns as well as recognizing the geometry for the object of a certain category.</p><p>In a multi-class detection case, we also leverage the semantics from a 2D detector for better instance segmentation. For example, if we know the object of interest is a pedestrian, then the segmentation network can use this prior to find geometries that look like a person. Specifically, in our architecture we encode the semantic category as a one-hot class vector (k dimensional for the pre-defined k categories) and concatenate the one-hot vector to the intermediate point cloud features. More details of the specific architectures are described in the supplementary.</p><p>After 3D instance segmentation, points that are classified as the object of interest are extracted ("masking" in Having obtained these segmented object points, we further normalize its coordinates to boost the translational invariance of the algorithm, following the same rationale as in the frustum proposal step. In our implementation, we transform the point cloud into a local coordinate by subtracting XYZ values by its centroid. This is illustrated in <ref type="figure">Fig. 4</ref> (c). Note that we intentionally do not scale the point cloud, because the bounding sphere size of a partial point cloud can be greatly affected by viewpoints and the real size of the point cloud helps the box size estimation.</p><p>In our experiments, we find that coordinate transformations such as the one above and the previous frustum rotation are critical for 3D detection result as shown in Tab. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Amodal 3D Box Estimation</head><p>Given the segmented object points (in 3D mask coordinate), this module estimates the object's amodal oriented 3D bounding box by using a box regression PointNet together with a preprocessing transformer network.</p><p>Learning-based 3D Alignment by T-Net Even though we have aligned segmented object points according to their centroid position, we find that the origin of the mask coordinate frame ( <ref type="figure">Fig. 4 (c)</ref>) may still be quite far from the amodal box center. We therefore propose to use a light-weight regression PointNet (T-Net) to estimate the true center of the complete object and then transform the coordinate such that the predicted center becomes the origin ( <ref type="figure">Fig. 4 (d)</ref>).</p><p>The architecture and training of our T-Net is similar to the T-Net in <ref type="bibr" target="#b24">[25]</ref>, which can be thought of as a special type of spatial transformer network (STN) <ref type="bibr" target="#b14">[15]</ref>. However, different from the original STN that has no direct supervision on transformation, we explicitly supervise our translation network to predict center residuals from the mask coordinate origin to real object center.</p><p>Amodal 3D Box Estimation PointNet The box estimation network predicts amodal bounding boxes (for entire object even if part of it is unseen) for objects given an object point cloud in 3D object coordinate ( <ref type="figure">Fig. 4 (d)</ref>). The network architecture is similar to that for object classification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>, however the output is no longer object class scores but parameters for a 3D bounding box.</p><p>As stated in Sec. 3, we parameterize a 3D bounding box by its center (c x , c y , c z ), size (h, w, l) and heading angle ? (along up-axis). We take a "residual" approach for box center estimation. The center residual predicted by the box estimation network is combined with the previous center residual from the T-Net and the masked points' centroid to recover an absolute center (Eq. 1). For box size and heading angle, we follow previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24]</ref> and use a hybrid of classification and regression formulations. Specifically we pre-define N S size templates and N H equally split angle bins. Our model will both classify size/heading (N S scores for size, N H scores for heading) to those pre-defined categories as well as predict residual numbers for each category (3 ? N S residual dimensions for height, width, length, N H residual angles for heading). In the end the net outputs 3 + 4 ? N S + 2 ? N H numbers in total.</p><formula xml:id="formula_0">C pred = C mask + ?C t?net + ?C box?net (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training with Multi-task Losses</head><p>We simultaneously optimize the three nets involved (3D instance segmentation PointNet, T-Net and amodal box estimation PointNet) with multi-task losses (as in Eq. 2). L c1?reg is for T-Net and L c2?reg is for center regression of box estimation net. L h?cls and L h?reg are losses for heading angle prediction while L s?cls and L s?reg are for box size. Softmax is used for all classification tasks and smooth-l 1 (huber) loss is used for all regression cases.</p><formula xml:id="formula_1">L multi?task =Lseg + ?(Lc1?reg + Lc2?reg + L h?cls + L h?reg + L s?cls + Ls?reg + ?Lcorner)<label>(2)</label></formula><p>Corner Loss for Joint Optimization of Box Parameters While our 3D bounding box parameterization is compact and complete, learning is not optimized for final 3D box accuracy -center, size and heading have separate loss terms. Imagine cases where center and size are accurately predicted but heading angle is off -the 3D IoU with ground truth box will then be dominated by the angle error. Ideally all three terms (center,size,heading) should be jointly optimized for best 3D box estimation (under IoU metric).</p><p>To resolve this problem we propose a novel regularization loss, the corner loss:</p><formula xml:id="formula_2">Lcorner = N S i=1 N H j=1 ?ijmin{ 8 k=1 P ij k ? P * k , 8 i=1 P ij k ? P * * k }<label>(3)</label></formula><p>In essence, the corner loss is the sum of the distances between the eight corners of a predicted box and a ground truth box. Since corner positions are jointly determined by center, size and heading, the corner loss is able to regularize the multi-task training for those parameters.</p><p>To compute the corner loss, we firstly construct N S ? N H "anchor" boxes from all size templates and heading angle bins. The anchor boxes are then translated to the estimated box center. We denote the anchor box corners as P ij k , where i, j, k are indices for the size class, heading class, and (predefined) corner order, respectively. To avoid large penalty from flipped heading estimation, we further compute distances to corners (P * * k ) from the flipped ground truth box and use the minimum of the original and flipped cases. ? ij , which is one for the ground truth size/heading class and zero else wise, is a two-dimensional mask used to select the distance term we care about.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Experiments are divided into three parts 1 . First we compare with state-of-the-art methods for 3D object detection on KITTI <ref type="bibr" target="#b9">[10]</ref> and SUN-RGBD <ref type="bibr" target="#b32">[33]</ref> (Sec 5.1). Second, we provide in-depth analysis to validate our design choices (Sec 5.2). Last, we show qualitative results and discuss the strengths and limitations of our methods (Sec 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparing with state-of-the-art Methods</head><p>We evaluate our 3D object detector on KITTI <ref type="bibr" target="#b10">[11]</ref> and SUN-RGBD <ref type="bibr" target="#b32">[33]</ref> benchmarks for 3D object detection. On both tasks we have achieved significantly better results compared with state-of-the-art methods.</p><p>KITTI Tab. 1 shows the performance of our 3D detector on the KITTI test set. We outperform previous state-of-theart methods by a large margin. While MV3D <ref type="bibr" target="#b5">[6]</ref> uses multiview feature aggregation and sophisticated multi-sensor fusion strategy, our method based on the PointNet <ref type="bibr" target="#b24">[25]</ref> (v1) and PointNet++ <ref type="bibr" target="#b26">[27]</ref> (v2) backbone is much cleaner in design. While out of the scope for this work, we expect that sensor fusion (esp. aggregation of image feature for 3D detection) could further improve our results.</p><p>We also show our method's performance on 3D object localization (bird's eye view) in Tab. 2. In the 3D localization task bounding boxes are projected to bird's eye view plane and IoU is evaluated on oriented 2D boxes. Again, our method significantly outperforms previous works which include DoBEM <ref type="bibr" target="#b41">[42]</ref> and MV3D <ref type="bibr" target="#b5">[6]</ref> that use CNNs on projected LiDAR images, as well as 3D FCN <ref type="bibr" target="#b16">[17]</ref> that uses 3D CNNs on voxelized point cloud.  <ref type="bibr" target="#b41">[42]</ref> and MV3D <ref type="bibr" target="#b5">[6]</ref> (previous state of the art) are based on 2D CNNs with bird's eye view LiDAR image. Our method, without sensor fusion or multi-view aggregation, outperforms those methods by large margins on all categories and data subsets. 3D bounding box IoU threshold is 70% for cars and 50% for pedestrians and cyclists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Cars The output of our network is visualized in <ref type="figure" target="#fig_5">Fig. 6</ref> where we observe accurate 3D instance segmentation and box prediction even under very challenging cases. We defer more discussions on success and failure case patterns to Sec. 5.3. We also report performance on KITTI val set (the same split as in <ref type="bibr" target="#b5">[6]</ref>) in Tab. 3 and Tab. 4 (for cars) to support comparison with more published works, and in Tab. 5 (for pedestrians and cyclists) for reference. that image CNNs can be easily applied. However, methods designed for bird's eye view may be incapable for indoor rooms where multiple objects often exist together in vertical space. On the other hand, indoor focused methods could find it hard to apply to sparse and large-scale point cloud from LiDAR scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUN-RGBD</head><p>In contrast, our frustum-based PointNet is a generic framework for both outdoor and indoor 3D object detection. By applying the same pipeline we used for KITTI data set, we've achieved state-of-the-art performance on SUN-RGBD benchmark (Tab. 6) with significantly higher mAP as well as much faster (10x-1000x) inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Architecture Design Analysis</head><p>In this section we provide analysis and ablation experiments to validate our design choices.</p><p>Experiment setup. Unless otherwise noted, all experiments in this section are based on our v1 model on KITTI data using train/val split as in <ref type="bibr" target="#b5">[6]</ref>. To decouple the influence of 2D detectors, we use ground truth 2D boxes for region proposals and use 3D box estimation accuracy (IoU threshold 0.7) as the evaluation metric. We will only focus on the car category which has the most training examples.   <ref type="table">Table 6</ref>. 3D object detection AP on SUN-RGBD val set. Evaluation metric is average precision with 3D IoU threshold 0.25 as proposed by <ref type="bibr" target="#b32">[33]</ref>. Note that both COG <ref type="bibr" target="#b29">[30]</ref> and 2D-driven <ref type="bibr" target="#b15">[16]</ref> use room layout context to boost performance while ours and DSS <ref type="bibr" target="#b34">[35]</ref> not. Compared with previous state-of-the-arts our method is 6.4% to 11.9% better in mAP as well as one to three orders of magnitude faster.</p><p>Comparing with alternative approaches for 3D detection. In this part we evaluate a few CNN-based baseline approaches as well as ablated versions and variants of our pipelines using 2D masks. In the first row of Tab. 7, we show 3D box estimation results from two CNN-based networks. The baseline methods trained VGG [32] models on ground truth boxes of RGB-D images and adopt the same box parameter and loss functions as our main method. While the model in the first row directly estimates box location and parameters from vanilla RGB-D image patch, the other one (second row) uses a FCN trained from the COCO dataset for 2D mask estimation (as that in Mask-RCNN <ref type="bibr" target="#b13">[14]</ref>) and only uses features from the masked region for prediction. The depth values are also translated by subtracting the median depth within the 2D mask. However, both CNN baselines get far worse results compared to our main method.</p><p>To understand why CNN baselines underperform, we vi-sualize a typical 2D mask prediction in <ref type="figure">Fig. 7</ref>. While the estimated 2D mask appears in high quality on an RGB image, there are still lots of clutter and foreground points in the 2D mask. In comparison, our 3D instance segmentation gets much cleaner result, which greatly eases the next module in finer localization and bounding box regression.</p><p>In the third row of Tab. 7, we experiment with an ablated version of frustum PointNet that has no 3D instance segmentation module. Not surprisingly, the model gets much worse results than our main method, which indicates the critical effect of our 3D instance segmentation module. In the fourth row, instead of 3D segmentation we use point clouds from 2D masked depth maps ( <ref type="figure">Fig. 7)</ref> for 3D box estimation. However, since a 2D mask is not able to cleanly segment the 3D object, the performance is more than 12% worse than that with the 3D segmentation (our main method in the fifth row). On the other hand, a combined usage of 2D and 3D masks -applying 3D segmentation on point cloud network arch. from 2D masked depth map -also shows slightly worse results than our main method probably due to the accumulated error from inaccurate 2D mask predictions. <ref type="figure">Fig. 4</ref>, our frustum PointNet takes a few key coordinate transformations to canonicalize the point cloud for more effective learning. Tab. 8 shows how each normalization step helps for 3D detection. We see that both frustum rotation (such that frustum points have more similar XYZ distributions) and mask centroid subtraction (such that object points have smaller and more canonical XYZ) are critical. In addition, extra alignment of object point cloud to object center by T-Net also contributes significantly to the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of point cloud normalization. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of regression loss formulation and corner loss.</head><p>In Tab. 9 we compare different loss options and show that a combination of "cls-reg" loss (the classification and residual regression approach for heading and size regression) and a regularizing corner loss achieves the best result.</p><p>The naive baseline using regression loss only (first row) achieves unsatisfactory result because the regression target is large in range (object size from 0.2m to 5m). In comparison, the cls-reg loss and a normalized version (residual normalized by heading bin size or template shape size) of it achieve much better performance. At last row we show that a regularizing corner loss further helps optimization.  <ref type="figure">Figure 7</ref>. Comparisons between 2D and 3D masks. We show a typical 2D region proposal from KITTI val set with both 2D (on RGB image) and 3D (on frustum point cloud) instance segmentation results. The red numbers denote depth ranges of points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Results and Discussion</head><p>In <ref type="figure" target="#fig_5">Fig. 6</ref> we visualize representative outputs of our frustum PointNet model. We see that for simple cases of nonoccluded objects in reasonable distance (so we get enough number of points), our model outputs remarkably accurate 3D instance segmentation mask and 3D bounding boxes. Second, we are surprised to find that our model can even predict correctly posed amodal 3D box from partial data (e.g. parallel parked cars) with few points. Even humans find it very difficult to annotate such results with point cloud data only. Third, in some cases that seem very challenging in images with lots of nearby or even overlapping 2D boxes, when converted to 3D space, the localization becomes much easier (e.g. P11 in second row third column).</p><p>On the other hand, we do observe several failure patterns, which indicate possible directions for future efforts. The first common mistake is due to inaccurate pose and size estimation in a sparse point cloud (sometimes less than 5 points). We think image features could greatly help esp. since we have access to high resolution image patch even for far-away objects. The second type of challenge is when there are multiple instances from the same category in a frustum (like two persons standing by). Since our current pipeline assumes a single object of interest in each frustum, it may get confused when multiple instances appear and thus outputs mixed segmentation results. This problem could potentially be mitigated if we are able to propose multiple 3D bounding boxes within each frustum. Thirdly, sometimes our 2D detector misses objects due to dark lighting or strong occlusion. Since our frustum proposals are based on region proposal, no 3D object will be detected given no 2D detection. However, our 3D instance segmentation and amodal 3D box estimation PointNets are not restricted to RGB view proposals. As shown in the supplementary, the same framework can also be extended to 3D regions proposed in bird's eye view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>This document provides additional technical details, extra analysis experiments, more quantitative results and qualitative test results to the main paper.</p><p>In Sec.B we provide more details on network architectures of PointNets and training parameters while Sec. C explains more about our 2D detector. Sec. D shows how our framework can be extended to bird's eye view (BV) proposals and how combining BV and RGB proposals can further improve detection performance. Then Sec. E presents results from more analysis experiments. At last, Sec. F shows more visualization results for 3D detection on SUN-RGBD dataset. <ref type="figure" target="#fig_1">(Sec 4.2, 4.3)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details on Frustum PointNets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Network Architectures</head><p>We adopt similar network architectures as in the original works of PointNet <ref type="bibr" target="#b24">[25]</ref> and PointNet++ <ref type="bibr" target="#b26">[27]</ref> for our v1 and v2 models respectively. What is different is that we add an extra link for class one-hot vector such that instance segmentation and bounding box estimation can leverage semantics predicted from RGB images. The detailed network architectures are shown in <ref type="figure" target="#fig_6">Fig. 8</ref>.</p><p>For v1 model our architecture involves point embedding layers (as shared MLP on each point independently), a max pooling layer and per-point classification multi-layer perceptron (MLP) based on aggregated information from global feature and each point as well as an one-hot class vector. Note that we do not use the transformer networks as in <ref type="bibr" target="#b24">[25]</ref> because frustum points are viewpoint based (not complete point cloud as in <ref type="bibr" target="#b24">[25]</ref>) and are already normalized by frustum rotation. In addition to XYZ , we also leverage LiDAR intensity as a fourth channel.</p><p>For v2 model we use set abstraction layers for hierarchical feature learning in point clouds. In addition, because Li-DAR point cloud gets increasingly sparse as it gets farther, feature learning has to be robust to those density variations. Therefore we used a robust type of set abstraction layers -multi-scale grouping (MSG) layers as introduced in <ref type="bibr" target="#b26">[27]</ref> for the segmentation network. With hierarchical features and learned robustness to varying densities, our v2 model shows superior performance than v1 model in both segmentation and box estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Data Augmentation and Training</head><p>Data augmentation Data augmentation plays an important role in preventing model overfitting. Our augmentation involves two branches: one is 2D box augmentation and the other is frustum point cloud augmentation.</p><p>We use ground truth 2D boxes to generate frustum point clouds for Frustum PointNets training and augment the 2D boxes by random translation and scaling. Specifically, we   <ref type="bibr" target="#b24">[25]</ref>. v2 models are based on PointNet++ <ref type="bibr" target="#b26">[27]</ref> set abstraction (SA) and feature propagation (FP) layers. The architecture for residual center estimation T-Net is shared for Ours (v1) and Ours (v2). The colors (blue for segmentaiton nets, red for T-Net and green for box estimation nets) of the network background indicate the coordinate system of the input point cloud. Segmentation nets operate in frustum coordinate, T-Net processes points in mask coordinate while box estimation nets take points in object coordinate. The small yellow square (or bar) concatenated with global features is class one-hot vector that tells the predicted category of the underlying object.</p><p>firstly compute the 2D box height (h) and width (w) and translate the 2D box center by random distances sampled from Uniform[?0.1w, 0.1w] and Uniform[?0.1h, 0.1h] in u,v directions respectively. The height and width are also augmented by two random scaling factor sampled from Uniform[0.9, 1.1].</p><p>We augment each frustum point cloud by three ways. First, we randomly sample a subset of points from the frustum point cloud on the fly (1,024 for KITTI and 2,048 for SUN-RGBD). For object points segmented from our predicted 3D mask, we randomly sample 512 points from it (if there are less than 512 points we will randomly resample to make up for the number). Second, we randomly flip the frustum point cloud (after rotating the frustum to the center) along the YZ plane in camera coordinate (Z is forward, Y is pointing down). Thirdly, we perturb the points by shifting the entire frustum point cloud in Z-axis direction such that the depth of points is augmented. Together with all data augmentation, we modify the ground truth labels for 3D mask and headings correspondingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI Training</head><p>The object detection benchmark in KITTI provides synchronized RGB images and LiDAR point clouds with ground truth amodal 2D and 3D box annotations for vehicles, pedestrians and cyclists. The training set contains 7,481 frames and an undisclosed test set contains 7,581 frames. In our own experiments (except those for test sets), we follow <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> to split the official training set to a train set of 3,717 frames and a val set of 3769 frames such that frames in train/val sets belong to different video clips. For models evaluated on the test set we train our model on our own train/val split where around 80% of the training data is used such that the model can achieve better generalization by seeing more examples.</p><p>To get ground truth for 3D instance segmentation we simply consider all points that fall into the ground truth 3D bounding box as object points. Although there are sometimes false labels from ground points or points from other closeby objects (e.g. a person standing by), the auto-labeled segmentation ground truth is in general acceptable.</p><p>For both of our v1 and v2 models, we use Adam optimizer with starting learning rate 0.001, with step-wise decay (by half) in every 60k iterations. For all trainable layers except the last classification or regression ones, we use batch normalization with a start decay rate of 0.5 and gradually decay the decay rate to 0.99 (step-wise decay with rate 0.5 in every 20k iterations). We use batch size 32 for v1 models and batch size 24 for v2 models. All three Point-Nets are trained end-to-end.</p><p>Trained on a single GTX 1080 GPU, it takes around one day to train a v1 model (all three nets) for 200 epochs while it takes around three days for a v2 model. We picked the early stopped (200 epochs) snapshot models for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUN-RGBD Training</head><p>The data set consists of 10,355 RGB-D images captured from various depth sensors for indoor scenes (bedrooms, dining rooms etc.). We follow the same train/val splits as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b29">30]</ref> for experiments. The data augmentation and optimization parameters are the same as that in KITTI.</p><p>As to auto-labeling of instance segmentation mask, however, data quality is much lower than that in KITTI because of strong occlusions and tight arrangement of objects in indoor scenes (see <ref type="figure" target="#fig_8">Fig. 11</ref> for some examples). Nonetheless we still consider all points within the ground truth boxes as object points for our training. For 3D segmentation we get only a 82.7% accuracy compared to around 90% in KITTI. Due to the heavy noise in segmentation mask label, we choose to only train and evaluate on v1 models that has more strength in global feature learning than v2 ones. For future works, we think higher quality in 3D mask labels can greatly help the instance segmentation network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details on RGB Detector (Sec 4.1)</head><p>For 2D RGB image detector, we use the encoder-decoder structure (e.g. DSSD <ref type="bibr" target="#b8">[9]</ref>, FPN <ref type="bibr" target="#b19">[20]</ref>) to generate region proposals from multiple feature maps using focal loss <ref type="bibr" target="#b20">[21]</ref> and use Fast R-CNN <ref type="bibr" target="#b11">[12]</ref> to predict final 2D detection bounding boxes from the region proposals.</p><p>To make the detector faster, we take the reduced VGG <ref type="bibr" target="#b31">[32]</ref> base network architecture from SSD <ref type="bibr" target="#b21">[22]</ref>, sample half of the channels per layer and change all max pooling layers to convolution layers with 3 ? 3 kernel size and stride of 2. Then we fine-tune it on ImageNet CLS-LOC dataset for 400k iterations with batch size of 260 on 10 GPUs. The resulting base network architecture has about 66.7% top-1 classification accuracy on the CLS-LOC validation dataset and only needs about 1.2ms to process a 224 ? 224 image on a NVIDIA GTX 1080.</p><p>We then add the feature pyramid layers <ref type="bibr" target="#b19">[20]</ref> from conv3 3, conv4 3, conv5 3, and fc7, which are used to predict region proposals with scales of 16, 32, 64, 128 respectively. We also add an extra convolutional layer (conv8) which halves the fc7 feature map size, and use it to predict proposals with scale of 256. We use 5 different aspect ratios { 1 3 , 1 2 , 1, 2, 3} for all layers except that we ignore { 1 3 , 3} for conv3 3. Following SSD, we also use normalization layer on conv3 3, conv4 3, and conv5 3 and initialize the norm 40. For Fast R-CNN part, we extract features from conv3 3, conv5 3, and conv8 for each region proposal and concatenate all the features to predict class scores and further adjust the proposals. We train this detector from COCO dataset with 384 ? 384 input image and have achieved 35.5 mAP on the COCO minival dataset, with only 10ms processing time for a 384 ? 384 image on a single GPU.</p><p>Finally, we fine-tune the detector on car, people, and bicycle from COCO dataset, and have achieved 48.5, 44.1, and 40.1 for these three classes on COCO. We take this model and further fine-tune it on car, pedestrian, and cyclist from KITTI dataset. The final model takes about 30ms to process a 384 ? 1280 image. To increase the recall of the detector, we also do detection from the center crop of the image besides the full image, and then merge the detections using non-maximum suppression.</p><p>Tab. 10 shows our detector's AP (2D) on KITTI test set. Our detector has achieved competitive or better results than current leading players on KITTI leader board. We've also reported our AP (2D) on val set in Tab. 11 for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Bird's Eye View PointNets (Sec 5.3)</head><p>In this section, we show that our 3D detection framework can also be extended to using bird's eye view proposals, which adds another orthogonal proposal source to achieve better overall 3D detection performance. We evaluate the results of car detection using LiDAR bird's eye view only proposals + point net (Ours(BV)), and combine frustum point net and bird's eye view point net using 3D nonmaximum suppression (NMS) (Ours(Frustum + BV)). The results are shown in <ref type="table" target="#tab_2">Table 12</ref>.</p><p>Bird's Eye View Proposal Similar to MV3D <ref type="bibr" target="#b5">[6]</ref> we use point features such as height, intensity and density, and train the bird's eye view 2D proposal net using the standard Faster-RCNN <ref type="bibr" target="#b28">[29]</ref>  discretize the projected point clouds into 2D grids with resolution of 0.1 meter and with the depth and width range 0 60 meters, which gives us the 600 ? 600 input size. For each cell, we take the intensity and the density of the highest point and divide the heights into 7 bins with the height of the highest point in each bin, which gives us 9 channels in total. In Faster R-CNN, we use the VGG-16 <ref type="bibr" target="#b31">[32]</ref> with 3 anchor scales <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">48</ref>) and 3 aspect ratios ( 1 2 , 1, 2). We train RPN and Fast R-CNN together using the approximate joint training.</p><p>To combine 3D detection boxes from frustum PointNets and the bird's eye view PointNets, we use 3D NMS with IoU threshold 0.8. We also apply a weight (0.5) to 3D boxes from BV PointNets since it is a weaker detector compared with our frustum one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bird's Eye View (BV) PointNets Similar to Frustum</head><p>PointNets that take point cloud in frustum, segment point cloud and estimate amodal bounding box, we can apply PointNets to points in bird's eye view regions. Since bird's eye view is based on orthogonal projection, the 3D space specified by a BV 2D box is a 3D cuboid (cut by minimum and maximum height) instead of a frustum.</p><p>Results Tab. 12 (Ours BV) shows the APs we get by using bird's eye view proposals only (without and RGB information). We compare with two previous LiDAR only methods (VeloFCN <ref type="bibr" target="#b17">[18]</ref> and MV3D (BV+FV) <ref type="bibr" target="#b5">[6]</ref>) and show that our BV proposal based detector greatly outperforms VeloFCN on all cases and outperforms MV3D (BV+FV) on moderate and hard cases by a significant margin.</p><p>More importantly, we show in the last row of Tab. 12 that bird's eye view and RGB view proposals can be combined to achieve an even better performance (3.8% AP improvement on hard cases). <ref type="figure">Fig. 9</ref> gives an intuitive explanation of why bird's eye view proposals could help. In the sample frame shown: while our 2D detector misses some highly occluded cars ( <ref type="figure">Fig. 9</ref>: left RGB image), bird's eye view based RPN successfully detects them ( <ref type="figure">Fig. 9</ref>: blue arrows in right LiDAR image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Easy Moderate Hard VeloFCN <ref type="bibr" target="#b17">[18]</ref> 15.20 <ref type="bibr" target="#b12">13</ref>  <ref type="table" target="#tab_2">Table 13</ref> compares PointNet <ref type="bibr" target="#b24">[25]</ref> (v1) and Point-Net++ <ref type="bibr" target="#b26">[27]</ref> (v2) architectures for instance segmentation and amodal box estimation. The v2 model outperforms v1 model on both tasks because 1) v2 model learns hierarchical features that are richer and more generalizable; 2) v2 model uses multi-scale feature learning that adapts to varying point densities. Note that the ours (v1) model corresponds to first row of <ref type="table" target="#tab_2">Table 13</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Effects of Training Data Size</head><p>Recently <ref type="bibr" target="#b36">[37]</ref> observed linear improvement in performance of deep learning models with exponential growth of data set size. In our Frustum PointNets we observe similar trend <ref type="figure">(Fig. 10)</ref>. This trend indicates a promising performance potential of our methods with larger datasets.</p><p>We train three separate group of Frustum PointNets on three sets of training data and then evaluate the model on a fixed validation set (1929 samples). The three data points in <ref type="figure">Fig. 10</ref> represent training set sizes of 1388, 2776, 5552 samples (0.185x, 0.371x, 0.742x of the entire trainval set) respectively. We augment the training data such that the total amount of samples are the same for each of the three cases (20x, 10x and 5x augmentation respectively). The training set and validation set are chosen such that they don't share frames from the same video clips.  <ref type="figure">Figure 10</ref>. Effects of training data size. Evaluation metric is 3D box estimation accuracy (IoU threshold 0.7). We see a clear trend of linear improvement in accuracy with exponential growth of training data size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Runtime and Model Size</head><p>In <ref type="table" target="#tab_2">Table 14</ref>, we show decomposed runtime cost (inference time) for our frustum PointNets (v1 and v2). The evaluation is based on TensorFlow <ref type="bibr" target="#b2">[3]</ref> with a NVIDIA GTX 1080 and a single CPU core. While for v1 model frustum proposal (with CNN and backprojection) takes the majority time, for v2 model since a PointNet++ <ref type="bibr" target="#b26">[27]</ref> model with multi-scale grouping is used, computation bottleneck shifts to instance segmentation. Note that we merge batch normalization and FC/convolution layers for faster inference (since they are both linear operation with multiply and sum), which results in close to 50% speedup for inference.</p><p>CNN model has size 28 MB. v1 PointNets have size 19MB. v2 PointNets have size 22MB. The total size is therefore 47MB for v1 model and 50MB for v2 model. <ref type="table" target="#tab_2">Box Est.  Total  v1  60 ms  18 ms  10 ms  88 ms  v2  60 ms  88 ms  19 ms  167 ms  Table 14</ref>. 3D detector runtime. Thirty-two region proposals used for frustum-based PointNets. 1,024 points are used for instance segmentation and 512 points are used for box estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Frustum Proposal 3D Seg</head><p>F. Visualizations for SUN-RGBD (Sec 5.1)</p><p>In <ref type="figure" target="#fig_8">Fig. 11</ref> we visualize some representative detection results on SUN-RGBD data. We can see that compared with KITTI LiDAR data, depth images can be popped up to much more dense point clouds. However even with such dense point cloud, strong occlusions of indoor objects as well as the tight arrangement present new challenges for detection in indoor scenes.</p><p>In <ref type="figure" target="#fig_1">Fig. 12</ref> we report the 3D AP curves of our Frustum PointNets on SUN-RGBD val set. 2D detection APs of our RGB detector are also provided in Tab. 11 for reference.   <ref type="table" target="#tab_2">Table 15</ref>. 2D and 3D object detection AP on SUN-RGBD val set. 2D IoU threshold is 0.5. Note that on some categories we get higher 3D AP (displayed in the table as well, the same results as in main paper) than 2D AP because our network is able to recover 3D geometry from very partial scan and is also due to a more loose 3D IoU threshold (0.25) in SUN-RGBD 3D AP evaluation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>depth to point cloud 2D region (from CNN) to 3D frustum 3D box (from PointNet)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Frustum PointNets for 3D object detection. We first leverage a 2D CNN object detector to propose 2D regions and classify their content. 2D regions are then lifted to 3D and thus become frustum proposals. Given a point cloud in a frustum (n ? c with n points and c channels of XYZ, intensity etc. for each point), the object instance is segmented by binary classification of each point. Based on the segmented object point cloud (m ? c), a light-weight regression PointNet (T-Net) tries to align points by translation such that their centroid is close to amodal box center. At last the box estimation net estimates the amodal 3D bounding box for the object. More illustrations on coordinate systems involved and network input, output are inFig. 4andFig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Challenges for 3D detection in frustum point cloud. Left: RGB image with an image region proposal for a person. Right: bird's eye view of the LiDAR points in the extruded frustum from 2D box, where we see a wide spread of points with both foreground occluder (bikes) and background clutter (building).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 Figure 5 .</head><label>25</label><figDesc>Basic architectures and IO for PointNets. Architecture is illustrated for PointNet++<ref type="bibr" target="#b26">[27]</ref> (v2) models with set abstraction layers and feature propagation layers (for segmentation). Coordinate systems involved are visualized inFig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visualizations of Frustum PointNet results on KITTI val set (best viewed in color with zoom in). These results are based on PointNet++ models<ref type="bibr" target="#b26">[27]</ref>, running at 5 fps and achieving test set 3D AP of 70.39, 44.89 and 56.77 for car, pedestrian and cyclist, respectively. 3D instance masks on point cloud are shown in color. True positive detection boxes are in green, while false positive boxes are in red and groundtruth boxes in blue are shown for false positive and false negative cases. Digit and letter beside each box denote instance id and semantic class, with "v" for cars, "p" for pedestrian and "c" for cyclist. See Sec. 5.3 for more discussion on the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Network architectures for Frustum PointNets. v1 models are based on PointNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(log scale)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Visualization of Frustum PointNets results on SUN-RGBD val set. First row: RGB image with 2D detection boxes. Second row: point cloud popped up from depth map and predicted amodal 3D bounding boxes (the numbers beside boxes correspond to 2D boxes on images). Green boxes are true positive. Red boxes are false positives. False negatives are not visualized. Third row: point cloud popped up from depth map and ground truth amodal 3D bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Precision recall (PR) curves for 3D object detection on SUN-RGBD val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>3D object detection 3D AP on KITTI test set. DoBEM</figDesc><table><row><cell>Method</cell><cell cols="3">Cars Easy Moderate Hard</cell><cell cols="3">Pedestrians Easy Moderate Hard</cell><cell cols="3">Cyclists Easy Moderate Hard</cell></row><row><cell>DoBEM [42]</cell><cell>7.42</cell><cell>6.95</cell><cell>13.45</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MV3D [6]</cell><cell>71.09</cell><cell>62.35</cell><cell>55.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (v1)</cell><cell>80.62</cell><cell>64.70</cell><cell>56.07</cell><cell>50.88</cell><cell>41.55</cell><cell>38.04</cell><cell>69.36</cell><cell>53.50</cell><cell>52.88</cell></row><row><cell>Ours (v2)</cell><cell>81.20</cell><cell>70.39</cell><cell>62.19</cell><cell>51.21</cell><cell>44.89</cell><cell>40.23</cell><cell>71.96</cell><cell>56.77</cell><cell>50.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .Table 3 .Table 4 .</head><label>234</label><figDesc>3D object localization AP (bird's eye view) on KITTI test set. 3D FCN<ref type="bibr" target="#b16">[17]</ref> uses 3D CNNs on voxelized point cloud and is far from real-time. MV3D<ref type="bibr" target="#b5">[6]</ref> is the previous state of the art. Our method significantly outperforms those methods on all categories and data subsets. Bird's eye view 2D bounding box IoU threshold is 70% for cars and 50% for pedestrians and cyclists. 3D object detection AP on KITTI val set (cars only). 3D object localization AP on KITTI val set (cars only).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pedestrians</cell><cell></cell><cell></cell><cell>Cyclists</cell><cell></cell></row><row><cell></cell><cell cols="3">Easy Moderate Hard</cell><cell cols="3">Easy Moderate Hard</cell><cell cols="3">Easy Moderate Hard</cell></row><row><cell>DoBEM [42]</cell><cell>36.49</cell><cell>36.95</cell><cell>38.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3D FCN [17]</cell><cell>69.94</cell><cell>62.54</cell><cell>55.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MV3D [6]</cell><cell>86.02</cell><cell>76.90</cell><cell>68.49</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (v1)</cell><cell>87.28</cell><cell>77.09</cell><cell>67.90</cell><cell>55.26</cell><cell>47.56</cell><cell>42.57</cell><cell>73.42</cell><cell>59.87</cell><cell>52.88</cell></row><row><cell>Ours (v2)</cell><cell>88.70</cell><cell>84.00</cell><cell>75.33</cell><cell>58.09</cell><cell>50.22</cell><cell>47.20</cell><cell>75.38</cell><cell>61.96</cell><cell>54.68</cell></row><row><cell>Method</cell><cell cols="3">Easy Moderate Hard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mono3D [4]</cell><cell>2.53</cell><cell>2.31</cell><cell>2.31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DOP [5]</cell><cell>6.55</cell><cell>5.07</cell><cell>4.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VeloFCN [17]</cell><cell>15.20</cell><cell>13.66</cell><cell>15.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MV3D (LiDAR) [6]</cell><cell>71.19</cell><cell>56.60</cell><cell>55.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MV3D [6]</cell><cell>71.29</cell><cell>62.68</cell><cell>56.56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (v1)</cell><cell>83.26</cell><cell>69.28</cell><cell>62.56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (v2)</cell><cell>83.76</cell><cell>70.92</cell><cell>63.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Easy Moderate Hard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mono3D [4]</cell><cell>5.22</cell><cell>5.19</cell><cell>4.13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DOP [5]</cell><cell>12.63</cell><cell>9.49</cell><cell>7.59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VeloFCN [17]</cell><cell>40.14</cell><cell>32.08</cell><cell>30.47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MV3D (LiDAR) [6]</cell><cell>86.18</cell><cell>77.32</cell><cell>76.33</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MV3D [6]</cell><cell>86.55</cell><cell>78.10</cell><cell>76.67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (v1)</cell><cell>87.82</cell><cell>82.44</cell><cell>74.77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (v2)</cell><cell>88.16</cell><cell>84.02</cell><cell>76.44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Most previous 3D detection works specialize either on outdoor LiDAR scans where objects are well separated in space and the point cloud is sparse (so that it's feasible for bird's eye projection), or on indoor depth maps that are regular images with dense pixel values such Performance on KITTI val set for pedestrians and cyclists.Model evaluated is Ours (v2).</figDesc><table><row><cell>Benchmark</cell><cell cols="3">Easy Moderate Hard</cell></row><row><cell>Pedestrian (3D Detection)</cell><cell>70.00</cell><cell>61.32</cell><cell>53.59</cell></row><row><cell cols="2">Pedestrian (Bird's Eye View) 72.38</cell><cell>66.39</cell><cell>59.57</cell></row><row><cell>Cyclist (3D Detection)</cell><cell>77.15</cell><cell>56.49</cell><cell>53.37</cell></row><row><cell>Cyclist (Bird's Eye View)</cell><cell>81.82</cell><cell>60.03</cell><cell>56.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .Table 8 .Table 9 .</head><label>789</label><figDesc>Comparing 2D and 3D approaches. 2D mask is from FCN on RGB image patch. 3D mask is from PointNet on frustum point cloud. 2D+3D mask is 3D mask generated by PointNet on point cloud poped up from 2D masked depth map. Effects of point cloud normalization. Metric is 3D box estimation accuracy with IoU=0.7. Effects of 3D box loss formulations. Metric is 3D box estimation accuracy with IoU=0.7.</figDesc><table><row><cell></cell><cell>mask</cell><cell cols="3">depth representation accuracy</cell></row><row><cell>ConvNet</cell><cell>-</cell><cell cols="2">image</cell><cell>18.3</cell></row><row><cell>ConvNet</cell><cell>2D</cell><cell cols="2">image</cell><cell>27.4</cell></row><row><cell>PointNet</cell><cell>-</cell><cell cols="2">point cloud</cell><cell>33.5</cell></row><row><cell>PointNet</cell><cell>2D</cell><cell cols="2">point cloud</cell><cell>61.6</cell></row><row><cell>PointNet</cell><cell>3D</cell><cell cols="2">point cloud</cell><cell>74.3</cell></row><row><cell>PointNet</cell><cell>2D+3D</cell><cell cols="2">point cloud</cell><cell>70.0</cell></row><row><cell cols="5">frustum rot. mask centralize t-net accuracy</cell></row><row><cell>-? -? ?</cell><cell></cell><cell>--? ? ?</cell><cell>----?</cell><cell>12.5 48.1 64.6 71.5 74.3</cell></row><row><cell cols="2">loss type</cell><cell cols="3">regularization accuracy</cell></row><row><cell cols="2">regression only</cell><cell>-</cell><cell></cell><cell>62.9</cell></row><row><cell></cell><cell>cls-reg</cell><cell>-</cell><cell></cell><cell>71.8</cell></row><row><cell cols="2">cls-reg (normalized)</cell><cell>-</cell><cell></cell><cell>72.2</cell></row><row><cell cols="2">cls-reg (normalized)</cell><cell cols="2">corner loss</cell><cell>74.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .Table 11 .</head><label>1011</label><figDesc>structure. The net outputs axis-aligned 2D bounding boxes in the bird's eye view. In detail, we 2D object detection AP on KITTI test set. Evaluation IoU threshold is 0.7. SWC is the first place winner on KITTI leader board for pedestrians and cyclists at the time of submission. Our 2D results are based on a CNN model on monocular RGB images. Our 2D object detection AP on KITTI val set.</figDesc><table><row><cell>Method</cell><cell cols="3">Cars Easy Moderate Hard</cell><cell cols="3">Pedestrians Easy Moderate Hard</cell><cell cols="3">Cyclists Easy Moderate Hard</cell></row><row><cell>SWC</cell><cell>90.82</cell><cell>90.05</cell><cell>80.59</cell><cell>87.06</cell><cell>78.65</cell><cell>73.92</cell><cell>86.02</cell><cell>77.58</cell><cell>68.44</cell></row><row><cell cols="2">RRC [28] 90.61</cell><cell>90.22</cell><cell>87.44</cell><cell>84.14</cell><cell>75.33</cell><cell>70.39</cell><cell>84.96</cell><cell>76.47</cell><cell>65.46</cell></row><row><cell>Ours</cell><cell>90.78</cell><cell>90.00</cell><cell>80.80</cell><cell>87.81</cell><cell>77.25</cell><cell>74.46</cell><cell>84.90</cell><cell>72.25</cell><cell>65.14</cell></row><row><cell>Subset</cell><cell cols="3">Easy Moderate Hard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">AP (2D) for cars 96.48</cell><cell>90.31</cell><cell>87.63</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>3D object detection AP on KITTI val set. Comparing Frustum PointNets and BV PointNets. This is a scene with lots of parallel parking cars (sample 5595 from val set). Left column shows 2D boxes from our 2D detector in image and 3D boxes from our Frustum PointNets in point cloud. Right column shows 3D boxes from BV PointNets in point cloud and the 2D boxes (projected from the 3D detection boxes) in image. Note that 2D detection boxes from Ours (Frustum) that have box height less than 25 pixels or contain no LiDAR points in the frustum are not shown in the image.</figDesc><table><row><cell>.66</cell><cell>15.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 .</head><label>13</label><figDesc>while the ours (v2) links to the last row. seg net box net seg acc. box acc. Effects of PointNet architectures. Metric is 3D box estimation accuracy with IoU=0.7.</figDesc><table><row><cell>v1</cell><cell>v1</cell><cell>90.6</cell><cell>74.3</cell></row><row><cell>v2</cell><cell>v1</cell><cell>91.0</cell><cell>74.7</cell></row><row><cell>v1</cell><cell>v2</cell><cell>90.6</cell><cell>76.0</cell></row><row><cell>v2</cell><cell>v2</cell><cell>91.0</cell><cell>77.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Details on network architectures, training parameters as well as more experiments are included in the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement The authors wish to thank the support of Nuro Inc., ONR MURI grant N00014-13-1-0341, NSF grants DMS-1546206 and IIS-1528025, a Samsung GRO award, and gifts from Adobe, Amazon, and Apple.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>2017-11-14 12PM. 2</idno>
		<ptr target="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" />
		<title level="m">Kitti 3d object detection benchmark leader board</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Kitti bird&apos;s eye view object detection benchmark leader board</title>
		<idno>2017-11-14 12PM. 2</idno>
		<ptr target="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=bev" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Amodal detection of 3d objects: Inferring 3d bounding boxes from 2d ones in rgb-depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1355" to="1361" />
		</imprint>
	</monogr>
	<note>In Robotics and Automation (ICRA</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>NIPS 2015. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">2d-driven 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08069</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07916</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06240</idno>
		<title level="m">Fpnn: Field probing neural networks for 3d data</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00496</idno>
		<title level="m">3d bounding box estimation using deep learning and geometry</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05009</idno>
		<title level="m">Learning deep 3d representations at high resolutions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="634" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02968</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Robotics: Science and Systems</title>
		<meeting>the Robotics: Science and Systems<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1903" to="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vehicle detection and localization on birds eye view elevation images using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Westfechtel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tadokoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Symposium on Safety, Security and Rescue Robotics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
