<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Make Heterophily Graphs Better Fit GNN: A Graph Rewiring Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07">July 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendong</forename><surname>Bi</surname></persName>
							<email>biwendong20@mails.ucas.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
							<email>lun.du@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Fu</surname></persName>
							<email>qifu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Wang</surname></persName>
							<email>yanlwang@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
							<email>shihan@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
							<email>dongmeiz@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendong</forename><surname>Bi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Make Heterophily Graphs Better Fit GNN: A Graph Rewiring Approach</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Washington, DC, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">17</biblScope>
							<date type="published" when="2017-07">July 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<note>KEYWORDS GNN, heterophily, graph rewiring, neighbor distribution * Work done while this author was an intern at Microsoft Research ? Corresponding Author ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn ACM Reference Format: 2022. Make Heterophily Graphs Better Fit GNN: A Graph Rewiring Ap-proach. In Proceedings of ACM Conference (Conference&apos;17). ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) are popular machine learning methods for modeling graph data. A lot of GNNs perform well on homophily graphs while having unsatisfied performance on heterophily graphs. Recently, some researchers turn their attentions to designing GNNs for heterophily graphs by adjusting message passing mechanism or enlarging the receptive field of the message passing. Different from existing works that mitigate the issues of heterophily from model design perspective, we propose to study heterophily graphs from an orthogonal perspective by rewiring the graph structure to reduce heterophily and making the traditional GNNs perform better. Through comprehensive empirical studies and analysis, we verify the potential of the rewiring methods. To fully exploit its potential, we propose a method named Deep Heterophily Graph Rewiring (DHGR) to rewire graphs by adding homophilic edges and pruning heterophilic edges. The detailed way of rewiring is determined by comparing the similarity of label/feature-distribution of node neighbors. Besides, we design a scalable implementation for DHGR to guarantee a high efficiency. DHRG can be easily used as a plug-in module, i.e., a graph pre-processing step, for any GNNs, including both GNN for homophily and heterophily, to boost their performance on the node classification task. To the best of our knowledge, it is the first work studying graph rewiring for heterophily graphs. Extensive experiments on 11 public graph datasets demonstrate the superiority of our proposed methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-structure data is ubiquitous in representing complex interactions between objects <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. Graph Neural Network (GNN), as a powerful tool for graph data modeling, has been widely developed for various real-world applications <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>. Based on the message passing mechanism, GNNs update node representations by aggregating messages from neighbors, thereby concurrently exploiting the rich information inherent in the graph structure and node attributes.</p><p>Traditional GNNs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31]</ref> mainly focus on homophily graphs that satisfy property of homophily (i.e. most of connected nodes belong to the same class). However, these GNNs usually can not perform well on graphs with heterophily (i.e. most of connected nodes belong to different classes) for the node classification problem, because message passing between nodes from different classes makes their representations less distinguishable, and thus leading to bad performance on node classification task. The aforementioned issues motivate considerable studies around GNNs for heterophily graph. For example, some studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref> adjust message passing mechanism for heterophily edges, while others <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43]</ref> enlarge the receptive field for the message passing. Note that, all these works mitigate the distinguishability issue caused by heterophily from the perspective of the GNN model design. While, there is another orthogonal perspective to mitigate the issue caused by heterophily, i.e., rewiring graph to reduce heterophily or increase homophily, which is still under-explored.</p><p>Graph rewiring <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref> is a kind of method that decouples the input graph from the graph for message passing and boost the performance of GNN on node classification tasks via changing the message passing structure. Many works have utilized graph rewiring for different tasks. However, most existing graph rewiring techniques have been developed for graphs under homophily assumption (sparsity <ref type="bibr" target="#b23">[24]</ref>, smoothness <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref> and low-rank <ref type="bibr" target="#b44">[45]</ref>), and thereby can not directly transfer to heterophily graphs. Different from existing solutions that design specific GNN architectures adapted to heterophily graphs, in this paper, we conduct comprehensive study on graph rewiring and propose an effective rewiring algorithm to reduce graph heterophily, which make GNNs perform better for both heterophily and homophily graphs.</p><p>First we demonstrate the effects of increasing homophily-level for heterophily graphs in Sec. 3 with comprehensive controlled experiments. Note that the homophily (and heterophily) level can be measured with Homophily Ratio (HR) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43]</ref>, which is formally defined as an average of the consistency of labels between each connected node-pair. From the analysis in Sec. 3.1, we find that both the node-level homophily ratio <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref> and node degree (reflects the recall of nodes from the same class) can affect the performance of GCN on the node classification task, where increasing either of the two variables can lead to better performance of GCN. This finding, i.e., classification performance of GCN on heterophily graphs can be increased by reducing the heterophily-level of graphs, motivates us to design a graph-rewiring strategy to increase homophily-level for heterophily graphs so that GNNs can perform better on the rewired graphs.</p><p>Then, we propose a learning-based graph rewiring approach on heterophily graphs, namely Deep Heterophily Graph Rewiring (DHGR). DHGR rewires the graph by adding/pruning edges on the input graph to reduce its heterophily-level. It can be viewed as a plug-in module for graph pre-processing that can work together with many kinds of GNN models including both GNN for homophily and heterophily, to boost their performance on node classification tasks. The key idea of DHGR is to reduce the heterophily while keeping the effectiveness by adding more homophilic edges and removing heterophilic edges. However, simply adding homophilic edges and removing heterophilic edges between nodes in the training set may increase the risk of overfitting and lead to poor performance (we prove this in Sec. 5.4). Another challenge is that unlike homophily graphs that can leverage Laplace Smooth to enhance the correlation between node features and labels, heterophily graphs do not satisfy the property of smoothness <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref>. In this paper, we propose to use label/feature-distribution of neighbors on the input graph as guidance signals to identify edge polarity (homophily/heterophily) and prove its effectiveness in 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Under the guidance of the neighbors' label-distribution, DHGR learns the similarity between each node-pair, which forms a similarity matrix. Then based on the learned similarity matrix, we can rewire the graph by adding edges between high-similarity nodepairs and pruning edges connecting low-similarity node-pairs. Then the learned graph structure can be further fed into GNNs for node classification tasks. Besides, we also design a scalable implementation of DHGR which avoids the quadratic time and memory complexity with respect to the numbers of nodes, making our method available for large-scale graphs. Finally, extensive experiments on 11 real-world graph datasets, including both homophily and heterophily graphs, demonstrate the superiority of our method.</p><p>We summarize the contributions of this paper as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>In this section, we give the definitions of some important terminologies and concepts appearing in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Networks</head><p>Let G = ( , ) denotes a graph, where is the node set, = | | is the number of nodes in G. Let ? R ? denote the feature matrix and the -th row of denoted as is the -dimensional feature vector of node . = {( , )| , ? and , is connected}. GNNs aim to learn representation for nodes in the graph. Typically, GNN models follow a neighborhood aggregation framework, where node representations are updated by aggregating information of its neighboring nodes. Let ? ( ) denotes the output vector of node at the -th hidden layer and let ? (0) = . The -th iteration of aggregation step can be written as:</p><formula xml:id="formula_1">? ( ) = COMBINE ? ( ?1) , AGG {? ( ?1) | ? N ( )}</formula><p>where N ( ) is the set of neighbors of . The AGG function indicates the aggregation function aimed to gather information from neighbors and the goal of the COMBINE function is to fuse the information from neighbors and the central node. For graph-level tasks, an additional READOUT function is required to get the global representation of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Rewiring</head><p>Given a graph G = ( , ) with node features ? R ? as the input, Graph Rewiring (GR) aims at learning an optimal G * = ( , * ) under a given criterion, where the edge set is updated and the node set is constant. Let , * ? R ? denote the adjacent matrix of G and G * , respectively. The rewired graph G * is used as input of GNNs, which is expected to be more effective than directly inputting the original graph G. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the pipeline of Graph Rewiring models usually involves two stages, the similarity learning and the graph rewiring based on the learned similarity between pairs of nodes. It is obvious that the criterion (i.e., objective function) design plays a critical role for the similarity learning stage. Thus, we first mine knowledge from data in the next section to abstract an effective criterion of graph rewiring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OBSERVATIONS FROM DATA</head><p>We observed from data that there exist two important properties of graph (i.e., node-level homophily ratio 1 and degree) that are strongly correlated with the performance of GNNs. And the two properties provide vital guidance so that we can optimize the graph structure by graph rewiring. However, we cannot directly calculate node-level homophily ratio because of the partially observable labels during training. Therefore, we introduce two other effective signals, i.e., neighbor's observable label/feature-distribution, which have strong correlations with the node-level homophily ratio. In this section, we first verify the relations between the two properties and the performance of GNNs. Then we verify the correlations between neighbor distribution and node-level homophily ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Effects of Node-level Homophily Ratio and Degree</head><p>First, we conduct validation experiments to verify the effects of node-level homophily ratio <ref type="bibr">[27?</ref> ] and node degree on the performance of GCN, as guidance for graph rewiring. Specifically, we first construct graphs by quantitatively controlling the node-level homophily ratio and node degree, and then verify the performance of GCN on the constructed graphs as a basis for measuring the quality of constructed graph structure. Note that considering the direction of message passing is from source nodes to target nodes, the node degree mentioned in this paper refers to the in-degree. For example, given node degree and node-level homophily ratio , we can constructed a directional Graph G , where each node on the G , has different neighboring nodes pointing to it and there are ? ? ? same-class nodes among the k neighbors, with other ? ? (1 ? )? neighbors randomly selected from remaining different-class nodes on the graph.</p><p>As shown in the <ref type="figure" target="#fig_1">Fig. 2</ref>, we conduct validation experiments on three different graph datasets, including one homophily graph (Cora) and two heterophily graphs (Chameleon, Actor). In this experiments, we construct graphs G , with node degree ranging from 5 to 25 and node-level homophily ratio ranging from 0.0 to 1.0, totally 35 constructed graphs for each dataset. And then for each constructed graph, we train vanilla GCN <ref type="bibr" target="#b18">[19]</ref> on it three times and calculate the average test accuracy on node classification task. From the <ref type="figure" target="#fig_1">Fig. 2</ref>, we find that both the homophily graph and the heterophily graph follow the same rule: when the degree is fixed, the accuracy of GCN increases with the increase of the node-level homophily ratio; when the homophily ratio is fixed, the accuracy of GCN increases with the increase of the degree. It should be noted that when the homophily ratio equals 0 (i.e., all neighboring nodes are from different classes), it may have a higher GCN accuracy than that when the homophily ratio is very small (around = 0.2). Besides, when the homophily ratio is largr than a threshold, the GCN accuracy converges to 100%. In general, the GCN accuracy almost varies monotonically with the node-level homophily ratio and node degree. And this motivates us to use graph rewiring as a way of increasing both node-level homophily ratio and degree.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effects of Neighbor's Label/Feature Distribution</head><p>From the Sec. 3.1, we conclude that graph rewiring can be used as a way of reducing heterophily to make GNNs perform well on both homophily and heterophily graphs. However, it is not easy to accurately identify the edge polarity (homophily or heterophily) on a heterophily graph so that we can estimate the node-level homophily ratio. For a homophily graph, we can leverage its homophily property and use Laplacian Smoothing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref> to make its node representation more distinguishable. However, heterophily graphs do not satisfy property of smoothness thus the information available is limited. A straightforward idea is to use node features to identify edge polarity, however, the information of this single signal is limited. In this paper, we propose to use similarity between the neighbor's label-distribution for node-pairs as a measure of edge polarity. Besides, considering that not all node labels are observable, we also introduce neighbor's feature-distribution (mean of neighbor features), which is completely observable, as signals in addition to neighbor's label-distribution.</p><p>Up to now, we have three signals (i.e. raw node features, labeldistribution and feature-distribution of neighbors) that can be used as measures for edge polarity. We quantitatively evaluate the effectiveness of the three signals and find that the distribution signals are more informative than the raw node feature through the following empirical experiments and analysis. To be specific, we consider the label/feature-distribution of the 1st-order and 2nd-order neighbors. Then we calculate the similarity between each node-pair with one of these signals and compute the mutual information between the node-pair similarity and edge polarity on the graph. The formula of Mutual information is written as follows:</p><formula xml:id="formula_2">( ; ) = ? ? ( , ) ( , ) ? , ( , ) ( ) ( )<label>(1)</label></formula><p>In the case of discrete random variables, the integral operation is replaced by the sum operation. As shown in the <ref type="figure" target="#fig_6">Fig. 3</ref>, we conduct statistical analysis on three datasets (i.e. Cora, Chameleon, Actor). From the <ref type="figure" target="#fig_6">Fig. 3</ref>, we find that both the similarity of neighbor's label-distribution and neighbor's feature-distribution have a strong correlation with edge polarity than that of the raw node features similarity, and neighbor's labeldistribution has a stronger correlation than neighbor's featuredistribution in most cases. And this rule applies to both homophily graphs and heterophily graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>Based on the observations mentioned above, we design the Deep Heterophily Graph Rewiring method (DHGR) for heterophily graphs, which can be easily plugged into various existing GNN models. Following the pipeline in <ref type="figure" target="#fig_0">Fig. 1</ref>, DHGR first learns a similarity matrix representing the similarity between each node-pair based on the neighbor distribution (i.e. label-distribution and featuredistribution of neighbors). Then we can rewire the graph structure by adding edges between high-similarity node-pair and pruning low-similarity edges on the original graph. Finally, the rewired graph is further fed into other GNN models for node classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Similarity Learner Based on Neighbor Distribution</head><p>Before rewiring the graph, we first learn the similarity between each pair of nodes. According to the analysis in Sec. 3.2, we design a graph learner that learns the node-pair similarity based on the neighbor distribution. Considering that in the training process, only the labels of nodes in the training set are available, we cannot observe the full label-distribution of neighbors. Therefore, we also leverage the feature-distribution of neighbors which can be fully observed to enhance this similarity learning process with the intuition that node features have correlations to labels for an attributed-graph. Besides, the results shown in Sec. 3.2 also validate the effectiveness of neighbor's feature-distribution. The overview of similarity learner used in DHGR is shown in <ref type="figure" target="#fig_7">Fig. 4</ref>. Specifically, for an attributed graph, we can first calculate its observable label-distribution ( ) and feature-distribution ( ) for the -hop neighbors of each node using node-labels in the training set and all node-features:</p><formula xml:id="formula_3">( ) = ( ?1 ) , ( ) = ( ?1 )<label>(2)</label></formula><p>( ) and ( ) is respectively the label-distribution and featuredistribution of -order neighbors in the graph, is the maximum neighbor-order and we use ? {1, 2} in this paper. is the one-hot label matrix, the -th row of is the one-hot label vector of node if belongs to the training set, else use a zero-vector instead. is the adjacent matrix and is the corresponding degree diagonal matrix. Then for each node, we can get the observed labeldistribution vector and feature-distribution vector of its neighbors. Next we calculate the cosine similarity between each node-pair with respect to both label-distribution and feature-distribution, and we can get the similarity matrix of label-distribution and similarity matrix of feature-distribution .</p><formula xml:id="formula_4">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? [ , ] = =1 ( ) [ , :], ( ) [ , :] [ , ] = =1 ( ) [ , :], ( ) [ , :]<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">( , ) = ? | | ? | | , = ? 1 | | ?? ? (4)</formula><p>Note that before calculating cosine similarity, we first decentralize the input variable by subtracting the mean of this variable for all nodes. Considering that not all nodes have an observed label-distribution, e.g., if all neighbors of node do not belong to the training set, then the observed label-distribution of is a zero vector. Obviously, this is not ideal, so we compensate for this with the feature-distribution of neighbors. In addition, we restrict the utilization condition of neighbor label-distribution by using a mask. Specifically, for node ? , we leverage its neighbor label-distribution only when the percentage of its neighbors in the training set is larger than a threshold :</p><formula xml:id="formula_6">( ) = 1 , &gt; 0 , ? , where = |N ( ) ? | |N ( )|<label>(5)</label></formula><p>? R ?1 is the mask vector, N ( ) is the neighbor set of , is the set of nodes in the training set. Then our similarity learner targets at learning the similarity of node-pairs based on the neighbor distribution. Specifically, the similarity learner first aggregates and transforms the feature of neighboring nodes and then uses the aggregated node representation to calculate cosine similarity for each node-pair:</p><formula xml:id="formula_7">, = =1 ? ( ) , ? ( ) , ? ( ) = ( ?1 ) ? ?<label>(6)</label></formula><p>, denotes the similarity between and , and similarity of all node-pairs form a similarity matrix . In practice, we also optionally use the concatenation of distribution feature ? and ? (transformed feature of node itself) for similarity calculation in Eq. 6. Finally, we use the and calculated in advance to guide the training of . We have the following two objective functions with respect and :</p><formula xml:id="formula_8">1 ( , ) = || ? || 2 (7) 2 ( , ) = ||( ? ) ? ( ? )|| 2<label>(8)</label></formula><p>In practice, we first use to reconstruct as the pretraining process and then further use to reconstruct under as the finetuning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Scalable Implementation of DHGR</head><p>However, directly optimizing the objective function mentioned above has quadratic computational complexity. For node attributes ? R ? , the ( 2 ) complexity is unacceptable for large graphs when &gt;&gt; . So we design a scalable training strategy with stochastic mini-batch. Specifically, we randomly select 1 ? 2 nodepairs as a batch and optimize the similarity matrix by a ( 1 ? 2 )sized sliding window in each iteration. We can assign small numbers to 1, 2 ? [1, ]. We give the pseudocode in Algorithm 1. Update with the gradient of ||( ? )|| 2 ); <ref type="bibr" target="#b12">13</ref> Obtain the entry , with Eq. 6. ? Final similarity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph Rewiring with Learned Similarity</head><p>After we obtain the similarity of each node-pair, we can use the learned similarity to rewire the graph. Specifically, we add edges between node-pairs with high-similarity and remove edges with low-similarity on the original graph. Three parameters are set to control this process: indicates the maximum number of edges that can be added for each node; constrains that the similarity of node-pairs to add edges mush larger than a threshold . Finally another parameter is set for pruning edges with similarity smaller than . The details of the Graph Rewiring process are given in Algorithm 2. Finally, we can feed the rewired graph G( , ) into any GNN-based models for node classification tasks. Remove edges ( , ) from G;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Complexity Analysis</head><p>We analyze the computational complexity of Algorithm 1 and Algorithm 2 with respect to the number of nodes | |. For Algorithm 1, the complexity of random sampling 1 + 2 nodes is ( 1 + 2 )).</p><p>Lets denote the feature dimension as and denote the one-hot label dimension as |C| &lt; . Considering that the complexity of calculating cosine similarity between two -dimension vectors is ( ), the complexity of calculating the similarity matrix , , ? R 1 ? 2 is ( 1 2 ). The complexity of calculating 1 and 2 equals to ( 1 2 ). Therefore, the final computational complexity of one epoch of Algorithm 1 is ( 1 2 ) where 1 , 2 are two constants. For Algorithm 2, we use Ball-Tree to compute the top-K nearest neighbors, the complexity of one top-K query is ( ? | | (| |))). Therefore, the time complexity of the first -loop which performs the topK algorithm is approximately ( ? | | (| |) + ? | |). The second -loop filters each edge in the original Graph and thus its complexity is (| |)). Therefore the final complexity of Algorithm 2 is ( ? | | (| |) + ? | | + | |).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first give the experimental configurations, including the introduction of datasets, baselines and setups used in this paper. Then we give the results of experiments comparing DHGR with other graph rewiring methods on the node classification task under transductive learning scenarios. Besides, we also conduct extensive hyper-parameter studies and ablation studies to validate the effectiveness of DHGR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate the performanes of DHGR and the existing methods on eleven real-world graphs. To demonstrate the effectiveness of DHGR , we select eight heterophily graph datasets (i.e. Chameleon, Squirrel, Actor, Cornell, Texas, Wisconsin <ref type="bibr" target="#b26">[27]</ref>, FB100 <ref type="bibr" target="#b29">[30]</ref>, Flickr <ref type="bibr" target="#b40">[41]</ref>) and three homophily graph datasets (i.e. Cora, CiteSeer, PubMed <ref type="bibr" target="#b18">[19]</ref>). The detailed information of these datasets are presented in the <ref type="table" target="#tab_4">Table 1</ref>. For graph rewiring methods, we use both the original graphs and the rewired graphs as the input of GNN models to validate their performance on the node classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>DHGR can be viewed as a plug-in module for other state-of-the-art GNN models. And we select five GNN models tackling homophily, including GCN <ref type="bibr" target="#b18">[19]</ref>, GAT <ref type="bibr" target="#b30">[31]</ref>, GraphSAGE <ref type="bibr" target="#b15">[16]</ref>, APPNP <ref type="bibr" target="#b19">[20]</ref> and GCNII <ref type="bibr" target="#b3">[4]</ref>. To demonstrate the significant improvement on heterophily graphs caused by DHGR, we also choose two GNNs tackling heterophily (i.e. GPRGNN <ref type="bibr" target="#b7">[8]</ref>, H2GCN <ref type="bibr" target="#b42">[43]</ref>). Besides, to validate the effectiveness of DHGR as a graph rewiring method, we also compare DHGR with two Graph Structure Learning (GSL) methods (i.e. LDS <ref type="bibr" target="#b13">[14]</ref> and IDGL <ref type="bibr" target="#b6">[7]</ref>) and one Graph Rewiring methods (i.e. SDRF <ref type="bibr" target="#b28">[29]</ref>), which are all aimed at optimizing the graph structure. For GPRGNN and H2GCN, we use the implementation from the benchmark <ref type="bibr" target="#b22">[23]</ref>, and we use the official implementation of other GNNs provided by Torch Geometric. For all Graph Rewiring methods except SDRF whose code is not available, we all use their official implementations proposed in the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Setup</head><p>For datasets in this paper, we all use their public released data splits. For Chameleon, Squirrel, Actor, Cornell, Texas ,and Wisconsin, ten random generated splits of data are provided by <ref type="bibr" target="#b26">[27]</ref>, and we therefore train models on each data split with 3 random seeds for model initialization (totally 30 trails for each dataset) and finally we calculate the average and standard deviation of all 30 results. And we use the official splits of other datasets (i.e. Cora <ref type="bibr" target="#b18">[19]</ref>, PubMed <ref type="bibr" target="#b18">[19]</ref>, CiteSeer <ref type="bibr" target="#b18">[19]</ref>, Flickr <ref type="bibr" target="#b40">[41]</ref>, FB100 <ref type="bibr">[</ref>  <ref type="bibr" target="#b13">[14]</ref>, IDGL <ref type="bibr" target="#b6">[7]</ref>) , we adjust their hyper-parameters according to the configurations used in their papers. For GNNs used in this paper, we adjust the hyper-parameters in the same searching space for fairness. We search the hidden dimensions in {32, 64} for all GNNs and set the number of model layers to 2 for GNNs except for GCNII <ref type="bibr" target="#b3">[4]</ref> which is designed with deeper depth and we search the number of layers for GCN2 in {2, 64} according to its official implementation. We train 200/300/400 epoch for all models and  <ref type="table" target="#tab_4">Nodes  2277  5201  7600  41554  89250  183  183  251  2708  3327  19717  Edges  36101  217073  30019  2724458  899756  298  325  511  10556  9104  88648  Features  2325  2089  93 2  4814  500  1703  1703  1703  1433  3703  500  Classes  5  5  5  2  7  5  5</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Main Results</head><p>We conduct experiments of node classification task on both heterophily and homophily graph datasets, and the results are presented in <ref type="table" target="#tab_6">Table 2</ref> and <ref type="table" target="#tab_7">Table 3</ref> respectively. We evaluate the performance of DHGR by comparing the classification accuracy of GNN with original graphs and graphs rewired by DHGR respectively. We also calculate the average gain (AG) of DHGR for all models on each dataset. The formula of average gain is given as follows:</p><formula xml:id="formula_9">= 1 |M | ?? ?M ( G) ? ( G)<label>(9)</label></formula><p>where M is the set of GNN models. is the short form of accuracy. G is the original graph and G is the graph rewired by DHGR. We also compare the proposed DHGR with other Graph Rewiring methods on their performance and running time, and the results of different graph rewiring methods are reported in <ref type="table" target="#tab_8">Table 4</ref> and <ref type="figure" target="#fig_8">Fig. 5</ref>. By analyzing these results, we have the following observations:</p><p>(1) All GNNs enhanced by DHGR, including GNNs for homophily and GNNs for heterophily, outperform their vanilla versions on the eight heterophily graph datasets. The average gain of DHGR on heterophily graph can be up to 32.44% on Squirrel. However, vanilla GCN on Squirrel only has 26.39% classification accuracy on the test set. Even with the sate-of-the-art GNNs for heterophily (i. <ref type="figure" target="#fig_1">e. GPRGNN, H2GCN)</ref>, an test accuracy of no more than 40% can be achieved. The H2GCN enhanced by DHGR can achieve an astonishing 72.24% test accuracy on Squirrel, almost doubling. For most other heterophily datasets, GNN with DHGR can provide significant accuracy improvements. It demonstrates the importance of graph rewiring strategy for improving GNN's performance on heterophily graphs. Besides, the significant average gain by DHGR also demonstrates the effectiveness of DHGR. For large-scale and edge-dense datasets such as Flickr and FB100 ( &gt;&gt; ), graph rewiring with DHGR can still provide a competitive boost for GNNs,  which verifies the effectiveness and scalability of DHGR on largescale graphs.</p><p>(2) For homophily graphs (i.e., Cora, Citeseer, Pubmed), the proposed DHGR can still provide competitive gain of node classification performance for the GNNs. Note that homophily graphs usually have a higher homophily ratio (i.e. 81%, 74%, 80% for Cora, Cite-Seer and PubMed), so even vanilla GCNs can achieve great results and thus the benefit of adjusting the graph structure to achieve a higher homophily ratio is less than that for heterophily graphs. To be specific, DHGR gains best average gain on Cora, e.g., the classification accuracy of vanilla GCN on Cora is improved from 81.1% to 82.6%. For another two datasets, DHGR also provide average gain no less than 0.5% accuracy for all GNN models. These results demonstrate that our method can provide significant improvements for heterophily graphs while maintaining competitive improvements for homophily graphs.</p><p>(3) To demonstrate the effectiveness of DHGR as a method of graph rewiring, we also compare the proposed approach with other graph rewiring methods (i.e. LDS, IDGL, SDRF). Besides, we also use two random graph structure transformation by adding or removing edges on the original graph with a probability of 0.5, namely RandAddEdge and RandDropEdge. To validate the effect of adding edges between same-class nodes with training label, we also design a method named RandAddEdge train Y that randomly adds edges between same-class nodes within the training set (for we can only observe labels of node in the training set) with a probability of 0.5. As shown in <ref type="table" target="#tab_8">Table 4</ref>, GCN with DHGR outperform GCN with other graph transformation methods on the presented four heterophily datasets. Note that RandAddEdge train Y which only use training label to add edges, though increases the homophily ratio, it cannot add edges beyond nodes in the training set. Only adding homophilic edges within the training set cannot guarantee an improvement of GCN's performance and make the nodes in the training set easier to distinguish, increasing the risk of overfitting. The significant improvements made by DHGR demonstrates the effectiveness of DHGR as a graph rewiring method. (4) Note that the traditional paradigm of GSL methods (e.g., LDS, IDGL.) is training a graph learner and a GNN through an end2end manner and based on the dense matrix optimization, which have larger complexity. The running time of DHGR and two other GSL methods is presented in <ref type="figure" target="#fig_8">Fig. 5</ref>, we find that the running time of DHGR is significantly smaller than that of GSL methods under the same device environment. We did not present the running time of SDRF because its code has not been released publicly yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyper-Parameter Study</head><p>To demonstrate the robustness of the proposed approach, we study the effect of the four main hyper-parameter of DHGR, i.e. Batchsize, (maximum number of added edges for each node), (the threshold of lowest-similarity when adding edges) and training ratio of datasets in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.5.1</head><p>The effect of batchsize and training set ratio. <ref type="table" target="#tab_9">Table 5</ref> shows the results of GCN with DHGR on two heterophily datasets varying with different batchsize for DHGR and training ratio (percentage of nodes in the training set.). The batchsize is ranging from [100?100] to [ ? ], where is the number of nodes and [ ? ] indicates using full-batch for training. Note that for the Squirrel dataset which has only 5201 nodes, the batchsize of 10000 ? 10000 equals full-batch. The results in <ref type="table" target="#tab_9">Table 5</ref> show that the proposed approach has stable improvements with different batchsize and training ratio.</p><p>To be specific, GCN with DHGR only has a 3% decrease in accuracy when decreasing the batchsize to 100, which is extremely small and with no more than 2% decrease in accuracy with training ratio ranging from 40% to 10%. Besides, we usually set the batchsize of DHGR ranging from 5000 to 10000 in real applications, because the overhead of 10000?10000 matrix storage and operation is completely acceptable. These results demonstrate the robustness of DHGR when adjusting the batchsize and training ratio.  <ref type="figure">Figure 6</ref>: Results of experiments with different and . is the maximum number of edges that can be added for each node. is the minimum similarity threshold of node-pairs between which edges can be added. Note that we remove all edges on the original graph for this experiment and only to verify the effects of edges added by DHGR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.5.2</head><p>The effect of and . We have two important hyperparameters when rewiring graphs with DHGR, the maximum number of edges added for each node (denoted as ) and the threshold of lowest-similarity when adding edges (denoted as ). Given the learned similarity by DHGR, the two hyper-parameters almost determines the degree and homophily ratio of the rewired graph. Motivated by the obversations presented in Sec. 3, we verify the effectiveness of DHGR for graph rewiring by using different and . <ref type="figure">Fig. 6 (a)</ref> shows the homophily ratio of rewired graphs using different and and <ref type="figure">Fig. 6 (b)</ref> shows the node classification accuracy of GCN on the rewired graphs using different and . We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study</head><p>Considering that DHGR leverage three different types of information (i.e. raw feature, label-distribution, feature-distribution), we also verify the effectiveness of each type of formation by removing them from DHGR and designing three variants of it. DHGR \label_dist indicates removing the using of neighbor's labeldistribution (the finetuning process). DHGR \feat_dist indicates removing the using of neighbor's feature-distribution (the pretraining process). DHGR \feat_self indicates do not use the concatenation of distribution feature ? and ? (transformed feature of node itself) for similarity calculation in Eq. 6 (only use the distribution feature ? ). As shown in <ref type="table" target="#tab_11">Table 6</ref>, the node classification of GCN with rewired graphs from almost all variants deteriorates to some extent on the four selected datasets (i.e. Cora, Cornell, Texas, FB100). For the Texas dataset, the results of DHGR \feat_dist that do not utilize neighbors feature-distribution have slight improvement over the full DHGR and we think it is caused by the poor performance of feature-distribution reflected by the results of DHGR \label_dist , which only leverages the feature-distribution and feature of node itself on this dataset. And the result of DHGR on Texas dataset only decreases slightly with 0.2% accuracy compared with DHGR \feat_dist . The results of the ablation study demonstrate the effectiveness of neighbor label-distribution for modeling heterophily graphs. Also, it demonstrates that the proposed approach makes full use of the useful information from neighbor distribution and raw feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK 6.1 Graph Representation Learning</head><p>Graph Neural Networks (GNNs) have been popular for modeling graph data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>. GCN <ref type="bibr" target="#b18">[19]</ref> proposed to use graph convolution based on neighborhood aggregation. GAT <ref type="bibr" target="#b30">[31]</ref> proposed to use attention mechanism to learn weights for neighbors. Graph-SAGE <ref type="bibr" target="#b15">[16]</ref> was proposed with graph sampling for inductive learning on graphs. These early methods are designed for homophily graphs, and they perform poorly on heterophily graphs. Recently, some studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43]</ref> propose to design GNNs for modeling heterophily graphs. MixHop <ref type="bibr" target="#b0">[1]</ref> was proposed to aggregate representations from multi-hops neighbors to alleviate heterophily. Geom-GCN <ref type="bibr" target="#b26">[27]</ref> proposed a bi-level aggregation scheme considering both node embedding and structural neighborhood. GPR-GNN <ref type="bibr" target="#b7">[8]</ref> proposed to adaptively learn the Generalized PageRank (GPR) weights to jointly optimize node feature and structural information extraction. More recently, GBK-GNN <ref type="bibr" target="#b10">[11]</ref> was designed with bi-kernels for homophilic and heterophilic neighbors respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Graph Rewiring</head><p>The traditional message passing GNNs usually assumes that messages are propagated on the original graph <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31]</ref>. Recently, there is a trend to decouple the input graph from the graph used for message passing. For example, graph sampling methods for inductive learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42]</ref>, motif-based methods <ref type="bibr" target="#b24">[25]</ref> or graph filter leveraging multi-hop neighbors <ref type="bibr" target="#b0">[1]</ref>, or changing the graph either as a preprocessing step <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> or adaptively for the downstream task <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref>. Besides, Graph Structure Learning (GSL) methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref> aim at learning an optimized graph structure and its corresponding node representations jointly. Such methods of changing graphs for better performance of downstream tasks are often generically named as graph rewiring <ref type="bibr" target="#b28">[29]</ref>. The works of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref> proposed rewiring the graph as a way of reducing the bottleneck, which is a structural property in the graph leading to over-squashing. Some GSL methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref> directly make adjacent matrix a learnable parameter and optimize it with GNN. Other GSL methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> use a bilevel optimization pipeline, in which the inner loop denotes the downstream tasks and the outer loop learns the optimal graph structure with a structure learner. Some studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">40]</ref> also use transformer-like GNNs to construct global connections between all nodes. However, both GSL methods and graph transformer-based methods usually have a higher time and space complexity than other graph rewiring methods. Most of existing Graph Rewiring methods are under the similar assumption (e.g., sparsity <ref type="bibr" target="#b23">[24]</ref>, low-rank <ref type="bibr" target="#b44">[45]</ref>, smoothness <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref>) on graphs. However, the property of low-rank and smoothness are not satisfied by heterophily graphs. Thus, graph rewiring methods for modeling heterophily graphs still need to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we propose a new perspective of modeling heterophily graphs by graph rewiring, which targets at improving the homophily ratio and degree of the original graphs and making GNNs gain better performance on the node classification task. Besides, we design a learnable plug-in module of graph rewiring for heterophily graphs namely DHGR which can be easily plugged into any GNN models to improve their performance on heterophily graphs. DHGR improves homophily of graph by adjusting structure of the original graph based on neighbor's label-distribution. And we design a scalable optimization strategy for training DHGR to guarantee a linear computational complexity. Experiments on eleven real-world datasets demonstrate that DHGR can provide significant performance gain for GNNs under heterophily, while gain competitive performance under homophily. The extensive ablation studies further demonstrate the effectiveness of the proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Pipeline of Graph Rewiring for heterophily graphs. Red and blue circles denote nodes from different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Graph rewiring validation experiments on three datasets. Each block in the heatmap denotes a rewired graph with node degree and node-level homophily ratio. The values in the block denote node classification accuracy of vanilla GCN on the test set (average accuracy of 3 runs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F</head><label></label><figDesc>label distribution 2nd-order label distribution 1st-order feature distribution 2nd-order feature distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Mutual Information (MI) between different signals and edge polarity (i.e.homophily or heterophily).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Overview of the Similarity Learner for Graph Rewiring in DHGR. ? R ? denotes the raw feature matrix and ? R ? denotes the adjacent matrix. Note that Node Pair-wise Cosine Similarity in the yellow block indicates the cosine similarity with decentralization calculated for each pair of nodes in the graph, which is defined in Eq. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Running time of GCN with DHGR and other GSL methods (i.e. LDS, IDGL). Note that for DHGR, we use the sum of the running time of DHGR and vanilla GCN as the final running of DHGR for fair comparison with GSL methods. We train 200 epoch for all methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Sample 2 nodes 2 ? , | 2 | = 2 ; Calculate the similarity matrix , ? R 1 ? 2 between 1 and 2 . (see Eq. 6 and Eq. 3); Update with ? 1 ( , ) (see Eq. 7); Calculate the similarity matrix , ? R 1 ? 2 between 1 and 2 . (see Eq. 6 and Eq. 3);</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>-ordinal</cell></row><row><cell></cell><cell></cell><cell>, MaxIteration,</cell><cell>Epoch1, Epoch2.</cell></row><row><cell></cell><cell cols="2">Output: Similarity matrix</cell></row><row><cell cols="3">1 for epoch from 1 to Epoch1 do</cell></row><row><cell>2</cell><cell cols="3">for i from 1 to MaxIteration do</cell></row><row><cell cols="2">7 Node set</cell><cell>? { ? |</cell><cell>[ ] = 1}.</cell></row><row><cell>12</cell><cell></cell><cell></cell></row></table><note>Algorithm 1: Training DHGR with stochastic mini-batch Input: graph G( , ), Node set , label , batch-size [ 1 , 2 ], min-percentage , max neighbor3 Sample 1 nodes 1 ? , | 1 | = 1 ;4568 for epoch from 1 to Epoch2 do9 Sample 1 nodes 1 ? , | 1 | = 1 ;10 Sample 2 nodes 2 ? , | 2 | = 2 ;11</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>23]) from the corresponding papers. We train our DHGR models with 200 epochs for pretraining and 30 epochs for finetuning in all datasets. And we search the hyper-parameters of DHGR in the same space for all datasets.(the max order of neighbors) is searched in {1, 2}, (the growing threshold) is searched in {3, 6, 8, 16} and (the pruning threshold) is searched in {0., 0.3, 0.6}, where we do not prune edges for homophily datasets which equals to set to -1.0. The batch size for training DHGR is searched in {5000, 10000}. For other GSL methods (i.e. LDS</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>The stastical information of the datasets used to evaluate our model. H.R. indicates the overall homophily ratio<ref type="bibr" target="#b26">[27]</ref> of the dataset, which means the percentage of homophilic edges in all edges of the graph.</figDesc><table><row><cell>Dataset</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Actor</cell><cell>FB100</cell><cell>Flickr</cell><cell>Cornell</cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Node classification accuracy (%) on the test set of heterophily graph datasets. The bold numbers indicate that our method improves the base model. The dash symbols indicate that we were not able to run the experiments due to memory issue. 90?0.57 49.68?0.45 74.34?0.20 55.56?3.21 61.96?1.27 52.35?7.07 DHGR 70.83?2.03 67.15?1.43 36.29?0.12 51.01?0.25 77.01?0.14 67.38?5.33 81.78?0.89 76.47?3.62 10?0.57 49.67?0.81 70.01?0.66 56.22?6.02 60.36?5.55 49.61?6.20 DHGR 72.11?2.87 62.37?1.78 34.71?0.48 50.40?0.09 79.41?5.13 70.09?6.77 83.78?3.37 73.20?4.89 07?0.15 50.21?0.31 75.99?0.09 80.08?2.96 82.03?2.77 81.36?3.91 DHGR 69.57?1.28 68.08 ?1.55 37.17?0.11 50.85?0.05 76.56?0.10 82.88?5.56 85.68?2.72 83.16?1.72 APPNP vanilla 40.44?2.02 29.20?1.45 30.02?0.89 49.05?0.10 74.22?0.11 56.76?4.58 55.10?6.23 54.59?6.13 DHGR 70.35?2.62 60.31?1.51 36.93?0.86 49.36?0.05 75.46?0.11 68.11?6.59 81.58?4.36 77.65?3.06 GCNII vanilla 57.37?2.35 39.51?1.63 31.05?0.14 50.34?0.22 77.06?0.12 61.70?5.91 62.43?7.37 52.75?4.23 DHGR 74.57?2.56 58.38?1.79 36.03?0.12 50.73?0.31 78.38?0.91 72.97?6.73 81.08?6.02 78.24?4.99 GPRGNN vanilla 41.56?1.66 30.03?1.11 35.72?0.19 49.76?0.10 78.58?0.23 72.78?6.05 69.37?1.27 76.08?5.86 DHGR 71.58?1.59 64.82?2.07 37.43?0.78 50.56?0.32 82.28?0.56 76.56?5.77 83.98?2.54 79.41?4.98</figDesc><table><row><cell>GNN Model</cell><cell>\</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Actor</cell><cell>Flickr</cell><cell>FB100</cell><cell>Cornell</cell><cell>Texas</cell><cell>Wisconsin</cell></row><row><cell cols="5">GCN 28.GAT vanilla 37.68?3.06 26.39?0.88 vanilla 44.34?1.42 29.82?0.98 29.GraphSAGE vanilla 49.06?1.88 36.73?1.21 vanilla 49.21?2.57 34.58?1.61 35.61?0.31 35.H2GCN DHGR 69.19?1.913 72.24?1.52 36.51?0.67</cell><cell>--</cell><cell>--</cell><cell cols="3">79.06?6.36 80.27?5.41 80.20?4.51 82.06?6.27 84.86?5.01 85.01?5.51</cell></row><row><cell>Avg Gain</cell><cell>\</cell><cell>25.51 ?</cell><cell>32.44 ?</cell><cell>4.23 ?</cell><cell>0.70 ?</cell><cell>3.15 ?</cell><cell>8.27 ?</cell><cell>15.89 ?</cell><cell>15.17 ?</cell></row><row><cell cols="5">select the best parameters via the validation set. The learning rate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">is searched in {1e-2, 1e-3, 1e-4}, the weight decay is searched in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">{1e-4, 1e-3, 5e-3}, and we use Adam optimizer to optimize all the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">models on Nvidia Tesla V100 GPU.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Node classification accuracy (%) on the test set of homophily graphs. The bold numbers indicate that our method improves the base model.</figDesc><table><row><cell>GNN Model</cell><cell>\</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>GCN</cell><cell cols="4">vanilla 81.09?0.39 DHGR 82.70?0.41 70.79?0.12 79.10?0.33 70.13?0.45 78.38?0.39</cell></row><row><cell>GAT</cell><cell cols="4">vanilla 81.90?0.73 DHGR 82.93?0.51 70.43?0.65 78.81?0.93 69.60?0.63 78.1?0.63</cell></row><row><cell>GraphSAGE</cell><cell cols="4">vanilla 80.62?0.47 DHGR 81.30?0.26 71.11?0.65 77.63?0.16 70.30?0.57 77.1?0.23</cell></row><row><cell>APPNP</cell><cell cols="4">vanilla 83.25?0.42 DHGR 83.86?0.40 71.60?0.35 79.61?0.53 70.46?0.31 78.9?0.45</cell></row><row><cell>GCNII</cell><cell cols="4">vanilla 83.11?0.37 DHGR 83.93?0.28 71.96?0.67 79.49?0.39 70.90?0.73 79.46?0.33</cell></row><row><cell>Avg Gain</cell><cell>\</cell><cell>0.95 ?</cell><cell>0.90 ?</cell><cell>0.54 ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Node classification accuracy (%) of GCN with different graph rewiring methods. Model with * means we use the results from the original paper (under the same settings of datasets) for their code is unavailable. The bold numbers indicate that our method improves the base model. 68?3.06 26.39?0.88 28.90?0.57 61.96?1.27 RandAddEdge 32.17?6.06 22.77?5.05 26.68?2.26 55.85?1.68 RandDropEdge 39.01?2.47 26.48?1.09 29.54?0.36 66.76?1.52 RandAddEdge train Y 37.01?3.36 27.89?2.28 29.57?1.17 60.08?2.13 LDS 36.12?2.89 28.02?1.78 27.58?0.97 58.75?5.57 IDGL 37.28?3.36 23.57?2.07 27.17?0.85 67.57?5.85 SDRF* 44.46?0.17 41.47?0.21 29.85?0.07 70.35?0.60 DHGR 70.83?2.03 67.15?1.43 36.29?0.12 81.78?0.89</figDesc><table><row><cell>Methods</cell><cell>Chameleon Squirrel</cell><cell>Actor</cell><cell>Texas</cell></row><row><cell>Vanilla GCN</cell><cell>37.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Node classification of GCN enhanced by DHGR with different training ratio and batch size. For each dataset under certain training ratio, we randomly generate 3 data splits and calculate the average accuracy.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Squirrel</cell><cell></cell><cell></cell><cell>FB100</cell><cell></cell></row><row><cell>Batchsize</cell><cell>40%</cell><cell>20%</cell><cell>10%</cell><cell>40%</cell><cell>20%</cell><cell>10%</cell></row><row><cell>100?100</cell><cell cols="6">64.57 64.01 63.31 75.36 75.02 74.78</cell></row><row><cell>1000?1000</cell><cell cols="6">66.01 65.68 64.53 76.21 76.30 75.01</cell></row><row><cell>5000?5000</cell><cell cols="6">66.57 66.21 66.17 76.58 76.37 75.97</cell></row><row><cell cols="7">10000?10000 67.79 67.66 66.32 77.32 76.57 76.32</cell></row><row><cell>?</cell><cell cols="6">67.79 67.66 66.32 77.23 76.87 76.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Node classification accuracy (%) of the ablation studies to compare GCN with DHGR and its variants which remove certain component from the original DHGR architecture. DHGR \label_dist 80.97?0.05 65.38?5.53 79.67?1.79 75.95?0.16 DHGR \feat_dist 81.3?0.13 67.08?6.08 82.02?1.06 76.68?0.56 DHGR \feat_self 81.7?0.11 62.21?4.49 67.85?1.02 75.65?0.26 DHGR 82.63?0.41 67.38?5.33 81.78?0.89 77.01?0.14observe that the homophily ratio usually increases when increasing with fixed , while decreases when increasing with fixed . Besides, the change of GCN node classification accuracy basically matches the change of homophily ratio with different and . This demonstrates the effectiveness and robustness of the rewired graphs learned by DHGR.</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell>Cornell</cell><cell>Texas</cell><cell>FB100</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Node-level homophily ratio is the homophily ratio of one specific node, which equals the percent of the same-class neighbors in all neighboring nodes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Conference'17, July 2017, Washington, DC, USA Wendong Bi, Lun Du, Qiang Fu, Yanlin Wang, Shi Han, and Dongmei Zhang</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Conference'17, July 2017, Washington, DC, USA</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine :earning</title>
		<meeting>International Conference on Machine :earning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
	<note>Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05205</idno>
		<title level="m">On the bottleneck of graph neural networks and its practical implications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendong</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07012</idno>
		<title level="m">MM-GNN: Mix-Moment Graph Neural Network towards Modeling Neighborhood Feature Distribution</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Qingqing Long, and Kunqing Xie. 2021. Fast Hierarchy Preserving Graph Embedding via Subspace Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3580" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TSSRGCN: Temporal Spectral Spatial Retrieval Graph Convolutional Network for Traffic Flow Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaigui</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunqing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM50108.2020.00108</idno>
		<ptr target="https://doi.org/10.1109/ICDM50108.2020.00108" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="954" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19314" to="19326" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive Universal Generalized PageRank Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding and Improvement of Adversarial Training for Network Embedding from an Optimization Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="230" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TabularNet: A neural network architecture for understanding semantic structures of tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GBK-GNN: Gated Bi-Kernel Graph Neural Networks for Modeling Both Homophily and Heterophily</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhou</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1550" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Traffic events oriented dynamic traffic assignment model for expressway network: a network flow approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengfei</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyuan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Transportation Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="107" to="120" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07875</idno>
		<title level="m">Graph neural networks with learnable structural and positional representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International conference on machine learning</title>
		<meeting>International conference on machine learning</meeting>
		<imprint>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring structure-adaptive graph learning for robust semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2020 IEEE International Conference on Multimedia and Expo (ICME</title>
		<meeting>2020 IEEE International Conference on Multimedia and Expo (ICME</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How to learn a graph from smooth signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Kalofolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics</title>
		<meeting>Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="920" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Differentiable graph module (dgm) for graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05485</idno>
		<title level="m">Diffusion improves graph learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3546" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01404</idno>
		<title level="m">New benchmarks for learning on non-homophilous graphs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01312</idno>
		<title level="m">Learning sparse neural networks through _0 regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Motifnet: a motifbased graph convolutional network for directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2018 IEEE Data Science Workshop</title>
		<meeting>2018 IEEE Data Science Workshop</meeting>
		<imprint>
			<publisher>DSW</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph signal processing: Overview, challenges, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Kova?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="808" to="828" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geom-gcn: Geometric graph convolutional networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inferring explicit and implicit social ties simultaneously in mobile social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Paul</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14522</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Social structure of Facebook networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Traud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason A</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="page" from="4165" to="4180" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Kokel</surname></persName>
		</author>
		<title level="m">Graph Sparsification via Meta-Learning. DLG@ AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Powerful Graph Convolutioal Networks with Adaptive Propagation Mechanism for Homophily and Heterophily</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13562</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cocogum: Contextual code summarization with multi-relational gnn on umls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ensheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>MSR-TR-2020-16</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Tag2Gauss: Learning Tag Representations via Gaussian Distribution in Tagged Networks.. In IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3799" to="3805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two sides of the same coin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
	</analytic>
	<monogr>
		<title level="m">Heterophily and oversmoothing in graph convolutional neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Domain adaptive classification on heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1410" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Traj-GAT: A Graph-based Long-term Dependency Modeling Approach for Trajectory Similarity Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingping</forename><surname>Bi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2275" to="2285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Do Transformers Really Perform Badly for Graph Representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<title level="m">Graphsaint: Graph sampling based inductive learning method</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bayesian graph convolutional neural networks for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyasundar</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Ustebay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5829" to="5836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03036</idno>
		<title level="m">2022. A Survey on Graph Structure Learning: Progress and Opportunities</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Cagnn: Cluster-aware graph neural networks for unsupervised graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01674</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
