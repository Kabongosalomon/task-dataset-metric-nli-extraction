<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time!</p><p>We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128?128 images, and 79.8% with one trained at 224?224.</p><p>A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224?224 images and further optimized with our technique for test resolution 320?320 achieves 86.4% top-1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks <ref type="bibr" target="#b20">[21]</ref> (CNNs) are used extensively in computer vision tasks such as image classification <ref type="bibr" target="#b19">[20]</ref>, object detection <ref type="bibr" target="#b29">[30]</ref>, inpainting <ref type="bibr" target="#b41">[42]</ref>, style transfer <ref type="bibr" target="#b10">[11]</ref> and even image compression <ref type="bibr" target="#b30">[31]</ref>. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called "center crop". This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN.</p><p>Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately <ref type="bibr" target="#b7">[8]</ref>. In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift.</p><p>Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs. For instance, for a target test resolution of 224?224, training at resolution 160?160 provides better results than the standard practice of training at resolution 224?224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224?224 for the test resolution 320?320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet.</p><p>Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Image classification is a core problem in computer vision. It is used as a benchmark task by the community to measure progress. Models pre-trained for image classification, usually on the ImageNet database <ref type="bibr" target="#b8">[9]</ref>, transfer to a variety of other applications <ref type="bibr" target="#b26">[27]</ref>. Furthermore, advances in image classification translate to improved results on many other tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Recent research in image classification has demonstrated improved performance by considering larger networks and higher resolution images <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>. For instance, the state of the art in the ImageNet ILSVRC 2012 benchmark is currently held by the ResNeXt-101 32x48d <ref type="bibr" target="#b24">[25]</ref> architecture with 829M parameters using 224?224 images for training. The state of the art for a model learned from scratch is currently held by the EfficientNet-b7 <ref type="bibr" target="#b36">[37]</ref> with 66M parameters using 600?600 The red region of classification is resampled as a crop that is fed to the neural net. For objects that have as similar size in the input image, like the white horse, the standard augmentations typically make them larger at training time than at test time (second column). To counter this effect, we either reduce the train-time resolution, or increase the test-time resolution (third and fourth column). The horse then has the same size at train and test time, requiring less scale invariance for the neural net. Our approach only needs a computationally cheap fine-tuning.</p><p>images for training. In this paper, we focus on the ResNet-50 architecture <ref type="bibr" target="#b12">[13]</ref> due to its good accuracy/cost tradeoff (25.6M parameters) and its popularity. We also conduct some experiments using the PNASNet-5-Large <ref type="bibr" target="#b23">[24]</ref> architecture that exhibits good performance on ImageNet with a reasonable training time and number of parameters (86.1M) and with the ResNeXt-101 32x48d <ref type="bibr" target="#b24">[25]</ref> weakly supervised because it was the network publicly available with the best performance on ImageNet.</p><p>Data augmentation is routinely employed at training time to improve model generalization and reduce overfitting. Typical transformations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35]</ref> include: random-size crop, horizontal flip and color jitter. In our paper, we adopt the standard set of augmentations commonly used in image classification. As a reference, we consider the default models in the PyTorch library. The accuracy is also improved by combining multiple data augmentations at test time, although this means that several forward passes are required to classify one image. For example, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref> used ten crops (one central, and one for each corner of the image and their mirrored versions). Another performance-boosting strategy is to classify an image by feeding it at multiple resolutions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>, again averaging the predictions. More recently, multi-scale strategies such as the feature pyramid network <ref type="bibr" target="#b22">[23]</ref> have been proposed to directly integrate multiple resolutions in the network, both at train and test time, with significant gains in category-level detection.</p><p>Feature pooling. A recent approach <ref type="bibr" target="#b4">[5]</ref> employs p-pooling instead of average pooling to adapt the network to test resolutions significantly higher than the training resolution. The authors show that this improves the network's performance, in accordance with the conclusions drawn by Boureau et al. <ref type="bibr" target="#b5">[6]</ref>. Similar pooling techniques have been employed in image retrieval for a few years <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref>, where high-resolution images are required to achieve a competitive performance. These pooling strategies are combined <ref type="bibr" target="#b37">[38]</ref> or replace <ref type="bibr" target="#b28">[29]</ref> the RMAC pooling method <ref type="bibr" target="#b37">[38]</ref>, which aggregates a set of regions extracted at lower resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20%</head><p>40% 60% 80% frequency % of the image area train PDF test PDF The data augmentation schemes are the standard ones used at training and testing time for CNN classifiers. The spiky distribution at test time is due to the fact that RoCs are center crops and the only remaining variability is due to the different image aspect ratios. Notice that the distribution is very different at training and testing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Region selection and scale statistics</head><p>Applying a Convolutional Neural Network (CNN) classifier to an image generally requires to pre-process the image. One of the key steps involves selecting a rectangular region in the input image, which we call Region of Classification (RoC). The RoC is then extracted and resized to a square crop of a size compatible with the CNN, e.g., AlexNet requires a 224 ? 224 crop as input. While this process is simple, in practice it has two subtle but significant effects on how the image data is presented to the CNN. First, the resizing operation changes the apparent size of the objects in the image (section 3.1). This is important because CNNs do not have a predictable response to a scale change (as opposed to translations). Second, the choice of different crop sizes (for architectures such as ResNet that admit non-fixed inputs) has an effect on the statistics of the network activations, especially after global pooling layers (section 3.2). This section analyses in detail these two effects. In the discus-sion, we use the following conventions: The "input image" is the original training or testing image; the RoC is a rectangle in the input image; and the "crop" is the pixels of the RoC, rescaled with bilinear interpolation to a fixed resolution, then fed to the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scale and apparent object size</head><p>If a CNN is to acquire a scale-invariant behavior for object recognition, it must learn it from data. However, resizing the input images in pre-processing changes the distribution of objects sizes. Since different pre-processing protocols are used at training and testing time 2 , the size distribution differs in the two cases. This is quantified next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Relation between apparent and actual object sizes</head><p>We consider the following imaging model: the camera projects the 3D world onto a 2D image, so the apparent size of the objects is inversely proportional to their distance from the camera. For simplicity, we model a 3D object as an upright square of height and width R ? R at a distance Z from the camera, and fronto-parallel to it. Hence, its image is a r ? r rectangle, where the apparent size r is given by r = f R/Z where f is the focal length of the camera. Thus we can express the apparent size as the product r = f ? r 1 of the focal length f , which depends on the camera, and of the variable r 1 = R/Z, whose distribution p(r 1 ) is camera-independent. While the focal length is variable, the field of view angle ? FOV of most cameras is usually in the [40 ? , 60 ? ] range. Hence, for an image of size H ? W one can write f = k ? HW where k ?1 = 2 tan(? FOV /2) ? 1 is approximately constant. With this definition for f , the apparent size r is expressed in pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Effect of image pre-processing on the apparent object size</head><p>Now, we consider the effect of rescaling images on the apparent size of objects. If an object has an extent of r ? r pixels in the input image, and if s is the scaling factor between input image and the crop, then by the time the object is analysed by the CNN, it will have the new size of rs ? rs pixels. The scaling factor s is determined by the pre-processing protocol, discussed next.</p><p>Train-time scale augmentation. As a prototypical augmentation protocol, we consider RandomResizedCrop in PyTorch, which is very similar to augmentations used by other toolkits such as Caffe and the original AlexNet.</p><p>RandomResizedCrop takes as input an H ? W image, selects a RoC at random, and resizes the latter to output a K train ? K train crop. The RoC extent is obtained by first sampling a scale parameter ? such that ? 2 ? U ([? 2 ? , ? 2 + ]) and an aspect ratio ? such that ln ? ? U ([ln ? ? , ln ? + ]). Then, the size of the RoC in the input image is set to H RoC ? W RoC = ? ? 2 ?HW ? ? 2 HW/?. The RoC is resized anisotropically with factors (K train /H RoC , K train /W RoC ) to generate the output image. Assuming for simplicity that the input image is square (i.e. H = W ) and that ? = 1, the scaling factor from input image to output crop is given by:</p><formula xml:id="formula_0">s = ? K train K train ? H RoC W RoC = 1 ? ? K train ? HW .</formula><p>(1)</p><p>By scaling the image in this manner, the apparent size of the object becomes</p><formula xml:id="formula_1">r train = s ? r = sf ? r 1 = kK train ? ? r 1 .<label>(2)</label></formula><p>Since kK train is constant, differently from r, r train does not depend on the size H ? W of the input image. Hence, preprocessing standardizes the apparent size, which otherwise would depend on the input image resolution. This is important as networks do not have built-in scale invariance.</p><p>Test-time scale augmentation. As noted above, test-time augmentation usually differs from train-time augmentation. The former usually amounts to: isotropically resizing the image so that the shorter dimension is K image test and then extracting a K test ? K test crop (CenterCrop) from that. Under the assumption that the input image is square (H = W ), the scaling factor from input image to crop rewrites as</p><formula xml:id="formula_2">s = K image test / ? HW , so that r test = s ? r = kK image test ? r 1 .<label>(3)</label></formula><p>This has a a similar size standardization effect as the train-time augmentation.</p><p>Lack of calibration. Comparing eqs. <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>, we conclude that the same input image containing an object of size r 1 results in two different apparent sizes if training or testing pre-processing is used. These two sizes are related by:</p><formula xml:id="formula_3">r test r train = ? ? K image test K train .<label>(4)</label></formula><p>In practice, for standard networks such as AlexNet K image test /K train ? 1.15; however, the scaling factor ? is sampled (with the square law seen above) in a range [? ? , ? + ] = [0.28, 1]. Hence, at testing time the same object may appear as small as a third of what it appears at training time. For standard values of the pre-processing parameters, the expected value of this ratio w.</p><formula xml:id="formula_4">r.t. ? is E r test r train = F ? K image test K train ? 0.80, F = 2 3 ? ? 3 + ? ? 3 ? ? 2 + ? ? 2 ? ,<label>(5)</label></formula><p>where F captures all the sampling parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scale and activation statistics</head><p>In addition to affecting the apparent size of objects, preprocessing also affects the activation statistics of the CNN, especially if its architecture allows changing the size of the input crop. We first look at the receptive field size of a CNN activation in the previous layer. This is the number of input spatial locations that affect that response. For the convolutional part of the CNN, comprising linear convolution, subsampling, ReLU, and similar layers, changing the input crop size is almost neutral because the receptive field is unaffected by the input size. However, for classification the network must be terminated by a pooling operator (usually average pooling) in order to produce a fixed-size vector. Changing the size of the input crop strongly affects the activation statistics of this layer. Activation statistics. We measure the distribution of activation values after the average pooling in a ResNet-50 in <ref type="figure" target="#fig_2">fig. 3</ref>. As it is applied on a ReLU output, all values are non-negative. At the default crop resolution of K test = K train = 224 pixels, the activation map is 7?7 with a depth of 2048. At K test = 64, the activation map is only 2?2: pooling only 0 values becomes more likely and activations are more sparse (the rate of 0's increases form 0.5% to 29.8%). The values are also more spread out: the fraction of values above 2 increases from 1.2% to 11.9%. Increasing the resolution reverts the effect: with K test = 448, the activation map is 14?14, the output is less sparse and less spread out.</p><p>This simple statistical observations shows that if the distribution of activations changes at test time, the values are not in the range that the final classifier layers (linear &amp; softmax) were trained for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Larger test crops result in better accuracy</head><p>Despite the fact that increasing the crop size affects the activation statistics, it is generally beneficial for accuracy, since as discussed before it reduces the train-test object size mismatch. For instance, the accuracy of ResNet-50 on the ImageNet validation set as K test is changed (see section 5) are: Thus for K test = 288 the accuracy is 78.4%, which is greater than 77.0% obtained for the native crop size K test = K train = 224 used in training. In <ref type="figure" target="#fig_4">fig. 5</ref>, we see this result is general: better accuracy is obtained with higher resolution crops at test time than at train time. In the next section, we explain and leverage this discrepancy by adjusting the network's weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>Based on the analysis of section 3, we propose two improvements to the standard setting. First, we show that the difference in apparent object sizes at training and testing time can be removed by increasing the crop size at test time, which explains the empirical observation of section 3.3. Second, we slightly adjust the network before the global average pooling layer in order to compensate for the change in activation statistics due to the increased size of the input crop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Calibrating the object sizes by adjusting the crop size</head><p>Equation <ref type="formula" target="#formula_4">(5)</ref>  is increased by a factor ? (where ? ? 1/0.80 = 1.25 in the example) then at test time, the apparent size of the objects is increased by the same factor. This equalizes the effect of the training pre-processing that tends to zoom on the objects. However, increasing K image test with K test fixed means looking at a smaller part of the object. This is not ideal: the object to identify is often well framed by the photographer, so the crop may show only a detail of the object or miss it altogether. Hence, in addition to increasing K image test , we also increase the crop size K test to keep the ratio K image test /K test constant. However, this means that K test &gt; K train , which skews the activation statistics (section 3.2). The next section shows how to compensate for this skew.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adjusting statistics before spatial pooling</head><p>At this point, we have selected the "correct" test resolution for the crop but we have skewed activation statistics. Hereafter we explore two approaches to compensate for this skew.</p><p>Parametric adaptation. We fit the output of the average pooling layer (section 3.2) with a parametric Fr?chet distribution at the original K train and final K test resolutions. Then, we define an equalization mapping from the new distribution back to the old one via a scalar transformation, and apply it as an activation function after the pooling layer (see Appendix A). This compensation provides a measurable but limited improvement on accuracy, probably because the model is too simple and does not differentiate the distributions of different components going through the pooling operator.</p><p>Adaptation via fine-tuning. Increasing the crop resolution at test time is effectively a domain shift. A natural way to compensate for this shift is to fine-tune the model. In our case, we fine-tune on the same training set, after switching from K train to K test . Here we choose to restrict the fine-tuning to the very last layers of the network.</p><p>A take-away from the distribution analysis is that the sparsity should be adapted. This requires at least to include the batch normalization that precedes the global pooling into the fine-tuning. In this way the batch statistics are adapted to the increased resolution. We also use the test-time augmentation scheme during fine-tuning to avoid incurring further domain shifts. <ref type="figure" target="#fig_3">Figure 4</ref> shows the pooling operator's activation statistics before and after fine-tuning. After fine-tuning the activation statistics closely resemble the train-time statistics. This hints that adaptation is successful. However, as discussed above, this does not imply an improvement in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Benchmark data. We experiment on the ImageNet-2012 benchmark <ref type="bibr" target="#b31">[32]</ref>, reporting validation performance as top-1 accuracy. It has been argued that this measure is sensitive to er- rors in the ImageNet labels <ref type="bibr" target="#b33">[34]</ref>. However, the top-5 metrics, which is more robust, tends to saturate with modern architectures, while the top-1 accuracy is more sensitive to improvements in the model.</p><p>To assess the significance of our results, we compute the standard deviation of the top-1 accuracy: we classify the validation images, split the set into 10 folds and measure the accuracy on 9 of them, leaving one out in turn. The standard deviation of accuracy over these folds is ? 0.03% for all settings. Thus we report 1 significant digit in the accuracy percentages.</p><p>In the supplemental material, we also report results on the Fine-Grained Visual Categorization challenges iNaturalist and Herbarium.</p><p>Architectures. We use standard state-of-the-art neural network architectures with no modifications, We consider in particular ResNet-50 <ref type="bibr" target="#b12">[13]</ref>. For larger experiments, we use PNASNet-5-Large <ref type="bibr" target="#b23">[24]</ref>, learned using "neural architecture search" as a succession of interconnected cells. It is accurate (82.9% Top-1) with relatively few parameters (86.1 M). We use also ResNeXt-101 32x48d <ref type="bibr" target="#b24">[25]</ref>, pre-trained in weaklysupervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets. It is accurate (85.4% Top-1) with lot of parameters (829 M).</p><p>Training protocol. We train ResNet-50 with SGD with a learning rate of 0.1 ? B/256, where B is the batch size, as in <ref type="bibr" target="#b14">[15]</ref>. The learning rate is divided by 10 every 30 epochs. With a Repeated Augmentation of 3, an epoch processes 5005 ? 512/B batches, or ?90% of the training images, see <ref type="bibr" target="#b4">[5]</ref>. In the initial training, we use B = 512, 120 epochs and the default PyTorch data augmentation: horizontal flip, random resized crop (as in section 3) and color jittering. To finetune, the initial learning rate is 0.008 same decay, B = 512, 60 epochs. The data-augmentation used for fine-tuning is described in the next paragraph. For ResNeXt-101 32x48d we use the pretrained version from PyTorch hub repository <ref type="bibr" target="#b1">[2]</ref>. We use almost the same fine-tuning as for the ResNet-50. We also use a ten times smaller learning rate and a batch size two times smaller. For PNASNet-5-Large we use the pretrained version from Cadene's GitHub repository [1]. The difference with the ResNet-50 fine-tuning is that we modify the last three cells, in one epoch and with a learning rate of 0.0008. We run our experiments on machines with 8 Tesla V100 GPUs and 80 CPU cores to train and fine-tune our ResNet-50.</p><p>Fine-tuning data-augmentation. We experimented three data-augmentation for fine-tuning: The first one (test DA) is resizing the image and then take the center crop, The second one (test DA2) is resizing the image, random horizontal shift of the center crop, horizontal flip and color jittering. The last one (train DA) is the train-time data-augmentation as described in the previous paragraph.</p><p>A comparison of the performance of these data augmentation is made in the section C.</p><p>The test DA data-augmentation described in this paragraph being the simplest. Therefore test DA is used for all the results reported with ResNet-50 and PNASNet-5-Large in this paper except in <ref type="table">Table 2</ref> where we use test DA2 to have slightly better performances in order to compare ours results with the state of the art.</p><p>For ResNeXt-101 32x48d all reported results are obtained with test DA2. We make a comparison of the results obtained between testDA, testDA2 and train DA in section C.</p><p>The baseline experiment is to increase the resolution without adaptation. Repeated augmentations already improve the default PyTorch ResNet-50 from 76.2% top-1 accuracy to 77.0%. <ref type="figure" target="#fig_4">Figure 5</ref>(left) shows that increasing the resolution at test time increases the accuracy of all our networks. E.g., the accuracy of a ResNet-50 trained at resolution 224 increases from 77.0 to 78.4 top-1 accuracy, an improvement of 1.4 percentage points. This concurs with prior findings in the literature <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>Improvement of our approach on a ResNet-50. <ref type="figure" target="#fig_4">Figure 5(right)</ref> shows the results obtained after fine-tuning the last batch norm in addition to the classifier. With fine-tuning we get the best results (79%) with the classic ResNet-50 trained at K train = 224. Compared to when there is no fine-tuning, the K test at which the maximal accuracy is obtained increases from K test = 288 to 384. If we prefer to reduce the training resolution, K train = 128 and testing at K train = 224 yields 77.1% accuracy, which is above the baseline trained at full test resolution without fine-tuning.</p><p>Multiple resolutions. To improve the accuracy, we classify the image at several resolutions and average the classification scores. Thus, the training time remains the same but there is a modest increase in inference time compared to processing only the highest-resolution crop.  and K test = [384, 352], we improve the single-crop result of 79.0% to 79.5%.</p><p>Application to larger networks. The same adaptation method can be applied to any convolutional network. In Table 1 we report the result on the PNASNet-5-Large and the IG-940M-1.5k ResNeXt-101 32x48d <ref type="bibr" target="#b24">[25]</ref>. For the PNASNet-5-Large, we found it beneficial to fine-tune more than just the batch-normalization and the classifier. Therefore, we also experiment with fine-tuning the three last cells. By increasing the resolution to K test = 480, the accuracy increases by 1 percentage point. By combining this with an ensemble of 10 crops at test time, we obtain 83.9% accuracy. With the ResNeXt-101 32x48d increasing the resolution to K test = 320, the accuracy increases by 1.0 percentage point. We thus reached 86.4% top-1 accuracy.</p><p>Speed-accuracy trade-off. We consider the trade-off between training time and accuracy (normalized as if it was run on 1 GPU). The full table with timings are in supplementary Section C. In the initial training stage, the forward pass is 3 to 6 times faster than the backward pass. However, during fine-tuning the ratio is inverted because the backward pass is applied only to the last layers.</p><p>In the low-resolution training regime (K train = 128), the additional fine-tuning required by our method increases the training time from 111.8 h to 124.1 h (+11%). This is to obtain an accuracy of 77.1%, which outperforms the network trained at the native resolution of 224 in 133.9 h. We produce a finetuned network with K test = 384 that obtains a higher accuracy than the network trained natively at that resolution, and the training is 2.3? faster: 151.5 h instead of 348.5 h.</p><p>Ablation study. We study the contribution of the different choices to the performance, limited to K train = 128 and K train = 224. By simply fine-tuning the classifier (the fully connected layers of ResNet-50) with test-time augmentation, we reach 78.9% in Top-1 accuracy with the classic ResNet-50 initially trained at resolution 224. The batch-norm fine-tuning and improvement in data augmentation advances it to 79.0%. The higher the difference in resolution between training and testing, the more important is batch-norm fine-tuning to adapt to the data augmentation. The full results are in the supplementary Section C. <ref type="table">Table 2</ref> compares our results with competitive methods from the literature. Our ResNet-50 is slightly worse than ResNet50-D and MultiGrain, but these do not have exactly the same architecture. On the other hand our ResNet-50 CutMix, which has a classic ResNet-50 architecture, outperforms others ResNet-50 including the slightly modified versions. Our fine-tuned PNASNet-5 outperforms the MultiGrain version. To the best of our knowledge our ResNeXt-101 32x48d surpasses all other models available in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Beyond the current state of the art</head><p>With 86.4% Top-1 accuracy and 98.0% Top-5 accuracy it is the first model to exceed 86.0% in Top-1 accuracy and 98.0% in Top-5 accuracy on the ImageNet-2012 benchmark <ref type="bibr" target="#b31">[32]</ref>. It exceeds the previous state of the art <ref type="bibr" target="#b24">[25]</ref> by 1.0% absolute in Top-1 accuracy and 0.4% Top-5 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Transfer learning tasks</head><p>We have used our method in transfer learning tasks to validate its effectiveness on other dataset than ImageNet. We evaluated it on the following datasets: iNaturalist 2017 <ref type="bibr" target="#b15">[16]</ref>, Stanford Cars <ref type="bibr" target="#b18">[19]</ref>, CUB-200-2011 <ref type="bibr" target="#b40">[41]</ref>, Oxford 102 Flowers <ref type="bibr" target="#b25">[26]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b27">[28]</ref>, NABirds <ref type="bibr" target="#b39">[40]</ref> and Birdsnap <ref type="bibr" target="#b3">[4]</ref>. We used our method with two types of networks for transfer learning tasks: SENet-154 <ref type="bibr" target="#b2">[3]</ref> and InceptionResNet-V2 <ref type="bibr" target="#b35">[36]</ref>.</p><p>For all these experiments, we proceed as follows.</p><p>(1) we initialize our network with the weights learned on ImageNet (using models from [1]). (2) we train it entirely for several epochs at a certain resolution. (3) we fine-tune with a higher resolution the last batch norm and the fully connected layer. <ref type="table" target="#tab_4">Table 3</ref> summarizes the models we used and the performance we achieve. We can see that in all cases our method improves the performance of our baseline. Moreover, we notice that the higher the image resolution, the more efficient the method is. This is all the more relevant today, as the quality of the images increases from year to year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the network's pooling activations. We have shown that, by adjusting the crop resolution and via  a simple and light-weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown that researchers waste resources when both training and testing strong networks at resolution 224 ? 224; We introduce a method that can "fix" these networks post-facto and thus improve their performance. An open-source implementation of our method is available at https://github.com/ facebookresearch/FixRes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material for "Fixing the train-test resolution discrepancy"</head><p>In this supplementary material we report details and results that did not fit in the main paper. This includes the estimation of the parametric distribution of activations in Section A, a small study on border/round-off effects of the image size for a convolutional neural net in Section B and more exhaustive result tables in Section C. Section E further demonstrates the interest of our approach through our participation to two competitive challenges in fine-grained recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Fitting the activations A.1 Parametric Fr?chet model after average-pooling</head><p>In this section we derive a parametric model that fits the distribution of activations on output of the spatial pooling layer.</p><p>The output the the last convolutional layer can be well approximated with a Gaussian distribution. Then the batch-norm centers the Gaussian and reduces its variance to unit, and the ReLU replaces the negative part with 0. Thus the ReLU outputs an equal mixture of a cropped unit Gaussian and a Dirac of value 0.</p><p>The average pooling sums n = 2 ? 2 to n = 14 ? 14 of those distributions together. Assuming independence of the inputs, it can be seen as a sum of n cropped Gaussians, where n follows a discrete binomial distribution. Unfortunately, we found this composition of distributions is not tractable in close form.</p><p>Instead, we observed experimentally that the output distribution is close to an extreme value distribution. This is due to the fact that only the positive part of the Gaussians contributes to the output values. In an extreme value distribution that is the sum of several (arbitrary independent) distributions, the same happens: only the highest parts of those distributions contribute. Thus, we model the statistics of activations as a Fr?chet (a.k.a. inverse Weibull) distribution. This is a 2-parameter distribution whose CDF has the form:</p><formula xml:id="formula_5">P (x, ?, ?) = e ?(1+ ? ? (x??)) ?1/?</formula><p>With ? a positive constant, ? ? R, ? ? R * + . We observed that the parameter ? can be kept constant at 0.3 to fit the distributions. <ref type="figure" target="#fig_5">Figure 6</ref> shows how the Fr?chet model fits the empirical CDF of the distribution. The parameters were estimated using least-squares minimization, excluding the zeros, that can be considered outliers. The fit is so exact that the difference between the curves is barely visible.</p><p>To correct the discrepancy in distributions at training and test times, we compute the parameters ? ref , ? ref of the distribution observed on training images time for K test = K train . Then we increase K test to the target resolution and measure the parameters ? 0 , ? 0 again. Thus, the transformation is just an affine scaling, still ignoring zeros.</p><p>When running the transformed neural net on the Imagenet evaluation, we obtain accuracies: Hence, the accuracy does not improve with respect to the baseline. This can be explained by several factors: the scalar distribution model, however good it fits to the observations, is insufficient to account for the individual distributions of the activation values; just fitting the distribution may not be enough to account for the changes in behavior of the convolutional trunk.   A.2 Gaussian model before the last ReLU activation</p><p>Following the same idea as what we did previously we looked at the distribution of activations by channel before the last ReLU according to the resolution. We have seen that the distributions are different from one resolution to another. With higher resolutions, the mean tends to be closer to 0 and the variance tends to become smaller. By acting on the distributions before the ReLU, it is also possible to affect the sparsity of values after spatial-pooling, which was not possible with the previous analysis based on Frechet's law. We aim at matching the distribution before the last ReLU with the distribution of training data at lower resolution. We compare the effect of this transformation before/after fine tuning with the learnt batch-norm approach. The results are summarized in <ref type="table" target="#tab_6">Table 4</ref>.</p><p>We can see that adapting the resolution by changing the distributions is effective especially in the case of small resolutions. Nevertheless, the adaptation obtained by fine-tuning the the batch norm improves performs better in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Border and round-off effects</head><p>Due to the complex discrete nature of convolutional layers, the accuracy is not a monotonous function of the input resolution. There is a strong dependency on the kernel sizes and strides used in the first convolutional layers. Some resolutions will not match with these parameters so we will have a part of the images margin that will not be taken into account by the convolutional layers.</p><p>In <ref type="figure" target="#fig_6">Figure 7</ref>, we show the variation in accuracy when the resolution of the crop is increased by steps of 1 pixel. Of course, it is possible to do padding but it will never be equivalent to having a resolution image adapted to the kernel and stride size.</p><p>Although the global trend is increasing, there is a lot of jitter that comes from those border effects. There is a large drop just after resolution 256. We observe the drops at each multiple of 32, they correspond to a changes in the top-level activation map's resolution. Therefore we decided to use only sizes that are multiples of 32 in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Result tables</head><p>Due to the lack of space, we report only the most important results in the main paper. In this section, we report the full result tables for several experiments. <ref type="table" target="#tab_7">Table 5</ref> report the numerical results corresponding to <ref type="figure" target="#fig_4">Figure 5</ref> in the main text. <ref type="table" target="#tab_8">Table 6</ref> reports the full ablation study results (see Section 5.1). <ref type="table">Table 7</ref> reports the runtime measurements that Section 5.1 refers to. <ref type="table">Table 8</ref> reports a comparaison between test DA and test DA2 that Section 5 refers to.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Impact of Random Resized Crop</head><p>In this section we measure the impact of the RandomResizedCrop illustrated in the section 5. To do this we did the same experiment as in section 5 but we replaced the RandomResizedCrop with a Resize followed by a random crop with a fixed size. The <ref type="figure" target="#fig_7">figure 8</ref> and <ref type="table" target="#tab_9">table 9</ref> shows our results. We can see that the effect observed in the section 5 is mainly due to the Random Resized Crop as we suggested with our analysis of the section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Fine-Grained Visual Categorization contests: iNaturalist &amp; Herbarium</head><p>In this section we summarize the results we obtained with our method during the CVPR 2019 iNaturalist <ref type="bibr" target="#b15">[16]</ref> and Herbarium <ref type="bibr" target="#b6">[7]</ref> competitions <ref type="bibr" target="#b2">3</ref> . We used the approach lined out in Subsection 5.3, except that we adapted the preprocessing to each dataset and added a few "tricks" useful in competitions (ensembling, multiple crops).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Challenges</head><p>The iNaturalist Challenge 2019 dataset contains images of 1010 animal and plant species, with a training set of 268,243 images and a test set of 35,351 images. The main difficulty is that the species are very similar within the six main families (Birds, Reptiles, Plants, Insects, Fungi and Amphibians) contained in the dataset. There is also a very high variability within the classes as the appearance of males, females and juveniles is often very different. What also complicates the classification is the size of the area of interest which is very variable from one image to another, sometimes the images are close-ups on the subject, sometimes we can hardly distinguish it. As a preprocessing, all images have been resized to have a maximum dimension of 800 pixels. The Herbarium contest requires to identify melastome species from 683 herbarium specimenina. The training set contain 34,225 images and the test set contain 9,565 images. The main difficulty is that the specimina are very similar and not always intact. In this challenge the particularity is that there is no variability in the background: each specimen is photographed on a white sheet of paper. All images have been also resized to have a maximum dimension of 800 pixels.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Ensemble of classifiers</head><p>In both cases we used 4 different CNNs to do the classification and we averaged their results, which are themselves from 10 crops of the image. We chose 4 quite different in their architectures in order to obtain orthogonal classification results. We tried to include the ResNet-50, but it was significantly worse than the other models, even when using an ensemble of models, probably due to its limited capacity. We used two fine-tuning stages: (1) to adapt to the new dataset in 120 epochs and (2) to adapt to a higher resolution in a few epochs. We chose the initial training resolution with grid-search, within the computational constraints. We did not skew the sampling to balance the classes. The rationale for this is that the performance measure is top-1 accuracy, so the penalty to misclassify infrequent classes is low. <ref type="table" target="#tab_3">Table 10</ref> summarizes the parameters of our submission and the results. We report our top-performing approach, 3 and 1 points behind the winners of the competition. Note that we just used our method off-the-shelf and therefore used much fewer evaluations on the public part of the test set (5 for iNaturalist and 8 for Herbarium). The number of CNNs that at are combined in our ensemble is also smaller that two best performing ones. In addition, for iNaturalist we did not train on data from the 2018 version of the contest. In summary, our participation was a run with minimal if no tweaking, where we obtain excellent results (5th out of more than 200 on iNaturalist), thanks to the test-time resolution adaptation exposed in this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Selection of the image regions fed to the network at training time and testing time, with typical data-augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Empirical distribution of the areas of the RoCs as a fraction of the image areas extracted by data augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Cumulative density function of the vectors components on output of the spatial average pooling operator, for a standard ResNet-50 trained at resolution 224, and tested at different resolutions. The distribution is measured on the validation images of Imagenet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>CDF of the activations on output of the average pooling layer, for a ResNet-50, when tested at different resolutions K test . Compare the state before and after fine-tuning the batch-norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Top-1 accuracy of the ResNet-50 according to the test time resolution. Left: without adaptation, right: after resolution adaptation. The numerical results are reported in Appendix C. A comparison of results without random resized crop is reported in Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Fitting of the CDF of activations with a Fr?chet distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Evolution of the top-1 accuracy of the ResNet-50 trained with resolution 224 according to the testing resolution (no finetuning). This can be considered a zoom of figure 5 with 1-pixel increments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Top-1 accuracy of the ResNet-50 according to the test time resolution. ResNet-50 train with resize and random crop with a fixed size instead of random resized crop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>estimates the change in the apparent object sizes during training and testing. If the size of the intermediate image K image test</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Application to larger networks: Resulting top-1 accuracy</figDesc><table><row><cell>Model</cell><cell>Train</cell><cell>Fine-tuning</cell><cell></cell><cell>Test resolution</cell><cell></cell></row><row><cell>used</cell><cell cols="2">resolution Classifier Batch-norm</cell><cell>Three last Cells</cell><cell>331 384 395 416</cell><cell>448</cell><cell>480</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Transfer learning task with our method and comparison with the state of the art. We only compare ImageNet-based transfer learning results with a single center crop for the evaluation (if available, otherwise we report the best published result) without any change in architecture compared to the one used on ImageNet. We report Top-1 Accuracy(%).</figDesc><table><row><cell>Dataset</cell><cell>Models</cell><cell cols="2">Baseline With Our Method</cell><cell>State-Of-The-Art Models</cell><cell></cell></row><row><cell>iNaturalist 2017 [16]</cell><cell>SENet-154</cell><cell>74.1</cell><cell>75.4</cell><cell>IncResNet-V2-SE [16]</cell><cell>67.3</cell></row><row><cell>Stanford Cars [19]</cell><cell>SENet-154</cell><cell>94.0</cell><cell>94.4</cell><cell>EfficientNet-B7 [37]</cell><cell>94.7</cell></row><row><cell>CUB-200-2011 [41]</cell><cell>SENet-154</cell><cell>88.4</cell><cell>88.7</cell><cell>MPN-COV [22]</cell><cell>88.7</cell></row><row><cell cols="2">Oxford 102 Flowers [26] InceptionResNet-V2</cell><cell>95.0</cell><cell>95.7</cell><cell>EfficientNet-B7 [37]</cell><cell>98.8</cell></row><row><cell>Oxford-IIIT Pets [28]</cell><cell>SENet-154</cell><cell>94.6</cell><cell>94.8</cell><cell cols="2">AmoebaNet-B (6,512) [17] 95.9</cell></row><row><cell>NABirds [40]</cell><cell>SENet-154</cell><cell>88.3</cell><cell>89.2</cell><cell>PC-DenseNet-161 [10]</cell><cell>82.8</cell></row><row><cell>Birdsnap [4]</cell><cell>SENet-154</cell><cell>83.4</cell><cell>84.3</cell><cell>EfficientNet-B7 [37]</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Matching distribution before the last Relu application to ResNet-50: Resulting top-1 accuracy % on ImageNet validation set</figDesc><table><row><cell>Model</cell><cell>Train</cell><cell>Adapted</cell><cell>Fine-tuning</cell><cell></cell><cell>Test resolution</cell></row><row><cell>used</cell><cell cols="3">resolution Distribution Classifier Batch-norm</cell><cell>64</cell><cell>224 288 352 384</cell></row><row><cell></cell><cell>78.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>76.5 77.0 77.5 78.0 Top 1 accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>76.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">224 232 240 248 256 264 272 280 288 Test resolution (pixels)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Top-1 validation accuracy for different combinations of training and testing resolution. Left: with the standard training procedure, (no finetuning, no adaptation of the ResNet-50). Right: with our data-driven adaptation strategy and test-time augmentations.</figDesc><table><row><cell>Train</cell><cell>Fine-tuning</cell><cell></cell><cell></cell><cell cols="4">Test resolution (top-1 accuracy)</cell></row><row><cell cols="3">resolution Classifier Batch-norm Data aug.</cell><cell>64</cell><cell>128</cell><cell>224</cell><cell>288</cell><cell>384</cell><cell>448</cell></row><row><cell>-</cell><cell>-</cell><cell>n/a</cell><cell cols="5">48.3 73.3 75.7 73.8 69.6 65.8</cell></row><row><cell></cell><cell>-</cell><cell>train DA</cell><cell cols="5">52.8 73.3 77.1 76.3 73.2 71.7</cell></row><row><cell>128</cell><cell>-</cell><cell>test DA</cell><cell cols="5">53.3 73.4 77.1 76.4 74.4 72.3</cell></row><row><cell></cell><cell></cell><cell>train DA</cell><cell cols="5">53.0 73.3 77.1 76.5 74.4 71.9</cell></row><row><cell></cell><cell></cell><cell>test DA</cell><cell cols="5">53.7 73.4 77.1 76.6 74.8 73.0</cell></row><row><cell>-</cell><cell>-</cell><cell>n/a</cell><cell cols="5">29.4 65.4 77.0 78.4 77.7 76.6</cell></row><row><cell></cell><cell>-</cell><cell>train DA</cell><cell cols="5">39.9 67.5 77.0 78.6 78.9 78.0</cell></row><row><cell>224</cell><cell>-</cell><cell>test DA</cell><cell cols="5">40.6 67.3 77.1 78.6 78.9 77.9</cell></row><row><cell></cell><cell></cell><cell>train DA</cell><cell cols="5">40.4 67.5 77.0 78.6 78.9 78.0</cell></row><row><cell></cell><cell></cell><cell>test DA</cell><cell cols="5">41.7 67.7 77.1 78.6 79.0 78.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation study: Accuracy when enabling or disabling some components of the training method. Train DA: trainingtime data augmentation during fine-tuning, test DA: test-time one.</figDesc><table><row><cell cols="4">Resolution Train time per batch (ms) Resolution fine-tuning (ms)</cell><cell>Performance</cell></row><row><cell>train test</cell><cell>backward</cell><cell>forward backward</cell><cell cols="2">forward Total time (h) accuracy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Top-1 validation accuracy for different combinations of training and testing resolution. ResNet-50 train with resize and random crop with a fixed size instead of random resized crop.</figDesc><table><row><cell>80</cell><cell></cell></row><row><cell>70</cell><cell></cell></row><row><cell>10 20 30 40 50 60 Top-1 accuracy</cell><cell>64 96 128 160 Test resolution (pixels) 224 256 Train resolution 64 Train resolution 128 384 440 Train resolution 224 Train resolution 384 Accuracy with train resolution Best accuracy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Our best ensemble results for the Herbarium and INaturalist competitions.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Update: Since the publication of this paper at Neurips, we have improved this state of the art by applying our method to EfficientNet. See our note<ref type="bibr" target="#b38">[39]</ref> for results and details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">At training time, the extraction and resizing of the RoC is used as an opportunity to augment the data by randomly altering the scale of the objects, in this manner the CNN is stimulated to be invariant to a wider range of object scales.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.kaggle.com/c/herbarium-2019-fgvc6 https://www.kaggle.com/c/inaturalist-2019-fgvc6</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">State of the art on ImageNet with ResNet-50 architectures and with all types of architecture (Single Crop evaluation) Models Extra Training Data Train Test # Parameters Top-1 (%) Top-5 (%)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="https://pytorch.org/hub/facebookresearch_WSL-Images_resnext" />
		<title level="m">Pytorch hub models</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu Andli Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Squeeze-andexcitation networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale finegrained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Tan Kiat Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrose</forename><surname>Yulong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tulig</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belongie</forename><surname>Melissa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05372</idno>
		<title level="m">The herbarium challenge 2019 dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training with confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08016</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end learning of deep visual representations for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="254" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The inaturalist challenge 2017 dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hy-Oukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpipe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08974</idno>
		<title level="m">Do better imagenet models transfer better</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hinton. Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Is second-order information helpful for large-scale visual recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08050</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time adaptive image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<title level="m">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05879</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fixing the train-test resolution discrepancy: Fixefficientnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Billion-scale semisupervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Ismet Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cutmix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04899</idno>
		<title level="m">Regularization strategy to train strong classifiers with localizable features</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Training and fine-tuning times are reported for a batch of size 32 for training and 64 for fine-tuning, on one GPU. Fine-tuning uses less memory than training therefore we can use larger batch size. The total time is the total time spent on both, with 120 epochs for training and 60 epochs of fine-tuning on ImageNet. Our approach corresponds to fine-tuning of the batch-norm and the classification layer</title>
	</analytic>
	<monogr>
		<title level="m">Table 7: Execution time for the training</title>
		<imprint/>
	</monogr>
	<note>1 test DA (%) Top-1 test DA2 (%)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Comparisons of performance between data-augmentation test DA and test DA2 in the case of fine-tuning batch-norm and classifier</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
