<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Decebal</forename><forename type="middle">Constantin</forename><surname>Mocanu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
						</author>
						<title level="a" type="main">Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a new perspective on training deep neural networks capable of stateof-the-art performance without the need for the expensive over-parameterization by proposing the concept of In-Time Over-Parameterization (ITOP) in sparse training. By starting from a random sparse network and continuously exploring sparse connectivities during training, we can perform an Over-Parameterization over the course of training, closing the gap in the expressibility between sparse training and dense training. We further use ITOP to understand the underlying mechanism of Dynamic Sparse Training (DST) and discover that the benefits of DST come from its ability to consider across time all possible parameters when searching for the optimal sparse connectivity. As long as sufficient parameters have been reliably explored, DST can outperform the dense neural network by a large margin. We present a series of experiments to support our conjecture and achieve the state-of-the-art sparse training performance with ResNet-50 on ImageNet. More impressively, ITOP achieves dominant performance over the overparameterization-based sparse methods at extreme sparsities. When trained with ResNet-34 on CIFAR-100, ITOP can match the performance of the dense model at an extreme sparsity of 98%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over-Parameterization has been shown to be crucial to the dominating performance of deep neural networks in practice, despite the fact that the training objective function  is usually non-convex and non-smooth <ref type="bibr" target="#b24">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b5">Brutzkus et al., 2017;</ref><ref type="bibr" target="#b38">Li &amp; Liang, 2018;</ref><ref type="bibr" target="#b56">Safran &amp; Shamir, 2018;</ref><ref type="bibr" target="#b61">Soudry &amp; Carmon, 2016;</ref><ref type="bibr" target="#b0">Allen-Zhu et al., 2019;</ref><ref type="bibr" target="#b15">Du et al., 2019;</ref><ref type="bibr" target="#b73">Zou et al., 2020;</ref><ref type="bibr" target="#b72">Zou &amp; Gu, 2019)</ref>. Meanwhile, advanced deep models <ref type="bibr" target="#b59">(Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b31">He et al., 2016;</ref><ref type="bibr" target="#b12">Devlin et al., 2018;</ref><ref type="bibr" target="#b4">Brown et al., 2020;</ref><ref type="bibr" target="#b14">Dosovitskiy et al., 2021)</ref> are continuously achieving state-of-the-art results in numerous machine-learning tasks. While achieving impressive performance, the size of the state-of-the-art models is also exploding. The resources required to train and deploy those highly over-parameterized models are prohibitive.</p><p>Motivated for inference, a large body of research <ref type="bibr" target="#b53">(Mozer &amp; Smolensky, 1989;</ref><ref type="bibr" target="#b27">Han et al., 2015)</ref> attempts to discover a sparse model that can sufficiently match the performance of the corresponding dense model while substantially reduce the number of parameters. While effective, these techniques involve pre-training a highly over-parameterized model for either at least a full converged training time (full dense overparameterization) <ref type="bibr" target="#b32">(Janowsky, 1989;</ref><ref type="bibr" target="#b35">LeCun et al., 1990;</ref><ref type="bibr" target="#b29">Hassibi &amp; Stork, 1993;</ref><ref type="bibr" target="#b49">Molchanov et al., 2017;</ref><ref type="bibr" target="#b28">Han et al., 2016;</ref><ref type="bibr" target="#b23">Gomez et al., 2019;</ref><ref type="bibr" target="#b6">Dai et al., 2018a)</ref> or a partial converged training time (partial dense over-parameterization) <ref type="bibr" target="#b45">(Louizos et al., 2017;</ref><ref type="bibr" target="#b71">Zhu &amp; Gupta, 2017;</ref><ref type="bibr" target="#b22">Gale et al., 2019;</ref><ref type="bibr" target="#b58">Savarese et al., 2019;</ref><ref type="bibr" target="#b34">Kusupati et al., 2020;</ref><ref type="bibr" target="#b68">You et al., 2019)</ref>. Given the fact that the training costs of state-of-the-art models e.g., <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> and Vision Transformer <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>, have increasingly exploded, this heavily over-parameterized dependency leads to a situation where state-of-the-art models are beyond the reach of the majority of the machine learning community.</p><p>Recently, the lottery ticket hypothesis (LTH) <ref type="bibr" target="#b19">(Frankle &amp; Carbin, 2019)</ref> shows the possibility to train a sub-network from scratch (sparse training) to match the performance of the dense network. However, these "winning tickets" are found under the guidance of a fully dense overparameterized process (iterative pruning a fully converged network), and solutions that are discovered through either a partial dense over-parameterization (pruning at initialization <ref type="bibr" target="#b64">Wang et al., 2020;</ref><ref type="bibr" target="#b9">de Jorge et al., 2020)</ref>) or no over-parameterization (randomly-initialized static sparse training <ref type="bibr" target="#b47">(Mocanu et al., 2016;</ref><ref type="bibr" target="#b16">Evci et al., 2019)</ref>) typically are not able to match the accuracy achieved by their dense counterpart. A common-sense explanation would be that in comparison with dense training, sparse training, especially at extreme high sparsities, does not have the overparameterization property, and hence suffers from a poor expressibility. One approach to address this problem is to leverage the knowledge learned from dense training, e.g., LTH <ref type="bibr" target="#b19">(Frankle &amp; Carbin, 2019)</ref>. While effective, the computational costs and memory requirements attached to the over-parameterized dense training are prohibitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Our Contribution</head><p>In this paper, we propose a concept that we call In-Time Over-Parameterization 1 to close the gap in overparameterization along with expressibility between sparse training and dense training, illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Instead of inheriting weights from a dense and pre-trained model, allowing a continuous parameter exploration across the training time performs an over-parameterization in the 1 https://github.com/Shiweiliuiiiiiii/ In-Time-Over-Parameterization space-time manifold, which can significantly improve the expressibility of sparse training.</p><p>We find the concept of In-Time Over-Parameterization useful (1) in exploring the expressibility of sparse training, especially for extreme sparsities, (2) in reducing training and inference costs (3) in understanding the underlying mechanism of dynamic sparse training (DST) <ref type="bibr" target="#b48">(Mocanu et al., 2018;</ref><ref type="bibr" target="#b17">Evci et al., 2020a)</ref>, (4) in preventing overfitting and improving generalization.</p><p>Based on In-Time Over-Parameterization, we improve the state-of-the-art sparse training performance with ResNet-50 on ImageNet. We further assess the ITOP concept by applying it to the main class of sparse training methods, DST, in comparison with the overparameterization-based sparse methods including LTH, gradual magnitude pruning (GMP), and pruning at initialization (PI). Our results show that, when a sufficient and reliable parameter exploration is reached (as required by ITOP), DST consistently outperforms those overparameterization-based methods. Since ITOP eliminates the dense Over-Parameterization throughout the whole course of training, it can match the performance of the corresponding dense networks with much fewer training FLOPs, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Dense Over-Parameterization</head><p>Sparsity-inducing techniques that depend on dense overparameterization (dense-to-sparse training) have been extensively studied. We divide them into three categories according to their degrees of dependence on the dense overparameterization.</p><p>Full dense over-parameterization. Techniques sought to inherit weights from a fully pre-trained dense model have a long history and were first introduced by <ref type="bibr" target="#b32">Janowsky (1989)</ref> and <ref type="bibr" target="#b53">Mozer &amp; Smolensky (1989)</ref>, autonomously evolving as the iterative pruning and retaining method. The basic idea of iterative pruning and retaining involves a three-step pro-cess: (1) fully pre-training a dense model until converged, (2) pruning the weights or the neurons that have the lowest influence on the performance, and (3) re-training the pruned model to further improve the performance. The pruning and retraining cycle is required at least once <ref type="bibr" target="#b44">(Liu et al., 2019)</ref>, and usually many times <ref type="bibr" target="#b28">(Han et al., 2016;</ref><ref type="bibr" target="#b26">Guo et al., 2016;</ref><ref type="bibr" target="#b19">Frankle &amp; Carbin, 2019)</ref>. The criteria used for pruning includes but are not limited to magnitude <ref type="bibr" target="#b53">(Mozer &amp; Smolensky, 1989;</ref><ref type="bibr" target="#b28">Han et al., 2016;</ref><ref type="bibr" target="#b26">Guo et al., 2016)</ref>, <ref type="bibr">Hessian (LeCun et al., 1990;</ref><ref type="bibr" target="#b29">Hassibi &amp; Stork, 1993)</ref>, mutual information <ref type="bibr" target="#b6">(Dai et al., 2018a)</ref>, Taylor expansion <ref type="bibr" target="#b50">(Molchanov et al., 2016;</ref>. Except for pruning, other techniques including variational dropout <ref type="bibr" target="#b49">(Molchanov et al., 2017)</ref>, targeted dropout , reinforcement learning <ref type="bibr" target="#b39">(Lin et al., 2017)</ref> also yield a sparse model from a pre-trained dense model. Partial dense over-parameterization. Another class of methods start from a dense network and continuously sparsify the model during training. Gradual magnitude pruning (GMP) <ref type="bibr" target="#b54">(Narang et al., 2017;</ref><ref type="bibr" target="#b71">Zhu &amp; Gupta, 2017;</ref><ref type="bibr" target="#b22">Gale et al., 2019)</ref> was proposed to reduce the number of pruning-andretaining rounds by pruning the dense network to the target sparsity gradually over the course of training. There are some examples <ref type="bibr" target="#b45">Louizos et al. (2017)</ref> and <ref type="bibr" target="#b65">Wen et al. (2016)</ref> that utilize L 0 and L 1 regularization to gradually learn the sparsity by explicitly penalizing parameters for being different from zero, respectively. Recently, <ref type="bibr" target="#b62">Srinivas et al. (2017)</ref>; <ref type="bibr" target="#b40">Liu et al. (2020a)</ref>; <ref type="bibr" target="#b58">Savarese et al. (2019)</ref>; <ref type="bibr" target="#b67">Xiao et al. (2019)</ref>; <ref type="bibr" target="#b34">Kusupati et al. (2020)</ref>; <ref type="bibr" target="#b70">Zhou et al. (2021)</ref> moved further by introducing trainable masks to learn the desirable sparse connectivity during training. Since these techniques start from a dense model, the training cost is smaller than training a dense network, depending on the stage at which the final sparse models are learned.</p><p>One-Shot dense over-parameterization. Very recently, works on pruning at initialization (PI) <ref type="bibr" target="#b64">Wang et al., 2020;</ref><ref type="bibr" target="#b63">Tanaka et al., 2020;</ref><ref type="bibr" target="#b10">de Jorge et al., 2021)</ref> have emerged to obtain trainable sparse neural networks before the main training process based on some salience criteria. These methods fall into the category of dense over-parameterization mainly because the dense model is required to train for at least one iteration to obtain those trainable sparse networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">In-Time Over-Parameterization</head><p>Dynamic Sparse Training. Evolving in parallel with LTH, DST is a growing class of methods to train sparse networks from scratch with a fixed parameter count throughout training (sparse-to-sparse training). This paradigm starts from a (random) sparse neural network and allows the sparse connectivity to evolve dynamically during training. It has been first introduced in Mocanu (2017) and became well-established in <ref type="bibr" target="#b48">Mocanu et al. (2018)</ref> by proposing the Sparse Evolutionary Training (SET) algorithm which achieves better performance than static sparse neural networks. In addition to the proper classification performance, it also helps to detect important input features <ref type="bibr" target="#b2">(Atashgahi et al., 2020)</ref>. <ref type="bibr" target="#b3">Bellec et al. (2018)</ref> proposed Deep Rewiring to train sparse neural networks with a strict connectivity constraint by sampling sparse configurations and weights from a posterior distribution. Follow-up works further introduced weight redistribution <ref type="bibr" target="#b52">(Mostafa &amp; Wang, 2019;</ref><ref type="bibr" target="#b11">Dettmers &amp; Zettlemoyer, 2019;</ref><ref type="bibr" target="#b43">Liu et al., 2021)</ref>, gradient-based weight growth <ref type="bibr" target="#b11">(Dettmers &amp; Zettlemoyer, 2019;</ref><ref type="bibr" target="#b17">Evci et al., 2020a)</ref>, and extra weights update in the backward pass <ref type="bibr" target="#b55">(Raihan &amp; Aamodt, 2020;</ref><ref type="bibr" target="#b33">Jayakumar et al., 2020)</ref> to improve the sparse training performance. By relaxing the constraint of the fixed parameter count, <ref type="bibr" target="#b8">Dai et al. (2019;</ref><ref type="bibr" target="#b7">2018b)</ref> proposed a grow-and-prune strategy based on gradient-based growth and magnitude-based pruning to yield an accurate, yet very compact sparse network. More recently, <ref type="bibr" target="#b41">Liu et al. (2020b)</ref> illustrated for the first time the true potential of using dynamic sparse training. By developing an independent framework, they can train truly sparse neural networks without masks with over one million neurons on a typical laptop.</p><p>Understanding Dynamic Sparse Training. Concurrently, some works attempt to understand Dynamic Sparse Training. <ref type="bibr" target="#b42">Liu et al. (2020c)</ref> found that DST gradually optimizes the initial sparse topology towards a completely different one. Although there exist many low-loss sparse solutions that can achieve similar loss, they are very different in the topological space. <ref type="bibr" target="#b18">Evci et al. (2020b)</ref> found that sparse neural networks that are initialized by a dense initialization e.g., <ref type="bibr" target="#b30">He et al. (2015)</ref>, suffer from a poor gradient flow, whereas DST can improve the gradient flow during training significantly. Although promising, the capability of sparse training has not been fully explored and the mechanism underlying DST is not clear yet. Questions like: Why Dynamic Sparse Training can improve the performance of sparse training? How Dynamic Sparse Training can enable sparse neural network models to match -and even to outperform -their dense counterparts? are required to be answered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">In-Time Over-Parameterization</head><p>In this section, we describe in detail In-Time Over-Parameterization, a concept that we proposed to be an alternative way to train deep neural networks without the expensive over-parameterization. We refer In-Time Over-Parameterization as a variant of dense over-parameterization, which can be achieved by encouraging a continuous parameter exploration across the training time. Note that different from the over-parameterization of dense models which refers to the spatial dimensionality of the parameter space, In-Time Over-Parameterization refers to the overall dimen-sionality explored in the space-time manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">In-Time Over-Parameterization Hypothesis</head><p>Based on In-Time Over-Parameterization, we propose the following hypothesis to understand Dynamic Sparse Training:</p><p>Hypothesis. The benefits of Dynamic Sparse Training come from its ability to consider across time all possible parameters when searching for the optimal sparse neural network connectivity. Concretely, this hypothesis can be divided into three main pillars which can explain the performance of DST:</p><p>1. Dynamic Sparse Training can significantly improve the performance of sparse training mainly due to the parameter exploration across the training time.</p><p>2. The performance of Dynamic Sparse Training is highly related to the total number of the reliably explored parameters throughout training. The reliably explored parameters refer to those newly-explored (newly-grown) weights that have been updated for long enough to exceed the pruning threshold.</p><p>3. As long as there are sufficient parameters that have been reliably explored, sparse neural network models trained by Dynamic Sparse Training can match or even outperform their dense counterparts by a large margin, even at extremely high sparsity levels.</p><p>We name our hypothesis as In-Time Over-Parameterization hypothesis for convenience.</p><p>Formally, given a dataset containing N samples</p><formula xml:id="formula_0">D = {(x i , y i )} N i=1</formula><p>and a dense network f (x; ?) parameterized by ?. We train the dense network to minimize the loss function N i=1 L(f (x i ; ?), y i ). When optimizing with a certain optimizer, f (x; ?) reaches a minimum validation loss function l with a test accuracy a. Differently, sparse training starts with a sparse neural network f (x; ? s ) parameterized by a fraction of parameters ? s . The basic mechanism of Dynamic Sparse Training is to train the sparse neural net-</p><formula xml:id="formula_1">work f (x; ? s ) to minimize the loss N i=1 L(f (x i ; ? s ), y i )</formula><p>while periodically update the sparse connectivity ? s every ?T iterations based on some criteria. f (x; ? u s ) reaches a minimum validation loss l at sparse connectivity update u with a test accuracy a , where ? u s is the sparse connectivity parameters obtained at the iteration u. Let us denote R s as the ratio of the total number of reliably explored parameters during training to the total number of parameters, or simply In-Time Over-Parameterization rate, computed as</p><formula xml:id="formula_2">R s = ? 1 s ?? 2 s ?...?? u s 0 ? 0 , where ? 0 is the 0 -norm.</formula><p>Our hypothesis states that when ?T ? T 0 , ?R 0 as long as R s ? R 0 , for which a ? a (commensurate accuracy) and ? u s 0 ? 0 (fewer parameters in the final sparse model), where T 0 is the minimum threshold of update interval to guarantee the reliable parameter exploration, and R 0 is the threshold of In-Time Over-Parameterization rate where DST can match the performance of the dense model. Similar to dense training, there are many factors affecting the performance of dynamic sparse training, learning rate, batch size, regularization, optimizers, sparsity distribution, etc. In this paper, we limit our study to parameter exploration, since it is the fundamental difference between dynamic sparse training and static sparse training. By "reliable", we mainly focus on the newly-activated weights that are updated for a long time (guaranteed by ?T ? T 0 ) so that they are not pruned in the next update iteration. We believe this is a good starting point, since even in this simple setting, our community does not have a satisfactory answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hypothesis Evaluation</head><p>In this section, we work through the In-Time Over-Parameterization hypothesis and study the effect of In-Time Over-Parameterization on the performance of DST. We choose Sparse Evolutionary Training (SET) as our DST method as SET activates new weights in a random fashion which naturally considers all possible parameters to explore. It also helps to avoid the dense over-parameterization bias introduced by the gradient-based methods e.g., The Rigged Lottery (RigL) <ref type="bibr" target="#b17">(Evci et al., 2020a)</ref> and Sparse Networks from Scratch (SNFS) <ref type="bibr" target="#b11">(Dettmers &amp; Zettlemoyer, 2019)</ref>, as the latter utilize dense gradients in the backward pass to explore new weights. To work through the proposed hypothesis, we conduct a set of step-wise fashion experiments with image classification. We study Multi-layer Perceptron (MLP) on CIFAR-10, VGG-16 on CIFAR-10, ResNet-34 on CIFAR-100, and ResNet-50 on ImageNet. We use PyTorch as our library. All results are averaged from three different runs and reported with the mean and standard deviation. See Appendix A for the experimental details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">TYPICAL TRAINING TIME</head><p>Our first evaluation of the In-Time Over-Parameterization hypothesis is to see what happens when different overparameterization rates R s are reached during training within a typical training time (200 or 250 epochs). A direct way to control R s is to vary ?T , a hyperparameter that determines the update interval of sparse connectivities (the number of iterations between two sparse connectivity updates). We train MLP, VGG-16, and ResNet-34 with various ?T and report the test accuracy.</p><p>Expected results. Gradually decreasing ?T will explore more parameters, and thus lead to increasingly higher test accuracy. However, when ?T gets smaller than the reliable exploration threshold T 0 , the test accuracy will start  to decrease since the new weights can not receive enough updates to exceed the pruning threshold.</p><p>Experimental results. For a better overview, we plot the performance achieved at different sparsities together in the leftmost column of <ref type="figure" target="#fig_2">Figure 3</ref>. To understand better the relationship between R s and test accuracy, we report the final R s associated with ?T separately in the rest columns of <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Overall, a similar pattern can be found existing in all lines. Starting from the static sparse training, sparse training consistently benefits from the increased R s as ?T decreases. However, the test accuracy starts to drop rapidly after it reaches a peak value, especially at high sparsities (yellow and blue lines). For example, even if MLPs and ResNet-34 eventually reach a 100% exploration rate with extremely small ?T values (e.g., 10, 30), their performance is much worse than the static sparse training. This behavior is perfectly in line with our hypothesis. While small ?T encourages sparse models to maximally explore the search space spanned over the dense model, the benefits provided by In-Time Over-Parameterization is heavily limited by the unreliable parameter exploration. Interestingly, the negative effect of the unreliable exploration on lower sparsities (green lines) is less than the one on high sparsities (yellow lines). We regard this as trivial sparsities <ref type="bibr" target="#b20">(Frankle et al., 2020a)</ref> as the remaining models are still over-parameterized to fit the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">EXTENDED TRAINING TIME</head><p>Until now, we have already learned the trade-off between test accuracy and R s for the typical training time. A direct approach to alleviating this trade-off is to extend the training time while using large ?T . We train MLP, VGG-16, and ResNet-34 for an extended training time with a large ?T . We safely choose ?T as 1500 for MLPs, 2000 for VGG-16, and 1000 for ResNet-34 according to the trade-off shown in <ref type="figure" target="#fig_2">Figure 3</ref>. In addition to the training time, the anchor points of the learning rate schedule are also scaled by the same factor.</p><p>Expected results. In this setting, we expect that, in addition to the benefits brought by the extended training time, sparse training would benefit significantly from the increased R s .</p><p>Experimental results. The results are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Static sparse training without parameter exploration consistently achieves the lowest accuracy. However, all models at different sparsities substantially benefit from an extended training time accompanied by an increased R s . In other words, reliably exploring the parameter space in time continuously improves the expressibility of sparse training. Importantly, after matching the performance of the dense baseline (black line), the performance of sparse training continues to improve, yielding a notable improvement over the dense baseline. Furthermore, the models with lower sparsities require less time to match their full accuracy plateau than  higher sparsities; the cause appears to be that models with lower sparsity can explore more parameters in the same training time.</p><p>To show that the performance gains are not only caused by the longer training time, we make a controlled experiment by stopping the parameter exploration immediately after the typical training time (the sparse connectivity remains fixed after a typical training time), shown as the orange dashed lines. As we can see, even though improved, the accuracy is much lower than the accuracy achieved by In-Time Over-Parameterization.</p><p>We also report the performance of dense models with an extended training time as the dashed black lines. Training a dense model with an extended time leads to either inferior (MLPs and VGG-16), or equal solutions (ResNet-34). Different from the dense over-parameterization where overfitting usually occurs when the model has been overtrained for long enough, the test accuracy of dynamic sparse training is continuously increasing as R s increases until a plateau is reached with a full In-Time Over-Parameterization. This observation highlights the advantage of In-Time Over-Parameterization to prevent overfitting over the dense overparameterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Effect of Hyperparameter Choices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effect of Weight Growth methods on ITOP</head><p>We next investigate the effect of gradient-based weight growth (used in RigL and SNFS) and random-based weight growth (used in SET) on In-Time Over-Parameterization. Since gradient-based methods have access to the dense overparameterization in the backward pass (occasionally using dense gradients to activate new weights), we hypothesize that they can reach a converged accuracy without a high R s . We make a comparison between RigL and SET for both the typical training and the extended training in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>We study them on MLPs where the model size is relatively small so that we can easily achieve a full In-Time Over-Parameterization and have a better understanding of these two methods.</p><p>Typical Training Time. It is clear that RigL also heavily suffers from the unreliable exploration. As ?T decreases, the test accuracy of RigL presents a trend of rising, falling, and rising again. Compared with the random-based growth, RigL receives larger gains from the reliable parameter exploration and also a larger forfeit from the unreliable exploration. These differences are potentially due to that RigL grows new weights with high gradient magnitude, which leads to a faster loss decrease when the exploration is faithful, but also requires a higher ?T to guarantee a faithful  exploration as the weight with large gradients is likely to end up with high magnitude, resulting in a large pruning threshold.</p><p>Extended Training Time. For RigL, we choose ?T = 4000 to ensure the reliable exploration (the performance of RigL with a smaller ?T = 1500 is much worse as shown in Appendix D). We can see that RigL also significantly benefits from an increased R s . Surprisingly, although RigL achieves higher accuracy than the SET with a limited training time, it ends up with lower accuracy than SET with a sufficient training time. From the perspective of R s , we can see that the R s of RigL is much smaller than SET, indicating that gradient weight growth drives the sparse connectivity into some similar structures and in turn limits its expressibility. On the contrary, random growth naturally considers the whole search space to explore parameters and has a larger possibility of finding better local optima. Similar results are also reported for sparse Recurrent Neural Networks (RNNs) in <ref type="bibr" target="#b43">Liu et al. (2021)</ref>. However, similar results are not shared with large-scale architectures on large datasets. For instance, RigL achieves better performance than SET with ResNet-50 on ImageNet. This result is reasonable since the dense gradients help RigL easily find the most promising weights at each sparse connectivity update. In contrast, it would take a much longer time (high R s ) for SET (random weight growth) to discover these promising weights within large-scale architectures, especially at high sparsities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effect of Batch Size on ITOP</head><p>Intuitively, our hypothesis uncovers ways to improve the existing DST methods within a limited training time. A direct way to reliably explore more parameters within a typical training time is to train with a small batch size. Using a smaller batch size equally means having more updates, and therefore leads to a higher R s . We simply demonstrate the effectiveness of this conjecture on SET with ?T = 1500 in <ref type="figure">Figure 6</ref> (see Appendix E for RigL). With a large batch size, the parameter exploration is insufficient to achieve a high In-Time Over-Parameterization rate R s , and the test accuracy is subsequently much lower than the dense model. As we expected, the reduction in batch size consistently increases R s as well as the test accuracy, until the batch size gets smaller than 16. However, the performance of the dense model remarkably decreases as the batch size decreases. More interestingly, when the batch size is smaller than 16, the performance of sparse models flips and the sparsest model starts to achieve the highest accuracy. The performance drop is likely caused by the increased "noise scale" of SGD where extremely small batch sizes lead to large noise scale and large accuracy drop <ref type="bibr" target="#b60">(Smith et al. (2017)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effect of Pruning Rate on ITOP</head><p>The initial pruning rate (denoted as P ) of parameter exploration also affects the overall number of parameters visited during training. Relatively large pruning rates encourage a large range of exploration, resulting in higher accuracy, whereas a too-large pruning rate hurts the model capacity as it prunes too many parameters. We confirm this with ResNet-18 on CIFAR-10 trained with various initial pruning rates P ? [0.1, 0.3, 0.5, 0.7, 0.9] as shown in  <ref type="figure">Figure 6</ref>. Test accuracy of SET with various batch sizes. The update interval ?T is set as 1500. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Boosting the Performance of DST</head><p>Based on the above-mentioned insights, we demonstrate the state-of-the-art sparse training performance with ResNet-50 on ImageNet. More precisely, we choose an update interval ?T of 4000, a batch size of 64, and an initial pruning rate of 0.5 so that we can achieve a high R s within a typical training time. We briefly name the improved method as RigL-ITOP. Please see Appendix B for the implementation details. <ref type="table" target="#tab_6">Table  2</ref> shows that without any advanced techniques, our method boosts the accuracy of RigL over the overparameterizationbased method (GMP and Lottery Ticket Rewinding (LTR) <ref type="bibr" target="#b20">(Frankle et al., 2020a)</ref>). More importantly, our method requires only 2? training time to match the performance of dense ResNet-50 at 80% sparsity, far less than RigL (5? training time) <ref type="bibr" target="#b17">(Evci et al., 2020a)</ref>.</p><p>Instead of using small batch size, another trick to encourage parameter exploration is sampling from the non-activated weights first when growing new weights. We demonstrate the effectiveness of this idea in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The Versatility of ITOP</head><p>Although we mainly focus on understanding DST from the ITOP point of view, ITOP can be potentially generalized to other sparsity-inducing categories. Here, we demonstrate its versatility by applying ITOP to two recently popular methods, LTH and PI. We choose SNIP  as the PI method, as it consistently performs well among different methods for pruning at initialization as shown by <ref type="bibr" target="#b21">Frankle et al. (2020b)</ref>. Compared with ITOP, LTH and SNIP are two overparameterization-based methods designed for a better initial subnetwork but without consulting any information yielded during training. We choose magnitudebased weight pruning and random-based weight growth for SNIP and LTH to achieve ITOP and name the corresponding methods as SNIP-SET-ITOP and LTH-SET-ITOP. To make a fair comparison between different pruning criteria, we use global and one-shot pruning for both SNIP and LTH. We train all models for 200 epochs and report the best test accuracy in <ref type="figure" target="#fig_5">Figure 7</ref>. See Appendix C for the experimental details. With a high In-Time Over-Parameterization rate, SET-ITOP consistently outperforms the overparameterization-based methods as well as the dense training, by a large margin. For instance, SET-ITOP can easily match the performance of the corresponding dense models with at most 5% parameters. More importantly, SET-ITOP has dominant performance at the extreme sparsity (98%) over LTH and SNIP, indicating the potential of In-Time Over-Parameterization to address the poor expressibility problem of the extremely sparse neural networks.  We further observe the ability of In-Time Over-Parameterization to improve generalization. <ref type="figure" target="#fig_6">Figure 8</ref> shows that the generalization error (the difference between the training accuracy and the test accuracy) of In-Time Over-Parameterization (SET-ITOP and RigL-ITOP) and the dense over-parameterization with MLPs on CIFAR-10. It is clear to see that models with the In-Time Over-Parameterization property generalize much better than the corresponding dense models. The generalization error gradually increases as the model gets denser. Together with the results in <ref type="figure" target="#fig_5">Figure  7</ref>, we can see that the reductions in sparsity lead to better classification performance but worse generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>In this paper, we propose In-Time Over-Parameterization, a variant of dense over-parameterization in the space-time manifold, to be an alternative way to train deep neural networks without the prohibitive dense over-parameterized dependency. We demonstrate the ability of In-Time Over-Parameterization (1) to improve the expressibility of sparse training, (2) to accelerate both training and inference, (3) to understand the underlying mechanism of DST, (4) to prevent overfitting and improve generalization. In addition, we empirically found that, with a sufficient and reliable parameter exploration, randomly-initialized sparse models consistently achieve better performance over those speciallyinitialized static sparse models. Our paper suggests that it is more effective and efficient to allocate the limited resources to explore more the sparse connectivity space, rather than allocating all resources to find a good sparse initialization.</p><p>Our paper discovers the importance of parameter exploration for sparse training. Even though we adjust the hyperparameters of RigL and reach the state of art sparse training performance, the usage of small batch size slows down the training speed of modern architectures. It is interesting to pursue a high In-Time Over-Parameterization rate with large batch size under a typical training time. Moreover, we believe that ITOP has potentials to help people to interpret the networks' decisions <ref type="bibr" target="#b66">(Wong et al., 2021)</ref>, to improve the robustness out of distribution and uncertainty performance , to detect non-spurious correlation <ref type="bibr" target="#b57">(Sagawa et al., 2020)</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings of Hypothesis Evaluation</head><p>In this Appendix, we describe the experimental settings of the hypothesis evaluation in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Models</head><p>We use MLP on CIFAR-10, VGG-16 on CIFAR-10, ResNet-34 on CIFAR-100 to work through our hypothesis. We describe these models in detail as follows:</p><p>MLP. MLP is a clean three-layer MLP with ReLU activation for CIFAR-10. The number of neurons of each layer is 1024, 512, 10, respectively. No other regularization such as dropout or batch normalization is used further.</p><p>VGG-16. VGG-16 is a modified CIFAR-10 version of the original VGG model introduced by <ref type="bibr" target="#b36">Lee et al. (2019)</ref>. The size of the fully-connected layer is reduced to 512 and the dropout layers are replaced with batch normalization to avoid any other sparsification.</p><p>ResNet-34. ResNet-34 is the CIFAR-100 version of ResNet with 34 layers introduced by <ref type="bibr" target="#b31">He et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Algorithm</head><p>We choose Sparse Evolutionary Training (SET) <ref type="bibr" target="#b48">(Mocanu et al., 2018)</ref> as the DST method to evaluate our hypothesis. SET helps to avoid the dense over-parameterization bias introduced by the gradient-based methods e.g., RigL and SNFS, as the latter utilize dense gradients in the backward pass to explore new weights. SET starts from a random sparse topology (Erd?s-R?nyi), and optimize the sparse connectivity towards a scale-free topology during training.</p><p>This algorithm contains three key steps:</p><p>1. Initializing a sparse neural network with Erd?s-R?nyi random graph at a sparsity of S.</p><p>2. Training the sparse neural network for ?T iterations.</p><p>3. Removing weights according to the standard magnitude pruning and growing new weights in a random fashion.</p><p>Steps 2 and 3 will be repeated iteratively until the end of the training. By doing this, SET maintains a fixed parameter count throughout training. We basically follow the experimental settings from <ref type="bibr" target="#b11">Dettmers &amp; Zettlemoyer (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Training</head><p>For models trained for a typical time, we train them with various update interval ?T reported in <ref type="figure" target="#fig_2">Figure 3</ref>. We use a set of 10% training data as the validation set and train on the remaining training data. Weight growth is guided by random sampling and weight pruning is guided by magnitude. We do not specifically finetune the starting point and the finishing point of the parameter exploration. The exploring operation is performed throughout training. The initial sparse connectivity is sampled by the Erd?s-R?nyi distribution introduced in <ref type="bibr" target="#b48">Mocanu et al. (2018)</ref>. We set the initial pruning rate as 0.5 and gradually decay it to 0 with a cosine annealing, as introduced in <ref type="bibr" target="#b11">Dettmers &amp; Zettlemoyer (2019)</ref>. The remaining training hyperparameters are set as follows:</p><p>MLP. We train sparse MLPs for 200 epochs by momentum SGD with a learning rate of 0.01 and a momentum coefficient of 0.9. We use a small learning rate 0.01 rather than 0.1, as the dense MLP doesn't converge with a learning rate of 0.1. We decay the learning rate by a factor of 10 every 24000 iterations. We set the batch size as 128. The weight decay is set as 5.0e-4.</p><p>VGG-16. We strictly follow the experimental settings from <ref type="bibr" target="#b11">Dettmers &amp; Zettlemoyer (2019)</ref> for VGG-16. All sparse models are trained with momentum SGD for 250 epochs with a learning rate of 0.1, decayed by 10 every 30000 mini-batches. We use a batch size of 128 and weight decay to 5.0e-4.</p><p>ResNet-34. We train sparse ResNet-34 for 200 epochs with momentum SGD with a learning rate of 0.1, decayed by 10 at the 100 and 150 epoch. We use a batch size of 128 and weight decay to 1.0e-4.</p><p>For models trained for an extended training time, we simply extend the training time and the anchor epochs of the learning rate schedule, while using a large ?T . The update interval ?T is chosen according to the trade-off shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Besides the learning steps, the anchor epochs of the learning rate schedule and the pruning rate schedule are also scaled by the same factor. For each training time, the accuracy are averaged over 3 seeds with mean and standard deviation. More detailed training hyperparameters are shared in <ref type="table" target="#tab_7">Table 3</ref>.</p><p>B. Implementation Details of RigL-ITOP in Section 4.2 In this Appendix, we describe our replication of RigL <ref type="bibr" target="#b17">(Evci et al., 2020a)</ref> and the hyperparameters we used for RigL-ITOP.</p><p>RigL is a state-of-the-art DST method growing new weights that are expected to receive gradient with high magnitude in the next iteration. Besides, it shows the proposed sparse distribution Erd?s-R?nyi-Kernel (ERK) improves the sparse performance over the Erd?s-R?nyi (ER). Since RigL is originally implemented with TensorFlow, we replicate it with PyTorch based on the implementation from <ref type="bibr" target="#b11">Dettmers &amp; Zettlemoyer (2019)</ref>. We note that RigL tunes the starting epoch and the ending point of the mask update. To encourage more exploration, we do not follow this strategy and explore sparse connectivities throughout training. We train sparse ResNet-50 for 100 epochs, the same as Dettmers &amp; Zettlemoyer (2019); <ref type="bibr" target="#b17">Evci et al. (2020a)</ref>. The learning rate is linearly increased to 0.1 with a warm-up in the first 5 epochs and decreased by a factor of 10 at epochs 30, 60, and 90. To reach a high and reliable In-Time Over-Parameterization rate, we use a small batch size of 64 and an update interval of 4000. Batch sizes lower than 64 lead to worse test accuracy. ImageNet experiments were run on 2x NVIDIA Tesla V100. With more fine-tuning, the results of RigL-ITOP (e.g. extended training time) can likely be improved, but we lack the resources to do it. We share the hyperparameters of RigL-ITOP in <ref type="table" target="#tab_8">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details in Section 5</head><p>In this Appendix, we describe the hyperparameters of SET-ITOP used in 5 in <ref type="table" target="#tab_8">Table 4</ref>. The replication details of LTH and SNIP are given below.</p><p>LTH. Lottery Ticket Hypothesis (LTH) <ref type="bibr" target="#b19">(Frankle &amp; Carbin, 2019)</ref> shows that there exist sub-networks that can match the accuracy of the dense network when trained with their original initializations. We follow the PyTorch implementation provide by <ref type="bibr" target="#b44">Liu et al. (2019)</ref> on GitHub 1 to replicate LTH.</p><p>Give the fact that the iterative pruning process of LTH would lead to much larger training resource costs than SNIP and static sparse training, we use one-shot pruning for LTH. For the typical training time setting, we first train a dense model for 200 epochs, after which we use global and one-shot magnitude pruning to prune the model to the target sparsity and retrain the pruned model with its original initializations for 200 epochs.</p><p>SNIP. Single-shot network pruning (SNIP) proposed in <ref type="bibr" target="#b36">Lee et al. (2019)</ref>, is a method that attempts to prune at initialization before the main training based on the connection sensitivity score s i = | ?L ?wi w i |. The weights with the smallest score are pruned. We replicate SNIP based on the PyTorch implementation on GitHub 1 . Same as <ref type="bibr" target="#b36">Lee et al. (2019)</ref>, we use a mini-batch of data to calculate the important scores and obtain the sparse model in a one-shot fashion before the initialization. After that, we train the sparse model without any sparse exploration for 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extended Training Performance of RigL with ?T = 1500</head><p>According to the results from <ref type="figure" target="#fig_4">Figure 5</ref>, we can see the ?T = 4000 is a good choice for the update interval of RigL. What if we choose a small update interval, e.g., ?T = 1500? Here we compare the extended training performance of RigL with two different update intervals 1500 and 4000. The results are shown in <ref type="figure" target="#fig_7">Figure 9</ref>. It is clear to see models trained with ?T = 1500 fall short of models trained with ?T = 4000, which indicates small update intervals is not sufficient for newly weights to catch up the existing weights in terms of magnitude. More importantly, although expected to perform sparse exploration more frequently, models trained with ?T = 1500 end up with a lower R s than the ones trained with ?T = 4000. These results highlight the importance of the sufficient training time for the new weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Test Accuracy of RigL with Various Batch Sizes</head><p>In this Appendix, we evaluate the performance of RigL with different batch sizes. We choose MLP as our model and the update interval ?T = 4000. The results are shown in <ref type="figure" target="#fig_0">Figure 10</ref>. Similar with SET, the performance of RigL also increases as the batch size decrease from 256 to 32. After that, the performance starts to drop due to the noisy input caused by the extreme small batch sizes. The In-Time Over-Parameterization rate (R s ) of RigL is again bounded up to some values. We also provide the comparison between RigL (solid lines) and SET (dashed lines) in this setting. We find a similar pattern with the extended training time, that is, RigL outperforms SET when R s is small but falls short of SET when sufficient parameters have been reliably explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Regrowing from the Non-Activated Weights First</head><p>One direct way to increase the In-Time Over-Parameterization rate during a typical training time is to sample from the non-activated weights first when performing weight growing. We evaluate this idea with SET by regrowing the non-activated weights first and report the results as SET+ with (mean ? std, R s ) in <ref type="table" target="#tab_9">Table 5</ref>. We training sparse ResNet-18 on CIFAR-10 for 250 epochs with a learning rate of 0.1 decayed by 10x at 125, 187 epochs, a batch size of 128, a pruning rate of 0.5.</p><p>When the parameter exploration is insufficient (small R s ), SET+ consistently achieves higher accuracy and higher R s than SET. Whiling effective, the R s increase achieved by this modification is relatively limited. This observation highlights an important direction for future work to achieve high R s within a typical training time. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>As the figure proceeds, we perform an Over-Parameterization in time. Blue lines refer to the currently activated connections. Pink lines are the connections that have been activated previously. While exploring In-Time Over-Parameterization, the parameter count (blue lines) of the sparse model is fixed throughout training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Training FLOPs of sparse models trained with In-Time Over-Parameterization. ?T = 1500 and batch size is 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Effect of In-Time Over-Parameterization on sparse training MLPs (top), VGG-16 (middle), and ResNet-34 (bottom) with a typical training time. All sparse models are trained with SET. Each line is averaged from three different runs. "Static" refers to the static sparse training without parameter exploration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Effect of In-Time Over-Parameterization on sparse training MLPs (top), VGG-16 (middle) and ResNet-34 (bottom) with an extended training time. All sparse models are trained with SET. Each line is averaged from three different runs. "Static" refers to the static sparse training without parameter exploration. "Dense extended" refers to training a dense model for an extended time. "Sparsity 0.95 w/o exploration" means we train the model for the same extended time but stop exploring parameters after a typical training time (200 or 250 epochs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Comparisons between RigL and SET with MLP on CIFAR-10. We vary the update interval ?T for the typical training time setting, and keep it fixed for the extended training time setting (1500 for SET and 4000 for RigL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Effect of In-Time Over-Parameterization on SNIP and LTH. "Typical" means training a model for a typical time of 200 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Generalization errors of SET-ITOP, RigL-ITOP, and the dense models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Extended training performance of RigL with update interval ?T = 1500 and ?T = 4000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Test accuracy of RigL with various batch sizes. The update interval ?T is set as 4000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Department of Mathematics and Computer Science, Eindhoven University of Technology, 5600 MB Eindhoven, the Netherlands 2 Faculty of Electrical Engineering, Mathematics andComputer Science, University of Twente, Enschede 7522NB,The Netherlands . Correspondence to: Shiwei Liu &lt;s.liu3@tue.nl&gt;. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>The similar pattern as we expected is shared across all update intervals.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">In-Time Over-Parameterization Rate</cell><cell></cell><cell></cell><cell cols="5">In-Time Over-Parameterization Rate</cell><cell></cell><cell></cell><cell cols="5">In-Time Over-Parameterization Rate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.27</cell><cell>0.44</cell><cell>0.66</cell><cell>0.87</cell><cell>0.98</cell><cell>1.0</cell><cell>1.0</cell><cell>0.47</cell><cell>0.69</cell><cell>0.89</cell><cell>0.98</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>0.73</cell><cell>0.91</cell><cell>0.99</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy [%]</cell><cell>62 64 66 68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Accuracy [%]</cell><cell>62 64 66 68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Accuracy [%]</cell><cell>62 64 66 68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Accuracy [%]</cell><cell>62 64 66 68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>32</cell><cell>16</cell><cell>8</cell><cell>4</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>32</cell><cell>16</cell><cell>8</cell><cell>4</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>32</cell><cell>16</cell><cell>8</cell><cell>4</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>32</cell><cell>16</cell><cell>8</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Batch Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Batch Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Batch Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Batch Size</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sparsity 0.95</cell><cell></cell><cell cols="2">Sparsity 0.90</cell><cell cols="2">Sparsity 0.80</cell><cell></cell><cell>Dense</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>Performance of sparse ResNet-18 on CIFAR-10 with various pruning rates. The results are run three times and reported with (mean ? std, Rs). The highest test accuracies are marked in bold.</figDesc><table><row><cell>Sparsity</cell><cell>Method</cell><cell>P</cell><cell>?T = 15000</cell><cell>?T = 10000</cell><cell>?T = 8000</cell><cell>?T = 5000</cell><cell>?T = 3000</cell></row><row><cell></cell><cell>SET</cell><cell>0.9</cell><cell>(93.53 ? 0.02, 0.113)</cell><cell>(93.57 ? 0.05, 0.150)</cell><cell>(93.44 ? 0.12, 0.176)</cell><cell>(93.70 ? 0.19, 0.247)</cell><cell>(93.78 ? 0.09, 0.353)</cell></row><row><cell></cell><cell>SET</cell><cell>0.7</cell><cell>(93.54 ? 0.09, 0.100)</cell><cell>(93.54 ? 0.18, 0.130)</cell><cell>(93.76 ? 0.07, 0.151)</cell><cell>(93.91 ? 0.17, 0.210)</cell><cell>(93.63 ? 0.01, 0.300)</cell></row><row><cell>0.95</cell><cell>SET</cell><cell>0.5</cell><cell>(93.51 ? 0.01, 0.086)</cell><cell>(93.77 ? 0.21, 0.109)</cell><cell>(93.84 ? 0.10, 0.125)</cell><cell>(93.93 ? 0.09, 0.170)</cell><cell>(93.94 ? 0.08, 0.241)</cell></row><row><cell></cell><cell>SET</cell><cell>0.3</cell><cell>(93.28 ? 0.01, 0.071)</cell><cell>(93.66 ? 0.05, 0.086)</cell><cell>(93.80 ? 0.01, 0.096)</cell><cell>(93.75 ? 0.18, 0.126)</cell><cell>(93.86 ? 0.10, 0.174)</cell></row><row><cell></cell><cell>SET</cell><cell>0.1</cell><cell>(93.24 ? 0.02, 0.056)</cell><cell>(93.35 ? 0.18, 0.061)</cell><cell>(93.29 ? 0.06, 0.065)</cell><cell>(93.50 ? 0.03, 0.076)</cell><cell>(93.34 ? 0.03, 0.096)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Performance of sparse ResNet-50 on ImageNet dataset with a typical training time. All results of other methods are obtained from<ref type="bibr" target="#b17">Evci et al. (2020a)</ref> except LTR which is the late-rewinding LTH version obtained from<ref type="bibr" target="#b18">Evci et al. (2020b)</ref>. RigL-ITOP2? is obtained by extending the training time by 2 times.</figDesc><table><row><cell cols="2">Methods</cell><cell cols="2">Top-1 Acc</cell><cell>R s</cell><cell>Training</cell><cell>Test</cell><cell>Top-1 Acc</cell><cell>R s</cell><cell>Training</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FLOPs</cell><cell>FLOPs</cell><cell></cell><cell></cell><cell>FLOPs</cell><cell>FLOPs</cell></row><row><cell>Dense</cell><cell></cell><cell cols="2">76.8 ? 0.09</cell><cell>1.00</cell><cell>1? (3.2e18)</cell><cell>1? (8.2e9)</cell><cell>76.8 ? 0.09</cell><cell>1.00</cell><cell>1? (3.2e18)</cell><cell>1? (8.2e9)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">sparsity=0.9</cell><cell></cell><cell></cell><cell cols="2">sparsity=0.8</cell></row><row><cell>Static</cell><cell></cell><cell cols="2">67.7 ? 0.12</cell><cell>0.10</cell><cell>0.24?</cell><cell>0.24?</cell><cell>72.1 ? 0.04</cell><cell>0.20</cell><cell>0.42?</cell><cell>0.42?</cell></row><row><cell>SET</cell><cell></cell><cell cols="2">69.6 ? 0.23</cell><cell>-</cell><cell>0.10?</cell><cell>0.10?</cell><cell>72.9 ? 0.39</cell><cell>-</cell><cell>0.23?</cell><cell>0.23?</cell></row><row><cell>SNFS</cell><cell></cell><cell cols="2">72.9 ? 0.06</cell><cell>-</cell><cell>0.50?</cell><cell>0.24?</cell><cell>75.2 ? 0.11</cell><cell>-</cell><cell>0.61?</cell><cell>0.42?</cell></row><row><cell>RigL</cell><cell></cell><cell cols="2">73.0 ? 0.04</cell><cell>-</cell><cell>0.25?</cell><cell>0.24?</cell><cell>75.1 ? 0.05</cell><cell>-</cell><cell>0.42?</cell><cell>0.42?</cell></row><row><cell>GMP</cell><cell></cell><cell>73.9</cell><cell></cell><cell>-</cell><cell>0.56?</cell><cell>0.23?</cell><cell>75.6</cell><cell>-</cell><cell>0.51?</cell><cell>0.10?</cell></row><row><cell>LTR</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.75 ? 0.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">RigL-ITOP</cell><cell cols="2">73.82 ? 0.08</cell><cell>0.83</cell><cell>0.25?</cell><cell>0.24?</cell><cell>75.84 ? 0.05</cell><cell>0.93</cell><cell>0.42?</cell><cell>0.42?</cell></row><row><cell cols="2">RigL-ITOP 2?</cell><cell cols="2">75.50 ? 0.09</cell><cell>0.89</cell><cell>0.50?</cell><cell>0.24?</cell><cell>76.91 ? 0.07</cell><cell>0.97</cell><cell>0.84?</cell><cell>0.42?</cell></row><row><cell cols="7">It is maybe more interesting that In-Time Over-</cell><cell></cell><cell></cell></row><row><cell cols="7">Parameterization brings large benefits to SNIP and LTH</cell><cell></cell><cell></cell></row><row><cell cols="7">as well. While LTH and SNIP fall short of SET-ITOP, SNIP-</cell><cell></cell><cell></cell></row><row><cell cols="7">SET-ITOP and LTH-SET-ITOP can match or even exceed</cell><cell></cell><cell></cell></row><row><cell cols="7">the performance of SET-ITOP with both MLP and ResNet-</cell><cell></cell><cell></cell></row><row><cell cols="7">34. This observation confirms that ITOP is a foundational</cell><cell></cell><cell></cell></row><row><cell cols="7">concept and can potentially improve any existing sparse</cell><cell></cell><cell></cell></row><row><cell cols="2">training methods.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">6. Generalization Improvement of ITOP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.98</cell><cell>0.95</cell><cell>0.9</cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Sparsity [%]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Experiment hyperparameters of the hypothesis evaluation in Section 3.2.</figDesc><table><row><cell>The hyperparameters include Learning Rate (LR), Batch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Experiment hyperparameters in Section 4.2 and Section 5. The hyperparameters include Learning Rate (LR),</figDesc><table><row><cell>Batch Size (typical</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Performance of sparse ResNet-18 on CIFAR-10 with various pruning rates. The results are run three times and reported with (mean ? std, Rs). The highest test accuracies are marked in bold.</figDesc><table><row><cell>Sparsity</cell><cell>Method</cell><cell>P</cell><cell>?T = 15000</cell><cell>?T = 10000</cell><cell>?T = 8000</cell><cell>?T = 5000</cell><cell>?T = 3000</cell></row><row><cell>0.9</cell><cell>SET SET+</cell><cell>0.5 0.5</cell><cell>(94.30 ? 0.16, 0.162) (94.43 ? 0.14, 0.169)</cell><cell>(94.47 ? 0.14, 0.201) (94.59 ? 0.11, 0.215)</cell><cell>(94.25 ? 0.10, 0.228) (94.54 ? 0.28, 0.247)</cell><cell>(94.36 ? 0.08, 0.302) (94.38 ? 0.07, 0.342)</cell><cell>(94.54 ? 0.05, 0.411) (94.53 ? 0.03, 0.492)</cell></row><row><cell>0.95</cell><cell>SET SET+</cell><cell>0.5 0.5</cell><cell>(93.57 ? 0.16, 0.086) (93.66 ? 0.14, 0.088)</cell><cell>(93.46 ? 0.04, 0.108) (93.70 ? 0.03, 0.114)</cell><cell>(93.67 ? 0.04, 0.124) (93.66 ? 0.15, 0.133)</cell><cell>(93.60 ? 0.04, 0.170) (93.78 ? 0.04, 0.186)</cell><cell>(93.61 ? 0.09, 0.241) (94.00 ? 0.08, 0.272)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Eric-mingjie/rethinking-network-pruning 1 https://github.com/mil-ad/snip</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Time Over-Parameterization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Quick and robust feature selection: the strength of energyefficient sparse training for autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Atashgahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Veldhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00560</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep rewiring: Training very sparse deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJ_wN01C-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brutzkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10174</idno>
		<title level="m">Sgd learns over-parameterized networks that provably generalize on linearly separable data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compressing neural networks using the variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Grow and prune compact, fast, and accurate lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11797</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nest: A neural network synthesis tool based on a grow-and-prune paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1487" to="1497" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Progressive skeletonization: Trimming more fat from a network at initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09081</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Progressive skeletonization: Trimming more fat from a network at initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=9GsFOUyUPi" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sparse networks from scratch: Faster training without losing performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04840</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient descent finds global minima of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1675" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The difficulty of training sparse neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10732</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rigging the lottery: Making all tickets winners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03533</idno>
		<title level="m">Gradient flow in sparse neural networks and how lottery tickets win</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJl-b3RcF7" />
	</analytic>
	<monogr>
		<title level="m">In International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Linear mode connectivity and the lottery ticket hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3259" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pruning neural networks at initialization: Why are we missing the mark?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08576</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09574</idno>
		<title level="m">The state of sparsity in deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning sparse networks using targeted dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kamalakara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13678</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Qualitatively characterizing neural network optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saxe</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Time Over-Parameterization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic network surgery for efficient dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1379" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pruning versus clipping in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Janowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review A</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">6600</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Top-kast: Top-k always sparse training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Soft threshold weight reparameterization for learnable sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1VZqjAcYX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A signal propagation perspective for pruning neural networks at initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJeTo2VFwH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning overparameterized neural networks via stochastic gradient descent on structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8157" to="8166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2181" to="2191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic sparse training: Find efficient sparse network from scratch with trainable masked layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>So</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJlbGJrtDB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R R</forename><surname>Matavalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Neural Computing and Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Topological insights into sparse neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Atashgahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferrar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mocanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09048</idno>
		<title level="m">Selfish sparse rnn training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning sparse neural networks through l 0 regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01312</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Network computations in artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
		<respStmt>
			<orgName>Technische Universiteit Eindhoven</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A topological insight into restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="243" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Variational dropout sparsifies deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06440</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Importance estimation for neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11264" to="11272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Using relevance to reduce network size automatically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Exploring sparsity in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Raihan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01969</idno>
		<title level="m">Sparse weight activation training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spurious local minima are common in two-layer relu neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Safran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4433" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An investigation of why overparameterization exacerbates spurious correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8346" to="8356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Winning the lottery with continuous sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maire</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04427</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Don&apos;t decay the learning rate, increase the batch size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00489</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">No bad local minima: Data independent training error guarantees for multilayer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08361</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Training sparse neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05467</idno>
		<title level="m">Pruning neural networks without any data by iteratively conserving synaptic flow</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Picking winning tickets before training by preserving gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkgsACVKPH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Leveraging sparse linear layers for debuggable deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04857</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Automatic network pruning by regularizing auxiliary parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoprune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13681" to="13691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Drawing early-bird tickets: Towards more efficient training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11957</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Can subnetwork structure be the key to out-of-distribution generalization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Learning n: M fine-grained structured sparse neural networks from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04010</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01878</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">An improved analysis of training overparameterized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2055" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Gradient descent optimizes over-parameterized deep relu networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="467" to="492" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
