<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Gebele</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Science Kempten</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonifaz</forename><surname>Stuhr</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Science Kempten</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Autonomous University of Barcelona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Haselberger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Science Kempten</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Technische Universit?t Berlin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised Domain Adaptation demonstrates great potential to mitigate domain shifts by transferring models from labeled source domains to unlabeled target domains. While Unsupervised Domain Adaptation has been applied to a wide variety of complex vision tasks, only few works focus on lane detection for autonomous driving. This can be attributed to the lack of publicly available datasets. To facilitate research in these directions, we propose CARLANE, a 3way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE encompasses the single-target datasets MoLane and TuLane and the multi-target dataset MuLane. These datasets are built from three different domains, which cover diverse scenes and contain a total of 163K unique images, 118K of which are annotated. In addition we evaluate and report systematic baselines, including our own method, which builds upon Prototypical Cross-domain Self-supervised Learning. We find that false positive and false negative rates of the evaluated domain adaptation methods are high compared to those of fully supervised baselines. This affirms the need for benchmarks such as CARLANE to further strengthen research in Unsupervised Domain Adaptation for lane detection. CARLANE, all evaluated models and the corresponding implementations are publicly available at https://carlanebenchmark.github.io.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vision-based deep learning systems for autonomous driving have made significant progress in the past years <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Recent state-of-the-art methods achieve remarkable results on public, real-world benchmarks but require labeled, large-scale datasets. Annotations for these datasets are often hard to acquire, mainly due to the high expenses of labeling in terms of cost, time, and difficulty. Instead, simulation environments for autonomous driving, such as CARLA <ref type="bibr" target="#b5">[6]</ref>, can be utilized to generate abundant labeled images automatically. However, models trained on data from simulation often experience a significant performance drop in a different domain, i.e., the real world, mainly due to the domain shift <ref type="bibr" target="#b6">[7]</ref>. Unsupervised Domain Adaptation (UDA) methods <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> try to mitigate the domain shift by transferring models from a fully-labeled source domain to an unlabeled target domain. This eliminates the need for annotating images but assumes that the target domain is accessible at training time. While UDA has been applied to complex tasks for autonomous driving such as object detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> and semantic segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, only few works focus on lane detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5]</ref>. This can be attributed to the lack of public UDA datasets for lane detection. To compensate for this data scarcity and encourage future research, we introduce CARLANE, a sim-to-real domain adaptation benchmark for lane detection. We use the CARLA simulator for data collection in the source domain with a free-roaming waypoint-based agent and data from two distinct real-world domains as target domains. This enables us to construct a benchmark that consists of three datasets:</p><p>(1) MoLane focuses on abstract lane markings in the domain of a 1/8th Model vehicle. We collect 80K labeled images from simulation as the source domain and 44K unlabeled real-world images from several tracks with two lane markings as the target domain. Further, we apply domain randomization as well as data balancing. For evaluation, we annotate 2,000 validation and 1,000 test images with our labeling tool.</p><p>(2) TuLane incorporates 24K balanced and domain-randomized images from simulation as the source domain and the well-known TuSimple <ref type="bibr" target="#b19">[20]</ref> dataset with 3,268 real-world images from U.S. highways with up to four labeled lanes as the target domain. The target domain of MoLane is a real-world abstraction from the target domain of TuLane, which may result in interesting insights about UDA.</p><p>(3) MuLane is a balanced combination of MoLane and TuLane with two target domains. For the source domain, we randomly sample 24K images from MoLane and combine them with TuLane's synthetic images. For the target domains, we randomly sample 3,268 images from MoLane and combine them with TuSimple. This allows us to investigate multi-target UDA for lane detection. To establish baselines and investigate UDA on our benchmark, we evaluate several adversarial discriminative methods, such as DANN <ref type="bibr" target="#b11">[12]</ref>, ADDA <ref type="bibr" target="#b12">[13]</ref> and SGADA <ref type="bibr" target="#b20">[21]</ref>. Additionally, we propose SGPCS, which builds upon PCS <ref type="bibr" target="#b21">[22]</ref> with a pseudo labeling approach to achieve state-ofthe-art performance.</p><p>Our contributions are three-fold: <ref type="bibr" target="#b0">(1)</ref> We introduce CARLANE, a 3-way sim-to-real benchmark, allowing single-and multi-target UDA. <ref type="bibr" target="#b1">(2)</ref> We provide several dataset tools, i.e., an agent to collect images with lane annotations in CARLA and a labeling tool to annotate the real-world images manually. <ref type="bibr" target="#b2">(3)</ref> We evaluate several well-known UDA methods to establish baselines and discuss results on both single-and multi-target UDA. To the best of our knowledge, we are the first to adapt a lane detection model from simulation to multiple real-world domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Generation for Sim-to-Real Lane Detection</head><p>In recent years, much attention has been paid to lane detection benchmarks in the real world, such as CULane <ref type="bibr" target="#b2">[3]</ref>, TuSimple <ref type="bibr" target="#b19">[20]</ref>, LLAMAS <ref type="bibr" target="#b22">[23]</ref>, and BDD100K <ref type="bibr" target="#b23">[24]</ref>. Despite the popularity of these benchmarks, there is few research that focuses on sim-to-real lane detection datasets. Garnett et al. <ref type="bibr" target="#b3">[4]</ref> propose a method for generating synthetic images with 3D lane annotations in the open-source engine blender. Their synthetic-3D-lanes dataset contains 300K train, 1,000 validation and 5,000 test images, while their real-world 3D-lanes dataset consists of 85K images, which are annotated in a semi-manual manner. Utilizing the data generation method from <ref type="bibr" target="#b3">[4]</ref>, Garnett et al. <ref type="bibr" target="#b18">[19]</ref> collect 50K labeled synthetic images to perform sim-to-real domain adaptation for 3D lane detection. At this point, the source domain of the dataset is not publicly available.</p><p>Recently, Hu et al. <ref type="bibr" target="#b4">[5]</ref> investigated UDA techniques for 2D lane detection. Their proposed data generation method relies on CARLA's built-in agent to automatically collect 16K synthetic images.</p><p>However, the dataset is not publicly available at this point. In comparison, our method leverages an efficient and configurable waypoint-based agent. Furthermore, in contrast to the aforementioned works, considering only single-source single-target UDA, we additionally focus on multi-target UDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Domain Adaptation</head><p>Unsupervised Domain Adaptation has been extensively studied in recent years <ref type="bibr" target="#b8">[9]</ref>. In an early work, Ganin et al. <ref type="bibr" target="#b7">[8]</ref> propose a gradient reversal layer between the features extractor and a domain classifier to learn similar feature distributions for distinct domains. Early discrepancy-based methods employ a distance metric to measure the discrepancy of the source and target domain <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. A prominent example is DAN <ref type="bibr" target="#b9">[10]</ref> which uses maximum mean discrepancies (MMD) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> to match embeddings of different domain distributions. Recently, DSAN <ref type="bibr" target="#b10">[11]</ref> builds upon DAN with local MMD and exploits fine-grained features to align subdomains accurately.</p><p>Domain alignment can also be achieved through adversarial learning <ref type="bibr" target="#b27">[28]</ref>. Adversarial discriminative methods such as DANN <ref type="bibr" target="#b11">[12]</ref> or ADDA <ref type="bibr" target="#b12">[13]</ref> employ a domain classifier or discriminator, encouraging the feature extractor to produce domain-invariant representations. While these methods mainly rely on feature-level alignment, adversarial generative methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> operate on pixel-level.</p><p>In a recent trend, self-supervised learning methods are leveraged as auxiliary tasks to improve domain adaptation effectiveness and to capture in-domain semantic structures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>. Furthermore, self-supervised learning is utilized for cross-domain alignment as well, by matching class-discriminative features <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>, task-discriminative features <ref type="bibr" target="#b33">[34]</ref>, class prototypes <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b21">22]</ref> or equivalent samples in the domains <ref type="bibr" target="#b35">[36]</ref>.</p><p>Furthermore, other recent works mitigate optimization inconsistencies by minimizing the gradients discrepancy of the source samples and target samples <ref type="bibr" target="#b36">[37]</ref> or by applying a meta-learning scheme between the domain alignment and the targeted classification task <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Generation</head><p>To construct our benchmark, we gather image data from a real 1/8th model vehicle, and the CARLA simulator <ref type="bibr" target="#b5">[6]</ref>. Ensuring the verification of results and transferability to real driving scenarios, we extend our benchmark with the TuSimple dataset <ref type="bibr" target="#b19">[20]</ref>. This enables gradual testing, starting from simulation, followed by model cars, and ending with full-scale real word experiments. Data variety is achieved through domain randomization in all domains. However, naively performing domain randomization might lead to an imbalanced dataset. Therefore, similar driving scenarios are sampled across all domains, and a bagging approach is utilized to uniformly collect lanes by their curvature with respect to the camera position. We strictly follow TuSimple's data format <ref type="bibr" target="#b19">[20]</ref> to maintain consistency across all our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Real-World Environment</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we build six different 1/8th race tracks, where each track is available in two different surface materials (dark and light gray). We vary between dotted and solid lane markings, which are pure white and 50 mm thick. The lanes are constantly 750 mm wide, and the smallest inner radius is 250 mm. The track layouts are designed to roughly contain the same proportion of straight and curved segments to obtain a balanced label distribution. We construct these tracks in four locations with alternating backgrounds and lighting conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Real-World Data Collection</head><p>Raw image data is recorded from a front-facing Stereolabs ZEDM camera with 30 FPS and a resolution of 1280 ? 720 pixels. A detailed description of the 1/8th car can be found in the Appendix. The vehicle is moved with a quasi-constant velocity clockwise and counter-clockwise to cover both directions of each track. All collected images from tracks (e) and (f) are used for the test subset. In addition, we annotate lane markings with our labeling tool for validation and testing, which is made publicly available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Simulation Environment</head><p>We utilize the built-in APIs from CARLA to randomize multiple aspects of the agent and environment, such as weather, daytime, ego vehicle position, camera position, distractor vehicles, and world objects (i.a., walls, buildings, and plants). Weather and daytime are varied systematically by adapting parameters for cloud density, rain intensity, puddles, wetness, wind strength, fog density, sun azimuth, and sun altitude. For further details, we refer to our implementation. To occlude the lanes similar to real-world scenarios, up to five neighbor vehicles are spawned randomly in the vicinity of the agent. We consider five different CARLA maps in urban and highway environments (Town03, Town04, Town05, Town06, and Town10) to collect our dataset, as the other towns' characteristics are not suitable for our task (i.a., mostly straight lanes). In addition, we collect data from the same towns without world objects to strengthen the focus on lane detection, similar to our model vehicle target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Simulation Data Agent</head><p>We implement an efficient agent based on waypoint navigation, which roams randomly and reliably in the aforementioned map environments and collects 1280 ? 720 images. In each step, the waypoint navigation stochastically traverses the CARLA road map with a fixed lookahead distance of one meter. In addition, we sample offset values ?y k from the center lane within the range ?1.20 m. To avoid saturation at the lane borders, which would occur with a sinusoidal function, we use the triangle wave function:</p><formula xml:id="formula_0">?y k = 2m ? arcsin(sin(i k ))<label>(1)</label></formula><p>where m is the maximal offset and i k is incremented by 0.08 for each simulation step k. Per frame, our agent moves to the next waypoint with an increment of one meter, enabling the collection of highly diverse data in a fast manner. We use a bagging approach for balancing, which allows us to define lane classes based on their curvature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The CARLANE Benchmark</head><p>The CARLANE Benchmark consists of three distinct sim-to-real datasets, which we build from our three different domains. The details of the individual subsets can be found in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>MoLane consists of images from CARLA and the real 1/8th model vehicle. For the abstract real-world domain, we collect 46,843 images with our model vehicle, of which 2,000 validation and 1,000 test images are labeled. For the source domain, we use our simulation agent to gather 84,000 labeled images. To match the label distributions between both domains, we define five lane classes based on the relative angle ? of the agent to the center lane for our bagging approach: strong left curve </p><formula xml:id="formula_1">(? ??45?), soft left curve (?45?&lt; ? ? ?15?), straight (?15?&lt; ? &lt; 15?), soft right curve (15?? ? &lt; 45?</formula><p>) and strong right curve (45?? ?). In total, MoLane encompasses 130,843 images. TuLane consists of images from CARLA, and a cleaned version of the TuSimple dataset <ref type="bibr" target="#b19">[20]</ref>, which is licensed under the Apache License, Version 2.0. To clean test set annotations, we utilize our labeling tool to ensure that the up to four lanes closest to the car are correctly labeled. We adapt the bagging classes to align the source dataset with TuSimple's lane distribution: left curve (?12?&lt; ? ? 5?), straight (?5?&lt; ? &lt; 5?) and right curve (5?? ? &lt; 12?).</p><p>MuLane is a multi-target UDA dataset and is a balanced mixture of images from MoLane and TuLane. For MuLane's entire training set and its source domain validation and test set, we use all available images from TuLane and sample the same amount of images from MoLane. We adopt the 1,000 test images from MoLane's target domain and sample 1,000 test images from TuSimple to form MuLane's test set. For the validation set, we use the 2,000 validation images from MoLane and 2,000 of the remaining validation and test images of TuLane's target domain. In total, MuLane consists of 65,336 images.</p><p>To further analyze CARLANE, we visualize the ground truth lane distributions in <ref type="figure" target="#fig_2">Figure 3</ref>. We observe that the lane distributions of source and target data from our datasets are well aligned.</p><p>MoLane, TuLane, and MuLane are publicly available at https://carlanebenchmark.github.io and licensed under the Apache License, Version 2.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Format</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Tasks</head><p>The main task of our datasets is UDA for lane detection, where the goal is to predict lane annotations</p><formula xml:id="formula_2">Y t ? R R?G?N given the input image X t ? R H?W ?3 from the unlabeled target domain D T = {(X t )} t?T .</formula><p>R defines the number of row anchors, G the number of griding cells, and N the number of lane annotations available in the dataset, where the definition of Y t follows <ref type="bibr" target="#b19">[20]</ref>. During training time, the images X s ? R H?W ?3 , corresponding labels Y s ? R H?W ?C from the source domain D S = {(X s , Y s )} s?S , and the unlabeled target images X t are available. Additionally, MuLane focuses on multi-target UDA, where</p><formula xml:id="formula_3">D T = {(X t1 ) ? (X t2 )} t1?T1,t2?T2 .</formula><p>Although we focus on sim-to-real UDA, our datasets can be used for unsupervised and semisupervised tasks and partially for supervised learning tasks. Furthermore, a real-to-real transfer can be performed between the target domains of our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Benchmark Experiments</head><p>We conduct experiments on our CARLANE Benchmark for several UDA methods from the literature and our proposed method. Additionally, we train fully supervised baselines on all domains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Metrics</head><p>For evaluation, we use the following metrics:</p><p>(1) Lane Accuracy (LA) <ref type="bibr" target="#b1">[2]</ref> is defined by LA = pc py , where p c is the number of correctly predicted lane points and p y is the number of ground truth lane points. Lane points are considered as correct if their L 1 distance is smaller than the given threshold t pc = 20 cos(a yl ) , where a yl is the angle of the corresponding ground truth lane.</p><p>(2) False Positives (FP) and False Negatives (FN) <ref type="bibr" target="#b1">[2]</ref>: To further determine the error rate and to draw more emphasis on mispredicted or missing lanes, we measure false positives with FP = l f lp and false negatives with FN = lm ly , where l f is the number of mispredicted lanes, l p is the number of predicted lanes, l m is the number of missing lanes and l y is the number of ground truth lanes. Following <ref type="bibr" target="#b1">[2]</ref>, we classify lanes as mispredicted, if the LA &lt; 85%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We use Ultra Fast Structure-aware Deep Lane Detection (UFLD) <ref type="bibr" target="#b1">[2]</ref> as baseline and strictly adopt its training scheme and hyperparameters. UFLD treats lane detection as a row-based classification problem and utilizes the row anchors defined by TuSimple <ref type="bibr" target="#b19">[20]</ref>. To achieve a lower bound for the evaluated UDA methods, we train UFLD as a supervised baseline on the source simulation data (UFLD-SO). Furthermore, we train our baseline on the labeled real-world training data for a surpassable fully-supervised performance in the target domain (UFLD-TO). Since the training images from MoLane and MuLane have no annotations, we train UFLD-TO in these cases on the labeled validation images and validate our model on the entire test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Compared UDA Methods</head><p>We evaluate the following feature-level UDA methods on the CARLANE Benchmark by adopting their default hyperparameters and tuning them accordingly. Each model is initialized with the pretrained feature encoder of our baseline model (UFLD-SO). The optimized hyperparameters can be found in <ref type="table">Table 2</ref>.</p><p>(1) DANN <ref type="bibr" target="#b11">[12]</ref> is an adversarial discriminative method that utilizes a shared feature encoder and a dense domain classifier connected via a gradient reversal layer.</p><p>(2) ADDA [13] employs a feature encoder for each domain and a dense domain discriminator. Following ADDA, we freeze the weights of the pre-trained classifier of UFLD-SO to obtain final predictions.</p><p>(3) SGADA <ref type="bibr" target="#b20">[21]</ref> builds upon ADDA and utilizes its predictions as pseudo labels for the target training images. Since UFLD treats lane detection as a row-based classification problem, we reformulate the pseudo label selection mechanism. For each lane, we select the highest confidence value from the griding cells of each row anchor. Based on their griding cell position, the confidence values are divided into two cases: absent lane points and present lane points. Thereby, the last griding cell <ref type="table">Table 2</ref>: Optimized hyperparameters to achieve the reported results. C denotes domain classifier parameters, D denotes domain discriminator parameters, adv the adversarial loss from <ref type="bibr" target="#b12">[13]</ref> and cls the classifier loss, sim the similarity loss and aux the auxiliary loss from <ref type="bibr" target="#b1">[2]</ref>. Loss weights are set to 1.0 unless stated otherwise. represents absent lane points as in <ref type="bibr" target="#b1">[2]</ref>. For each case, we calculate the mean confidence over the corresponding lanes. We then use the thresholds defined by SGADA to decide whether the prediction is treated as a pseudo label.</p><p>(4) SGPCS (ours) builds upon PCS <ref type="bibr" target="#b21">[22]</ref> and performs in-domain contrastive learning and crossdomain self-supervised learning via cluster prototypes. Our overall objective function comprises the in-domain and cross-domain loss from PCS, the losses defined by UFLD, and our adopted pseudo loss from SGADA. We adjust the momentum for memory bank feature updates to 0.5 and use spherical K-means <ref type="bibr" target="#b38">[39]</ref> with K = 2, 500 to cluster them into prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Implementation Details</head><p>We implement all methods in PyTorch 1.8.1 and train them on a single machine with four RTX 2080 Ti GPUs. Tuning all methods took a total amount of compute of approximately 3.5 petaflop/s-days.</p><p>The training times for each model range from 4-13 days for UFLD baselines and 6-44 hours for domain adaption methods. In addition, we found that applying output scaling on the last linear layer of the model yields slightly better results. Therefore, we divide the models' output by 0.5. Our implementation is publicly available at https://carlanebenchmark.github.io.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation</head><p>Quantitative Evaluation. In <ref type="table" target="#tab_2">Table 3</ref> we report the results on MoLane, TuLane, and MuLane across five different runs. We observe that UFLD-SO is able to generalize to a certain extent to the target domain. This is mainly due to the alignment of semantic structure from both domains. ADDA, SGADA, and our proposed SGPCS manage to adapt the model to the target domain slightly and consistently. However, DANN suffers from negative transfer <ref type="bibr" target="#b39">[40]</ref> when trained on MoLane and MuLane. The negative transfer of DANN for complex domain adaptation tasks is also observed in other works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr">42]</ref> and can be explained by the source domain's data distribution and the model complexity <ref type="bibr" target="#b39">[40]</ref>. In our case, the source domain contains labels not present in the target domain, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>, which is more pronounced in MoLane and MuLane.</p><p>We want to emphasize that with an accuracy gain of a maximum of 4.55% (SGPCS) and high false positive and false negative rates, the domain adaptation methods are not able to achieve comparable results to the supervised baselines (UFLD-TO). Furthermore, we observe that false positive and false negative rates increase significantly on MuLane, indicating that the multi-target dataset forms the most challenging task. False positives and false negatives represent wrongly detected and missing lanes which can lead to crucial impacts on autonomous driving functions. These results affirm the need for the proposed CARLANE Benchmark to further strengthen the research in UDA for lane detection.</p><p>Qualitative Evaluation. We use t-SNE [43] to visualize the features of the features encoders for the source and target domains of MuLane in <ref type="figure" target="#fig_3">Figure 4</ref>. t-SNE visualizations of MoLane and TuLane can be found in the Appendix. In accordance with the quantitative results, we observe only a slight adaptation of the source and target domains features for ADDA, SGADA, and SGPCS compared to the supervised baseline UFLD-SO. Consequently, the examined well-known domain adaptation methods have no significant effect on feature alignment. In addition, we show results from the evaluated methods in <ref type="figure" target="#fig_4">Figure 5</ref> and observe that the models are able to predict target domain lane  annotations in many cases but are not able to achieve comparable results to the supervised baseline (UFLD-TO).</p><p>In summary, we find quantitatively and qualitatively that the examined domain adaptation methods do not significantly improve the performance of lane detection and feature adaptation. For this reason, we believe that the proposed benchmark could facilitate the exploration of new domain adaptation methods to overcome these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present CARLANE, the first UDA benchmark for lane detection. CARLANE was recorded in three domains and consists of three datasets: the single-target datasets MoLane and TuLane and the multi-target dataset MuLane, which is a balanced combination of both. Based on the UFLD model, we conducted experiments with different UDA methods on CARLANE and found that the selected methods are able to adapt the model to target domains slightly and consistently. However, none of the methods achieve comparable results to the supervised baselines. The most significant performance differences are noticeable in the high false positive and false negative rates of the UDA methods compared to the target-only baselines, which is even more pronounced in the MuLane multi-target task. These false-positive and false-negative rates can negatively impact autonomous driving functions since they represent misidentified and missing lanes. Furthermore, as shown in the t-SNE plots of <ref type="figure" target="#fig_3">Figure 4</ref>, the examined well-known domain adaptation methods have no significant effect on feature alignment. The current difficulties of the examined UDA methods to adequately align the source and target domains confirm the need for the proposed CARLANE benchmark. We believe that CARLANE eases the development and comparison of UDA methods for lane detection.</p><p>In addition, we open-source all tools for dataset creation and labeling and hope that CARLANE facilitates future research in these directions.</p><p>Limitations. One limitation of our work is that we only use a fixed set of track elements within our 1/8th scaled environment. These track elements represent only a limited number of distinct curve radii. Furthermore, neither buildings nor traffic signs exist in MoLane's model vehicle target domain. Moreover, the full-scale real-world target domain of TuLane is derived from TuSimple. TuSimple's data was predominantly collected under good and medium conditions and lacks variation in weather and time of day. In addition, we want to emphasize that collecting data for autonomous driving is still an ongoing effort and that datasets such as TuSimple do not cover all possible real-world driving scenarios to ensure safe, practical use. For the synthetically generated data, we limited ourselves to using existing CARLA maps without defining new simulation environments. Despite these limitations, CARLANE serves as a supportive dataset for further research in the field of UDA. Ethical and Responsible Use. Considering the limitations of our work, UDA methods trained on TuLane and MuLane should be tested with care and under the right conditions on a full-scale car. However, real-world testing with MoLane in the model vehicle domain can be carried out in a safe and controlled environment. Additionally, TuLane contains open-source images with unblurred license plates and people. This data should be treated with respect and in accordance with privacy policies. In general, our work contributes to the research in the field of autonomous driving, in which a lot of unresolved ethical and legal questions are still being discussed. The step-by-step testing possibility across three domains makes it possible for our benchmark to include an additional safety mechanism for real-world testing.</p><p>[ [43] L. Van der Maaten and G. Hinton, "Visualizing data using t-sne.," Journal of machine learning research, vol. 9, no. 11, 2008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Example Usage of the CARLANE Benchmark</head><p>A Jupyter Notebook with a tutorial to read the datasets for usage in PyTorch can be found at https://carlanebenchmark.github.io.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Model Vehicle Description</head><p>In <ref type="figure">Figure 6</ref>, the self-built 1/8th model vehicle is shown, which we used to gather the images for the 1/8th scaled target domain. A NVIDIA Jetson AGX is the central computation unit powered by a separate Litionite Tanker Mini 25000mAh battery. For image collection, we utilize the software framework ROS Melodic and a Stereolabs ZEDM stereo camera with an integrated IMU. The camera is directly connected to the AGX and captures images with a resolution of 1280 ? 720 pixels and a rate of 30 FPS. <ref type="figure">Figure 6</ref>: Picture of the 1/8th model vehicle we built to capture images in our 1/8th target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Reproducibility of the Baselines</head><p>To ensure reproducibility, we strictly follow UFLD <ref type="bibr" target="#b1">[2]</ref> and the corresponding UDA method for model architecture and hyperparameters. Thereby, we utilize UFLD as an encoder for the UDA method. We provide a detailed table of the tuned hyperparameters, architecture changes, and objectives in the main text. In addition, the trained weights of our baselines, their entire implementation, and the configuration files of our baselines are made publicly available at https://carlanebenchmark.github.io.</p><p>Initialization. We initialize convolutional layer weights with kaiming normal and their biases with 0.0. Linear layer weights are initialized with normal (mean = 0.0, std = 0.01), batch normalization weights and biases are initialized with 1.0. t-SNE feature clustering. <ref type="figure" target="#fig_5">Figure 7</ref> shows the t-SNE feature clustering of the trained baselines for the MoLane and TuLane dataset, respectively. We observe that few features of both domains spread over the entire plot for higher-performing UDA methods. However, there are still large clusters of features from one domain, indicating that the domain adaptation only occurred slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UFLD-SO</head><p>Qualitative results. We randomly sample results from our baselines and show them in Figures 9, 10, and 11. Compared to UFLD-SO, the UDA baselines ADDA, SGADA, and SGPCS increase performance consistently. UFLD-TO samples show the best results on the target domain.  In <ref type="table" target="#tab_4">Table 4</ref>, we compare CARLANE with the datasets created by related work. The main differentiators are that our dataset contains three distinct domains, including a scaled model vehicle, and is publicly available. To further compare our synthetic datasets with related work, the applied variations during the data collection process are summarized in <ref type="table" target="#tab_5">Table 5</ref>. Additionally, we highlight noticeable differences in the visual quality of the simulation engines in <ref type="figure">Figure 8</ref>. Scenes captured in Carla are more realistic and detailed. <ref type="bibr" target="#b3">[4]</ref> ours <ref type="figure">Figure 8</ref>: Visual comparison of simulation images from the custom blender simulation used in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> and the Carla simulation used by <ref type="bibr" target="#b4">[5]</ref> and our work. We observe that scenes captured in Carla are more detailed and realistic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Comparison to Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasheet for the CARLANE Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.</p><p>CARLANE was created to be the first publicly available single-and multi-target Unsupervised Domain Adaptation (UDA) benchmark for lane detection to facilitate future research in these directions. However, in a broader sense, the datasets of CARLANE were also created for unsupervised and semi-supervised learning and partially for supervised learning. Furthermore, a real-to-real transfer can be performed between the target domains of our datasets.</p><p>Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?</p><p>As released on June 17, 2022, the initial version of CARLANE was created by Julian Gebele, Bonifaz Stuhr, and Johann Haselberger from the Institute for Driver Assistance Systems and Connected Mobility (IFM). The IFM is a part of the University of Applied Sciences Kempten. Furthermore, CARLANE was created by Bonifaz Stuhr as part of his Ph.D. at the Autonomous University of Barcelona (UAB) and by Johann Haselberger as part of his Ph.D. at the Technische Universit?t Berlin (TU Berlin).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Who funded the creation of the dataset?</head><p>If there is an associated grant, please provide the name of the grantor and the grant name and number.</p><p>There is no specific grant for the creation of the CARLANE Benchmark. The datasets were created as part of the work at the IFM and the University of Applied Sciences Kempten.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composition</head><p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.</p><p>The instances are drives on diverse roads in simulation, in an abstract 1/8th real world, and in full-scale real-world scenarios, along with lane annotations of the up to four nearest lanes to the vehicle.</p><p>How many instances are there in total (of each type, if appropriate)? The datasets of CARLANE contain samples of driving scenarios and lane annotations encountered in simulation and the real world. The datasets are not representative of all these driving scenarios, as the distribution of the latter is highly dynamic and diverse. Instead, the motivation was to resemble the variety and shifts of different domains in which such scenarios occur to strengthen the systematic study of UDA methods for lane detection. Therefore, CARLANE should be considered as an UDA benchmark with step-by-step testing possibility across three domains, which makes it possible to include an additional safety mechanism for real-world testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What data does each instance consist of?</head><p>"Raw" data (e.g., unprocessed text or images) or features? In either case, please provide a description. Each unlabeled instance consists of an 1280?720 image from a driving scenario and a .txt file entry for the corresponding subset.</p><p>Is there a label or target associated with each instance? If so, please provide a description.</p><p>As described above, the labels per instance are discretized lane annotations and lane segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is any information missing from individual instances?</head><p>If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.</p><p>Everything is included. No data is missing.</p><p>Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? If so, please describe how these relationships are made explicit.</p><p>There are no relationships made explicit between instances. However, some instances are part of the same drive and therefore have an implicit relationship.</p><p>Are there recommended data splits(e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.</p><p>Each domain is split into training and validation subsets. Details are shown in The full-scale real-world target domain from TuSimple could implicitly reveal sensitive information printed or put on the vehicles or people's wearings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection Process</head><p>How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.</p><p>The source domain images of driving scenarios and the corresponding lane annotations were directly recorded from the simulation. Lanes were manually labeled for the directly recorded realworld images. For the images collected from the model vehicle, the authors annotated the data with a labeling tool created for this task. The labeling tool is publicly available at https: //carlanebenchmark.github.io. The labeling tool is utilized to clean up the annotations of the test set in the real-world domain. The authors do not have information about the labeling process of the full-scale target domain since its data is derived from the TuSimple dataset.</p><p>What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?</p><p>The source domain data was collected using the CARLA simulator and its APIs with a resolution of 1280 ? 720 pixels. The real-world 1/8th target domain was collected with a Stereolabs ZEDM camera with 30 FPS and a resolution of 1280?720 pixels. The lane distributions were additionally balanced with a bagging approach, and lanes were annotated with a labeling tool. More information can be found in the corresponding paper and the implementation. The implementation and all used tools are publicly available at https://carlanebenchmark.github.io.</p><p>If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?</p><p>Source domain dataset entries are sampled based on the relative angle ? of the agent to the center lane. For MoLane, five lane classes are defined for the bagging approach: strong left curve (? ??45?), soft left curve (?45?&lt; ? ? ?15?), straight (?15?&lt; ? &lt; 15?), soft right curve (15?? ? &lt; 45?) and strong right curve (45?? ?).</p><p>For TuLane, three lane classes are defined for the bagging approach: left curve (?12?&lt; ? ? 5?), straight (?5?&lt; ? &lt; 5?) and right curve (5?? ? &lt; 12?).</p><p>Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?</p><p>Only the authors were involved in the collection process. The authors do not have information about the people involved in collecting the TuSimple dataset.</p><p>Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.</p><p>MoLane's data was collected and annotated from June 2021 to August 2021. Data for TuLane's source domain was collected in February 2022.</p><p>Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and/or under applicable terms o fuse (ToU)?</head><p>If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Images sampled from our CARLANE Benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our track types for MoLane. (a) -(d) show the black version of the training and validation tracks. These tracks are also constructed using a light gray surface material. (e) and (f) depict our test tracks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Lane annotation distributions of the three subsets of CARLANE. Since the real-world training data of MoLane and MuLane is unlabeled, we utilize their validation data for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>t-SNE visualization of MuLane dataset. The source domain is marked in blue, the real-world model vehicle target domain in red, and the TuSimple domain in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of target domain predictions. Ground truth lane annotations are marked in blue, predictions in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>t-SNE visualizations of the MoLane dataset (top) and the TuLane dataset (bottom). The source domain is marked in blue, the real-world model vehicle target domain in red, and TuLane's target domain in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 10 :Figure 11 :</head><label>91011</label><figDesc>Qualitative results of target domain predictions. Images are randomly sampled. Ground truth lane annotations are marked in blue, predictions in red. Qualitative results of target domain predictions. Images are randomly sampled. Ground truth lane annotations are marked in blue, predictions in red. Qualitative results of target domain predictions. Images are randomly sampled. Ground truth lane annotations are marked in blue, predictions in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset overview. Unlabeled images denoted by *, partially labeled images denoted by **</figDesc><table><row><cell>Dataset</cell><cell>domain</cell><cell>total images</cell><cell>train</cell><cell>validation</cell><cell>test</cell><cell>lanes</cell></row><row><cell>MoLane</cell><cell>CARLA simulation model vehicle</cell><cell>84,000 46,843</cell><cell>80,000 43,843*</cell><cell>4,000 2,000</cell><cell>-1,000</cell><cell>? 2 ? 2</cell></row><row><cell>TuLane</cell><cell>CARLA simulation TuSimple [20]</cell><cell>26,400 6,408</cell><cell>24,000 3,268</cell><cell>2,400 358</cell><cell>-2,782</cell><cell>? 4 ? 4</cell></row><row><cell>MuLane</cell><cell>CARLA simulation model vehicle + TuSimple [20]</cell><cell>52,800 12,536</cell><cell>48,000 6,536**</cell><cell>4,800 4,000</cell><cell>-2,000</cell><cell>? 4 ? 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance on the test set. Lane accuracy (LA), false positives (FP), and false negatives (FN) are reported in %.</figDesc><table><row><cell>ResNet-18</cell><cell>LA</cell><cell cols="2">MoLane FP &amp; FN</cell><cell>LA</cell><cell>TuLane FP</cell><cell>FN</cell><cell>LA</cell><cell>MuLane FP</cell><cell>FN</cell></row><row><cell>UFLD-SO</cell><cell>89.39</cell><cell></cell><cell>25.25</cell><cell>87.43</cell><cell>34.21</cell><cell>23.48</cell><cell>88.02</cell><cell>50.24</cell><cell>26.08</cell></row><row><cell>DANN [12]</cell><cell cols="2">87.65?0.48</cell><cell>29.97?1.21</cell><cell>88.74?0.32</cell><cell>32.71?0.52</cell><cell>21.64?0.65</cell><cell>86.01?0.67</cell><cell>55.33?1.22</cell><cell>36.30?1.90</cell></row><row><cell>ADDA [13]</cell><cell cols="2">92.85?0.17</cell><cell>10.61?0.77</cell><cell>90.72?0.15</cell><cell>29.73?0.36</cell><cell>17.67?0.42</cell><cell>89.83?0.33</cell><cell>46.79?0.43</cell><cell>20.57?0.63</cell></row><row><cell>SGADA [21]</cell><cell cols="2">93.82?0.10</cell><cell>7.13?0.22</cell><cell>91.70?0.13</cell><cell>28.42?0.34</cell><cell>16.10?0.43</cell><cell>90.71?0.10</cell><cell>45.13?0.32</cell><cell>17.26?0.36</cell></row><row><cell>SGPCS (ours)</cell><cell cols="2">93.94?0.04</cell><cell>7.16?0.16</cell><cell>91.55?0.13</cell><cell>28.52?0.21</cell><cell>16.16?0.26</cell><cell>91.57?0.22</cell><cell>45.49?0.63</cell><cell>17.39?0.88</cell></row><row><cell>UFLD-TO</cell><cell>97.35</cell><cell></cell><cell>0.50</cell><cell>94.97</cell><cell>18.05</cell><cell>3.84</cell><cell>96.57</cell><cell>34.06</cell><cell>2.49</cell></row><row><cell>ResNet-34</cell><cell>LA</cell><cell></cell><cell>FP &amp; FN</cell><cell>LA</cell><cell>FP</cell><cell>FN</cell><cell>LA</cell><cell>FP</cell><cell>FN</cell></row><row><cell>UFLD-SO</cell><cell>90.35</cell><cell></cell><cell>22.25</cell><cell>89.42</cell><cell>32.35</cell><cell>21.19</cell><cell>89.17</cell><cell>48.86</cell><cell>23.67</cell></row><row><cell>DANN [12]</cell><cell cols="2">90.91?0.42</cell><cell>19.73?1.51</cell><cell>91.06?0.14</cell><cell>30.17?0.20</cell><cell>18.54?0.25</cell><cell>88.76?0.22</cell><cell>48.93?0.47</cell><cell>24.16?0.89</cell></row><row><cell>ADDA [13]</cell><cell cols="2">92.39?0.26</cell><cell>12.17?0.84</cell><cell>91.39?0.16</cell><cell>28.76?0.30</cell><cell>16.63?0.36</cell><cell>90.22?0.39</cell><cell>45.84?0.54</cell><cell>19.49?0.90</cell></row><row><cell>SGADA [21]</cell><cell cols="2">93.31?0.10</cell><cell>9.41?0.16</cell><cell>92.04?0.09</cell><cell>28.18?0.20</cell><cell>15.99?0.24</cell><cell>91.63?0.03</cell><cell>44.18?0.12</cell><cell>16.23?0.16</cell></row><row><cell>SGPCS (ours)</cell><cell cols="2">93.53?0.25</cell><cell>8.24?0.91</cell><cell>93.29?0.18</cell><cell>25.68?0.48</cell><cell>12.73?0.59</cell><cell>91.55?0.17</cell><cell>44.75?0.28</cell><cell>16.41?0.44</cell></row><row><cell>UFLD-TO</cell><cell>97.21</cell><cell></cell><cell>0.30</cell><cell>94.43</cell><cell>20.74</cell><cell>7.20</cell><cell>96.54</cell><cell>33.76</cell><cell>2.03</cell></row><row><cell cols="2">UFLD-SO</cell><cell></cell><cell>DANN</cell><cell></cell><cell>ADDA</cell><cell></cell><cell>SGADA</cell><cell></cell><cell>SGPCS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>41] H. Fan, X. Chang, W. Zhang, Y. Cheng, Y. Sun, and M. Kankanhalli, "Self-supervised globallocal structure modeling for point cloud domain adaptation with reliable voted pseudo labels," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6377-6386, 2022. [42] D. Kim, K. Saito, T.-H. Oh, B. A. Plummer, S. Sclaroff, and K. Saenko, "Cross-domain self-supervised learning for domain adaptation with few source labels," arXiv preprint arXiv:2003.08264, 2020.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of CARLANE (ours) with datasets created by related work.</figDesc><table><row><cell cols="2">Dataset Year</cell><cell>Publicly Available</cell><cell>Domains</cell><cell cols="2">Simulation Resolution</cell><cell>Total Images</cell><cell>Annotations</cell></row><row><cell>[4]</cell><cell>2019</cell><cell></cell><cell>sim, real</cell><cell>blender</cell><cell>480 ? 360</cell><cell>391K</cell><cell>3D</cell></row><row><cell>[19]</cell><cell>2020</cell><cell></cell><cell>sim, real</cell><cell>blender</cell><cell>480 ? 360</cell><cell>586K</cell><cell>3D</cell></row><row><cell>[5]</cell><cell>2022</cell><cell></cell><cell>sim, real</cell><cell>Carla</cell><cell>1280 ? 720</cell><cell>23K</cell><cell>2D</cell></row><row><cell>ours</cell><cell>2022</cell><cell></cell><cell>sim, real, scaled</cell><cell>Carla</cell><cell>1280 ? 720</cell><cell>163K</cell><cell>2D</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of applied variations for the collection of the synthetic datasets.</figDesc><table><row><cell>Dataset</cell><cell>Ego Vehicle</cell><cell>Camera Position</cell><cell>Lane Deviation</cell><cell>Traffic</cell><cell>Pedestrians</cell><cell>World Objects</cell><cell>Daytime</cell><cell>Weather</cell><cell>City</cell><cell>Rural</cell><cell>Highway</cell><cell>Terrain</cell><cell>Lane Topology</cell><cell>Road Appearance</cell></row><row><cell>[4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[19]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[5]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>shows the per-domain and per-subset breakdown of CARLANE instances. TuSimple is available at https://github.com/TuSimple/ tusimple-benchmark under the Apache License Version 2.0, January 2004.</figDesc><table><row><cell>Does the dataset contain all possible in-</cell></row><row><cell>stances or is it a sample (not necessarily ran-</cell></row><row><cell>dom) of instances from a larger set? If the</cell></row><row><cell>dataset is a sample, then what is the larger set? Is</cell></row><row><cell>the sample representative of the larger set (e.g., ge-</cell></row><row><cell>ographic coverage)? If so, please describe how this</cell></row><row><cell>representativeness was validated/verified. If it is not</cell></row><row><cell>representative of the larger set, please describe why</cell></row><row><cell>not (e.g., to cover a more diverse range of instances,</cell></row><row><cell>because instances were withheld or unavailable).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Dataset overview. Unlabeled images denoted by *, partially labeled images denoted by **.</figDesc><table><row><cell>Dataset</cell><cell>domain</cell><cell>total images</cell><cell>train</cell><cell>validation</cell><cell>test</cell><cell>lanes</cell></row><row><cell>MoLane</cell><cell>CARLA simulation model vehicle</cell><cell>84,000 46,843</cell><cell>80,000 43,843*</cell><cell>4,000 2,000</cell><cell>-1,000</cell><cell>? 2 ? 2</cell></row><row><cell>TuLane</cell><cell>CARLA simulation TuSimple</cell><cell>26,400 6,408</cell><cell>24,000 3,268</cell><cell>2,400 358</cell><cell>-2,782</cell><cell>? 4 ? 4</cell></row><row><cell>MuLane</cell><cell>CARLA simulation model vehicle + TuSimple</cell><cell>52,800 12,536</cell><cell>48,000 6,536**</cell><cell>4,800 4,000</cell><cell>-2,000</cell><cell>? 4 ? 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 the dataset contain data that might be considered confidential (e.g., data that is pro- tected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?</head><label>6</label><figDesc>If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.CARLANE is entirely self-contained. If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.No.</figDesc><table><row><cell>. The target</cell></row><row><cell>domains for UDA additionally include test sets,</cell></row><row><cell>which were recorded from separate tracks (model</cell></row><row><cell>vehicle) or driving scenarios (TuSimple). Since</cell></row><row><cell>UDA aims to adapt models to target domains,</cell></row><row><cell>only the target domains include a test set.</cell></row><row><cell>Are there any errors, sources of noise, or re-</cell></row><row><cell>dundancies in the dataset? If so, please provide</cell></row><row><cell>a description.</cell></row><row><cell>CARLANE was recorded from different drives</cell></row><row><cell>through simulation and real-world domains.</cell></row><row><cell>Therefore there are images captured from the</cell></row></table><note>Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?Does the dataset identify any subpopulations (e.g., by age, gender)?</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing/cleaning/labeling</head><p>Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.</p><p>As described above, lane annotations were labeled or cleaned using a labeling tool and sampled based on the relative angle ? of the agent to the center lane.</p><p>Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data.</p><p>No.</p><p>Is the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point.</p><p>Yes, the software is available at https:// carlanebenchmark.github.io.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uses</head><p>Has the dataset been used for any tasks already? If so, please provide a description.</p><p>The datasets were used to create UDA baselines for the corresponding paper presenting the CAR-LANE Benchmark.</p><p>Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.</p><p>Yes, the baselines presented in the corresponding paper are available at https:// carlanebenchmark.github.io.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What(other) tasks could the dataset be used for?</head><p>In a broader sense, the datasets of CARLANE can also be used for unsupervised and semisupervised learning and partially for supervised learning.</p><p>Is there anything about the composition of the dataset or the way it was collected and pre-processed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?</p><p>Yes, TuLane and MuLane contain open-source images with unblurred license plates and people. This data should be treated with respect and in accordance with privacy policies. In general, CARLANE contributes to the research in the field of autonomous driving, in which many unresolved ethical and legal questions are still being discussed. The step-by-step testing possibility across three domains makes it possible for our benchmark to include an additional safety mechanism for real-world testing. This can help the consumer to mitigate the risks and harms to some extent.</p><p>Are there tasks for which the dataset should not be used? If so, please provide a description.</p><p>Since CARLANE focuses on UDA for lane detection and spans a limited number of driving scenarios, consumers should not solely really on this dataset to train models for fully autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution</head><p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.</p><p>Yes, CARLANE is publicly available on the internet for anyone interested in using it.</p><p>How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? The datasets have been available on kaggle since June 17, 2022.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Will the dataset be distributed under a copyright or other intellectual property (IP) license,</head><p>CARLANE is licensed under the Apache License Version 2.0, January 2004.</p><p>Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.</p><p>TuSimple, which is used for TuLanes and Mu-Lanes target domains, is licensed under the Apache License Version 2.0, January 2004. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.</p><p>Unknown to authors of the datasheet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maintenance Who will be supporting/hosting/maintaining the dataset?</head><p>CARLANE is hosted on kaggle and supported and maintained by the authors.</p><p>How can the owner/curator/manager of the dataset be contacted (e.g., email address)?</p><p>The curators of the datasets can be contacted under carlane.benchmark@gmail.com.</p><p>Is there an erratum? If so, please provide a link or other access point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No.</head><p>Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub)?</p><p>New versions of CARLANE's datasets will be shared and announced on our homepage (https: //carlanebenchmark.github.io) and at kaggle if corrections are necessary.</p><p>Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.</p><p>Yes, we plan to support versioning of the datasets so that all the versions are available to potential users. We maintain the history of versions via our homepage (https://carlanebenchmark. github.io) and at kaggle. Each version will have a unique DOI assigned.</p><p>If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.</p><p>Others can extend/augment/build on CARLANE with the support of the open-source tools provided on our homepage. Besides these tools, there will be no mechanism to validate or verify the extended datasets. However, others are free to release their extension of the CARLANE Benchmark or its datasets under the Apache License Version 2.0.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SSTN: Self-Supervised Domain Adaptation Thermal Object Detection for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ultra Fast Structure-aware Deep Lane Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatial As Deep: Spatial CNN for Traffic Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D-LaneNet: End-to-End 3D Multiple Lane Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pe&amp;apos;er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sim-to-Real Domain Adaptation for Lane Detection and Classification in Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ethier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Sharman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rayside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Melek</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">CARLA: An Open Urban Driving Simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Transferable Features with Deep Adaptation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Subdomain Adaptation Network for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-Adversarial Training of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>March</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial Discriminative Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-Supervised Domain Adaptation for Computer Vision Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="156694" to="156706" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised Domain Adaptation through Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15426" to="15436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-source Domain Adaptation for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Synthetic-to-Real Domain Adaptation for Lane Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Uziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">TuSimple-benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tusimple</surname></persName>
		</author>
		<ptr target="https://github.com/TuSimple/tusimple-benchmark/tree/master/doc/lane_detection" />
		<imprint>
			<biblScope unit="page" from="2021" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-Training Guided Adversarial Domain Adaptation for Thermal Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Halici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="4322" to="4331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised Labeled Lane Markers Using Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soussan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Return of Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Kernel Method for the Two-Sample-Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Kernel Two-Sample Test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle Consistent Adversarial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cross-domain Self-supervised Learning for Domain Adaptation with Few Source Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised cyclegan for objectpreserving image-to-image domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="498" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Toalign: Task-oriented alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13834" to="13846" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A prototypeoriented framework for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanwisuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17194" to="17208" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reducing the covariate shift by mirror samples in cross domain alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9546" to="9558" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cross-domain gradient discrepancy minimization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3937" to="3946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Metaalign: Coordinating domain alignment and classification for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16643" to="16653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Characterizing and avoiding negative transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11293" to="11302" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
