<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">General Partial Label Learning via Dual Bipartite Graph Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
							<email>bo.wu@columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">General Partial Label Learning via Dual Bipartite Graph Autoencoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We formulate a practical yet challenging problem: General Partial Label Learning (GPLL). Compared to the traditional Partial Label Learning (PLL) problem, GPLL relaxes the supervision assumption from instance-level -a label set partially labels an instance -to group-level: 1) a label set partially labels a group of instances, where the within-group instance-label link annotations are missing, and 2) crossgroup links are allowed -instances in a group may be partially linked to the label set from another group. Such ambiguous group-level supervision is more practical in real-world scenarios as additional annotation on the instance-level is no longer required, e.g., face-naming in videos where the group consists of faces in a frame, labeled by a name set in the corresponding caption. In this paper, we propose a novel graph convolutional network (GCN) called Dual Bipartite Graph Autoencoder (DB-GAE) to tackle the label ambiguity challenge of GPLL. First, we exploit the cross-group correlations to represent the instance groups as dual bipartite graphs: within-group and cross-group, which reciprocally complements each other to resolve the linking ambiguities. Second, we design a GCN autoencoder to encode and decode them, where the decodings are considered as the refined results. It is worth noting that DB-GAE is self-supervised and transductive, as it only uses the group-level supervision without a separate offline training stage. Extensive experiments on two real-world datasets demonstrate that DB-GAE significantly outperforms the best baseline over absolute 0.159 F1-score and 24.8% accuracy. We further offer analysis on various levels of label ambiguities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Labels are not always clean, complete, and unequivocal. As illustrated in <ref type="figure">Figure 1 (top)</ref>, given a training instance , it corresponds to a candidate label set <ref type="bibr">[ , ]</ref> where only one of them is correct. Learning from such ambiguous labels is known as Partial Label Learning (PLL) <ref type="bibr" target="#b1">(Cour, Sapp, and Taskar 2011)</ref>, which is a practical problem since it significantly reduces the human label effort compared to other one-to-one supervisions.</p><p>However, the assumption of PLL is still hardly feasible in large-scale scenarios: if we have millions of frames in videos or Web images, the instance-level label annotation <ref type="figure">Figure 1</ref>: Top: Traditional PLL problem -each face is associated with one of the names in the caption. Bottom: The proposed GPLL problem addressed in this paper -there are general cases of faces without names or names without faces. See <ref type="figure">Figure 2</ref> for illustrative formulation differences. Images are from MPII-MD <ref type="bibr" target="#b17">(Rohrbach et al. 2017)</ref> and M-VAD <ref type="bibr" target="#b16">(Pini et al. 2019)</ref> of PLL will be prohibitively expensive. <ref type="figure">Figure 1 (bottom)</ref> shows several examples of the relaxation from instance-level to group-level: a group of instances and the candidate label set <ref type="bibr">[ , ]</ref>. Compared to the tradition PLL, this is more complex and ambiguous: 1) within-group annotations are dropped, 2) cross-group links are allowed -appears in the candidate label set of another group [ ], and 3) there are some instances with null label that is not in any label set. Such relaxed supervision is more appealing since it requires NO extra annotation on the instance-level. To this end, we propose a novel problem: General Partial Label Learning (GPLL), whose training annotation only comes from the inherent data pair <ref type="figure">(Figure 1 bottom)</ref>, and thus is very challenging. <ref type="figure">Figure 2</ref> illustrates some related problems with progressively relaxed supervisions.</p><p>A straightforward approach to GPLL is to consider some within-/cross-group heuristics such as : 1) Instances with similar features across groups likely belong to the same label. 2) Similar instances cooccur with the same label across groups implies that the label is likely assigned to those instances. 3) An instance cannot belong to multi-labels ? , <ref type="figure">Figure 2</ref>: From (a) to (d), on the evolution of relaxing the supervision but bringing in more label ambiguity challenges. (a) Multi-Label <ref type="bibr" target="#b6">(Huang, Gao, and Zhou 2018)</ref>: each instance is labeled with more than one labels. (b) Multi-Instance <ref type="bibr" target="#b25">(Wu et al. 2015)</ref>: at least one instance in a group belongs to the label. (c) Partial-Label <ref type="bibr" target="#b3">(Feng and An 2019)</ref>: a candidate label set partially labels an instance, only one of them is correct. Note that the label set may vary from instance to instance. So far, the supervision is on instance-level. (d) General Partial Label Learning (our focus): group-level supervision. Instances or labels (blue or green shaded) may link to another group; there are also null instances and labels (grey shaded) with no links at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>, and distinct instances cannot be the same label within a group. However, these heuristics are too weak to address the extreme ambiguity. In fact, as we will show in Ablation Study, modeling such heuristics to construct the initial links only achieves 62.9% accuracy.</p><p>We believe that the key to solve GPLL is how to exploit the aforementioned cross-group correlations unsupervisedly to construct initial links and then refine them with stronger group contextual representations. To this end, we propose a novel graph convolutional network, called Dual Bipartite Graph Autoencoder (DB-GAE). As its name implies, DB-GAE explicitly learns richer within-group and cross-group representations which serve as a reciprocal complement to each other. The within-group representation resolves the ambiguity in a group, and the cross-group one renders additional global group context for further disambiguation. In particular, we first represent the initial links as the proposed within-group and cross-group bipartite graphs, and then use GCN <ref type="bibr" target="#b0">(Berg, Kipf, and Welling 2017)</ref> to encode and decode them to refine the dual links to obtain the results, where the reconstruction loss is only referenced to the within-group graph input as this is the only supervision we have in GPLL. Therefore, it is worth noting that DB-GAE is self-supervised and transductive, which is appealing as it requires NO additional training data and an offline training stage.</p><p>We compare the proposed DB-GAE to other baselines on both GPLL and PLL benchmarks. Our method outperforms the best baseline with absolute 0.158 F1-score and 24.7% accuracy. We also analyze the model performances on the varying levels of label ambiguity. The contributions are summarized as follows:</p><p>? We introduce the new learning problem of GPLL, which generalizes the existing PLL formulation to more realistic, challenging, and ambiguous annotation scenarios.</p><p>? We propose a novel graph neural networks called DB-GAE, which aims to disambiguate and predict instancelabel links within and across groups.</p><p>? We set up a new benchmark for the proposed GPLL task. Experiments demonstrate that DB-GAE significantly outperforms over strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Partial Label Learning (PLL) <ref type="bibr" target="#b15">(Nguyen and Caruana 2008;</ref><ref type="bibr" target="#b1">Cour, Sapp, and Taskar 2011;</ref><ref type="bibr" target="#b26">Xie and Huang 2018</ref>) also called superset label learning (Gong et al. 2017) had been viewed as a weakly-supervised learning framework with implicit labeling information which assumes there is always exactly one ground-truth among the candidate label set. Therefore, one disambiguation strategy is building a certain parametric model and regarding ground-truth label as a latent variable. The model is iteratively refined by optimizing certain objectives, such as the maximum likelihood criterion <ref type="bibr" target="#b12">(Kupfer and Zorn 2019;</ref><ref type="bibr" target="#b14">Liu and Dietterich 2014)</ref>, or the maximum margin criterion <ref type="bibr" target="#b28">(Yu and Zhang 2016)</ref>. Another strategy assumes equal importance for all kinds of candidate labels and predicts label scores by averaging their modeling outputs <ref type="bibr" target="#b1">(Cour, Sapp, and Taskar 2011;</ref><ref type="bibr" target="#b21">Tang and Zhang 2017;</ref><ref type="bibr" target="#b24">Wu and Zhang 2018;</ref><ref type="bibr" target="#b23">Wang, Li, and Zhang 2019;</ref><ref type="bibr" target="#b27">Xu, Lv, and Geng 2019)</ref>. Compared to the PLL problem, GPLL is much more challenging and needs to resolve grouplevel disambiguation, which is more general and practical for real-world scenarios. Graph Neural Networks (GNNs) were introduced in <ref type="bibr" target="#b5">(Gori, Monfardini, and Scarselli 2005;</ref><ref type="bibr" target="#b19">Scarselli et al. 2008)</ref>, and mainly focus on supervised node classification or link prediction problem based on convolutional graph networks <ref type="bibr" target="#b2">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b9">Kipf and Welling 2016a;</ref><ref type="bibr" target="#b10">2016b;</ref><ref type="bibr" target="#b29">Zhang and Chen 2018)</ref>. More recently, graph autoencoder networks <ref type="bibr" target="#b0">(Berg, Kipf, and Welling 2017)</ref> were proposed to perform unsupervised link prediction, which we adopt for our label disambiguation problem. Unlike previous link prediction problems where the weights of observed links were given by the data. Our weights were initially estimated by clustering algorithms, which is the only information we have. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation</head><p>In GPLL setting, the data is provided in the form of K groups G = {g i } K i=1 . Each group g i is a collection of instances and a candidate label set where</p><formula xml:id="formula_0">g i = {X (i) , L (i) }. X (i) is the set of M (i) instances, X (i) = {x (i) m } M (i) m=1 , and x (i) m is the instance feature where x (i) m ? R d . The asso- ciated candidate label set L (i) is the set of N (i) labels, L (i) = {l (i) n } N (i) n=1 , where l (i) n ? Y. The class set Y contains (C + 1) classes where Y = {1, , .</formula><p>.., C, null}, since some instances might be from background classes that never appear in the dataset labels. As shown in the GPLL examples in Introduction, the correct label for an instance in X (i) may exist in its candidate label set L (i) or even in another candidate label set L (j) of another group, where i = j. The instance in X (i) will have a null label if its correct label doesn't exist in any candidate label set L in G. In a nutshell, the input of this problem is a set of groups which consist of instances and labels G =</p><formula xml:id="formula_1">{X (i) , L (i) } K i=1 .</formula><p>The output is the predicted label l ? Y for each instance x ? X . In addition, we assume that some instances and labels repetitively co-occur across groups for the model to learn the association pattern. Moreover, the problem is naturally in a self-supervised and transductive scenario, i.e., there is no train/test split, and the data G is all we have for labeling the instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, we describe our method for GPLL task and elaborate on each part with details: (b) In Dual Bipartite Graph, we take instances and labels as nodes and construct Within-Group or Cross-Group Link for two bipartite graphs with uncertain links. (c) We propose a Graph Autoencoder to learn the embedding representations of the bipartite graphs and iteratively refine the bipartite link weights. (d) In the final stage, we propose an Instance Label Pooling to predict the correct instance-label link for each instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual Bipartite Graph</head><p>Formally, we define our uncertain bipartite graph as G = {[X , L], M }, G is a weighted graph with instance nodes X and label nodes L. M denotes the uncertain links between instance X and label L with likelihood values. The likelihood of each link refers to whether a label is correct for an instance. We will construct the dual bipartite graph M = [M within , M cross ] with complementary information. M within and M cross will be the edges of the Within-Group Graph and Cross-Group Graph. Within-Group Graph Construction. As shown in <ref type="bibr">Figure 3(b)</ref>, we consider the second and third heuristics described in the third paragraph of the Introduction to estimate the link likelihood within a group to construct the Within-Group Graph. The within-group link weight initialization contains three steps: 1) Given a instance x i ? X and label l j ? L, we represent the within-group link q ij by concatenating instance and label features to form a tuple [x i ; l j ]. 2) We create all possible links within each of group between instances and labels and perform DBSCAN <ref type="bibr" target="#b18">(Sander et al. 1998</ref>) to cluster the links by their link features. We choose the cluster size c ij for each link to describe the cooccurrence frequency of instance-label pairs. The number is the times that x i co-occurs with the label l j in the entire dataset, which assigned as the likelihood of l j being the correct label of x i . 3) We will refine the likelihood by considering the contradictory relation of links within a group.We define the contradictory link for each link q ij : the links with only one shared node (instance x i or label l j ) in the same group. We will refine the likelihood by dividing the total likelihood of the link q ij and its contradictory links. The within-group link weight is defined as follow:</p><formula xml:id="formula_2">w ij = c ij ( u?Ni c iu + v?Nj c vj ) ? c ij<label>(1)</label></formula><p>where N i and N j are the neighbor nodes set for node i and j.</p><p>We acquire all the within-group link weights by calculating all the weights between the instances and labels within the same group and denote the weighted adjacency matrix as M within ? [0, 1] U ?V where U is the number of instances in X and V is the number of labels in L. Cross-Group Graph Construction. Given the withingroup weights M inner and the first heuristic mentioned in the third paragraph of Introduction, we can initialize the cross-group link weight and construct Cross-Group Graph shown in <ref type="figure" target="#fig_0">Figure 3(b)</ref>. Given an instance, we measure the l2-distance for instances and select similar instances by a certain threshold d and define those nodes as homogeneous neighbor node. In addition, the homogeneous neighbors of each instance has their candidate labels, and we link the instance to these cross-group candidate labels as a cross-group link. The likelihood values of cross-group links were initialized in the previous step. We use such within-group link weight to be the likelihood between the instance and the label of its homogeneous node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Autoencoder</head><p>To predict the unknown likelihood of instance-label pairs for uncertain graph, we design a novel graph autoencoder architecture called DB-GAE. The model has the ability to 1) Encode the graph G with heterogeneous within/cross-group links to a low-dimensional embedding space. 2) Dynamically update the instance-label relation while learning a new representation of the instance and label nodes. 3) Predict the link weights between instances and labels by reconstructing the observed links we initialize. Graph Convolution Encoder. Given the node features [X , L] and the link weights [M within , M cross ] initialized in the previous step, we aim to encode such information in node representation for further prediction. The graph convolution model incorporates the neighbor information by propagating the message to form a new representation of a node. We utilized this characteristic to use the link information during propagation to obtain a more representative embedding. For expressing the propagation of within-group links, a single hidden layer GCN is given by</p><formula xml:id="formula_3">H i = f (H i?1 , M within )<label>(2)</label></formula><p>where H 0 = [X , L] and f is a propagation rule. Each layer H i corresponds to the instance and label feature matrix [X , L] and where each row is a feature representation of a node. This operation is similar to a filtering operation in the CNN (LeCun, Bengio, and others 1995), and the features become increasingly more abstract at each consecutive layer. We aggregate the feature representation of each node by its associated neighbors. Moreover, the neighbors were weighted by the within-group weight w ij and transformed by applying the weights W before propagation. To avoid the interference between within-group and cross-group link propagation, we have distinct propagation rules of dual bipartite GCNs for within-group links and cross-group links separately. The propagation rule for within-group and crossgroup can be denoted as:</p><formula xml:id="formula_4">? j?i = w ij W l j ? j ?i = w ij W l j<label>(3)</label></formula><p>where the w ij and w ij are link weights computed from the previous section and j is a cross-group label. W is a learnable parameter. This operation is similar to the spectral rule propagation <ref type="bibr" target="#b9">(Kipf and Welling 2016a)</ref> where the propagation is normalized based on the degree of both i and j. Instead, our propagation is normalized by the link weight of both i and j. We aggregate incoming messages for each of instance from label nodes by accumulating all neighbors N i to represent the node, denoted as:</p><formula xml:id="formula_5">h within i = ?( j?Ni ? j?i ) h cross i = ?( j ?Ni ? j ?i ) (4) h within i</formula><p>is the hidden vector that represents the instance node i by within-group and h cross i is the hidden vector that represents the instance node i by cross-group. To arrive at the final embedding of instance node i and label node j, we apply the concatenate operation over the hidden vector updated from the within-group and cross-group. The model has a non-linear transformation to transform the concatenated representations for each node by dual path GCN to a unified embedding representation. After concatenation, the feature will feed into a dense layer to obtain the final representation, denoted as:</p><formula xml:id="formula_6">u i = ?(W u [h within i ; h cross i ; f i ]) (5) v j = ?(W v [h within j ; h cross j ; n j ])<label>(6)</label></formula><p>?(?) denotes an ReLU activation function. W u and W v are learnable parameters. We use the transformation functions of f i = ?(W f x i + b) and n j = ?(W n l j + b) in our paper. The output of the encoder will be the updated representations [U, V ] for the instances and labels. Graph Attention for Within/Cross-Group Propagation.</p><p>The graph convolution is based on the probability value of in the dual bipartite graph, which is fixed during the graph propagation process. Moreover, we want to continuously update the representations of nodes to predict the link weights by learning from links with uncertainty. Hence, dynamically adjust the propagation weight between instances and labels by considering the features itself is essential. To this end, we can employ some form of attention mechanism <ref type="bibr" target="#b22">(Veli?kovi? et al. 2017)</ref> which actively learn how to propagate the information to optimize our result. To perform the attention on nodes, attention coefficients can be calculated by:</p><p>e ij = a(W a x i , W a l j ) (7) It indicates the importance of label node l j 's features to instance node x i , and W a is its learnable weight matrix, and a is a feed-forward network. We inject the graph structure into the mechanism by performing masked attention which means we compute ? ij for nodes j ? N i , where N i is the neighbor nodes of node i in the graph. To make coefficients easily comparable across different nodes, we normalize them across all choices of j using the softmax function:</p><formula xml:id="formula_7">? ij = exp(e ij ) k?Ni exp(e ik )<label>(8)</label></formula><p>We learn two kinds of link information by graph attention, including the within-group link weight and cross-group link weight. Therefore, the propagation rule in Equation 3 can be extended to:</p><formula xml:id="formula_8">? j?i = ? ij w ij W l j ? j ?i = ? ij w ij W l j<label>(9)</label></formula><p>where ? is the context vector which represents the normalized contribution of label j to instance i. After aggregating the information though the neighbors by summation and apply averaging on the K transformation attention by multihead attention <ref type="bibr" target="#b32">(Zitnik and Leskovec 2017)</ref>, the Equation <ref type="formula">4</ref> becomes:</p><formula xml:id="formula_9">h within i = ?( 1 K K k=1 j?Ni ? k j?i ) (10) h cross i = ?( 1 K K k=1 j ?Ni ? k j ?i )<label>(11)</label></formula><p>Bi-linear Decoder for Link Resolution. To predict the link values between instances and labels, we decode the updated embedding that contains the within-group, cross group, and feature information. In addition, we can use Bi-linear decoder model <ref type="bibr" target="#b11">(Kiros, Salakhutdinov, and Zemel 2014)</ref> to reconstruct links of the bipartite graph by considering the node feature similarity. The reconstruction model is M = ?(U T V ), and likelihood between instance i and label j i? M ij . The learning objective is to reconstruct the weights of the observed links (estimated from the within-group initialization) and predict the weights of unobserved links (crossgroup link). The score function of the decoder is:</p><formula xml:id="formula_10">p(M ij = r) = e u T i Qrvj ? s?R e u T i Qsvj ,<label>(12)</label></formula><p>where Q r is the trainable parameter matrix of shape E ? E, and E is the dimension of hidden representations. r is weighting scale from 0 to 1 which represents the likelihood of the link. The predicted rating is computed as:</p><formula xml:id="formula_11">Mij = g(ui, vj) = E p(M ij =r) [r] = r?R rp(Mij = r) (13)</formula><p>Within Group Reconstruction Loss. To optimize the proposed graph inference networks, we follow the loss function defined in <ref type="bibr" target="#b0">(Berg, Kipf, and Welling 2017)</ref> to minimize the reconstruction loss by negative log likelihood of the predicted likelihood:</p><formula xml:id="formula_12">L = ? i,j;?ij =1 R r=1 I[r = M within ij ] log p(M ij = r),<label>(14)</label></formula><p>where the matrix ? ? 0, 1 is a mask for unobserved links in the within-group matrix M within . We optimize over the observed links to predict the likelihood of the matrixM which contains observed links and unobserved links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Prediction by Instance Label Pooling</head><p>To infer the label of each instance, we use the predicted link weightM ij generated by DB-GAE. As shown in <ref type="figure" target="#fig_0">Figure3(d)</ref>, given an instance, we aggregate all the weights of the links it connects by different classes. For within-group link weight, we directly use the predicted link weight. For cross-group link weight, we multiply the predicted link weight with the cosine similarity of the instance i's feature and its homogeneous neighbor's feature. That is because the link weight should be lower if the feature similarity is low. The weight of a class o being the label of instance i is calculated by:</p><formula xml:id="formula_13">W i o = j?o;?ij =1 ?(M ij ) + j?o;? ij =1 ?(M ij f i ? f i ||f i ||||f i || )<label>(15)</label></formula><p>where the ? represents a ReLU over a sigmoid function. ? ? 0, 1 is a mask for unobserved links in the cross group matrix M cross . We aggregate the link weight by the same class and pool the class with the maximum weighted score p i = argmax o?Y (W i o ) as the predicted label of instance x i . If the score is equal to 0, it means there is no prediction, we predict it as a null.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Partial Label Learning Datasets</head><p>We evaluate the performance of our model for the automatic face naming problem on two real-world datasets: MPII-MD <ref type="bibr" target="#b17">(Rohrbach et al. 2017)</ref> and M-VAD <ref type="bibr" target="#b16">(Pini et al. 2019)</ref>. MPII-MD dataset in GPLL setting, which contains only group-level supervision with cross group labels and null labels. The M-VAD dataset is constructed for PLL setting, which has less ambiguity but larger-scale data. MPII-MD: MPII Movie Description Dataset. The MPII-MD dataset consists of face images and ambiguous labels automatically extracted using the screenplays provided in 13 movies with 806 different faces and 181 possible names from 558 image-caption pairs. We select the frames from the data with detectable faces and corresponding captions. The percentage of the faces with a null label in the dataset are 21%. M-VAD: Montreal Video Annotation Dataset. In M-VAD Names dataset contains fully annotated faces in images with names in the captions from 55 movies. It consists of 222,58 detected faces and 591 possible names from 17,533 imagecaption pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Data Distribution over Ambiguity Ratio</head><p>Data Distribution over Ambiguity Ratio. To explore the data difficulty of the datasets, we define ambiguity ratio and show the histogram over different levels of ambiguity, as shown in <ref type="figure">Figure 4</ref>. The metric refers to a fraction of all possible instance-label links that are incorrect, the ambiguity ratio of a label with class c ? Y is defined by:</p><formula xml:id="formula_14">R o = 1 ? K i=1 |s (i) t | K i=1 |s (i) t | + |s (i) f |<label>(16)</label></formula><p>where i is the group index. s (i) t is the set of correct instancelabel links in the group g (i) with class o. s <ref type="bibr">(i)</ref> f is the set of wrong links in group g (i) which connect to instance node or label node with class o.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>Cluster Voting <ref type="bibr" target="#b18">(Sander et al. 1998)</ref>: For each instance, the method selects candidate labels from the same cluster. The correct label is determined by majority voting over all the candidates. To cluster faces by visual features for face naming datasets, we apply DBSCAN (with = 1, n = 2). Pair Clustering <ref type="bibr" target="#b18">(Sander et al. 1998)</ref>: As in the Within-Group Graph Construction, we perform pair clustering to estimate the likelihood of a link. Given an instance, we find its link with the largest cluster size and pick its label as the prediction, We also perform DBSCAN (with = 1, n = 2) for pair clustering. IPAL <ref type="bibr" target="#b30">(Zhang and Yu 2015)</ref>: IPAL is an instance-based PLL model and disambiguates candidate labels by an iterative label propagation procedure. PL-LEAF <ref type="bibr" target="#b31">(Zhang, Zhou, and Liu 2016)</ref>: PL-LEAF is a feature-aware approach which learns the manifold structure of feature space and performs regularized multi-output regression over the generated labeling confidences. SURE <ref type="bibr" target="#b3">(Feng and An 2019)</ref>: SURE proposes a unified formulation with the maximum infinity norm regularization to train the desired model and perform pseudo-labeling jointly. PL-AGGD <ref type="bibr" target="#b23">(Wang, Li, and Zhang 2019)</ref>: PL-AGGD proposes a unified framework which jointly optimizes the ground-truth labeling confidences, similarity graph, and model parameters to achieve generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>For all runs, we use pre-trained FaceNet <ref type="bibr" target="#b20">(Schroff, Kalenichenko, and Philbin 2015)</ref> to extract the visual embedding for detected faces with 512 dimensions from the images and apply a threshold d = 1 suggested in the paper <ref type="bibr" target="#b20">(Schroff, Kalenichenko, and Philbin 2015)</ref> for l 2 distance to determine two faces are the same person. We encode names using one-hot vectors. In DB-GAE, we use the Adam optimizer <ref type="bibr" target="#b8">(Kingma and Ba 2014)</ref> with a learning rate of 0.001. The layer sizes of graph convolution (with ReLU) is 1000 and 100 for the dense layer. We run 1000 epochs on both datasets with the runtime of 10min on MPII-MD dataset and 5hr 10min on the M-VAD dataset on CPU. Since M-VAD dataset has sufficient training data, we perform 10-fold cross-validation on the partial label learning method with 9:1 train/test split. For our method, we only use the same testing data (10% data) because our method does not require additional training data. More details of the model architecture, parameter list, and analysis of the time complexity of the model will be included in the arXiv version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Performance Comparison</head><p>From <ref type="table" target="#tab_0">Table 1</ref>, our proposed method outperforms the baselines in both datasets with a significant margin by 0.158 absolute improvements of F1-score for GPLL dataset and 3.8% absolute improvement of accuracy in PLL dataset. In the experimental results on MPII-MD dataset, the best baseline reach about 0.61 on F1-score and 49% on accuracy, and proposed model can achieve the best performance with over 0.76 on F1-score and 73% on accuracy. The significant improvements show that our model is powerful enough to deal with generalized ambiguity, and the baseline methods will fail to disambiguate distractors. As shown in the results on M-VAD dataset, PLL methods performed much better when using sufficient training data for PLL setting. Our model is able to achieve the best performance among the methods with only one-tenth of data in the transductive setting. This result shows its ability to resolve the extreme ambiguity caused by data sparsity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition Controlled Experiments</head><p>In addition to overall performance results, we show the comparison with baselines for different levels of data difficulty. The effect of ambiguity ratio on performance. In the <ref type="figure" target="#fig_1">Figure 5(a)</ref>, the baselines can reach comparable performances with the proposed model when the ambiguity is not severe (ambiguity ratio &lt; 0.4). Their performances will drop a lot when they meet high levels of data ambiguity (ambiguity ratio &gt; 0.4). In less ambiguity dataset (PLL setting) <ref type="figure" target="#fig_1">Figure 5(b)</ref>, we can see that the proposed method is comparable with the state-of-the-art PPL method without addtional training data. The effect of ground-truth frequency on performance. The ground-truth frequency is the number of face-name pairs with the same class co-occur in the same group throughout the dataset. In <ref type="figure" target="#fig_2">Figure 6</ref> (a), we can see that our model performs better than other methods in general. Almost all of the methods exhibit a slight drop in the groundtruth frequency of 14, because the average ambiguity ratio at frequency interval is higher than the middle ambiguity ratio of frequency 7. In <ref type="figure" target="#fig_2">Figure 6(b)</ref>, the approach performs better than other methods. The accuracy of the model will con-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>To reveal the contribution of each component, we test the performance of DB-GAE by removing different parts: Graph Autoencoder, Dual Bipartite Graph Architecture (apply GAE and set all weights for instance label links to be averaged <ref type="bibr" target="#b7">(H?llermeier and Beringer 2006;</ref><ref type="bibr" target="#b30">Zhang and Yu 2015)</ref>), Cross-Group Link, and Graph Attention. The comparison results of ablation study are shown in the <ref type="table" target="#tab_1">Table 2</ref>, we can see that the Graph Autoencoder contributes the most of performance improvement. The GAE (w/o Dual Bipartite Graph Architecture) encounters an accuracy drop in the MPII-MD dataset because it is unable to deal with null distractors since the weights fed into the GAE are all links with a high likelihood. The consideration of Cross-Group Link will help the model to deal with the group-level ambiguity. The F1-score of DB-GAE (w/o Cross-Group Link) drops obviously on MPII-MD. The improvement of DB-GAE over DB-GAE (w/o Graph Attention) verifies our hypothesis that graph attention can help to capture a better representation of the graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head><p>As shown in <ref type="figure" target="#fig_3">Figure 7</ref>, compared to the best baseline: PL-LEAF in our MPII-MD experiment, our model can correctly predict the null labels and cross-group labels even when the number of distractors is large within a group. Also, the performance for within-group label prediction is also better since the model incorporates cross-group knowledge to resolve the within-group ambiguity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure Cases</head><p>From <ref type="figure" target="#fig_4">Figure 8</ref>, we can conclude that 1) The visual recognition rate may affect the within-group linking performance. In the first example, didn't link to , which was limited by the feature representation. 2) Also, it will affect the cross-group linking due to the failure of finding similar faces.</p><p>should link to the name to find its correct label but the model can't find as similar faces.</p><p>3) The current model is based on correlation and thus lacks reasoning ability, for example, we humans may rule out other faces and predict the correct link, but our method fails. For example, when predicting , since we know and were linked, we can infer is more likely to be .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We introduced the General Partial Label Learning (GPLL) problem, which is more realistic and general than the traditional PLL. The proposed approach DB-GAE was de-signed to tackle the challenges of GPLL by disambiguating the within-/cross-group instance-label links with richer contextual graph representations. We contributed two GPLL benchmarks on automatic face naming tasks. We found that DB-GAE outperformed the best baseline with absolute 0.159 F1-score and 24.8% accuracy. Further analysis shows the robustness of DB-GAE in generalized ambiguity scenarios and the effect of various ambiguity levels. Moving forward, we are going to frame more tasks into GPLL such as cross-domain co-reference resolution in NLP, and push the envelope of DB-GAE in other fields.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The framework of the proposed method can be demonstrated as four parts: (a) Problem Formulation with Instance-Label Groups. (b) Dual Bipartite Graph. (c) Graph Autoencoder. (d) Instance Label Pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>F1-score and accuracy curves versus ambiguity ratios on MPII-MD and M-VAD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>F1-score and accuracy curves versus ground-truth frequency on MPII-MD and M-VAD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative examples from MPII-MD dataset. The blue/grey box represents the correct prediction of a name/null label. Red box represents the wrong prediction. The orange line visualize the predicted link between face and name.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Failure cases from MPII-MD dataset. The dashed line represents the ground-truth link between face and name which the model misses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance Comparison on MPII-MD and M-VAD</figDesc><table><row><cell></cell><cell cols="2">MPII-MD</cell><cell>M-VAD</cell></row><row><cell>Method</cell><cell cols="3">F1-score Accuracy Accuracy</cell></row><row><cell>Pair Clustering</cell><cell>0.539</cell><cell>45.0 %</cell><cell>78.2 %</cell></row><row><cell>Cluster Voting</cell><cell>0.558</cell><cell>46.5 %</cell><cell>78.9 %</cell></row><row><cell>SURE</cell><cell>0.605</cell><cell>48.7 %</cell><cell>85.7 %</cell></row><row><cell>IPAL</cell><cell>0.608</cell><cell>48.3 %</cell><cell>86.1 %</cell></row><row><cell>PL-LEAF</cell><cell>0.610</cell><cell>48.6 %</cell><cell>86.3 %</cell></row><row><cell>PL-AGGD</cell><cell>0.598</cell><cell>47.6 %</cell><cell>86.5 %</cell></row><row><cell>Our method</cell><cell>0.768</cell><cell>76.5 %</cell><cell>90.3 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Ablation Study of Proposed Method DB-GAE</cell></row><row><cell></cell><cell cols="2">MPII-MD</cell><cell>M-VAD</cell></row><row><cell>Method</cell><cell cols="3">F1-score Accuracy Accuracy</cell></row><row><cell>DB-GAE</cell><cell>0.768</cell><cell>76.5%</cell><cell>90.3%</cell></row><row><cell>w/o Graph Autoencoder</cell><cell>0.614</cell><cell>62.9%</cell><cell>81.6%</cell></row><row><cell>w/o Dual Bipartite Graph</cell><cell>0.724</cell><cell>68.4 %</cell><cell>88.3 %</cell></row><row><cell>w/o Cross-Group Link</cell><cell>0.730</cell><cell>73.9 %</cell><cell>89.2 %</cell></row><row><cell>w/o Graph Attention</cell><cell>0.743</cell><cell>76.4 %</cell><cell>87.3 %</cell></row><row><cell cols="4">tinually increase even beyond 95% if the correct face-name</cell></row><row><cell cols="2">co-occur frequently enough.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning from partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1501" to="1536" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Partial label learning with selfguided retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03045</idno>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Gong, C</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A regularization approach for instance-based superset label learning</title>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="967" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>eeding of 2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast multiinstance multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning from ambiguously labeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>H?llermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="419" to="439" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Valuable information in early sales proxies: The use of google search ranks in portfolio optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kupfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learnability of the superset label learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1629" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classification with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="551" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">M-vad names: a dataset for video captioning with naming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bolelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="14007" to="14027" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating descriptions with grounded and co-referenced people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Density-based clustering in spatial databases: The algorithm gdbscan and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="194" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Confidence-rated discriminative partial label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive graph guided disambiguation for partial label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards enabling binary decomposition for partial label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2868" to="2874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and autoannotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3460" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Partial multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Partial label learning via label enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Maximum margin partial label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="96" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Solving the partial label learning problem: An instance-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Partial label learning via feature-aware disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
