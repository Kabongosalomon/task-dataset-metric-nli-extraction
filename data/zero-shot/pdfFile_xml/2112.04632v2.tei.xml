<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Glimpse-based Decoder for Detection with Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
							<email>zhe.chen1@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>jing.zhang1@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Glimpse-based Decoder for Detection with Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although detection with Transformer (DETR) is increasingly popular, its global attention modeling requires an extremely long training period to optimize and achieve promising detection performance. Alternative to existing studies that mainly develop advanced feature or embedding designs to tackle the training issue, we point out that the Region-of-Interest (RoI) based detection refinement can easily help mitigate the difficulty of training for DETR methods. Based on this, we introduce a novel REcurrent Glimpse-based decOder (REGO) in this paper. In particular, the REGO employs a multi-stage recurrent processing structure to help the attention of DETR gradually focus on foreground objects more accurately. In each processing stage, visual features are extracted as glimpse features from RoIs with enlarged bounding box areas of detection results from the previous stage. Then, a glimpse-based decoder is introduced to provide refined detection results based on both the glimpse features and the attention modeling outputs of the previous stage. In practice, REGO can be easily embedded in representative DETR variants while maintaining their fully end-to-end training and inference pipelines. In particular, REGO helps Deformable DETR achieve 44.8 AP on the MSCOCO dataset with only 36 training epochs, compared with the first DETR and the Deformable DETR that require 500 and 50 epochs to achieve comparable performance, respectively. Experiments also show that REGO consistently boosts the performance of different DETR detectors by up to 7% relative gain at the same setting of 50 training epochs. Code is available via https://github.com/zhechen/Deformable-DETR-REGO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection aims to locate and recognize foreground objects from images. In recent years, deep learning has made rapid development in object detection. With deep convolutional neural networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref>, various powerful detectors have been developed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>. <ref type="bibr">Stage</ref>   <ref type="figure">Figure 1</ref>. Concept of the proposed recurrent glimpse-based decoder (REGO) for augmenting the training of attention modeling in Detection with Transformer (DETR). Using original DETR results, the REGO performs a multi-stage Region-of-Interest (RoI) based attention modeling refinement procedure by gradually focusing on more accurate areas. In each stage, glimpse features are extracted and a glimpse-based decoder is employed to provide refined detection outputs based on both the glimpse features and the attention modeling output of the previous stage. The REGO maintains the fully end-to-end pipeline of different DETR methods and can improve their training performance promisingly.</p><p>In general, modern detectors produce redundant results and require Non-Maximum Suppression (NMS) to reduce the redundancy in detection. Different from this popular paradigm, Detection with Transformer (DETR) <ref type="bibr" target="#b2">[3]</ref> applied Transformer <ref type="bibr" target="#b39">[40]</ref> for detection and is the first fully end-toend detector that avoids the need for NMS. In particular, a Transformer is a powerful attention-based encoder-decoder pipeline for translating an input sequence to the target sequence. By formulating the detection task as a direct set prediction problem, the authors of DETR managed to translate visual features into a set of detection results based on the global attention modeling of a Transformer. Despite benefits, the DETR suffers from a difficult training problem. Using MS COCO dataset <ref type="bibr" target="#b23">[24]</ref>, the original DETR requires 500 training epochs to obtain promising performance, while the other popular detectors like FPN <ref type="bibr" target="#b21">[22]</ref> only require less than 36 epochs to get similar results. Even using a machine with 8 powerful V100 GPUs, a DETR detector costs more than 10 days to finish the training <ref type="bibr" target="#b2">[3]</ref>.</p><p>By addressing the training problem, researchers found that the lack of effective locality modeling could affect the training of attention modeling in DETR methods. For example, Zhu et al. <ref type="bibr" target="#b48">[49]</ref> analyzed that the Transformer would distribute almost uniform attentional weights to all features initially. It is then necessary to apply long training epochs to make the Transformer learn to focus on sparse and meaningful local areas. To tackle this issue, researchers developed advanced multi-scale feature encoding <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> and object embedding designs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42]</ref> to improve the locality modeling in Transformer before final detection, so that the attention of Transformer can be trained more efficiently and the detection results can be improved properly. Different from existing methods, we propose that the training of the attention modeling in DETR can be easily improved based on Region-of-Interest (RoI). More specifically, considering local areas around bounding boxes detected by DETR as RoIs that may contain objects, we can directly restrict the attention of DETR by only focusing on these RoIs. Therefore, modeling the features within RoIs can help introduce more locality inductive biases in DETR and thus improve its training efficiency effectively.</p><p>In fact, researchers have demonstrated that gradual refinements according to RoIs can boost training and detection performance for two-stage <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref> and multi-stage detectors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>. Nevertheless, these multi-stage detection methods mainly follow RCNN detection methodology <ref type="bibr" target="#b15">[16]</ref> for training and inference which still requires NMS. To our best knowledge, the RoI-based refinement for attention modeling in DETR has rarely been studied.</p><p>To develop a proper RoI-based DETR refinement method, we take inspiration from the glimpse mechanism as studied in the work <ref type="bibr" target="#b28">[29]</ref> which extracted features from a few selected local areas of different scales as glimpses and applied a recurrent network to encode the glimpse information. Similar to DETR, this glimpse method also formulates the visual understanding as a sequence translation task and has proven to be effective for image recognition. We follow this mechanism and propose a novel recurrent glimpse-based decoder (REGO) module to help existing DETR methods relieve training difficulty and improve detection performance.</p><p>The proposed REGO module refines DETR with multistage processing. Taking detection and attention modeling outputs of the original DETR as initial states, each stage of REGO first extracts glimpse features from local areas surrounding the detected bounding boxes. Then, a Transformer decoder is employed to translate the glimpse features based on previous attention modeling outputs into augmented attention modeling outputs and refined detection results. For early stages, we extract glimpse features from the local areas at larger scales w.r.t. the detected bounding box areas, enabling the incorporation of rich contexts to boost the detection that can possibly be unreliable in early stages.</p><p>After multiple stages of processing, the REGO performs a coarse-to-fine RoI-based refinement which is shown to be effective for improving the training of different DETR methods.</p><p>To sum up, the contributions of this paper are three-fold:</p><p>? We proposed a novel RoI-based refinement module that can effectively tackle the difficult training problem for the attention modeling in DETR and improve detection performance.</p><p>? The REGO is easy-to-implement and is a complementary module that can be embedded in different DETR variants. It keeps the fully end-to-end detection pipeline of DETR while accelerating convergence and improving detection performance for different DETR methods effectively.</p><p>? Extensive experiments show that the REGO helps deliver promising performance using only 36 training epochs with a DETR pipeline, which is 13? shorter than the first DETR method. Moreover, REGO also consistently boosts the performance of different DETR methods by up to 7% relative gain using the same 50 training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Detection Modern detectors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref> generally make dense detection of objects appeared on the images. For example, the widely used regional proposal network (RPN) <ref type="bibr" target="#b31">[32]</ref> scans every location on the feature map of the backbone like ResNet <ref type="bibr" target="#b17">[18]</ref> and generates proposal windows that may cover foreground objects. This produces plenty of redundant proposals, e.g., an object can be covered by different but highly overlapped proposals, which is disadvantageous to make sparse predictions. To alleviate this problem, one-stage methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref> develop augmented networks to ensure that they can directly provide compact detection results. Two-stage methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref> attempt to refine the proposal bounding boxes based on the features extracted with RoIPooling <ref type="bibr" target="#b14">[15]</ref> or RoIAlign <ref type="bibr" target="#b16">[17]</ref>. Nevertheless, both one-stage and two-stage detectors rely on the hand-designed NMS procedure to remove redundancy, which is heuristic and separated from the end-to-end learning pipeline, leading to many inaccurate predictions remained after NMS. Alternatively, recently introduced detection with Transformer <ref type="bibr" target="#b2">[3]</ref> can provide a set of object detection results without requiring NMS. However, DETR suffers from frustratingly difficult training problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvement of Transformer in Computer Vision</head><p>The training difficulty of DETR is a common issue in Transformer-based computer vision methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>. By addressing the training problem, many researchers found that locality modeling is important for improving the training of attention in DETR <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>. Some methods <ref type="bibr" target="#b29">[30]</ref> developed advanced local window-based method for improving efficiency. In object detection, researchers mainly develop advanced feature encoding and embedding designs to help tackle this problem. The Deformable DETR <ref type="bibr" target="#b48">[49]</ref> applied deformable operations <ref type="bibr" target="#b8">[9]</ref> to better focus on a few local areas at different scales in the Transformer. The method SMCA <ref type="bibr" target="#b11">[12]</ref> introduced multi-scale co-attention to improve DETR with refined local representations. In addition, other studies like Conditional DETR <ref type="bibr" target="#b27">[28]</ref> and Anchor DETR <ref type="bibr" target="#b41">[42]</ref> tend to improve the spatial embedding in Transformer to help accelerate training. These two methods enhance the locality modeling of Transformer by making attention focus on potentially valuable areas on the image learned with positional embeddings. Unlike these methods that require careful designs, we argue that the RoIs which naturally correspond to local areas can also improve the training of attention modeling in DETR. A more related method is the iterative refinement used in Deformable DETR <ref type="bibr" target="#b48">[49]</ref>. We note that this method does not use RoIs and it mainly improves performance by re-using all the regression outputs of DETR. Our RoI method can improve attention modeling and is orthogonal to this method. Experiments show that the cooperation of this method and our REGO achieves state-of-the-art performance.</p><p>RoI-based Improvement for Object Detection Researchers have proved that the detection results can be progressively improved by refining classification and localization w.r.t. RoIs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45]</ref>. For example, MR CNN <ref type="bibr" target="#b12">[13]</ref> introduced an iterative procedure to alternate between scoring and bounding box refinement based on RoIs. The CascadeRCNN <ref type="bibr" target="#b1">[2]</ref> repeated the RoI-based detection head of the Faster RCNN <ref type="bibr" target="#b31">[32]</ref> several times for refinement. Despite effectiveness, such type of RoI-based refinement methodology can not be directly applied to the fully end-toend pipeline of DETR because they rely on different optimization goals and still require NMS. More recently, some methods, like Efficient DETR <ref type="bibr" target="#b45">[46]</ref>, TSP-RCNN <ref type="bibr" target="#b34">[35]</ref>, and SparseRCNN <ref type="bibr" target="#b33">[34]</ref>, also uses RoIs to achieve improved performance with a Transformer and can also avoid the NMS. However, we argue that these methods are still based on the typical two-stage detection pipeline like Faster RCNN <ref type="bibr" target="#b31">[32]</ref> and they only apply Transformer mainly to approximate NMS. These methods do not directly tackle the difficult training problem for attention modeling in DETR.</p><p>In summary, exploring end-to-end RoI-based refinement for improving the training of attention modeling in DETR remains a missing part in literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary</head><p>Here, we briefly review the DETR. More details can be found at <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40]</ref>. Multi-head Attention Multi-head attention deals with query, key, and value inputs. It correlates query and key and then aggregates values according to the correlation results. Following <ref type="bibr" target="#b39">[40]</ref>, the multi-head attention splits features into different 'heads' and performs self-attention or cross-attention in each head. The features of different heads will be concatenated together and fed into a linear projection to obtain the final output. Formally, to help describe a multi-head attention, we suppose that X q ? R Lq?C is a query tensor where L q refers to its sequence length and C is its feature dimension. We follow the formulations of DETR <ref type="bibr" target="#b2">[3]</ref> and unify the key and value into the same tensor: X kv ? R L kv ?C which is the the key-value sequence of length L kv . The multi-head attention, abbreviated as "A", can be formulated as:</p><formula xml:id="formula_0">A(X q , X kv ) = W A A(X 1 q , X 1 kv ), . . . , A(X M q , X M kv ) , (1) where W A ? R C?C is a trainable linear projection matrix,</formula><p>M is the number of heads, and [. . .] refers to concatenation operation. The X i q ? R Lq?C and X i kv ? R L kv ?C are query and key-value tensors of the i-th head (i = 1, . . . M ), respectively, where C = C M . In each head, the following operation is performed:</p><formula xml:id="formula_1">A(X i q , X i kv ) = A i qkv X i kv ,<label>(2)</label></formula><p>where A i qkv represents the attentional weights:</p><formula xml:id="formula_2">A i qkv = Sof tmax( X i q (X i kv ) T ? C</formula><p>). DETR Pipeline The DETR applies an encoder-decoder pipeline to translate the input features into a set of detection results. During training, Hungarian matching <ref type="bibr" target="#b19">[20]</ref> is performed to assign the detection results with the most matched ground-truths. The encoder-decoder consists of a visual feature encoding phase and a detection result decoding phase. The feature encoding investigates the relations between visual features from different locations. It applies several encoding layers to augment the encoded representation. We suppose the backbone network extracts features into: X ? R H?W ?C where H, W represents the height and width, respectively, and C is the feature dimension. In each encoding layer, a multi-head self-attention module is employed, that is, query, key, and value tensors are the same: X q = X kv . The input feature X also integrates a positional embedding to encode position information. Suppose the output of the encoding phase is H enc ? R HW ?C . Then, detection decoding phase perform detection based on H enc . It begins from object query embeddings E box ? R N d ?C and applies cross-attention as described in Eq. <ref type="formula">(1)</ref>   <ref type="figure">Figure 2</ref>. The overview of the REGO (top row) and the detailed structure of the i-th processing stage in REGO.</p><p>respectively, where N c represents the number of object categories. To this end, we have:</p><formula xml:id="formula_3">O cls = F cls (H dec ) O box = F box (H dec ) ,<label>(3)</label></formula><p>where F cls and F box are functions that map the decoded feature H dec into the desired outputs respectively. The two functions are implemented based on linear projection and multi-layer perception, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Recurrent Glimpse-based Decoder</head><p>Different from existing methods, we propose a recurrent glimpse-based decoders (REGO) to perform RoI-based detection refinement method for improving attention modeling in DETR. The REGO consists of two major components. The first one is a multi-stage recurrent processing structure that progressively augments attention modeling outputs and improves the detection of DETR, and the second one is the glimpse-based decoder that is used in each stage to explicitly perform the refinement. <ref type="figure">Figure 2</ref> shows the detailed pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-stage Recurrent Processing</head><p>Built upon detection results and attention decoding outputs from original DETR, we propose a recurrent processing pipeline to help the DETR gradually attend to more meaningful areas to avoid long training periods for optimizing the attention of DETR. In general, the proposed recurrent processing structure is a multi-stage pipeline. In each stage, previously detected bounding boxes are used to obtain RoIs for extracting glimpse features. Then, glimpse features are translated according to previous attention decoding outputs into refined attention decoding outputs for describing detected objects. The refined attention decoding outputs can provide improved detection results. Thus, for the i-th processing stage, we propose to detect objects according to:</p><formula xml:id="formula_4">O cls (i) = F cls (H dec (i)) O box (i) = F box (H dec (i)) + O box (i ? 1) ,<label>(4)</label></formula><p>where O cls/box (i) represent the classification and bounding box regression outputs of the i-th recurrent processing stage, respectively, and H dec (i) represents the refined attention of this stage after decoding. Then, to obtain a proper representation of H dec (i), we use the following formulation:</p><formula xml:id="formula_5">H dec (i) = [H g (i), H dec (i ? 1)],<label>(5)</label></formula><p>where H g (i) is the translated glimpse features according to H dec (i ? 1), and [. . .] refers to concatenation operation. Reusing H dec (i ? 1) in Eq. (5) not only improves the attention of previous stages, but also help maintain consistency in the produced detection results across different stages, which could help reduce the variations in the Hungarian matching loss in later stages The study <ref type="bibr" target="#b34">[35]</ref> has proven that reducing the randomness of the matching loss is beneficial for accelerating convergence. The calculation of translated glimpse features H g (i) will be discussed with more details in the next section. For the first stage where i = 0, we use the outputs of original DETR, as described in Eq. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Glimpse-based Decoder</head><p>During the i-th processing stage, the glimpse-based decoder collects visual features from areas around the detected bounding boxes O box (i?1) from the previous stage. It then performs cross-attention to model the relations between the collected features and previous attention outputs and compute translated glimpse features H g (i) of current stage.</p><p>In particular, we denote the extracted visual features as V (i) for the i-th stage, terming as the glimpse feature. Then, we translate it according to the previous attention outputs into a refined attention modeling outputs for detection. Multi-head cross-attention is applied to fulfill the translation, i.e.,</p><formula xml:id="formula_6">H g (i) = A(V (i), H dec (i ? 1)).<label>(6)</label></formula><p>Note that we use the attention outputs from the last layer of the decoder in original DETR to define H dec (0). It is also worth mentioning that either the V (i) or the H dec (i ? 1) can be used as the query in A. Both settings can correlate glimpse features with previous attention outputs properly and can all improve the training of DETR. We simply found that above formulation achieves 0.5 point higher in AP on COCO dataset <ref type="bibr" target="#b23">[24]</ref>. To extract the glimpse features V (i), we perform the following operation based on O box (i ? 1):</p><formula xml:id="formula_7">V (i) = f ext X, R O box (i ? 1), ?(i) ,<label>(7)</label></formula><p>where the function f ext represents the feature extraction operation, R represents the RoI computation, and ?(i) a scalar factor. In particular, the function R computes RoIs by enlarging the areas of bounding boxes detected by O box (i ? 1) with a factor of ?. Then, we use the RoIAlign <ref type="bibr" target="#b16">[17]</ref> technique to implement f ext . The symbol X here represents the features obtained with the backbone network.</p><p>Since the original detection results could be unreliable at first, we tend to extract glimpse features from a larger area around each detection result for refinement in early stages, so that contexts can be incorporated and target objects can be properly captured within the glimpse areas. In later processing stages, we gradually narrow the area for extracting glimpse features to achieve more precise detection with more local details. In other words, the ?(i) in REGO starts from a large number and then decreases its value for later stages of the REGO. The detailed setting of ?(i) can be found in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>The proposed REGO is a plug-and-play module for different DETR methods. It only has two major hyperparameters, i.e. the number of recurrent stages and the enlarging ratios ? of each stage. To reduce the manual tuning efforts, we unify the two hyperparameters into a single one. More specifically, we constrain that the last recurrent stage has an enlarging ratio equals to 1. Then, when we add a new recurrent stage before the last stage, we increase the enlarging ratio by 1 for the added stage. In other words, if we use 3 recurrent stage, then ?(3), ?(2), ?(1) = 3, 2, 1, respectively. Therefore, we only need to investigate the influence of number of recurrent stages. In addition, we follow the original DETR and apply auxiliary losses to enhance the training of intermediate outputs of the glimpse-based decoders and apply LayerNorm <ref type="bibr" target="#b0">[1]</ref> to help regularize the decoded glimpse representation H dec (i).</p><p>For each recurrent stage of the REGO, we use the decoder architecture of the original DETR for glimpse feature translation, but we do not use encoders and only use 2 decoding layers for a decoder. In decoders, the selfattention of encoders brings marginal benefits but consumes more computational resources, e.g. for REGO-DeformabelDETR-R50, adding the self-attention layers for all stages only improves AP, AP 50 , and AP 75 by 0.1, -0.1, 0.2, respectively, while introducing around 4 more GFLOPs and 9M more parameters. Without encoders, the complexity of the decoder in REGO is much smaller than the decoder used in the original DETR methods. The complexity analysis is presented in the experiment section. Besides the number of stages, we present other implementation details as follows. Firstly, we follow the default settings of the RoIAlign <ref type="bibr" target="#b16">[17]</ref> and uses a 7 by 7 window for feature extraction. In addition, when extracting glimpse features, we attempt to use features from different levels of backbone in both multi-scale and single-scale DETR methods, but note that we do not use the FPN <ref type="bibr" target="#b21">[22]</ref> to save costs. Also, the number of RoIs depends on the output of DETR and the number of stages. We will present more details in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setup</head><p>We follow existing DETR methods <ref type="bibr" target="#b2">[3]</ref> and perform evaluation using the MS COCO <ref type="bibr" target="#b23">[24]</ref> dataset which has 118k training images and 5k validation images. We follow the MS COCO protocol and report the performance using the evaluation metrics of average precision (AP), AP at 0.5, AP at 0.75, and AP for small, medium, and large objects. The validation set is mainly used for evaluation.</p><p>We apply our method on the original DETR <ref type="bibr" target="#b2">[3]</ref> and Deformable DETR <ref type="bibr" target="#b48">[49]</ref> using their released codes. For training, we follow the original settings of the released codes for fair comparison, except that we also perform experiments with much fewer training epochs. For example, the original DETR detectors adopt 500 or 50 training epochs, while we mainly evaluate our method with 50 or 36 training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance Evaluation</head><p>In this section, we perform comprehensive comparison between the current DETR methods and our method. <ref type="table">Ta</ref>  <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b48">49]</ref>. * * Improve with iterative box refinement and two-stage processing. ? Reproduced using released code. ble 1 shows the overall results on MS COCO val dataset.</p><p>In particular, we thoroughly investigate the performance of applying REGO on different DETR methods using different backbone networks and different training epochs.</p><p>Comparison with different DETR methods We have applied our proposed REGO on two major DETR detectors for evaluation. This include the vanilla DETR <ref type="bibr" target="#b2">[3]</ref> method improved with 300 queries, reference points, and focal loss as described by <ref type="bibr" target="#b48">[49]</ref> and the Deformable DETR <ref type="bibr" target="#b48">[49]</ref>. We also presented the reported performance of RCNN-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> and other DETR variants <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46]</ref>. From the results in    est AP among many state-of-the-art object detectors. Convergence Analysis We further study the impact of REGO on actual convergence. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the detailed convergence curves of the Deformable DETR and the Deformable DETR with REGO. It shows that the REGO effectively speeds up the convergence and boosts the model performance promisingly comparing to the baseline. In particular, REGO helps achieve comparable performance with the baseline using only 30 epochs, i.e., 40% less than the complete 50 training epochs used in the baseline. Comparing to the first DETR which requires 500 epochs, the REGO can help reduce about 94% of the total training period. Complexity Analysis The extra computational complexity brought by the REGO is around 17 GFLOPs, which is only around 10% of the complexity of a Deformable DETR-R50 model while bringing around 28% acceleration in training (36 epochs v.s. 50 epochs). Furthermore, the complexity of our method remains the same when using larger and deeper backbone networks like R101 and X101 because of the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>Analysis of Different REGO Stages We first present the detection performance of applying different numbers of REGO stages in <ref type="table" target="#tab_4">Table 2</ref>. The evaluated stages range from 1 to 4 where the glimpse scale in each stage varies accordingly as described in Section 4.3. We also present the baseline Deformable DETR result as reported in the paper <ref type="bibr" target="#b48">[49]</ref>.</p><p>The results show that the REGO with different numbers of stages improves the baseline performance greatly. A single processing stage can increase the mAP by more than 1 point. Applying more stages leads to further improvement. The performance of 3 and 4 stages is the best among compared settings. Although REGO with 4 stages achieves favorable performance, its improvement over the setting of 3 stages is marginal, implying that adding more than 3 processing stages in REGO could result in diminished benefits. We also disentangle the enlarging ratio from the 3-stage setting, i.e. making ?=1,1,1, and only obtain 45.3 in mAP, showing that the glimpse design is useful. We also study the quality of detection results from different stages in a 3-stage REGO module. A histogram chart of the numbers of correct detection results at different settings of Intersect-over-Union (IoU) w.r.t. ground-truths is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. A detection result is correct only if its IoU to a ground-truth is higher than 0.5 and its predicted label aligns with the ground-truth. In addition, the numbers of total detection results for different stages are similar, i.e., around 30k boxes. Therefore, with the similar amount of correct detection results, the chart shows that all the REGO stages (red bars) help produce more accurate detection results than the baseline (blue bars) and their counterparts with fewer stages (yellow and green bars) for the right-most two groups of detection results. For example, the results of REGO with 3 stages contain more correct detection results whose IoU scores w.r.t. ground-truths are higher than 0.9. These results demonstrates that REGO with more stages continues to refine the detection by focusing on objects in the coarseto-fine ROIs and learning better feature representations. <ref type="table" target="#tab_5">Table 3</ref> shows the performance comparison of using different scales of glimpse area. The 1x, 1.5x, and 2x in the table represent the ratios of enlarging glimpse scales. For example, if using 2x and the default glimpse scales are 3.0, 2.0, 1.0 times larger than the previously detected bounding boxes, the actual glimpse scales are 6.0, 3.0, 2.0 times larger, respectively. We can find that the 1x setting already achieves the highest AP, and other settings achieve comparable but slightly lower AP. This suggests it is inappropriate to enlarge glimpse areas aggressively for implementing REGO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Different Scales of Glimpse Area</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative Results</head><p>We present some visual detection results to better illustrate the impact of REGO. We choose Deformable DETR <ref type="bibr" target="#b48">[49]</ref> with R50 as baseline. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the results. Note that we choose the detection results whose confidence scores are higher than 0.5 for better visualization. From the figure, we can observe that the REGO indeed helps reduce both false positive and false negative results for the baseline method. Besides, the REGO can also help investigate the relations between different detected bounding boxes with the help of glimpse-based decoders. We will present some visual examples of the object relations learned with REGO in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduce a novel and effective technique, called REcurrent Glimpse-based decOder (REGO), to improve the Detection with Transformer (DETR) methods. By incorporating recurrent processing structure and learning glimpse features from coarse-to-fine RoIs, the REGO shows to both accelerate the convergence speed and boost the detection performance of different DETR methods consistently. We hope this study can contribute to future research on end-toend and efficient detection methodologies.</p><p>Social Impacts and Limitations Our method can benefit various applications like self-driving. A potential limitation is that we still need several GPU days for training which is environmental costly. This can be mitigated by further improving the efficiency of both our REGO and the DETR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. More Descriptions</head><p>Processing Algorithm In general, the REGO follows the algorithm described in Alg. 1 to process the visual features within each stage. We will release the code shortly. More Implementation Details In addition to the details discussed in the paper, we would also like to mention the following aspects of implementation. In particular, regarding the extraction of multi-scale features, this can be easy for applying REGO on DETR methods like Deformable DETR <ref type="bibr" target="#b48">[49]</ref> that already extract multi-scale features for attention modeling. When applying the REGO on DETR methods like the original DETR <ref type="bibr" target="#b2">[3]</ref> that only extract the single-scale feature from the last convolutional stage of the backbone, we attach 1?1 convolutions on the output of different convolutional stages (stage level 2 to stage level 5) to obtain multi-scale features. In this case, the attached 1?1 convolutions reduce the channel numbers to 256. Note that we do not use FPN for extracting multi-scale features to save costs. Besides, for the 'DC5' DETR variants which use single-scale features but enlarge the scale of the last convolutional stage, we still attach 1?1 convolutions to extract features and reduce channel numbers, except that the features of the last convolutional stage is down-sampled to its normal scale (i.e., 1/32 of the input image) to save costs.</p><formula xml:id="formula_8">Algorithm 1 Processing of i-th REGO Stage Require: H dec (i ? 1), O box (i ? 1) Ensure: H dec (i), O box (i),</formula><p>In addition, in the REGO, we use different weight parameters to initialize the decoders of different stages, thus the decoder at each stage can be trained to be more sensitive to the glimpse features of the corresponding stage. At each stage, to stabilize training, we follow <ref type="bibr" target="#b36">[37]</ref> and do not back propagate gradients into the outputs of the previous stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Improve DETR For Free</head><p>As mentioned in our paper, the proposed REGO method can improve the DETR for free during inference. This means that, after training with REGO, the obtained DETR model can still achieve improved performance by removing the REGO from the detection pipeline during inference.   <ref type="table" target="#tab_8">Table 4</ref> shows the results evaluated on MS COCO val set. We use the Deformable DETR <ref type="bibr" target="#b48">[49]</ref> as our baseline DETR method.</p><p>From these results, we can find that the complete REGO method (training and inference) improves around 2 points in AP, and the Deformable DETR trained with REGO but tested without REGO still achieves around 1 point gain in AP comparing to the baseline method. This demonstrates that the REGO is effective to enhance the feature of the original DETR method based on the RoI-based refinement procedure, which also suggest that the proposed REGO method can indeed improve the attention modeling in DETR during optimization. By removing the REGO, the obtained 1 point improvement does not introduce any extra complexity for inference comparing to the baseline DETR method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Learned Object Relations</head><p>As described in the paper, the decoder of the REGO in each stage correlates the glimpse features extracted based on previously detected bounding boxes with the previous attention modeling outputs corresponding to the same set of detected bounding boxes. Therefore, the obtained correlation results can reveal the relations between any two detected bounding boxes of the previous stage, and the detection results with higher correlation weights can be considered as more important for refining the attention modeling outputs and detection results.</p><p>In <ref type="figure" target="#fig_5">Fig. 6</ref>, we show some examples of the objects with the highest correlation weights w.r.t. a detected object from the previous stage. From the presented figures, we can find that the most related objects discovered by the REGO generally have strong semantic connections. For example, in the first column, the 'fork' and 'pizza' are most correlated for refining the detection related to the 'fork' object; in the fourth column, the 'person' and the 'horses' are most correlated for refining the 'person' object who is driving the carriage. Both examples are intuitive to human as well. This illustrates that the REGO can help DETR explore the information from semantically meaningful areas without wasting attention on obviously irrelevant areas, which is beneficial for improving the training efficiency and effectiveness of attention modeling in DETR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detected Objects</head><p>Most Related Objects Discovered by REGO </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(3), to represent O cls (0), O box (0), and H dec (0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Convergence curves of Deformable DETR on the val set for whether using the proposed REGO. For REGO, we explore convergence performance by reducing the learning rate at the 15th, 20-th, and 30-th epoch. ?: Reproduced using released code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>IoU w.r.t. Ground-truth Number of Detections</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Histogram of correct detection results on the val set at different settings, i.e., different IoU w.r.t. ground-truths and in different REGO stages. Note that the number of correct detection results of different stages share a similar amount (? 30k boxes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visual detection results of the baseline Deformable DETR [49] and its variant with REGO. Green boxes are detection results while red boxes are ground-truths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Illustration of using the REGO to reveal object relations. Each figure of the top row shows one of the detected objects of the previous stage, and each figure of the bottom row shows the most related detection results of the previous stage discovered by the decoder of the REGO. Results are obtained from the last stage of the REGO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>with H enc for making predictions. The N d here represents the number of predicted objects. Suppose the decoded feature is H dec , then H dec = A(E box , H enc ). With the H dec , the decoding phase performs classification and bounding box coordinate regression, obtaining O cls ? R N d ?Nc and O box ? R N d ?4 ,</figDesc><table><row><cell></cell><cell cols="3">Detection Outputs</cell><cell></cell><cell>Detection Outputs</cell><cell></cell><cell>Detection Outputs</cell></row><row><cell>DETR</cell><cell></cell><cell>0 0 0</cell><cell>REGO Stage 1</cell><cell>?</cell><cell>REGO Stage</cell><cell>REGO Stage + 1</cell><cell>+ 1 + 1 + 1</cell></row><row><cell>Previous Detected Boxes</cell><cell></cell><cell></cell><cell cols="2">Glimpse Features</cell><cell></cell><cell></cell><cell>Detected Boxes</cell></row><row><cell>? 1 { , , , ? }</cell><cell>Enlarge</cell><cell>RoIs</cell><cell>Features Extract</cell><cell></cell><cell cols="2">Glimpse Decoding Output</cell><cell>{ , , , ? }</cell><cell>Next Stage</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Classification Output</cell></row><row><cell>Previous</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multi-head</cell><cell>MLP</cell><cell>{ }</cell></row><row><cell>Decoding Output</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Attention</cell><cell></cell><cell>Decoding Output</cell></row><row><cell>? 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Linear Projection</cell><cell></cell><cell></cell><cell>? 1</cell><cell>Next Stage</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Glimpse-based Decoder for the -th Processing Stage</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>-Table 1 .</head><label>1</label><figDesc>DetectorsBackbone Epochs AP AP 50 AP 75 AP S AP M AP L GFLOPs #Params (M) Results of different detectors on the MS COCO val split. Baseline results are shaded.</figDesc><table><row><cell>FCOS [38]</cell><cell>R50</cell><cell>36</cell><cell>41.0 59.8</cell><cell>44.1 26.2 44.6 52.2</cell><cell>177</cell><cell>32</cell></row><row><cell>Faster RCNN -FPN [22]</cell><cell>R50</cell><cell>36</cell><cell>40.2 61.0</cell><cell>43.8 24.2 43.5 52.0</cell><cell>180</cell><cell>42</cell></row><row><cell>Faster RCNN -FPN [22]</cell><cell>R101</cell><cell>36</cell><cell>42.0 62.5</cell><cell>45.9 25.2 45.6 54.6</cell><cell>246</cell><cell>61</cell></row><row><cell>Mask RCNN [17]</cell><cell>X101</cell><cell>36</cell><cell>44.5 64.9</cell><cell>48.7 27.6 48.3 57.7</cell><cell>457</cell><cell>102</cell></row><row><cell>Cascade Mask RCNN [2, 5]</cell><cell>X101</cell><cell>36</cell><cell>46.6 65.1</cell><cell>50.6 29.3 50.5 60.1</cell><cell>627</cell><cell>135</cell></row><row><cell>TSP-RCNN [35]</cell><cell>R50</cell><cell>96</cell><cell>45.0 64.5</cell><cell>49.6 29.7 47.7 58.0</cell><cell>188</cell><cell>-</cell></row><row><cell>Efficient DETR [46]</cell><cell>R50</cell><cell>36</cell><cell>44.2 62.2</cell><cell>48.0 28.4 47.5 56.6</cell><cell>159</cell><cell>35</cell></row><row><cell>Sparse RCNN [34]</cell><cell>R50</cell><cell>36</cell><cell>44.5 63.4</cell><cell>48.2 26.9 47.2 59.5</cell><cell>-</cell><cell>-</cell></row><row><cell>DETR [3]</cell><cell>R50</cell><cell>500</cell><cell>42.0 62.4</cell><cell>44.2 20.5 45.8 61.1</cell><cell>86</cell><cell>41</cell></row><row><cell>DETR-DC5 [3]</cell><cell>R50</cell><cell>500</cell><cell>43.3 63.1</cell><cell>45.9 22.5 47.3 61.1</cell><cell>187</cell><cell>41</cell></row><row><cell>UP-DETR [10]</cell><cell>R50</cell><cell>300</cell><cell>42.8 63.0</cell><cell>45.3 20.8 47.1 61.7</cell><cell>86</cell><cell>41</cell></row><row><cell>Conditional DETR [28]</cell><cell>R50</cell><cell>50</cell><cell>40.9 61.8</cell><cell>43.3 20.8 44.6 59.2</cell><cell>90</cell><cell>44</cell></row><row><cell>Anchor DETR [42]</cell><cell>R50</cell><cell>50</cell><cell>44.2 64.7</cell><cell>47.5 24.7 48.2 60.6</cell><cell>151</cell><cell>-</cell></row><row><cell>SMCA [12]</cell><cell>R50</cell><cell>50</cell><cell>43.7 63.6</cell><cell>47.2 24.2 47.0 60.4</cell><cell>152</cell><cell>40</cell></row><row><cell>SMCA [12]</cell><cell>R101</cell><cell>50</cell><cell>44.4 65.2</cell><cell>48.0 24.3 48.5 61.0</cell><cell>218</cell><cell>58</cell></row><row><cell>DETR  *  [3, 28, 49]  ? DETR  *  -DC5 [3, 28, 49]  ?</cell><cell>R50</cell><cell>50 50</cell><cell>39.3 60.3 41.3 62.8</cell><cell>41.4 18.5 42.4 57.5 43.6 21.0 44.5 59.4</cell><cell>88 189</cell><cell>44</cell></row><row><cell>REGO-DETR  *  (ours) REGO-DETR  *  -DC5 (ours)</cell><cell>R50</cell><cell>50 50</cell><cell>42.3 60.5 44.0 62.6</cell><cell>46.2 26.2 44.8 57.5 47.8 26.5 45.2 62.9</cell><cell>112 213</cell><cell>58</cell></row><row><cell>Deformable DETR [49]</cell><cell>R50</cell><cell>36  ? 50</cell><cell>42.7 61.4 43.8 62.6</cell><cell>46.7 25.9 46.2 56.6 47.7 26.4 47.1 58.0</cell><cell>173</cell><cell>40</cell></row><row><cell>REGO-Deformable DETR (ours)</cell><cell>R50</cell><cell>36 50</cell><cell>44.8 63.8 45.9 65.2</cell><cell>48.7 27.0 48.0 60.2 49.7 27.6 48.9 61.5</cell><cell>190</cell><cell>54</cell></row><row><cell></cell><cell>R50</cell><cell>50</cell><cell>46.4 65.3</cell><cell>50.6 30.0 49.8 61.4</cell><cell>173</cell><cell>40</cell></row><row><cell>Deformable DETR  *  *  [49]  ?</cell><cell>R101</cell><cell>50</cell><cell>47.2 66.6</cell><cell>51.1 28.5 50.9 62.4</cell><cell>240</cell><cell>59</cell></row><row><cell></cell><cell>X101</cell><cell>50</cell><cell>47.7 67.2</cell><cell>51.4 29.3 51.2 62.8</cell><cell>417</cell><cell>105</cell></row><row><cell></cell><cell>R50</cell><cell>50</cell><cell>47.6 66.8</cell><cell>51.6 29.6 50.6 62.3</cell><cell>190</cell><cell>54</cell></row><row><cell>REGO-Deformable DETR  *  *  (ours)</cell><cell>R101</cell><cell>50</cell><cell>48.5 67.0</cell><cell>52.4 29.5 52.0 64.4</cell><cell>257</cell><cell>73</cell></row><row><cell></cell><cell>X101</cell><cell>50</cell><cell>49.1 67.5</cell><cell>53.1 30.0 52.6 65.0</cell><cell>434</cell><cell>119</cell></row></table><note>* Improve with 300 queries, reference points, and focal loss</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 ,</head><label>1</label><figDesc>we can observe that our method consistently improves different R50based baseline methods by around 2 points in AP using 50 epochs. For example, using the original DETR, we boosts the performance from 39.3 AP to 42.3 AP at 50 training epochs. Moreover, by further cooperating with iterative box refinement and two-stage processing when using the Deformable DETR as baseline, the REGO helps improve the AP from 46.4 to 47.6 which is the highest score in all the compared DETR methods trained with 50 epochs and R50 backbone. This demonstrates that our proposed REGO is effective for improving DETR by gradually attending to more accurate object areas with RoIs. Comparison at Fewer Training Epochs By applying the REGO on the Deformable DETR, we also compare the detection performance obtained at 36 training epochs used by many traditional detection methods<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref>. According to theTable 1, we help the Deformable DETR achieve 44.8 AP using 36 training epochs while the original Deformable DETR only achieves 42.7 with 36 epochs. Under the same training period, our method also helps surpass FPN and FCOS greatly. We also performed an extra experiment of REGO-DeformableDETR-X101 also trained with 36 epochs, obtaining 48.1, 67.4, 52.0 in AP, AP 50 , and AP 75 , respectively, which are higher than the CascadeR-CNN<ref type="bibr" target="#b1">[2]</ref>, proving that our REGO can reduce training costs for DETR effectively.</figDesc><table><row><cell>Comparison with Different Backbones We also investi-</cell></row><row><cell>gate the effectiveness of REGO on different backbone net-</cell></row><row><cell>works, including R50 [18], R101 [18], and X101 [43]. Be-</cell></row><row><cell>sides the improvements over R50 network, the results in</cell></row><row><cell>Table 1 also show that REGO continues to improve the</cell></row><row><cell>baseline DETR method with both R101 and X101 networks</cell></row><row><cell>promisingly. In particular, with X101 backbone network,</cell></row><row><cell>our Deformable DETR + REGO detector achieves the high-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Glimpse StagesAP AP 50 AP 75 AP S AP M AP L Hyper-parameter study of the number of stages in REGO. Deformable DETR<ref type="bibr" target="#b48">[49]</ref> is used as baseline.Glimpse Scale AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell cols="4">Deformable DETR [49] 43.8 62.6</cell><cell>47.7 26.4 47.1 58.0</cell></row><row><cell>1 Stage (? = 1)</cell><cell></cell><cell cols="2">45.1 63.0</cell><cell>46.4 24.7 46.0 60.0</cell></row><row><cell>2 Stage (? = 2, 1)</cell><cell></cell><cell cols="2">45.6 65.1</cell><cell>49.3 27.4 48.7 61.2</cell></row><row><cell>3 Stage (? = 3, 2, 1)</cell><cell></cell><cell cols="2">45.9 65.2</cell><cell>49.7 27.6 48.9 61.5</cell></row><row><cell cols="2">4 Stage (? = 4, 3, 2, 1)</cell><cell cols="2">45.9 65.5</cell><cell>50.3 28.5 49.0 61.1</cell></row><row><cell>1x</cell><cell cols="2">45.9 65.2</cell><cell>49.7 27.6 48.9 61.5</cell></row><row><cell>1.5x</cell><cell cols="2">45.8 65.0</cell><cell>50.1 27.6 48.9 61.3</cell></row><row><cell>2x</cell><cell cols="2">45.7 65.0</cell><cell>49.9 27.8 48.6 59.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Hyper-parameter study of the glimpse scale in REGO.</figDesc><table><row><cell>REGO is implemented with 3 stages.</cell></row><row><cell>identical implementation. The extra complexity w.r.t. these</cell></row><row><cell>larger backbone network-based DETR are only around 7%,</cell></row><row><cell>while the REGO brings more improvement rather than in-</cell></row><row><cell>creasing depth of backbone networks. For example, the</cell></row><row><cell>X101 improves R50 with 1.3 points in AP (46.4 to 47.7)</cell></row><row><cell>for Deformable DETR at the cost of another 244 GFLOPs,</cell></row><row><cell>while the REGO achieves similar results (47.6) with only</cell></row><row><cell>extra 17 GFLOPs using the R50 backbone. Furthermore,</cell></row><row><cell>we can also show in the supplementary materials that the</cell></row><row><cell>Deformable DETR trained with REGO can already achieve</cell></row><row><cell>around 1 point higher AP even without using REGO dur-</cell></row><row><cell>ing inference, which means that the REGO directly helps</cell></row><row><cell>original DETR learn better attention and offers detection</cell></row><row><cell>improvement during inference for free.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>and O cls (i) 1. Calculate RoIs by enlarging bounding box areas of O box (i ? 1) according to a scale ?(i) 2. Extract glimpse features V (i) based on enlarged RoIs; 3. Perform multi-head attention on glimpse features V (i) and H dec (i ? 1) to obtain decoded features H g (i); 4. Concatenate H g (i) and H dec (i ? 1) to obtain refined attention modeling outputs H dec (i) of current stage; 5. Predict object bounding boxes O box (i) and corresponding labels O cls (i) using H dec (i);</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Inference Method AP AP 50 AP 75 AP S AP M AP L Deformable DETR [49] 43.8 62.6 47.7 26.4 47.1 58.0 Inference w/ REGO 45.9 65.2 49.7 27.6 48.9 61.5 Inference w/o REGO 44.9 65.0 48.7 26.5 48.4 61.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison of whether using REGO for inference. Deformable DETR<ref type="bibr" target="#b48">[49]</ref> is used as baseline. The models related to the performance of inference with or without REGO all use REGO for training.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. Dr. Zhe Chen is supported by IH-180100002, and Dr. Jing Zhang is supported by ARC FL-170100117.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sasa: Semantics-augmented set abstraction for point-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context refinement for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A shape transformation-based dataset augmentation framework for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1121" to="1138" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recursive context routing for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="160" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07448</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Attend refine repeat: Active box proposal generation via in-out localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04446</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Asts: A unified framework for arbitrary shape text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5924" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Conditional detr for fast training convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12450</idno>
		<title level="m">SparseR-CNN: End-to-end object detection with learnable proposals</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3611" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732,2020.3</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fpdetr: Detection transformer advanced by fully pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Anchor detr: Query design for transformer-based detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07107</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Craft objects from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6043" to="6051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Efficient detr: Improving end-to-end object detector with dense prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01318</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Splitattention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10108</idno>
		<title level="m">Vision transformer advanced by exploring inductive bias for image recognition and beyond</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
