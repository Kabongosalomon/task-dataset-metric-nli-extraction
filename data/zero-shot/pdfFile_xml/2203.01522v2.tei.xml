<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
							<email>baosheng.yu@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the success of deep neural networks, there are still many challenges in deep representation learning due to the data scarcity issues such as data imbalance, unseen distribution, and domain shift. To address the abovementioned issues, a variety of methods have been devised to explore the sample relationships in a vanilla way (i.e., from the perspectives of either the input or the loss function), failing to explore the internal structure of deep neural networks for learning with sample relationships. Inspired by this, we propose to enable deep neural networks themselves with the ability to learn the sample relationships from each minibatch. Specifically, we introduce a batch transformer module or BatchFormer, which is then applied into the batch dimension of each mini-batch to implicitly explore sample relationships during training. By doing this, the proposed method enables the collaboration of different samples, e.g., the head-class samples can also contribute to the learning of the tail classes for long-tailed recognition. Furthermore, to mitigate the gap between training and testing, we share the classifier between with or without the BatchFormer during training, which can thus be removed during testing. We perform extensive experiments on over ten datasets and the proposed method achieves significant improvements on different data scarcity applications without any bells and whistles, including the tasks of long-tailed recognition, compositional zero-shot learning, domain generalization, and contrastive learning. Code is made publicly available at https://github.com/zhihou7/BatchFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite the great success of deep neural networks for representation learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, it heavily relies on collecting large-scale training data samples, which turns out to be non-trivial in real-world applications. Therefore, how to form robust deep representation learning under the data  <ref type="figure">Figure 1</ref>. An illustration of the relationships between different images. Specifically, similar classes tend to share similar parts (e.g., cock, robin, vulture share body shape and claw shape) and transferable augmentation (e.g., angles). Therefore, transferring shared knowledge from head/seen classes to tail/unseen classes can facilitate long-tailed/zero-shot learning. In addition, exploring the invariant features between images belonging to the same class is also helpful for robust representation learning with a few samples.</p><p>scarcity by exploring the sample relationships has received a lot of attention from the community, especially for the tasks without good training data to guarantee the generalization, such as long-tailed recognition <ref type="bibr" target="#b72">[72]</ref>, zero-shot learning <ref type="bibr" target="#b47">[48]</ref>, and domain generalization <ref type="bibr" target="#b7">[8]</ref>. However, it still remains a great challenge to find a unified, flexible, and powerful way to explore the sample relationships for robust representation learning. An intuitive example revealing the effectiveness of sample relationships is shown in <ref type="figure">Figure 1</ref>.</p><p>Among recent data scarcity learning methods, sample relationships have been intensively explored using an explicit scheme from either regularization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b79">79]</ref> or knowledge transfer <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b88">88]</ref>. Specifically, a simple yet very effective way is to directly generate new data samples/domains from existing training data <ref type="bibr" target="#b41">[42]</ref>, such as mixup <ref type="bibr" target="#b79">[79]</ref>, copy-paste <ref type="bibr" target="#b18">[19]</ref>, crossgrad <ref type="bibr" target="#b58">[58]</ref>, and compositional learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>. Another way is to transfer knowledge between data samples, e.g., 1) transferring meta knowledge between head and tail classes for longtailed recognition <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b73">73]</ref>; 2) transferring the knowledge from seen classes to unseen classes for zero-shot learning <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b76">76]</ref>; and 3) transferring invariant knowledge for domain generalization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref>. However, the abovementioned methods explore sample relationships from either the input or output of deep neural networks, failing to enable deep neural networks themselves with the ability to explore sample relationships, i.e., there is no interaction from the view of batch dimension.</p><p>Enabling the learning on the batch dimension is not easy for deep neural networks due to the training and inference gap, i.e., we do not always have a mini-batch of data samples during testing. For example, batch normalization requires to always keep mini-batch training statistics, where the running mean and variance are then used to normalize testing samples <ref type="bibr" target="#b32">[33]</ref>. Another example uses the feature memory to keep category centers during training, which is then used to enhance the tail/unseen categories during testing <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b88">88]</ref>. Therefore, to explore sample relationships for robust representation learning, we propose to empower the deep neural networks with structural advances for sample relationship learning. Specifically, we try to capture and model the sample relationships in each mini-batch of training data samples by introducing a transformer into the batch dimension, and we refer to it as the Batch Transformer or BatchFormer. Furthermore, to mitigate the gap between training and testing, we utilize a shared classifier before and after the BatchFormer module to enforce the batch-invariant learning. By doing this, the BatchFormer module is only required during training, i.e., we do not need to change the inference structures of deep neural networks.</p><p>From the perspective of optimization, BatchFormer enables the information propagation of all features of the minibatch samples. Therefore, all samples can contribute to the learning on any object categories, and the insight might be that this implicitly enriches current training samples with hallucinated features from the whole mini-batch (e.g., the shared parts between two categories). For example, in longtailed recognition, the hallucinated features may improve the feature space of the tail classes. Meanwhile, the loss function also emphasizes on the rare classes via propagating larger gradients of rare classes on other features in the mini-batch. In particular, we also find that BatchFormer brings two obvious changes to the learned representation, i.e., it effectively facilitates the deep model to learn 1) comprehensive representations by focusing on almost all different parts of the object; and 2) invariant representations by focusing on the object itself rather than the complex back-ground cues (See more empirical evaluations in appendix).</p><p>In this paper, our main contributions can be summarized as follow: 1) we propose to explore sample relationships from the perspective of the internal structure of deep neural networks; 2) we devise a simple yet effective module termed as BatchFormer, which is a plug-and-play module to explore sample relationships in each mini-batch; and 3) without any bells and whistles, we perform extensive experiments to demonstrate the effectiveness of BatchFormer in a variety of visual recognition tasks, including long-tailed recognition, zero-shot learning, domain generalization, and self-supervised representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Sample Relationship. There are diverse and firm relationships among different samples, which have been widely used via various types of strategies <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b79">79]</ref>. Zhang et al. <ref type="bibr" target="#b79">[79]</ref> propose to regularize the network to favor simple linear behavior in-between training examples with mixup. Mixup <ref type="bibr" target="#b79">[79]</ref> merely considers the linear transformation between examples, while we investigate the relationship among examples in a non-linear way. The compositionality of samples has inspired massive approaches to improve the generalization of few-shot and zero-shot learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b62">62]</ref>, where the shared parts/attributes among different samples have been explored via prior label relationship knowledge. There are also some approaches via investigating sample/class relationships to conduct transductive inference <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>, e.g., transductive fewshot classification <ref type="bibr" target="#b43">[44]</ref>, meta embedding <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b88">88]</ref>. However, those approaches require to conduct inference with multiple samples (e.g., query set, or bank features). Meanwhile, massive domain generalization methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref> aims to find casual/invariant representations across domains, which we think internally utilizes the relationship among samples of the same class but different domains. However, those techniques are diverse and complex. By contrast, we propose to investigate relationships for robust representation learning in a simple yet effective way, and thus benefit those challenging tasks simultaneously.</p><p>Data Scarcity Learning. Learning with imperfect training data has turned out to be very challenging, which has been explored in a variety of data scarcity tasks. A very important problem is long-tailed recognition, where the data from different classes usually exhibit a long-tailed distribution: a large portion of classes have very few instances. Current long-tailed approaches can be roughly categorized into distribution re-balancing methods (e.g., resampling <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, re-weighting <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b82">82]</ref>), ensemble of diverse branches/experts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b83">83]</ref>, and knowledge transfer <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b88">88]</ref>. Many knowledge transfer approaches have been developed for long-tailed recognition via meta learning <ref type="bibr" target="#b73">[73]</ref>, memory features <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b88">88]</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?? ??</head><p>Transformer Encoder ?? Classifier</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Classifier</head><p>BatchFormer Backbone <ref type="figure">Figure 2</ref>. The main framework of representation learning with the proposed BatchFormer. Specifically, we apply a BatchFormer module between the feature extractor (e.g., ResNet) and the classifier layer to explore the samples relationships. Furthermore, with a shared classifier before and after the BatchFormer during training for batch-invariant learning, we can then remove BatchFormer during testing. and virtual data generation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b69">69]</ref>. Nevertheless, those works usually depend on complex memory bank <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b88">88]</ref>, or complex learning strategies <ref type="bibr" target="#b73">[73]</ref> or additional distillation step <ref type="bibr" target="#b25">[26]</ref>. Another data scarcity tasks include zeroshot learning and domain generalization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b85">85]</ref>, which require to recognize new categories with unseen training data. Specifically, zero-shot learning aims to recognize unseen classes that do not training samples, while domain generalization targets at generalizing classes of seen domains to unseen domains. Current zero-shot learning approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b76">76]</ref> usually transfer knowledge of seen classes to unseen classes via modern techniques (e.g., graph network <ref type="bibr" target="#b47">[48]</ref>, data generalization <ref type="bibr" target="#b89">[89]</ref>, and compositional learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48]</ref>) for unseen classes recognition. Recently, compositional zero-shot learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref> have been widely explored in different tasks, we thus mainly evaluate the proposed BatchFormer on compositional zero-shot learning <ref type="bibr" target="#b47">[48]</ref>. Domain generalization techniques usually include data augmentation <ref type="bibr" target="#b85">[85,</ref><ref type="bibr" target="#b86">86]</ref>, meta learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b81">81]</ref>, and disentangled/invariant representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>. BatchFormer facilitates invariant representation learning, and thus illustrates effectiveness on multiple datasets.</p><p>Contrastive Self-Supervised Learning. Contrastive Learning <ref type="bibr" target="#b19">[20]</ref> has increasingly achieved great success in computer vision for self-supervised learning, e.g., <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref> and <ref type="bibr" target="#b10">[11]</ref>. Contrastive Learning aims to learn representations that attract similar samples and dispel different samples, where the similar samples and different samples are known according to some priors. However, we implicitly mine the relationships with Transformer Encoder network. Specifically, we also demonstrates the effectiveness of BatchFormer in contrastive learning, e.g., MoCo-v2 <ref type="bibr" target="#b11">[12]</ref> and MoCo-v3 <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first provide an overview of the main deep representation learning framework with the proposed BatchFormer module. We then introduce the proposed BatchFormer module in detail, including the encoder and the shared classifier. Lastly, we discuss the insights behind the proposed BatchFormer from the view of gradient flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The relationships between different samples are various and complex, while previous approaches have explored sample relationships using a straightforward way, such as the joint manipulation of different input images and the knowledge transfer using meta embedding or loss functions. Among existing methods, sample relationships are usually required to be defined in an explicit scheme before they can be used for learning, thus failing to automatically learn sample relationships for representation learning.</p><p>We consider sample relationships from a learning perspective, i.e., we aim to enable deep neural networks themselves with the ability to learn sample relationships from each mini-batch sample during the end-to-end deep representation learning. The main deep representation learning framework with the proposed new module is shown in <ref type="figure">Figure</ref> 2. Specifically, a backbone network is first used to learn representations for individual data samples, i.e., there is no interaction between different samples in each mini-batch. After this, a new module is introduced to model the relationships between different samples by using the crossattention mechanism in transformer, and we thus referred to it as Batch Transformer or BatchFormer module. The output of BatchFormer is then used as the input of the final classifier. To fulfill the gap between training and testing, we also utilize an auxiliary classifier before the BatchFormer module, i.e., by sharing the weights between the final classifier and the auxiliary classifier, we are able to transfer the knowledge learned from sample relationships to the backbone and auxiliary classifier. Therefore, during testing we can remove the BatchFormer and directly use the auxiliary classifier for classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">BatchFormer</head><p>In this subsection, we introduce the detailed structures of the proposed BatchFormer. Specifically, the proposed BatchFormer module utilizes a stack of multiple transformer encoder layers to model the relationships between different samples.</p><p>Transformer Encoder. The transformer encoder includes multihead self-attention (MSA) and MLP blocks. A Layernorm (LN) is used after each block. Let X ? R N ?C denote a sequence of input features, where N is the length of the sequence and C indicates the dimension of input features. We then have the output of the transformer encoder as follows,X</p><formula xml:id="formula_0">l = LN (M SA(X l?1 ) + X l?1 ),<label>(1)</label></formula><formula xml:id="formula_1">X l = LN (M LP (X l ) +X l ),<label>(2)</label></formula><p>where l indicates the index of layers in the transformer encoder. The multi-head attention layers have been widely used to model the relationships from channel and spatial dimensions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b65">65]</ref>. Therefore, we argue that it can also be extended to explore the relationships in the batch dimension. As a result, different from typical usage of transformer layers, the input of BatchFormer will be first reshaped to enable the transformer layers working on the batch dimension of the input data. By doing this, the self-attention mechanism in transformer layers then becomes the cross-attention between different samples for BatchFormer. Shared Classifier. Since we can not assume batch statistics for testing, such as sample relationships, there might be a gap between the features before and after the Batch-Former module. That is, we can not perform inference on new samples by directly removing the BatchFormer. Therefore, apart from the final classifier, we also introduce a new auxiliary classifier to not only learn from the final classifier but also keep consistent with the features before the BatchFormer. To achieves this, we simply share the parameters/weights between the auxiliary classifier and the final classifier. We refer to this simple yet effective strategy as "shared classifier". With the proposed "shared classifier", we can thus remove the BatchFormer module during testing, while still benefiting from the sample relationship learning using BatchFormer. The proposed BatchFormer is a plug-and-play module for robust deep representation learning. During optimization, the proposed BatchFormer modules can be jointly optimized with other deep learning backbone networks (including both CNNs and Vision Transformers) in an end-to-end way. The proposed BatchFormer is also very easy to implement using typical deep learning softwares,. For example, we show how to implement BatchFormer with several lines of PyTorch code in Algorithm 1.</p><formula xml:id="formula_2">! " ! # ! $%&amp; ' " ' &amp; ' $%&amp; (! " (' " (! # (' # (! # (' " (! ) (' " (! " (' # (! " (' $%&amp; (! $%&amp; (' # (! # (' $%&amp; (! $%&amp; (' $%&amp;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">BatchFormer: A Gradient View</head><p>To better understand how the proposed BatchFormer helps representation learning by exploring sample relationships, we also provide an intuitive explanation from the perspective of gradient propagation for optimization. Intuitively, without BatchFormer, all losses only propagate gradients on the corresponding samples and categories, i.e., one-to-one, while there are gradients on other samples with BatchFormer (the dashed line ) as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Specifically, given samples X(X = X 0 , X 1 , X i , ..., X N ?1 ) and the corresponding losses L 0 , L 1 , L i , ..., L N ?1 in the mini-batch, with Batch-Former, we then have</p><formula xml:id="formula_3">?L i ?X = ?L i ?X i + N ?1 j? =i ?L i ?X j .<label>(3)</label></formula><p>That is, BatchFormer brings new gradient terms ?Li ?Xj , where i ? = j. From a perspective of gradient optimization, L i also optimizes the network according to sample X j (j ? = i), that is a significant difference compared to the model without BatchFormer. In other word, X j (j ? = i) can be regarded as a virtual sample <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b79">79]</ref> of y i , where y i is the label of X i . We consider that both BatchFormer and Mixup <ref type="bibr" target="#b79">[79]</ref> can be regarded as data-dependent augmentations. Batch-Former implicitly draws virtual examples from the vicinity distribution of samples via cross-attention module. From this perspective, BatchFormer has implicitly augmented N ? 1 virtual samples for each label y i via the relationship modeling among samples in the mini-batch. Previous approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b89">89]</ref> have demonstrated data augmentation is helpful for long-tailed recognition <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b69">69]</ref>, zero-shot learning <ref type="bibr" target="#b89">[89]</ref>, and domain generalization <ref type="bibr" target="#b85">[85,</ref><ref type="bibr" target="#b86">86]</ref>. the virtual samples are largely helpful for tail classes since those classes lack of samples. Our gradients analysis in Section 5 also demonstrates the tail classes have larger gradients on other samples compared to head classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we perform extensive experiments to demonstrate the effectiveness of BatchFormer for a variety of data scarcity learning tasks, including long-tailed recognition, zero-shot learning, domain generalization and contrastive learning. Appendix provides more results of other tasks, e.g., image classification (include Visual Transformer <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b63">63]</ref>), and more ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Long-Tailed Recognition</head><p>Datasets. We use four popular datasets for long-tailed recognition as follows: 1) CIFAR-100-LT has 50,000 training images and 10,000 validation images with 100 categories. 2) ImageNet-LT <ref type="bibr" target="#b44">[45]</ref> contains 115.8K images of 1000 classes from ImageNet 2012. The number of images in each class ranges from 5 to 1,280; 3) iNaturalist 2018 <ref type="bibr" target="#b64">[64]</ref> is a large-scale fine-grained dataset with 437.5K images from 8,142 categories; 4) Places-LT <ref type="bibr" target="#b44">[45]</ref> is a longtailed scene classification dataset derived from the Places dataset <ref type="bibr" target="#b84">[84]</ref> with 184.5K images from 365 categories with class instances ranging from 5 to 4,980.</p><p>Baselines. We use the following three baseline methods: Balanced Softmax <ref type="bibr" target="#b55">[56]</ref>, RIDE <ref type="bibr" target="#b72">[72]</ref> and PaCo <ref type="bibr" target="#b13">[14]</ref>. If not otherwise stated, we follow the same settings used in previous methods, and the proposed BatchFormer is removed during testing. Particularly, there is a small difference when comparing with RIDE <ref type="bibr" target="#b72">[72]</ref>. we train the network with a batch size of 400 on 4 V100 GPUs for 100 epochs with an initial learning rate of 0.1 on ImageNet-LT and 0.2 on iNaturalist 2018, respectively. The learning rare is decayed with cosine schedule on iNaturalist 2018. We report the performance of BatchFormer based on 3 experts RIDE. All experiments are conducted on V100 GPU with PyTorch. See other details in Appendix A.</p><p>Results on CIFAR-100-LT. <ref type="table" target="#tab_0">Table 1</ref> demonstrates the proposed BatchFormer module is orthogonal to the stateof-the-art methods, e.g., Balanced Softmax and Paco. We notice that BatchFormer largely improves Balanced Softmax by 2.4% for few classes when imbalance ratio is 100, and by 1.8% on medium classes and by 1.2% on few classes respectively when imbalance ratio is 200. Besides, the results of PaCo on imbalance ratio 200 also increase by 1.5% on medium classes and 0.7% on few classes with Batch-Former. For ratio 100, BatchFormer mainly improves many classes since Paco has achieved good performance on few classes. Please refer to Appendix for more explanations (i.e., the comparison without PaCo loss, but with strong data augmentation). Overall, BatchFormer improves the recognition of tail classes while maintaining the performance of head classes. Meanwhile BatchFormer is pluggable to current popular methods on CIFAR-100-LT.</p><p>Results on ImageNet-LT. As illustrated in <ref type="table">Table 2</ref>, BatchFormer largely improves Balanced Softmax by 2.4% on medium classes and 6.9% on few classes respectively under ResNet-10 backbone. Meanwhile, under ResNet-50 backbone, BatchFormer largely improves Balanced Softmax on the Few category by 5%.</p><p>When BatchFormer is applied in RIDE <ref type="bibr" target="#b72">[72]</ref>, the results on medium classes and few classes increase by 1.8% and 2.8% respectively under ResNet-10 backbone. Batch-Former also increases medium and few classes by 1% and 1.9% respectively under ResNet-50. BatchFormer achieves clear improvement overall, while the performance on many classes drops a bit. Furthermore, BatchFormer also effectively improves RIDE under two-stage training strategy (RIDE uses a larger model to teach small model with distilling loss). Here, different from RIDE, we use a pre-trained model (the same model) to initialize the model and train the model again with BatchFormer (See details in Appendix).</p><p>PaCo <ref type="bibr" target="#b13">[14]</ref> is recently introduced for Long-Tailed Recog- nition with Supervised Contrastive Learning. We also find BatchFormer is able to facilitate ImageNet-LT on medium classes and few classes. Noticeably, PaCo uses a strong data augmentation strategy from Supervised Contrastive Learning with 400 training epochs. We think data augmentation limits the improvement of BatchFormer on ImageNet-LT. Particularly, Appendix also shows BatchFormer achieves comparable results on balanced Full ImageNet.</p><p>Results on iNaturalist 2018. We mainly evaluate Batch-Former on RIDE since Balanced Softmax has limited performance on iNaturalist 2018 and PaCo requires over 36 GPU days to converge (See Appendix). We train RIDE with cosine decay learning-rate scheduler by one-stage and achieve a strong baseline with 72.5% (better than the reported) as illustrated in <ref type="table">Table 3</ref>. Particularly, BatchFormer further improves the Medium category and Few Category by 1.8% and 2.6% respectively. Here, for a fair comparison to previous work, we mainly evaluate BatchFormer RIDE with 3 experts which has similar GFlops to previous work.</p><p>Results on Places-LT. <ref type="table">Table 4</ref> illustrates BatchFormer improves BALMS on Few category. Besides, BatchFormer effectively improves PaCo <ref type="bibr" target="#b13">[14]</ref> which is the new stateof-the-art on Places-LT. Here, different from CIFAR-100-LT and ImageNet-LT, BatchFormer mainly improves Many category. This might be because the result of PaCo on Many category is very worse and BatchFormer can re-balance the imbalanced datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Zero-Shot Learning</head><p>We evaluate BatchFormer on compositional zero-shot learning <ref type="bibr" target="#b47">[48]</ref>. All experiments are evaluated with one V100 GPU. The learning rare, epochs, optimizer are fully similar to <ref type="bibr" target="#b47">[48]</ref>. See details in Appendix A.</p><p>Datasets. The experiments are performed on three datasets: 1) MIT-States <ref type="bibr" target="#b33">[34]</ref> consists of 30,000 training images of natural objects with 1,262 seen compositions (23.8 image per composition, 115 states and 245 objects), and 13,000 test images with 400 seen compositions and 400 unseen compositions; 2) UT-Zappos <ref type="bibr" target="#b78">[78]</ref> includes 23,000 training images of shoes catalogue and we use the splits from <ref type="bibr" target="#b54">[55]</ref>. UT-Zappos has 83 seen compositions for training (277.1 images per composition, 16 states, 12 objects) and 18 seen compositions and unseen compositions in test set; 3) C-GQA <ref type="bibr" target="#b47">[48]</ref> provides 26,000 training images with 6,963 seen compositions (3.7 images per composition, 453 states, 870 objects) and 3,000 test images with 18 seen compositions and unseen compositions.</p><p>Metrics. We adopt the evaluation protocol of <ref type="bibr" target="#b54">[55]</ref> and report the Area Under the Curve (AUC) (in %) between the accuracy on seen and unseen compositions. Similar to <ref type="bibr" target="#b47">[48]</ref>, we also report unseen accuracy and seen accuracy, as well as the best harmonic mean. <ref type="table">Table 5</ref> demonstrates BatchFormer effectively improves the AUC and HM among all the datasets compared to the baseline. Particularly, for a fair comparison, we use the released code under the same setting to reproduce <ref type="bibr" target="#b47">[48]</ref> as the baseline. We notice BatchFormer mainly improves the Seen category on MIT-States and C-GQA, while BatchFormer largely improves the unseen category by nearly 5%. This might be because the number of seen composition instances on MIT-States and C-GQA is few, e.g. 23.8 image per seen composition on MIT-States and 3.7 images per seen composition. In other words, the recognition of seen compositions on MIT-States and C-GQA is few-shot learning. We think BatchFormer on the two datasets mainly finds invariant features among images of the same class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Domain Generalization</head><p>By exploring sample relationships of the same class, it is easier to find the invariant features and thus improve the domain generalization. We first demonstrate BatchFormer effectively improves the baseline (without domain generalization techniques) on PACS <ref type="bibr" target="#b40">[41]</ref> under ResNet-18. Then, we apply BatchFormer to recent domain generalization, e.g., SWAD <ref type="bibr" target="#b7">[8]</ref>, and shows the effectiveness of BatchFormer.</p><p>Datasets. We illustrates the application of BatchFormer on several domain generalization datasets: 1) PACS <ref type="bibr" target="#b40">[41]</ref> covers 7 object categories and 4 domains (Photo, Art Paintings, Cartoon and Sketches), 2) VLCS <ref type="bibr" target="#b17">[18]</ref> (4 domains, 5 classes, 10,729 images), OfficeHome <ref type="bibr" target="#b66">[66]</ref> (4 domains, 65 classes and 15,588 images), and TerraIncognita <ref type="bibr" target="#b3">[4]</ref> (4 domains, 10 classes and 24,788 images).</p><p>Details. For baseline, we train the network under ResNet-18 with SGD (30 epochs, initial learning rate 0.001), and drop the learning rate at 24 epochs. We also use the popular data augmentations, e.g., flip, color jiter and scale. We train the network 5 times and report the average. Others methods are compared based on <ref type="bibr" target="#b35">[36]</ref>. For a fair comparison with SWAD <ref type="bibr" target="#b7">[8]</ref>, we follow the optimization methods of <ref type="bibr" target="#b7">[8]</ref> and use the released code of <ref type="bibr" target="#b7">[8]</ref> to evaluate BatchFormer on ResNet-18. See more details in Appendix. <ref type="table">Table 6</ref> illustrates BatchFormer consistently improves baseline , CORAL <ref type="bibr" target="#b60">[60]</ref> and MixStyle <ref type="bibr" target="#b87">[87]</ref>. BatchFormer also clearly improves recent work <ref type="bibr" target="#b7">[8]</ref> on four datasets in <ref type="table">Table 7</ref>. Particularly, BatchFormer improves <ref type="bibr" target="#b7">[8]</ref> by over 2% on OfficeHome and TerraIncognita. This illustrates Batch-Former is able to facilitate invariant representation learning, and improve the generalization across domains.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Self-Supervised Learning</head><p>Contrastive Learning aims to learn representations that attract similar samples and dispel different samples, while BatchFormer builds a Transformer Network among samples to implicitly explore the sample relationships for representation learning. BatchFormer can also be applied to Contrastive Learning. We mainly evaluate BatchFormer with MoCo-v2 <ref type="bibr" target="#b11">[12]</ref> and MoCo-v3 <ref type="bibr" target="#b12">[13]</ref> on linear classification protocol. Object detection result is provided in Appendix. <ref type="table">Table 8</ref> shows BatchFormer is also able to improve the representation learning of self-supervised learning, e.g., BatchFormer consistently improves MoCo-v2 and MoCo-v3 by around 1.% on ImageNet. <ref type="figure" target="#fig_3">Figure 4</ref> shows the linear classification of BatchFormer pre-trained model convergences faster, e.g., BatchFormer pre-trained model achieves the performance of moco-v3 with only 38 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>Batch size. The batch size represents the size of Transformer. <ref type="table" target="#tab_4">Table 9</ref> shows BatchFormer is less sensitive to the batch size when batch size is less than 128. We find batch size 512 achieves better performance on Few category. Shared Classifier. Interestingly, <ref type="table" target="#tab_0">Table 10</ref> demonstrates BatchFormer without shared classifier achieves similar performance on three categories, while BatchFormer with shared classifier maintains the performance on Many category. This demonstrates the effectiveness of shared classifier and the ability of re-balancing of BatchFormer.   <ref type="figure">Figure 6</ref>. The gradient of each class to other images in mini-batch on CIFAR-100-LT and ImageNet-LT (based on <ref type="bibr" target="#b55">[56]</ref>). For each class, we obtain the gradient normalization of the loss of each label to other images in the mini-batches, and then average the gradients of each class in the test set. The classes are sorted by descending order according to the number of instances in the training set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Visualization Analysis</head><p>Visualized Comparison. We illustrate the visualized comparison with Grad-CAM <ref type="bibr" target="#b56">[57]</ref> between Baseline and BatchFormer in <ref type="figure" target="#fig_4">Figure 5</ref>. We find BatchFormer focuses on more details of objects and ignores spurious correlations. On the one hand, when the image includes complex scence with many disturbing factors, BatchFormer effectively improves the attention of the network on the corresponding object regions (e.g., sea snake on the sandbeach, dog on the snow and insect on the leaf in <ref type="figure" target="#fig_4">Figure 5 right)</ref>. On the other hand, BatchFormer also pays more attention on regions of the object when the scene is clear (e.g., the bird, dog, and spider in <ref type="figure" target="#fig_4">Figure 5</ref> left). See more examples in Appendix.</p><p>Gradients Analysis. BatchFormer has increased new gradient backward: the loss of each label has the gradients on other images. In other words, we have implicitly augmented the samples for the class of each image in the mini-batches. The other images in the mini-batch can also be regarded as the virtual instances of current image class. The gradient is firmly related to the effect of each image label on other images. <ref type="figure">Figure 6</ref> illustrates the rarer the class is, the larger the gradients of the class have on other images in the mini-batch. Thus, BatchFormer actually utilizes the other images to facilitate low-shot recognition via increasing gradients of few-shot labels on other images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future work</head><p>We propose to enable deep neural networks themselves with the ability to explore the sample relationships from each mini-batch. Specifically, we regards each image (batch dimension) in the mini-batch as a node of a sequence, and then build a Transformer Encoder Network among the images to mine the relationships among the images in the mini-batch. BatchFormer enables the gradient propagation of each label to all images in the mini-batch, which can be regarded as virtual sample augmentation, and thus improve the representation learning. We further introduce a shared classifier before and after the BatchFormer during training, which can thus be removed during testing. We demonstrate the effectiveness of BatchFormer on over ten datasets and BatchFormer achieves significant improvements on different tasks, including long-tailed recognition, zero-shot learning, domain generalization, and contrastive learning.</p><p>Limitations The improvement of current BatchFormer on models with strong data augmentation and balanced distribution is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Experimental Details</head><p>In all our experiments, if not otherwise stated, we use one layer Transformer Encoder. Meanwhile, The proposed BatchFormer is inserted after the global average pooling layer in ResNet.</p><p>Long-Tailed Recognition. For Balanced Softmax <ref type="bibr" target="#b55">[56]</ref>, we directly insert BatchFormer before the classifier, and train the network with 128 batch size on 1 V100 GPU for 90 epochs. The initial learning rate for Balanced Softmax <ref type="bibr" target="#b55">[56]</ref> is 0.05 (it is linearly decreased according to the batch size) and we set the learning rate of the weights of BatchFormer module to 0.005 to avoid overfitting. For RIDE <ref type="bibr" target="#b72">[72]</ref>, we train the network with a batch size of 400 on 4 V100 GPUs for 100 epochs with an initial learning rare of 0.1 on ImageNet-LT and 0.2 on iNaturalist 2018 respectively. For a fair comparison, we verify BatchFormer based on 3 experts RIDE. Specifically, a shared BatchFormer is incorporated in all expert branches, i.e. the module of BatchFormer is shared among 3 expert heads. Experimentally, we find it is not necessary to utilized shared classifier for RIDE. We thus remove shared classifier in RIDE. We think this might be because the multiple experts internally maintain the features between before and after BatchFormer. For Paco <ref type="bibr" target="#b13">[14]</ref>, we use default hyper-parameters of Paco on ImageNet-LT with four V100 GPUs, on CIFAR-100-LT with a single V100 GPU, and on Places-LT with a single V100 GPU. We simply insert BatchFormer in the query encoder, momentum encoder and prediction head, respectively. Meanwhile, the proposed BatchFormer is shared among those places. See more details in the provided code for MoCo. We find it requires 9 days to train Paco on iNaturalist18 <ref type="bibr" target="#b64">[64]</ref> with 4 V100 GPUs. Thus, we directly evaluate BatchFormer on RIDE which only requires 32 hours.</p><p>Compositional Zero-Shot Learning. We would like to clarify some details when reproducing the results of <ref type="bibr" target="#b47">[48]</ref> on UT-Zap50K. Specifically, we can not fully reproduce the result of <ref type="bibr" target="#b47">[48]</ref> on UT-Zap50K due to the missing details. We also find the result with learnable feature extractor of <ref type="bibr" target="#b47">[48]</ref> on UT-Zap50K is much better than the result with fixed feature extractor in <ref type="table">Table 2</ref> of <ref type="bibr" target="#b47">[48]</ref>, which is largely different from the results on other datasets. Meanwhile, the same problem has been noticed by others on github issues of <ref type="bibr" target="#b47">[48]</ref>, i.e., the result can not be unable to fully reproduced (https://github.com/ExplainableML/czsl/issues/4). Therefore, all our experiments are based on the reproduced result for a fair comparison with <ref type="bibr" target="#b47">[48]</ref>.</p><p>Domain Generalization. We mainly evaluate Batch-Former on domain generalization by using the following two baseline methods: Transfer-Learning-Library <ref type="bibr" target="#b35">[36]</ref> and SWAD <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr" target="#b35">[36]</ref> provides massive traditional and recent methods for domain generalization, and we thus simply apply BatchFormer to those method and evaluate the effectiveness of BatchFormer. Specifically, we use the default setting of <ref type="bibr" target="#b35">[36]</ref> for each method, and reproduce the result of each method with <ref type="bibr" target="#b35">[36]</ref> for a fair comparison. All experiments are conducted three times and the results are averaged. For SWAD <ref type="bibr" target="#b7">[8]</ref>, we use the released code and default setting to reproduce the result of ResNet-18.For fair comparison, we also provide the result of ResNet-50.</p><p>Self-Supervised Learning. BatchFormer is also pluggable to contrastive learning. Specifically, with Batch-Former, we train MoCo-v2 <ref type="bibr" target="#b11">[12]</ref> and MoCo-v3 <ref type="bibr" target="#b12">[13]</ref> for 200 epochs and 300 epochs respectively. We follow all default training settings when comparsing with MoCo-v2 and MoCo-v3. Since it is different from supervised learning, we provide the code based on MoCo-v3. As the released code of MoCo requires 16 GPUs with 32GB memory, we conduct all contrastive learning experiments using two cluster nodes with 8 NVIDIA A100 GPUs (40GB) for each node. We keep all other experimental details same as <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b12">[13]</ref> for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiments</head><p>To better demonstrate the effectiveness of the proposed BatchFormer, we provide additional experimental results on more tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Generalized Zero-Shot Learning</head><p>We also evaluate BatchFormer on generalized zero-shot learning task. Specifically, we report the accuracy of "seen", "unseen", and the harmonic mean of them (unseen <ref type="table" target="#tab_0">Table 11</ref>. Illustration of BatchFormer on Generalized Zero-Shot Learning based on <ref type="bibr" target="#b70">[70]</ref>. Unseen and Seen are the Top-1 accuracies tested on unseen classes and seen classes, respectively, in GZSL. and seen). We perform experiments on one of the most popular datasets for generalized zero-shot learning, CUB <ref type="bibr" target="#b67">[67]</ref>, which includes 11,788 images from 200 bird species. We build a baseline with the released code of <ref type="bibr" target="#b70">[70]</ref> and achieve better results than <ref type="bibr" target="#b70">[70]</ref>. As shown in <ref type="table" target="#tab_0">Table 11</ref>, the proposed BatchFormer achieves a new state-of-the-art on Unseen and Harmonic mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Self-Supervised Learning</head><p>Object detection on VOC2007. We also evaluate Object Detection of MoCo on VOC2007 <ref type="bibr" target="#b16">[17]</ref> in <ref type="table" target="#tab_0">Table 12</ref>. Similar as MoCo-v2 <ref type="bibr" target="#b11">[12]</ref>, we use the pre-trained model to fine-tune Faster-RCNN on VOC2007 based on Detec-tron2 <ref type="bibr" target="#b75">[75]</ref>. We find MoCo-v3 achieves worse result on VOC2007. However, BatchFormer consistently improves the object detection on VOC2007. Here, we train MoCo-v2 for 200 epochs, and MoCo-v3 for 100 epochs. Specifically, we think that the number of training epochs (only 100 epochs) of MoCo-v3 might limit the performance on VOC2007. <ref type="table" target="#tab_0">Table 13</ref> demonstrates BatchFormer for Image Classification. We find BatchFormer achieves comparable performance among ResNet50. This shows BatchFormer does not degrade the performance when the distribution of data is balanced. <ref type="figure" target="#fig_6">Figure 7</ref>. Grad-Cam demonstration of BatchFormer on low-shot test images based on <ref type="bibr" target="#b55">[56]</ref>. The left images show BatchFormer enables the model pay attention on more details when the scene is simple, while the right images show BatchFormer facilitates the model ignore the spurious correlation in the image. This is clear version of <ref type="figure" target="#fig_4">Figure 5</ref> in the paper.  <ref type="bibr" target="#b40">[41]</ref>. Here, the baseline is from <ref type="bibr" target="#b35">[36]</ref>. SWAD <ref type="bibr" target="#b7">[8]</ref> is reproduced based on the released code of <ref type="bibr" target="#b7">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Image Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Domain Generalization</head><p>We provide more experimental results based on <ref type="bibr" target="#b35">[36]</ref> in <ref type="table" target="#tab_0">Table 14</ref>. Experiments on OfficeHome, VLCS, TerraIncognita are provided in <ref type="table" target="#tab_0">Table 15</ref>, <ref type="table" target="#tab_0">Table 16</ref> and <ref type="table" target="#tab_0">Table 17</ref> respec- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Domain Adaption</head><p>We also demonstrate BatchFormer on Domain Adaption on VisDA2017 <ref type="bibr" target="#b52">[53]</ref>. <ref type="table" target="#tab_0">Table 18</ref> shows BatchFormer effectively improves the corresponding baseline, i.e., MDD <ref type="bibr" target="#b80">[80]</ref>. <ref type="table" target="#tab_0">Table 18</ref>. Illustration of BatchFormer for Domain Adaption on VisDA2017 <ref type="bibr" target="#b52">[53]</ref>. The backbone is ResNet-101. Experiments are based on <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Synthetic ? &gt; Real MDD <ref type="bibr" target="#b80">[80]</ref> 76.8?1.5 +BatchFormer 77.8 ?2.0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>Number of Layers. We use one layer BatchFormer in our experiment. <ref type="table" target="#tab_0">Table 19</ref> demonstrates with more layers of BatchFormer, we do not observe larger improvement. We leave how to leverage more layers of BatchFormer to improve the performance in future work.</p><p>BatchFormer without PaCo loss <ref type="bibr" target="#b13">[14]</ref>. We notice BatchFormer mainly improves PaCo on Many category on CIFAR-100-LT (imbalance ratio 100). We thus conduct additional ablation study on BatchFormer for PaCo. Here, we remove the PaCo loss with Balanced loss <ref type="bibr" target="#b13">[14]</ref> to build the baseline. we observe consistent results in the <ref type="table" target="#tab_10">Table 20</ref>. Noticeably, the baseline without PaCo loss is even better than the one in the main paper.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualized Comparison</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Gradient propagation with BatchFormer in a mini-batch. Dashed lines represent new gradient propagation among samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Convergence of BatchFormer under linear classification.The batch size and optimizer are the same as<ref type="bibr" target="#b12">[13]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Grad-Cam demonstration of BatchFormer on low-shot test images based on<ref type="bibr" target="#b55">[56]</ref>. The first row is baseline, while the second row is BatchFormer. The left images show BatchFormer enables the model pay attention on more details when the scene is simple and clean, while the right images show BatchFormer faciliates the model ignore the spurious correlation in the image. More figures are in Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7</head><label>7</label><figDesc>provides clear figure of the Grad-Cam Figure in the main paper. More comparisons are included in Figure 8, Figure 9, Figure 10, where we choose the top 100 classes on ImageNet for demonstration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>More Grad-Cam illustration of BatchFormer on low-shot test images based on<ref type="bibr" target="#b55">[56]</ref>. The figures are also provided in the directory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>More Grad-Cam illustration of BatchFormer on low-shot test images based on<ref type="bibr" target="#b55">[56]</ref>. The figures are also provided in the directory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>More Grad-Cam illustration of BatchFormer on low-shot test images based on<ref type="bibr" target="#b55">[56]</ref>. The figures are also provided in the directory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Pytorch Code of BatchFormer.</figDesc><table><row><cell>def BatchFormer(x, y, encoder, is training):</cell></row><row><cell># x: input features with the shape [N, C]</cell></row><row><cell># encoder: TransformerEncoderLayer(C,4,C,0.5)</cell></row><row><cell>if not is training:</cell></row><row><cell>return x, y</cell></row><row><cell>pre x = x</cell></row><row><cell>x = encoder(x.unsqueeze(1)).squeeze(1)</cell></row><row><cell>x = torch.cat([pre x, x], dim=0)</cell></row><row><cell>y = torch.cat([y, y], dim=0)</cell></row><row><cell>return x, y</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Illustration of imbalance ratio 100 and 200 on CIFAR-100-LT. * means we train the method with the released code in one stage (e.g., balanced softmax<ref type="bibr" target="#b55">[56]</ref>). Med means Medium category. RIDE* means we use 3 experts. [56] 50.7 68.0 49.7 31.9 46.4 70.0 51.5 24.3 + BatchFormer 51.7 68.4 49.3 34.3 47.5 70.2 53.3 25.5 Paco [14] 51.9 63.9 53.0 36.5 47.1 68.1 51.5 27.5 + BatchFormer 52.4 68.4 52.1 34.0 47.8 68.1 53.0 28.2 Illustration of ResNet-10 and ResNet-50 on ImageNet-LT. * means we train the net work with the released code by onestage. RIDE-3e means we use three experts in RIDE. [56] 41.0 52.6 38.3 18.0 50.1 61.1 47.5 27.6 + BatchFormer ? 43.2 52.8 40.7 24.9 51.1 61.4 47.8 33.6 RIDE-3e * [72] 44.7 57.0 40.3 25.5 53.6 64.9 50.4 33.2 + BatchFormer 45.7 56.3 42.1 28.3 54.1 64.3 51.4 35.1 62.7 56.7 42.1 two stage RIDE-3e [72] 45.9 57.6 41.7 28.0 54.9 66.2 51.7 34.9 + BatchFormer 47.6 55.3 45.5 33.3 55.7 64.6 53.4 39.0</figDesc><table><row><cell>Method</cell><cell cols="5">100 All Many Med Few All Many Med Few 200</cell></row><row><cell>RIDE* [72]</cell><cell cols="4">48.0 68.1 49.2 23.9 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Balanced *  Method</cell><cell cols="5">ResNet-10 All Many Med Few All Many Med Few ResNet-50</cell></row><row><cell>OLTR [45]</cell><cell cols="4">35.6 43.2 35.1 18.5 -</cell><cell>-</cell><cell>-</cell></row><row><cell>LFME [77]</cell><cell cols="4">38.8 47.0 37.9 19.2 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Balanced *  PaCo [14]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-57.0 64.8 55.9 39.1</cell></row><row><cell>+ BatchFormer</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-57.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Illustration on iNaturalist 2018. * means we train the method with the released code in one stage. RIDE-3e means RIDE with 3 experts. Illustration of Places-LT (Backbone is ResNet-152). * means the result that we train the method with the released code.</figDesc><table><row><cell>Methods</cell><cell>All</cell><cell cols="3">Many Medium Few</cell></row><row><cell>BBN [83]</cell><cell>66.3</cell><cell>49.4</cell><cell>70.8</cell><cell>65.3</cell></row><row><cell>cRT [37]</cell><cell>65.2</cell><cell>69.0</cell><cell>66.0</cell><cell>63.2</cell></row><row><cell>RIDE-3e [72]</cell><cell>72.2</cell><cell>70.2</cell><cell>72.2</cell><cell>72.7</cell></row><row><cell>PaCo [14]</cell><cell>73.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">RIDE-3e *  [72] 72.5</cell><cell>68.1</cell><cell>72.7</cell><cell>73.2</cell></row><row><cell>+ Batchformer</cell><cell>74.1</cell><cell>65.5</cell><cell>74.5</cell><cell>75.8</cell></row><row><cell>Methods</cell><cell>All</cell><cell cols="3">Many Medium Few</cell></row><row><cell>OLTR [45]</cell><cell>35.9</cell><cell>44.7</cell><cell>37.0</cell><cell>25.3</cell></row><row><cell cols="2">? -normalized [37] 37.9</cell><cell>37.8</cell><cell>40.7</cell><cell>31.8</cell></row><row><cell>BALMS* [56]</cell><cell>37.8</cell><cell>41.4</cell><cell>38.8</cell><cell>29.1</cell></row><row><cell>+ Batchformer</cell><cell>38.2</cell><cell>39.5</cell><cell>38.3</cell><cell>35.7</cell></row><row><cell>PaCo [14]</cell><cell>41.2</cell><cell>37.5</cell><cell>47.2</cell><cell>33.9</cell></row><row><cell>+ Batchformer</cell><cell>41.6</cell><cell>44.0</cell><cell>43.1</cell><cell>33.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .Table 7 .</head><label>567</label><figDesc>Illustration of Batchformer on Compositional Zero-Shot Learning. * means we use the released code to reproduce the results. S means seen, U means unseen, s means state and o means object. The results of<ref type="bibr" target="#b42">[43]</ref> are copied from<ref type="bibr" target="#b47">[48]</ref>. CompCos<ref type="bibr" target="#b45">[46]</ref> 4.5 16.4 25.3 24.6 27.9 31.8 28.7 43.1 59.8 62.5 44.7 73.20.0 31.6 27.3 30.3 34.5 31.5 46.5 60.3 64.5 46.3 74.4 3.7 14.9 30.8 14.7 15.8 29.0 +BatchFormer 6.7 20.6 33.2 27.7 30.8 34.7 34.6 49.0 62.5 69.2 49.7 75.6 3.8 15.5 31.3 14.7 15.3 30.0 Illustration of BatchFormer for Domain Generalization under different works on PACS [41]. 4?0.2 73.8?2.0 68.6?1.8 96.3?0.2 79.8 CORAL [60] 79.2?1.7 75.5 ?1.1 71.4?3.1 94.7?0.3 80.2 +BatchFormer 80.6?0.9 74.7?1.9 73.1?0.3 95.1?0.3 80.9 MixStyle [87] 81.7?0.1 76.8?0.0 80.8?0.0 93.1?0.0 83.1 +BatchFormer 84.8 ?0.4 75.3?0.0 81.1 ?0.4 93.6?0.0 83.7 Illustration of BatchFormer for Domain Generalization based on recent work [8] (ResNet-18). We show the average results of different domains. Terra is TerraIncognita. See more results in Appendix.</figDesc><table><row><cell>Method</cell><cell cols="2">AUC HM</cell><cell cols="2">MIT-States S U</cell><cell>s</cell><cell>o</cell><cell>AUC HM</cell><cell>UT-Zap50K S U</cell><cell>s</cell><cell>o</cell><cell cols="2">AUC HM</cell><cell>C-GQA S U</cell><cell>s</cell><cell>o</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CGE [48]</cell><cell cols="13">6.5 21.4 32.8 28.0 30.1 34.7 33.5 60.5 64.5 71.5 48.7 76.2 3.6 14.5 31.4 14.0 15.2 30.4</cell></row><row><cell cols="6">CGE* 6.3 Methods art paint cartoon sketches</cell><cell cols="2">photo Avg.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell cols="7">79.9? 1.0 73.0?1.5 67.7? 3.0 95.7?0.4 79.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+BatchFormer 80.Methods</cell><cell cols="6">PACS VLCS OfficeHome Terra</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SWAD* [8]</cell><cell></cell><cell>82.9</cell><cell>76.3</cell><cell></cell><cell>62.1</cell><cell></cell><cell>42.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+ BatchFormer</cell><cell>83.7</cell><cell>76.9</cell><cell></cell><cell>64.3</cell><cell></cell><cell>44.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Table 8. Illustration of BatchFormer for Contrastive Learning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">(MoCo [12, 13, 24] under ResNet50) on linear classification.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell></cell><cell cols="5">Epochs Top-1 Top-5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MoCo-v2 [12]</cell><cell></cell><cell></cell><cell>200</cell><cell>67.5</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MoCo-v2 [12]+BatchFormer</cell><cell>200</cell><cell>68.4</cell><cell></cell><cell>88.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MoCo-v3 [13]</cell><cell></cell><cell></cell><cell>100</cell><cell>68.9</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MoCo-v3 [13]+BatchFormer</cell><cell>100</cell><cell>69.8</cell><cell></cell><cell>89.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 9 .</head><label>9</label><figDesc>Ablation Studies of Batch Size based on Balanced Softmax<ref type="bibr" target="#b55">[56]</ref>. The backbone is ResNet-10 under one GPU. learning rare is linearly decreased with batch size.</figDesc><table><row><cell>Batch Size</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell></row><row><cell>All</cell><cell cols="6">43.2 43.6 43.3 43.2 42.4 42.5</cell></row><row><cell>Many</cell><cell cols="6">53.8 53.7 52.9 52.8 52.2 52.0</cell></row><row><cell>Medium</cell><cell cols="6">40.2 40.7 40.8 40.7 39.5 39.6</cell></row><row><cell>Few</cell><cell cols="6">23.9 24.9 25.3 24.9 25.2 25.8</cell></row><row><cell cols="7">Table 10. Ablation Studies on ImageNet-LT based on Balanced</cell></row><row><cell cols="5">Softmax [56]. The backbone is ResNet-50.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>All</cell><cell cols="4">Many Medium Few</cell></row><row><cell>BatchFormer</cell><cell></cell><cell>50.9</cell><cell>60.7</cell><cell></cell><cell>47.7</cell><cell>34.1</cell></row><row><cell cols="3">w/o Shared Classifier 42.4</cell><cell>41.3</cell><cell></cell><cell>43.3</cell><cell>42.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Here, for a fair comparison, we use the the released code of MoCo-v2 and MoCo-v3 to run the experiments in the same setting, and obtain the baseline.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">CUB [67]</cell></row><row><cell></cell><cell></cell><cell cols="3">Unseen Seen Harmonic mean</cell></row><row><cell>IZF [59]</cell><cell></cell><cell>52.7</cell><cell>68.0</cell><cell>59.4</cell></row><row><cell cols="2">TF-VAEGAN [49]</cell><cell>52.8</cell><cell>64.7</cell><cell>58.1</cell></row><row><cell cols="2">CE-GZSL [70]</cell><cell>63.9</cell><cell>66.8</cell><cell>65.3</cell></row><row><cell cols="2">CE-GZSL*(reproduced)</cell><cell>67.5</cell><cell>65.1</cell><cell>66.3</cell></row><row><cell cols="2">+ BatchFormer</cell><cell>68.2</cell><cell>65.8</cell><cell>67.0</cell></row><row><cell>Table 12.</cell><cell cols="4">Illustration of BatchFormer for MoCo on</cell></row><row><cell cols="2">VOC2007 [17]. Methods</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell></cell><cell cols="2">MoCo-v2* [12] 56.4</cell><cell>82.1</cell><cell>63.1</cell></row><row><cell></cell><cell>+BatchFormer</cell><cell>56.7</cell><cell>82.0</cell><cell>63.6</cell></row><row><cell></cell><cell>MoCo-v3 [12]</cell><cell>46.6</cell><cell>78.2</cell><cell>48.9</cell></row><row><cell></cell><cell>MoCo-v3 [12]</cell><cell>48.0</cell><cell>78.8</cell><cell>51.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 13 .Table 14</head><label>1314</label><figDesc>Illustration of BatchFormer for Image Recognition.</figDesc><table><row><cell>Methods</cell><cell cols="3">Epochs Top-1 Top-5</cell></row><row><cell>ResNet50 [74]</cell><cell>200</cell><cell>78.9</cell><cell></cell></row><row><cell>ResNet50 + BatchFormer</cell><cell>200</cell><cell>78.9</cell><cell>-</cell></row></table><note>. Illustration of BatchFormer for Domain Generalization under different works on PACS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>3?0.7 76.1?0.6 75.5?2.6 95.4 ?0.2 82.0 +BatchFormer 82.4?1.5 76.4?1.2 75.7?1.0 95.1?0.4 82.4 CORAL [60] 79.2?1.7 75.5 ?1.1 71.4?3.1 94.7?0.3 80.2 +BatchFormer 80.6?0.9 74.7?1.9 73.1?0.3 95.1?0.3 80.9 IRM [1] 81.0?0.6 71.4?4.1 68.1?7.1 95.0?0.6 78.9 +BatchFormer 78.9?3.1 71.0?7.1 71.5?2.8 96.0?0.3 79.4 V-REx [39] 80.8?1.8 75.3?1.4 73.3?0.9 95.9?0.0 81.3 + BatchFormer 82.0?0.3 76.3?0.7 75.2?1.7 95.3?0.1 82.2 MixStyle [87] 81.7?0.1 76.8?0.0 80.8?0.0 93.1?0.0 83.1 +BatchFormer 84.8 ?0.4 75.3?0.0 81.1 ?0.4 93.6?0.0 83.7 SWAD* [8] 83.1?1.5 75.9?0.9 77.1?2.4 95.6?0.6 82.9 +BatchFormer 84.3?0.8 76.9 ?1.2 78.2?1.8 95.7?0.6 83.9 ResNet50 V-REx [39] 83.8?4.8 81.0?0.0 97.7?0.4 77.7?3.1 85.0 + BatchFormer 87.3 ?5.0 80.2?4.6 97.1?1.7 77.9?4.4 85.6 IRM [1] 88.2?0.6 79.8?1.0 97.6?0.5 77.6?0.7 85.8 + BatchFormer 89.0?0.98 80.1 ?1.0 98.0?0.4 79.8?0.4 86.8 SWAD [8] 89.4?0.7 83.7?1.2 97.7?0.6 82.5?0.8 88.1 +BatchFormer 90.2?0.5 84.0?1.0 97.3?0.3 83.0?0.6 88.6</figDesc><table><row><cell></cell><cell>art paint</cell><cell>cartoon sketches</cell><cell>photo Avg.</cell></row><row><cell>Baseline</cell><cell>81.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 15 .Table 16 .Table 17 .</head><label>151617</label><figDesc>Illustration of BatchFormer for Domain Generalization based on recent work [8] on OfficeHome Methods Art Clipart Product RealWorld Avg. SWAD* [8] 54.5?0.8 49.4?0.1 70.9?0.1 72.7?0.2 62.1 + BatchFormer 57.8?0.1 51.0?0.1 73.4?0.2 75.1?0.1 64.3 ResNet-50 IRM [1] 66.8?0.2 54.9?0.8 77.5?0.7 80.5?0.4 69.9 +BatchFormer 67.7 ?0.2 55.5?0.8 78.4?0.5 81.0?0.3 70.6 SWAD* [8] 65.9?0.8 58.0?0.1 78.5?0.5 80.2?0.7 70.6 + BatchFormer 66.7?0.3 57.9?0.3 79.2?0.4 80.6?0.7 71.1 Illustration of BatchFormer for Domain Generalization based on recent work [8] (ResNet-18) on VLCS 2?1.4 61.4?0.1 71.2?1.7 75.5?0.8 76.3 + BatchFormer 97.2?0.8 61.3?1.1 71.7?1.0 77.4?0.4 76.9 Illustration of BatchFormer for Domain Generalization based on recent work [8] (ResNet-18) on TerraIncognita Methods Art Clipart Product RealWorld Avg. SWAD* [8] 47.6?3.0 33.8?4.5 53.6?1.8 33.3?0.6 42.1 + BatchFormer 49.8?1.8 40.3?2.0 55.2?1.2 34.0?1.1 44.8 tively. The default backbone is ResNet-18.</figDesc><table><row><cell>Methods</cell><cell>Caltech101 LabelMe SUN09 SUN09 Avg.</cell></row><row><cell>SWAD* [8]</cell><cell>97.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 19 .</head><label>19</label><figDesc>Ablation Studies of different layers on ImageNet-LT based on Balanced Softmax<ref type="bibr" target="#b55">[56]</ref>. The backbone is ResNet-10.</figDesc><table><row><cell>Method</cell><cell>All</cell><cell cols="3">Many Medium Few</cell></row><row><cell cols="2">BatchFormer (1 layers) 43.2</cell><cell>52.8</cell><cell>40.4</cell><cell>25.6</cell></row><row><cell>2 layers</cell><cell>43.2</cell><cell>52.8</cell><cell>40.3</cell><cell>26.0</cell></row><row><cell>4 layers</cell><cell>42.7</cell><cell>52.1</cell><cell>40.1</cell><cell>25.4</cell></row><row><cell>8 layers</cell><cell>43.3</cell><cell>53.3</cell><cell>40.2</cell><cell>26.1</cell></row><row><cell>16 layers</cell><cell>43.1</cell><cell>52.9</cell><cell>40.0</cell><cell>26.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 20 .</head><label>20</label><figDesc>Illustration of BatchFormer without PaCo loss [14] on CIFAR-LT-100. Method 100 200 All Many Med Few All Many Med Few Baseline [52] 52.0 68.1 53.2 31.6 47.31 67.8 52.6 27.3 + BatchFormer 52.6 68.7 53.2 33.1 48.13 68.9 53.1 28.2</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A causal view of compositional zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using metaregularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="456" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalizing from several related classification tasks to a new unlabeled sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2178" to="2186" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What is the effect of importance weighting in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ace: Ally complementary experts for solving long-tailed recognition in one-shot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Swad: Domain generalization by seeking flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Cheol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to balance specificity and invariance for in and out of domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="301" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parametric contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2918" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Compas: Representation learning with compositional part sharing for fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11878</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distilling virtual examples for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Yin-Yin He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detecting human-object interaction via fabricated compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="14646" to="14655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-challenging improves cross-domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="124" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discovering states and transformations in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1383" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking classbalanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7610" to="7619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Transfer-learning-library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junguang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baixu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<ptr target="https://github.com/thuml/Transfer-Learning-Library" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Decoupling representation and classifier for long-tailed recognition. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Compositional learning for human object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keizo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno>ICML, 2021. 13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by betweenclass attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Metasaug: Meta semantic augmentation for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixiong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5212" to="5221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Symmetry and group in attribute-object compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11316" to="11325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Open world compositional zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Ferjad</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5222" to="5230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Mini-batch graphs for robust image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Kumar Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03237</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning graph embeddings for compositional zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mf Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Latent embedding feedback and discriminative features for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshita</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Visda: The visual domain adaptation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient domain generalization via common-specific low-rank decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praneeth</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="7728" to="7738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Task-driven modular networks for zero-shot compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3593" to="3602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Balanced meta-softmax for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunan</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grad-Cam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Invertible zero-shot recognition flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11662" to="11671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning compositional representations for few-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6372" to="6381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Generalizing to unseen domains: A survey on domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03097</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Rsg: A simple but effective module for learning imbalanced datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3784" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Contrastive learning based hybrid networks for longtailed image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A survey of zero-shot learning: Settings, methods, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<idno>2019. 12</idno>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">PAMI</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="247" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fine-grained visual comparisons with local learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning to generalize unseen domains via memory-based multi-source metalearning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6277" to="6286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Improving calibration for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16489" to="16498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02503</idno>
		<title level="m">Domain generalization: A survey</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning to generate novel domains for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Domain generalization with mixstyle. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Inflated episodic memory with region self-attention for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A generative adversarial approach for zero-shot learning from noisy texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
