<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging Unlabeled Data for Crowd Counting by Learning to Rank</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
							<email>xialei@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
							<email>andrew.bagdanov@unifi.it</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">MICC</orgName>
								<orgName type="institution" key="instit2">University of Florence Florence</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging Unlabeled Data for Crowd Counting by Learning to Rank</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel crowd counting approach that leverages abundantly available unlabeled crowd imagery in a learning-to-rank framework. To induce a ranking of cropped images , we use the observation that any sub-image of a crowded scene image is guaranteed to contain the same number or fewer persons than the super-image. This allows us to address the problem of limited size of existing datasets for crowd counting. We collect two crowd scene datasets from Google using keyword searches and queryby-example image retrieval, respectively. We demonstrate how to efficiently learn from these unlabeled datasets by incorporating learning-to-rank in a multi-task network which simultaneously ranks images and estimates crowd density maps. Experiments on two of the most challenging crowd counting datasets show that our approach obtains state-ofthe-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Crowd counting and crowd density estimation techniques aim to count the number of persons in crowded scenes. They are essential in video surveillance <ref type="bibr" target="#b2">[3]</ref>, safety monitoring, and behavior analysis <ref type="bibr" target="#b28">[29]</ref>. Person counting and density estimation are instances of a broader class of classical counting problems in computer vision. Counting semantic image features is important in medical and biological image processing <ref type="bibr" target="#b17">[18]</ref>, vehicle counting <ref type="bibr" target="#b24">[25]</ref>, and numerous other application contexts.</p><p>Despite the attention the crowd counting problem has received, both classically and in the recent computer vision literature, it remains a difficult task in practice. Perspective distortion, clutter, occlusion, non-uniform distribution of people, complex illumination, scale variation, and a host of other scene-incidental imaging conditions render person counting and crowd density estimation in unconstrained images an extremely daunting problem. Techniques for crowd counting have been recently improved using Convolutional Neural Networks (CNNs). These recent approaches We sample a decreasing sequence of sub-images I1, I2, and I3 from an unlabeled image. Though we do not know the exact person counts C(Ii), we use the fact that C(I1) ? C(I2) ? C(I3) as self-supervision to learn representations for person counting.</p><p>include scale-aware regression models <ref type="bibr" target="#b24">[25]</ref>, multi-column CNNs <ref type="bibr" target="#b36">[37]</ref>, and switching networks <ref type="bibr" target="#b0">[1]</ref>. As with most CNN architectures, however, these person counting and crowd density estimation techniques are highly data-driven. Even modestly deep architectures for visual recognition require massive amounts of labeled training data for learning. For person counting, the labeling burden is even more onerous than usual. Training data for person counting requires that each individual person be meticulously labeled in training images. It is for this reason that person counting and crowd density estimation datasets tend to have only a few hundred images available for training. As a consequence, the ability to train these sophisticated CNN-based models suffers. Recently, self-supervised learning has received more attention because it provides an alternative to collecting large hand-labeled datasets. Self-supervised learning is based on Methods Basic CNNs Scale-aware Context-aware Multi-task Fast inference   <ref type="bibr" target="#b34">[35]</ref> Shang et al. <ref type="bibr" target="#b27">[28]</ref> Marsden et al. <ref type="bibr" target="#b22">[23]</ref> Zhang et al. <ref type="bibr">(2016)</ref>  <ref type="bibr" target="#b36">[37]</ref> Babu Sam et al. <ref type="bibr" target="#b0">[1]</ref> Sindagi et al. <ref type="bibr" target="#b30">[31]</ref> Ours <ref type="table">Table 1</ref>. State-of-the-art crowd counting networks and their characteristics.</p><p>the idea of using an auxiliary task (different, but related to the original supervised task) for which data is freely available and no annotation is required. As a consequence, selfsupervised learning can be much more scalable and flexible. A network trained to estimate the relative location of patches in images was shown to automatically learn features discriminative for semantic concepts in <ref type="bibr" target="#b9">[10]</ref>. Other examples include methods that can generate color images from gray scale images and vice versa <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36]</ref>, recover a whole patch from the surrounding pixels by inpainting <ref type="bibr" target="#b25">[26]</ref>, and learn from equivalence relations <ref type="bibr" target="#b23">[24]</ref>.</p><p>In this paper, we propose a self-supervised task to improve the training of networks for crowd counting. Our approach leverages unlabeled crowd images at training time to significantly improve performance. Our key insight is that even though we do not have an exact count of the number of persons in a crowd image, we do know that crops sampled from a crowd image are guaranteed to contain the same or fewer persons than the original (see <ref type="figure">Figure 1</ref>). This gives a technique for generating a ranking of sub-images that can be used to train a network to estimate whether one image contains more persons than another image. The standard approach to exploiting self-supervised learning is to train the self-supervised task first, after which the resulting network is fine-tuned on the final task for which limited data is available. We show that this approach, which is used by the vast majority of self-supervised methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>, does not produce satisfactory results for crowd counting. Our proposed self-supervision, however, yields significant improvement over the state-ofthe-art when added as a proxy task to supervised crowd counting in a multi-task network.</p><p>The main contribution of this work is that we propose a method that can leverage unlabeled crowd imagery at training time. We propose two different approaches to automatically acquire this data from the Internet. In addition, we analyze three approaches to training using ranked image sets in combination with datasets of labeled crowd scenes. Finally, we demonstrate that our approach leads to state-ofthe-art results on two crowd counting datasets and obtains excellent results on a cross-dataset experiment.</p><p>The rest of this paper is organized as follows. In the next section we briefly review the literature related to crowd counting. Then, in Section 3 we describe how to systematically generate ranked images from unlabeled crowd imagery. In Section 4 we introduce our approach to exploiting this ranked imagery at training time. We follow in Section 5 with an extensive experimental evaluation of our approach and a comparative analysis with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We divide our discussion of related work into two main groups as in <ref type="bibr" target="#b31">[32]</ref>: traditional approaches and CNN-based methods. We focus on crowd counting in still images, but we refer the interested reader to the following papers for examples of crowd counting in video <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Various traditional approaches have been proposed to deal with the crowd counting problem. The main strategies are divided into the various categories as in <ref type="bibr" target="#b20">[21]</ref>. Most early work on crowd counting used detectors to detect the heads or full bodies of persons in the scene <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref>. This information can then be used to count. However, detectionbased approaches fail in extremely dense crowded scenes due to occlusion and low resolution of persons. To address these issues, researchers proposed to map features learned from the crowded scene or patches to the number of people <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. By counting using regression, the crowd counting problem is decomposed into two parts: feature extraction and a regression model. While regression-based approaches resulted in improvement, only global counting was considered without any spatial information (i.e. without estimating a density map). The authors of <ref type="bibr" target="#b17">[18]</ref> proposed to learn a mapping from patches to corresponding density maps, which achieved great success on a variety of counting problems.</p><p>As introduced in the review of <ref type="bibr" target="#b31">[32]</ref>, CNN-based approaches can be classified into different categories based on the properties of the CNN (see <ref type="table">Table 1</ref> for an overview of state-of-the-art networks and the properties they hold). Basic CNNs incorporate only basic CNN layers in their networks. The approaches in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref> use the AlexNet network <ref type="bibr" target="#b15">[16]</ref> to map from crowd scene patches to global number of people by changing the output of AlexNet from 1000 to 1. The resulting network can be trained end-to-end. Due to the large variations of density in different images, recent methods have focused on scale-awareness. The method proposed in <ref type="bibr" target="#b36">[37]</ref> trains a multi-column based architecture (MCNN) to capture the different densities by using different sizes of kernels in the network. Similarly, the authors of <ref type="bibr" target="#b24">[25]</ref> propose the Hydra-CNN architecture that takes different resolutions of patches as inputs and has multiple output layers (heads) which are combined in the end. Most recently, in <ref type="bibr" target="#b0">[1]</ref> the authors propose a switching CNN that can select an optimal head instead of combining the information from all network heads. Finally, context-aware models are networks that can learn from the context of images. In <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref> the authors propose to classify images or patches into one of five classes: very high density, high density, medium density, low density and very low density. However, the definition of these five classes varies across datasets and must be carefully chosen using knowledge of the statistics of each dataset.</p><p>Although CNN-based methods have achieved great success in crowd counting, due to lack of labeled data it is still challenging to train deep CNNs without over-fitting. The authors of <ref type="bibr" target="#b34">[35]</ref> propose to learn density map and global counting in an alternating sequence to obtain better local optima. The method in <ref type="bibr" target="#b14">[15]</ref> uses side information like groundtruth camera angle and height to help the network to learn. However, this side information is expensive to obtain and is not available in most existing crowd counting datasets.</p><p>There are several works which have studied how to learn to rank, and they focus on learning a ranking function from ground-truth rankings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref>. However, these are very different from our approach in which we aim to learn from rankings. Most related to our work is a recent paper <ref type="bibr" target="#b19">[20]</ref> proposing a method for image quality assessment using automatically generated rankings of distorted images. In that work the authors used ranking data to pre-train a network and then fine-tune on available labeled datasets. We will show that such an approach fails for crowd counting, and that only when posed as a multi-task learning problem is the network able to exploit the additional data from rankings.</p><p>In another recent paper <ref type="bibr" target="#b23">[24]</ref> a method is proposed where the self-supervised task is to learn to count. The authors propose two pretext tasks (scaling and tiling) which guide self-supervised training. The network aims to learn to count visual primitives in image regions. It is self-supervised by the fact that the number of visual primitives is not expected to change under scaling, and that the sum of all visual primitives in individual tiles should equal the total number of visual primitives in the whole image. Unlike our approach, they do not consider rankings of regions and their counts are typically very low (several image primitives). Also, their final tasks do not involve counting but rather unsupervised learning of features for object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generating ranked image sets for counting</head><p>As discussed in the introduction, acquiring data for crowd counting is laborious because images often contain hundreds of persons which require precise annotation. Instead, we propose a self-supervised task for crowd-counting which exploits crowd images which are not hand-labeled with person counts during training. Rather than regressing to the absolute number of persons in the image, we train a network which compares images and ranks them according to the number of persons in the images. In this section, we show how to cheaply collect rank-labeled data which can be used to train these methods.</p><p>The main idea is based on the observation that all patches contained within a larger patch must have a fewer or equal number of persons than the larger patch (see <ref type="figure">Figure 1</ref>). This observation allows us to collect large datasets of crowd images for which relative ranks exist. Rather than having to painstakingly annotate each person we are only required to verify if the image contains a crowd. Given a crowd image we extract ranked patches according to Algorithm 1.</p><p>To collect a large dataset of crowd images from the Internet, we use two different approaches:</p><p>? Keyword query: We collect a crowd scene dataset from Google Images by using different key words: Crowded, Demonstration, Train station, Mall, Studio, Beach, all of which have high likelihood of containing a crowd scene. Then we delete images not relevant to our problem. In the end, we collected a dataset containing 1180 high resolution crowd scene images, which is about 24x the size of the UCF CC 50 dataset, 2.5x the size of ShanghaiTech Part A, and 2x the size of ShanghaiTech Part B. Note that no other annotation of images is performed. Example images from this dataset are given in <ref type="figure" target="#fig_1">Figure 2</ref> (top row).</p><p>? Query-by-example image retrieval: For each specific existing crowd counting dataset, we collect a dataset by using the training images as queries with the visual image search engine Google Images. We choose the first ten similar images and remove irrelevant ones. For UCF CC 50 we collect 256 images, for ShanghaiTech Part A 2229 images, and for Shang-haiTech Part B 3819 images. An example of images returned for a specific query image is given in <ref type="figure" target="#fig_1">Figure 2</ref> (bottom row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning from ranked image sets</head><p>In this section we describe our approach to training a CNN to estimate the number of persons in dense crowd scenes. We use the ranked image set generation technique described in the previous section to generate data for the Algorithm 1 : Algorithm to generate ranked datasets.</p><p>Input: A crowd scene image, number of patches k and scale factor s.</p><p>Step 1: Choose an anchor point randomly from the anchor region. The anchor region is defined to be 1/r the size of the original image, centered at the original image center, and with the same aspect ratio as the original image. self-supervised task of ranking crowd images. We first introduce the network architectures used for counting and ranking, and then describe three different approaches to combining both losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Crowd density estimation network</head><p>Here we explain the network architecture which is trained on available crowd counting datasets with ground truth annotations. This network regresses to a crowd density image which indicates the number of persons per pixel (examples of such maps are given in <ref type="figure" target="#fig_5">Figure 5</ref>). A summation of all values in such a crowd density image gives an estimate of the number of people in the scene. In the experimental section we will consider this network as the baseline method to which we compare.</p><p>Our baseline network is derived from the VGG-16 network <ref type="bibr" target="#b29">[30]</ref>, which consists of 13 convolutional layers followed by three fully connected layers. We adapt the network to regress to person density maps. We remove its two fully connected layers, and the max-pooling layer (pool5) to prevent further reduction of spatial resolution. In their place we add a single convolutional layer (a single 3 ? 3 ? 512 filter with stride 1 and zero padding to maintain same size) which directly regresses to the crowd density map. As the counting loss, L c , we use the Euclidean distance between the estimated and ground truth density maps:</p><formula xml:id="formula_0">L c = 1 M M i=1 (y i ?? i ) 2<label>(1)</label></formula><p>where M is the number of images in a training batch, y i is ground truth person density map of the i-th image in the batch, and the prediction from the network as? i . The network is indicated in orange in <ref type="figure" target="#fig_3">Figure 3</ref>.  Ground truth annotations for crowd counting typically consist of a set of coordinates which indicate the 'center' (typically head center of a person). To convert this data to crowd density maps, we place a Gaussian with standard deviation of 15 pixels and sum these for all persons in the scene to obtain y i . This is a standard procedure and is also used in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>The fact that we derive our architecture from the VGG-16 network has the advantage of being able to use pretrained features from ImageNet. Given the large success of pre-trained features in neural networks, it is somewhat surprising to note that the vast majority of deep learning methods for crowd counting train from scratch <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">31]</ref>. We found, however, that using pre-trained features significantly improves results.</p><p>To further improve the performance of our baseline network, we introduce multi-scale sampling from the available labeled datasets. Instead of using the whole image as an input, we randomly sample square patches of varying size (from 56 to 448 pixels). In the experimental section we verify that this multi-scale sampling is important for good performance. Since we are processing patches rather than images, we will use? i to refer to the estimate of patch i from now on. The importance of multi-scale processing of crowd data was also noted in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Crowd ranking network</head><p>In the previous section we explained how to collect abundantly available data for crowd counting. This data does not have crowd density maps and only ranking data is avail-able via the sampling procedure described in Algorithm 1. This ranking indicates that an equal number or more persons are present in a patch when compared to another. Here we present the network which is trained based on this information. For this purpose, we replace the Euclidean loss by an average pooling layer followed by a ranking loss (network in blue in <ref type="figure" target="#fig_3">Figure 3</ref>). First, the average pooling layer converts the density map into an estimate of the number of persons per spatial unit?(I i ) according to:</p><formula xml:id="formula_1">c (I i ) = 1 M j? i (x j ),<label>(2)</label></formula><p>where x j are the spatial coordinates of the density map, and M = 14 ? 14 is the number of spatial units in the density map. The ranking which is on the total number of persons in the patch? i also directly holds for its normalized version c i , since? (I i ) = M ?? (I i ).</p><p>To enforce the ranking, we apply the pairwise ranking hinge loss which for a single pair is defined as:</p><formula xml:id="formula_2">L r = max (0,? (I 2 ) ?? (I 1 ) + ?),<label>(3)</label></formula><p>where ? is the margin, which is set to zero in our case. This loss increases with the difference between two count estimates when their order does not respect the correct ranking. Without loss of generality, we assume that the rank of?(I 1 ) is higher than?(I 2 ). The gradient with respect to the network parameters ? of the loss in Eq. 3 is given by:</p><formula xml:id="formula_3">? ? L r = 0 if? (I 2 )?? (I 1 ) + ? ? 0 ? ?? (I 2 ) ? ? ?? (I 1 ) otherwise<label>(4)</label></formula><p>When network outputs the correct ranking there is no backpropagated gradient. However, when the network estimates are not in accordance with the correct ranking the backpropagated gradient causes the network to increase its estimate for the patch with lower score and to decrease its estimate for the one with higher score (note that in backpropagation the gradient is subtracted).</p><p>A typical implementation of the ranking loss would involve a Siamese network <ref type="bibr" target="#b7">[8]</ref> where two images are sent through parallel branches of the network which share their parameters. However, in <ref type="bibr" target="#b19">[20]</ref> the authors show that it is computationally advantageous (and sometimes leads to better minima) to send the images in a batch through a single branch and combine them when computing the ranking loss. The ranking loss is then computed with:</p><formula xml:id="formula_4">L r = M i=1 j?S(i) max (0,? (I j ) ?? (I i ) + ?)<label>(5)</label></formula><p>where S (i) is the set of patches containing fewer people than patch i. Note that this relation is only defined for patches which are contained by patch i. In practice we sample minibatches of 25 images which contain 5 sets of 5 images which can be compared among them resulting in a total of 5 ? (4 + 3 + 2 + 1) = 50 pairs in one minibatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Combining counting and ranking data</head><p>Here we discuss three approaches to combining ground truth labeled crowd scenes with data for which only rank information is available. These three approaches are depicted in <ref type="figure" target="#fig_4">Figure 4</ref>. We shortly introduce them here:</p><p>? Ranking plus fine-tuning: In this approach the network is first trained on the large dataset of ranking data, and is next fine-tuned on the smaller dataset for which density maps are available. To the best of our knowledge this is the approach which is used by all self-supervised methods in vision <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20</ref>].</p><p>? Alternating-task training: While ranking plus finetuning works well when the two tasks are closely related, it might perform bad for crowd counting because no supervision is performed to indicate what the network is actually supposed to count. Therefore, we propose to alternate between the tasks of counting and ranking. In practice we perform train for 300 minibatches on a single task before switching to the other, then repeat.</p><p>? Multi-task training: In the third approach, we add the self-supervised task as a proxy to the supervised counting task and train both simultaneously. In each minibatch we sample data from both ranking and labeled datasets and train both tasks simultaneously as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. The loss function for multi-task training is:</p><formula xml:id="formula_5">L = L c + ?L r ,<label>(6)</label></formula><p>where ? sets the relative weight between the counting and ranking loss.</p><p>In the next section we compare these three approaches on several standard dataset for crowd counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We report on several experiments to evaluate our approach with respect to baselines and the state-of-the-art methods. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Experimental Protocol</head><p>We use two standard benchmark crowd counting datasets. The UCF CC 50 dataset is a very challenging dataset introduced by <ref type="bibr" target="#b12">[13]</ref>. It contains 50 annotated images of different resolutions, illuminations and scenes. The variation of densities is very large among images from 94 to 4543 persons with an average of 1280 persons per image.</p><p>The ShanghaiTech dataset introduced by <ref type="bibr" target="#b36">[37]</ref> is a largescale crowd counting dataset consisting of 1198 images with 330,165 annotated heads. This dataset includes two parts: 482 images in Part A which are randomly crawled from the Internet, and 716 images in Part B which are taken from busy streets. Both parts are further divided into training and evaluation sets. The training and test of Part A has 300 and 182 images, respectively, whereas that of Part B has 400 and 316 images, respectively.</p><p>Following existing work, we use the mean absolute error (MAE) and the mean squared error (MSE) to evaluate different methods. These are defined as follows:</p><formula xml:id="formula_6">M AE = 1 N N i=1 C (I i ) ?? (I i ) , M SE = 1 N N i=1 C (I i ) ?? (I i ) 2<label>(7)</label></formula><p>where N is the number of test images, C (I i ) is the ground truth number of persons in the ith image and? (I i ) is the predicted number of persons in the ith image. We use the Caffe <ref type="bibr" target="#b13">[14]</ref> framework and train using minibatch Stochastic Gradient Descent (SGD). The minibatch size for both ranking and counting is 25, and for multi-task training is 50. For the ranking plus fine-tuning method, the learning rate is 1e-6 for both ranking and fine-tuning. Similarly, for the alternating-task training method, the steps for training both tasks are 300 iterations. For the multi-task method, we found ? = 100 to provide good results on all datasets. Learning rates are decreased by a factor of 0.1 every 10K iterations for a total of 20K iterations. For both training phases we use 2 weight decay (set to 5e-4). During training we sample one sub-image from each training image per epoch. We perform down-sampling of three scales and up-sampling of one scale on the UCF CC 50 dataset and only up-sampling of one scale is used on the ShanghaiTech dataset. The number of ranked crops k = 5, the scale factor s = 0.75 and the anchor region r = 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study</head><p>We begin with an ablation study on the UCF CC 50 dataset. The aim is to evaluate the relative gain of the proposed improvements and to evaluate the use of a ranking loss against the baseline. The ranked images in this experiment are generated from the Keyword dataset. The results are summarized in <ref type="table" target="#tab_0">Table 2</ref>. We can observe the benefit of using a pre-trained ImageNet model in crowd counting, with a significant drop in MAE of around 28% compared to the model trained from scratch. By using both multi-scale data augmentation and starting from a pre-trained model, another improvement of around 6% is obtained.</p><p>Next, we compare the three methods we propose for combining the ranking and counting losses. The "Ranking plus fine-tuning" method, which is the approach applied by all self-supervised methods in the literature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20]</ref>, performs worse than directly fine-tuning from a pretrained ImageNet model. This is probably caused by the poorly-defined nature of the self-supervised task. To optimize this task the network could decide to count anything, e.g. 'hats', 'trees', or 'people', all of which would agree with the ranking constraints that are imposed. By jointly learning both the self-supervised and crowd counting tasks, the self-supervised task is forced to focus on counting persons. As a result the "Alternating-task training" method improves the MAE by about 12% when compared to the direct fine-tuning method. Moreover, the "Multi-task training" approach reduces the MAE further to 279.6. Given its excellent results we consider only the "Multi-task training" approach for the remainder of the experiments.</p><p>We also probe how performance scales with increasing training data. We ran an experiment in which we incrementally add supervised training data from Part A of the ShanghaiTech data. Our approach, using only 60% of the labeled data, yields about the same accuracy as training the counting objective alone on 100% of this data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with the state-of-the-art</head><p>We start with the results on the UCF CC 50 dataset. A five-fold cross-validation was performed for evaluating  the methods. Results are shown in <ref type="table" target="#tab_1">Table 3</ref>. Our multitask training method with the Keyword dataset reduces the MAE error from 295.8 to 279.6 compared to the state-ofthe-art. However the MSE of our method on UCF CC 50 dataset is worse then the state-of-the-art methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31]</ref>, which means our methods works better in general but has more extreme outliers. Compared to training on the Keyword dataset, learning from the Query-by-example dataset is slightly worse, which might be because most images from UCF CC 50 are black and white with low resolution, which often does not lead to satisfactory query results. An example of prediction in UCF CC 50 using our network is shown in <ref type="figure" target="#fig_5">Figure 5</ref>.</p><p>Next we compare with state-of-the-art on the two sets of the ShanghaiTech dataset. As shown in <ref type="table" target="#tab_2">Table 4</ref>, similar conclusions as on UCF CC 50 can be drawn. We see that using the our approach further improves by about 2% on ShanghaiTech. For both Part A and Part B, our approach surpasses the state-of-the-art method <ref type="bibr" target="#b30">[31]</ref>. An example of prediction by our network on ShanghaiTech is given in <ref type="figure" target="#fig_5">Figure 5</ref>. For comparison we also provide the results of our baseline method (including fine-tuning from a On Part B our baseline already obtains state-of-the-art, with the best results for the multi-task approach obtaining around a 30% improvement when compared to the state-of-the-art. It should also be noted that the method of <ref type="bibr" target="#b30">[31]</ref> is complementary to ours and an approach which combines both methods is expected to further improve results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation on transfer learning</head><p>As proposed in <ref type="bibr" target="#b36">[37]</ref>, to demonstrate the generalization of the learned model, we test our method in the transfer learning setting by using Part A of the ShanghaiTech dataset as the source domain and using UCF CC 50 dataset as the target domain. The model trained on Part A of ShanghaiTech is used to predict the crowd scene images from UCF CC 50 dataset, and the results can be seen in <ref type="table" target="#tab_3">Table 5</ref>. Using only counting information improves the MAE by 12% compared to reported results in <ref type="bibr" target="#b36">[37]</ref>. By combining both ranking and counting datasets, the MAE decreases from 349.5 to 337.6, and MSE decreases from 475.7 to 434.3. In conclusion, these results show that our method significantly outperforms the only other work reporting results on the task of cross-dataset crowd counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work we proposed a method for crowd counting. The main novelty is based on the observation that a crop which is contained within a larger crop must contain fewer or an equal number of persons than the larger crop. This allows us to address one of the main problems for crowd counting, namely the lack of large training datasets. Our approach enables the exploitation of abundantly available training data from the Internet by automatically generating rankings from them. We showed how this additional data can be leveraged with available annotated data in a multitask network. Experiments show that the proposed self-supervised task improves results significantly when compared to a network which is only trained on the annotated data. We show that incorporating the self-supervised task in a multitask approach obtains optimal results. Furthermore, we obtain state-of-the-art results on two challenging datasets for crowd counting, namely the ShanghaiTech and the UCF CC 50 dataset. Finally, we show that the learned models generalize well to other datasets, significantly outperforming the only other crowd counting method which reports on this transfer learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method MAE MSE</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 1 .</head><label>21</label><figDesc>Using ranked sub-images for self-supervised training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Example images from the retrieved crowd scene dataset. (top) Representative images using key words as query. (bottom) Representative images using training image as query image (the query image is depicted on the left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Step 2 :</head><label>2</label><figDesc>Find the largest square patch centered at the anchor point and contained within the image boundaries.Step 3: Crop k ? 1 additional square patches, reducing size iteratively by a scale factor s. Keep all patches centered at anchor point.Step 4: Resize all k patches to input size of network. Output: A list of patches ordered according to the number of persons in the patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The multi-task framework combining both counting and ranking information. This network can be trained end-to-end for crowd counting. VGG-conv refers to the convolutional layers of the VGG-16 network. See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Three ways to combine ranking and counting datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Examples of predicted density maps for the UCF CC 50 (Top row, true count: 3406 prediction: 3052) and ShanghaiTech datasets (Bottom row, true count: 361 prediction: 365). Left column: crowd image. Middle column: ground truth. Right column: prediction. pre-trained model and multi-scale data augmentation) on this dataset: M AE = 77.7 and M SE = 115.9 on Part A, and M AE = 14.7 and M SE = 24.7 on Part B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Split 1 Split 2 Split 3 Split 4 Split 5 Ave MAE Ave MSE MAE and MSE on the UCF CC 50 dataset with five-fold cross validation. The Basic CNN is trained from scratch on the training set. The second row is the VGG-16 network fine-tuned starting from a pre-trained ImageNet model. The third row is the VGG-16 network trained with multi-scale data augmentation and starting from a pre-trained model. Results for combining both ranking and counting datasets are in the last three rows.</figDesc><table><row><cell>Basic CNN</cell><cell>701.41 394.52 497.57 263.56 415.23</cell><cell>454.45</cell><cell>620.95</cell></row><row><cell>+ Pre-trained model</cell><cell>570.01 350.63 334.89 184.79 202.41</cell><cell>328.54</cell><cell>443.38</cell></row><row><cell>+ multi-scale</cell><cell>532.85 307.43 266.75 216.96 216.35</cell><cell>308.06</cell><cell>408.70</cell></row><row><cell cols="2">Ranking plus fine-tuning 552.68 375.38 241.28 211.66 247.70</cell><cell>325.73</cell><cell>429.28</cell></row><row><cell cols="2">Alternating-task training 454.33 350.63 172.52 214.03 235.70</cell><cell>285.44</cell><cell>401.29</cell></row><row><cell>Multi-task training</cell><cell>443.68 340.31 196.76 218.48 199.54</cell><cell>279.60</cell><cell>408.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>MAE and MSE error on the UCF CC 50 dataset.</figDesc><table><row><cell></cell><cell cols="3">Method MAE MSE</cell></row><row><cell cols="4">Idrees et al. [13] 419.5 541.6</cell></row><row><cell cols="4">Zhang et al. (2015) [35] 467.0 498.5</cell></row><row><cell cols="4">Zhang et al. (2016) [37] 377.6 509.1</cell></row><row><cell cols="4">Onoro et al. [25] 333.7 425.2</cell></row><row><cell cols="4">Walach et al. [33] 364.4 341.4</cell></row><row><cell cols="4">Babu Sam et al. [1] 318.1 439.2</cell></row><row><cell cols="4">Sindagi et al. [31] 295.8 320.9</cell></row><row><cell cols="4">Ours: Multi-task (Query-by-example) 291.5 397.6</cell></row><row><cell cols="4">Ours: Multi-task (Keyword) 279.6 388.9</cell></row><row><cell></cell><cell>Part A</cell><cell cols="2">Part B</cell></row><row><cell cols="4">Method MAE MSE MAE MSE</cell></row><row><cell cols="2">Zhang et al. (2015) [35] 181.8 277.7</cell><cell>32.0</cell><cell>49.8</cell></row><row><cell cols="2">Zhang et al. (2016) [37] 110.2 173.2</cell><cell>26.4</cell><cell>41.3</cell></row><row><cell>Babu Sam et al. [1]</cell><cell>90.4 135.0</cell><cell>21.6</cell><cell>33.4</cell></row><row><cell>Sindagi et al. [31]</cell><cell>73.6 106.4</cell><cell>20.1</cell><cell>30.1</cell></row><row><cell>Ours: Multi-task (Query-by-example)</cell><cell>72.0 106.6</cell><cell>14.4</cell><cell>23.8</cell></row><row><cell>Ours: Multi-task (Keyword)</cell><cell>73.6 112.0</cell><cell>13.7</cell><cell>21.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>MAE and MSE error on the ShanghaiTech dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Zhang et al. (2016) [37] 397.6 624.1 Ours: Counting only 349.5 475.7 Ours: Multi-task 337.6 434.3 Transfer learning across datasets. Models were trained on Part A of ShanghaiTech and tested on UCF CC 50.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Contributions with respect to the state-of-the-art: Basic CNNs are simple and fast to train, but usually achieve lower accuracy. Combining different scale-aware models and context-aware models has been shown to significantly increase performance, but the complexity of these models is high. In addition, considering the scarcity of large annotated datasets, ranked patches are used as side information to decrease the effect of over-fitting. The model we propose in this paper is fast to train, uses no side information, supports fast inference, is scale-aware, is multi-task, and outperforms the state-of-the-art. Our key contribution is in showing how to effectively exploit unlabeled crowd imagery for pre-training CNNs. InTable 1we list current state-of-the-art approaches to crowd counting along with ours and indicate the characteristics of each.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and models: https://github.com/xialeiliu/CrowdCountingCVPR18</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We acknowledge the Spanish project TIN2016-79717-R, the CHISTERA project M2CR (PCIN-2015-251) and the CERCA Programme / Generalitat de Catalunya. Xialei Liu acknowledges the Chinese Scholarship Council (CSC) grant No.201506290018. We also acknowledge the generous GPU donation from NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crowdnet: a deep convolutional network for dense crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM on Multimedia Conference</title>
		<meeting>ACM on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-S</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Counting people with lowlevel features and bayesian regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2160" to="2177" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ranking measures and loss functions in learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flow mosaicking: Real-time pedestrian counting without scene-specific learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1093" to="1100" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast crowd density estimation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06748</idno>
		<title level="m">Crowd counting by adapting convolutional neural networks with side information</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning from rankings for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rankiqa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Crowd counting and profiling: Methodology and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling, Simulation and Visual Analysis of Crowds</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="347" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Crossing the line: Crowd counting by integer programming with local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2539" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguiness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00220</idno>
		<title level="m">Fully convolutional crowd counting on highly congested scenes</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>L?pez-Sastre</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large scale learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Advances in Ranking</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end crowd counting via joint learning local and global count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Crowd counting via weighted vlad on dense attribute feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey of recent advances in cnn-based single image crowd counting and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to count with cnn boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM int. conf. on Multimedia</title>
		<meeting>ACM int. conf. on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Singleimage crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
