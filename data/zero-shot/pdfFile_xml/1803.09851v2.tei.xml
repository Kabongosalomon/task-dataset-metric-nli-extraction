<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attributes as Operators: Factorizing Unseen Attribute-Object Compositions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
							<email>tushar@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
							<email>grauman@fb.com*</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attributes as Operators: Factorizing Unseen Attribute-Object Compositions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new approach to modeling visual attributes. Prior work casts attributes in a similar role as objects, learning a latent representation where properties (e.g., sliced) are recognized by classifiers much in the way objects (e.g., apple) are. However, this common approach fails to separate the attributes observed during training from the objects with which they are composed, making it ineffectual when encountering new attribute-object compositions. Instead, we propose to model attributes as operators. Our approach learns a semantic embedding that explicitly factors out attributes from their accompanying objects, and also benefits from novel regularizers expressing attribute operators' effects (e.g., blunt should undo the effects of sharp). Not only does our approach align conceptually with the linguistic role of attributes as modifiers, but it also generalizes to recognize unseen compositions of objects and attributes. We validate our approach on two challenging datasets and demonstrate significant improvements over the state of the art. In addition, we show that not only can our model recognize unseen compositions robustly in an open-world setting, it can also generalize to compositions where objects themselves were unseen during training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Attributes are semantic descriptions that convey an object's properties-such as its materials, colors, patterns, styles, expressions, parts, or functions. Attributes have proven to be an effective representation for faces and people <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b31">32]</ref>, catalog products <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b16">17]</ref>, and generic objects and scenes <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b0">1]</ref>. Because they are expressed in natural language, attributes facilitate human-machine communication about visual content, e.g., for applications in image search <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24]</ref>, zero-shot learning <ref type="bibr" target="#b0">[1]</ref>, narration <ref type="bibr" target="#b24">[25]</ref>, or image generation <ref type="bibr" target="#b54">[55]</ref>.</p><p>Attributes and objects are fundamentally different entities: objects are physical things (nouns), whereas attributes are properties of those things (adjectives). Despite this fact, existing methods for attributes largely proceed in the same manner as state-of-the-art object recognition methods. Namely, image examples labeled according to the attributes present are used to train discriminative models, e.g., with a convolutional neural network <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>The latent vector encoding learned by such models is expected to capture an objectagnostic attribute representation. Yet, achieving this is problematic, both in terms of <ref type="figure">Fig. 1</ref>: Conceptual overview of our idea. Left: Unlike for objects, it is difficult to learn a predictable visual prototype for an attribute (e.g., "sliced" as shown here). Furthermore, standard visual recognition pipelines are prone to overfit to those object-attribute pairings observed during training. Right: We propose to model attributes as operators, learning how they transform objects rather than what they themselves look like. Once learned, the effects of the attribute operators are generalizable to new, unseen object categories.</p><p>data efficiency and generalization. Specifically, it assumes during training that 1) the attribute has been observed in combination with all potential objects (unrealistic and not scalable), and/or 2) an attribute's influence is manifested similarly across all objects (rarely the case, e.g., "old" influences church and shoe differently). We observe that with the attribute's meaning so intrinsically tied to the object it describes, an ideal attribute vector encoding may not exist. See <ref type="figure">Figure 1</ref>, left.</p><p>In light of these issues, we propose to model attributes as operators -with the goal of learning a model for attribute-object composition itself capable of explicitly factoring out the attributes' effect from their accompanying object representations.</p><p>First, rather than encode an attribute as a point in some embedding space, we encode it as a (learned) transformation that, when applied to an object encoding, modifies it to appropriately transform its appearance (see <ref type="bibr">Figure 1,</ref><ref type="bibr">right)</ref>. In particular, we formulate an embedding objective where compositions and images project into the same semantic space, allowing recognition of unseen attribute-object pairings in novel images. <ref type="bibr" target="#b2">3</ref> Second, we introduce novel regularizers during training that capitalize on the attributeas-operator concept. For example, one regularizer requires that the effect of applying an attribute and then its antonym to an object should produce minimal change in the object encoding (e.g., blunt should "undo" the effects of sharp); another requires commutativ-ity when pairs of attributes modify an object (e.g., a sliced red apple is equivalent to a red sliced apple).</p><p>We validate our approach on two challenging datasets: MIT-States <ref type="bibr" target="#b32">[33]</ref> and UT-Zappos <ref type="bibr" target="#b55">[56]</ref>. Together, they span hundreds of objects, attributes, and compositions. The results demonstrate the advantages of attributes as operators, in terms of the accuracy in recognizing unseen attribute-object compositions. We observe significant improvements over state-of-the-art methods for this task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref>, with absolute improvements of 3%-12%. Finally, we show that our method is similarly robust whether identifying unseen compositions on their own or in the company of seen compositions-which is of great practical value for recognition in realistic, open world settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual attributes. Early work on visual attributes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">36]</ref> established the task of inferring mid-level semantic descriptions from images. The research community has since explored many applications for attributes, including image search <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44]</ref>, zero-shot object categorization <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1]</ref>, sentence generation <ref type="bibr" target="#b24">[25]</ref> and fashion image analysis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref>. Throughout, the standard approach to learn attributes is very similar to that used to learn object categories: discriminative classifiers with labeled examples. In particular, today's best accuracies are obtained by training a deep convolutional neural network to classify attributes <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b46">47]</ref>. Multi-task attribute training methods account for correlations between different attributes <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44]</ref>. Our approach is a fundamental departure from all of the above: rather than consider attribute instances as points in some high-dimensional space that can be classified, we consider attributes as operators that transform visual data from one condition to another.</p><p>Composition in language and vision. In natural language processing, the composition of adjectives and nouns is modeled as single compositions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref> or transformations (i.e., an adjective transformation applied to the noun vector) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b45">46]</ref>. Bridging such linguistic concepts to visual data, some work explores the correlation between similarity scores for color-object pairs in the language and visual domains <ref type="bibr" target="#b34">[35]</ref>.</p><p>Composition in vision has been studied in the context of modeling compound objects <ref type="bibr" target="#b38">[39]</ref> (clipboard = clip + board), verb-object interactions <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b57">58]</ref> (riding a horse = person + riding + horse), and adjective-noun combinations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b8">9]</ref> (fluffy towel = towel modified by fluffy). All these approaches leverage the key insight that the characteristics of the composed entities could be very different from their constituents; however, they all subscribe to the traditional notion of representing constituents as vectors, and compositions as black-box modifications of these vectors. Instead, we model compositions as unique operators conditioned on the constituents (e.g., for attribute-object composition, a different modification for each attribute).</p><p>Limited prior work on attribute-object compositions considers unseen compositions, that is, where each constituent is seen during training, but new unseen compositions are seen at test time <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref>. Both methods construct classifiers for composite concepts using pre-trained linear classifiers for the "seen" primitive concepts, either with tensor completion <ref type="bibr" target="#b4">[5]</ref> or neural networks <ref type="bibr" target="#b32">[33]</ref>. Recent work extends this notion to expressions connected by logical operators <ref type="bibr" target="#b8">[9]</ref>. We tackle unseen compositions as well. However, rather than treat attributes and objects alike as classifier vectors and place the burden of learning on a single network, we propose a factored representation of the constituents, modeling attribute-object composition as an attribute-specific invertible transformation on object vectors. Our formulation also enables novel regularizers based on the attributes' linguistic meaning. Our model naturally extends to compositions where the objects themselves are unseen during training, unlike <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref> which requires an SVM classifier to be trained for every new object. In addition, rather than exclusively predict unseen compositions as in <ref type="bibr" target="#b32">[33]</ref>, we also study the more realistic scenario where all compositions are candidates for recognition.</p><p>Visual transformations. The notion of visual "states" has been explored from several angles. Given a collection of images <ref type="bibr" target="#b19">[20]</ref> or time-lapse videos <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b26">27]</ref>, methods can discover transformations that map between object states in order to create new images or visualize their relationships. Given video input, action recognition can be posed as learning the visual state transformation, e.g., how a person manipulates an object <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2]</ref> or how activity preconditions map to postconditions <ref type="bibr" target="#b50">[51]</ref>. Given a camera transformation, other methods visualize the scene from the specified new viewpoint <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b58">59]</ref>. While we share the general concept of capturing a visual transformation, we are the first to propose modeling attributes as operators that alter an object's state, with the goal of recognizing unseen compositions.</p><p>Low-shot learning with sample synthesis. Recent work explores ways to generate synthetic training examples for classes that rarely occur, either in terms of features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b60">61]</ref> or entire images <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b7">8]</ref>. One part of our novel regularization approach also involves hypothetical attribute-transformed examples. However, whereas prior work explicitly generates samples offline to augment the dataset, our feature generation is an implicit process to regularize learning and works in concert with other novel constraints like inverse consistency or commutativity (see Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our goal is to identify attribute-object compositions (e.g., sliced banana, fluffy dog) in an image. Conventional classification approaches suffer from the long-tailed distribution of complex concepts <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b29">30]</ref> and a limited capacity to generalize to unseen concepts. Instead, we model the composition process itself. We factorize out the underlying primitive concepts (attributes and objects) seen during training, and use them as building blocks to identify unseen combinations during inference. Our approach is driven by the fundamental narrative: if we've seen a sliced orange, a sliced banana, and a rotten banana, can we anticipate what a rotten orange looks like?</p><p>We model the composition process around the functional role of attributes. Rather than treat objects and attributes equally as vectors, we model attributes as invertible operators, and composition as an attribute-conditioned transformation applied to object vectors. Our recognition task then turns into an embedding learning task, where we project images and compositions into a common semantic space to identify the composition present. We guide the learning with novel regularizers that are consistent with the linguistic behavior of attributes. </p><formula xml:id="formula_0">? ? ? fc predict "sliced" predict "banana" g(p) f(x) -1 M M M M M M M M sliced sliced ripe ripe ripe ripe unripe -1 banana banana banana banana (b) Proposed regularizers.</formula><p>We propose several regularizers that conform with the linguistic meaning of attributes. Laux ensures that the identity of the attribute/object is not lost during composition; Linv swaps out attributes to implicitly synthesize new compositions for training; Lant models antonyms ("unripe" should undo the effects of "ripe"); and Lcomm models the commutative property of attributes (a ripe sliced banana is the same as a sliced ripe banana). In the following, we start by formally describing the embedding learning problem in Section 3.1. We then describe the details of our embedding scheme for attributes and objects in Section 3.2. We present our optimization objective and auxiliary loss terms in Section 3.3. Finally, we describe our training methodology in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unseen pair recognition as embedding learning</head><p>We train a model that learns a mapping from a set of images X to a set of attributeobject pairs P = A ? O. For example, "old-dog" is one attribute-object pairing. We divide the set of pairs into two disjoint sets: P s , which is a set of pairs that is seen during training and is used to learn a factored composition model, and P u , which is a set of pairs unseen during training, yet perfectly valid to encounter at test time. While P s and P u are completely disjoint, their constituent attributes and objects are observed in some 6 T. Nagarajan and K. Grauman (other) composition during training. Our images contain objects with a single attribute label associated with them, i.e., each image has a unique pair label p ? P.</p><p>During training, given an image x ? X and its corresponding pair label p ? P s , we learn two embedding functions f (x) and g(p) to project them into a common semantic space. For f (x), we use a pretrained ResNet18 <ref type="bibr" target="#b14">[15]</ref> followed by a linear layer. For g(p), we introduce an attribute-operator model, described in detail in Section 3.2.</p><p>We learn the embedding functions such that in this space, the Euclidean distance between the image embedding f (x) and the correct pair embedding g(p) is minimized, while the distance to all incorrect pairs is maximized. Distance in this space represents compatibility-i.e., a low distance between an image and pair embedding implies the pair is present in the image. Critically, once g(p) is learned, even an unseen pair can be projected in this semantic space, and its compatibility with an image can be assessed. See <ref type="figure" target="#fig_0">Figure 2a</ref>.</p><p>During inference, we compute and store the pair embeddings of all potential pair candidates from P using our previously learned composition function g(.). When presented with a new image, we embed it as usual using f (.), and identify which of the pair embeddings is closest to it. Note how P includes both pairs seen in training as well as unseen attribute-object compositions; recognizing the latter would not be possible if we were doing a simple classification among the previously seen combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attribute-operator model for composition</head><p>As discussed above, the conventional approach treats attributes much like objects, both occupying some point/region in an embedding space <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19</ref>].</p><p>On the one hand, it is meaningful to conjure a latent representation for an "attributefree object"-for example, dog exists as a concept before we specialize it to be a spotted or fluffy dog. In fact, in the psychology of perception, one way to characterize a socalled basic-level category is by its affordance of a single mental prototype <ref type="bibr" target="#b39">[40]</ref>. On the other hand, however, it is problematic to conjure an "object-free attribute". What does it mean to map "fluffy" as a concept in a semantic embedding space? What is the visual prototype of "fluffy"? See <ref type="figure">Figure 1</ref>.</p><p>We contend that a more natural way of describing attributes is in how they modify the objects they refer to. Images of a "dog" and a "fluffy dog" help us estimate what the concept "fluffy" refers to. Moreover, these modifications are strongly conditioned on the object they describe ("fluffy" exhibits itself significantly differently in "fluffy dog" compared to "fluffy pillow"). In this sense, attribute behavior bears some resemblance to geometric transformations. For example, rotation can be perfectly represented as an orthogonal matrix acting on a vector. Representing rotation as a vector, and its action as some additional function, would be needlessly complicated and unintuitive.</p><p>With this in mind, we represent each object category o ? O as a D-dimensional vector, which denotes a prototypical object instance. Specifically, we use GloVe word embeddings <ref type="bibr" target="#b37">[38]</ref> for the object vector space. Each attribute a ? A is a parametrized function g a : R D ? R D that modifies an object representation to exhibit that attribute, and brings it to the semantic space where images reside. For simplicity, we consider a linear transform for g a , represented by a D ? D matrix M a :</p><formula xml:id="formula_1">g(p) = g a (o) = M a o,<label>(1)</label></formula><p>though the proposed framework (excluding the inverse consistency regularizer) naturally supports more complex functions for g a as well. See <ref type="figure" target="#fig_0">Figure 2a</ref>, top right. Interesting properties arise from our attribute-operator design. First, factorizing composition as a matrix-vector product facilitates transfer: an unseen pair can be represented by applying a learned attribute operator to an appropriate object vector <ref type="figure" target="#fig_0">(Figure 2a</ref>, bottom left). Secondly, since images and compositions reside in the same space, it is possible to remove attributes from an image by applying the inverse of the transformation; multiple attributes can be applied consecutively to images; and the structure of the attribute space can be coded into how the transformations behave. Below we discuss how we leverage these properties to regularize the learning process (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning objective for attributes as operators</head><p>Our training set consists of n images and their pair labels, {(x 1 , p 1 ), . . . , (x n , p n )}. We design a loss function to efficiently learn to project images and composition pairs to a common embedding space. We begin with a standard triplet loss. The loss for an image x with pair label p = (a, o) is given by:</p><formula xml:id="formula_2">L triplet = max (0, d(f (x), M a o) ? d(f (x), M a o ) + m) , ? a = a ? o = o, (2)</formula><p>where d denotes Euclidean distance, and m is the margin value, which we keep fixed at 0.5 for all our experiments. In other words, the embedded image ought to be closer to its object transformed by the specified attribute a than other attribute-object pairings.</p><p>Thus far, the loss is similar in spirit to embedding based zero-shot learning methods <ref type="bibr" target="#b53">[54]</ref>, and more generally to triplet-loss based representation learning methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b42">43]</ref>. We emphasize that our focus is on learning a model for the composition operation; a triplet-loss based embedding is merely an appropriate framework that facilitates this. In the following, we extend this framework to effectively accommodate attributes as operators and inject our novel linguistic-based regularizers.</p><p>Object and attribute auxiliaries. In our model, both the attribute operator and object vector, and thereby their composition, are learnable parameters. It is possible that one element of the composition (either attributes or objects) will dominate during optimization, and try to capture all the information instead of learning a factorized model. This could lead to a composition representation, where one component does not adequately feature. To address this, we introduce an auxiliary loss term that forces the composed representation to be discriminative, i.e., it must be able to predict both the attribute and object involved in the composition:</p><formula xml:id="formula_3">L aux = ? i?A ? ai log(p i a ) ? i?O ? oi log(p i o ),<label>(3)</label></formula><p>where ? yi = 1 iff y = i, and p a and p o are the outputs of softmax linear classifiers trained to discriminate the attributes and objects, respectively. This auxiliary supervision ensures that the identity of the attribute and the object are not lost in the composed representation-in effect, strongly incentivizing a factorized representation.</p><p>Inverse consistency. We exploit the invertible nature of our attributes to implicitly synthesize new training instances to regularize our model further. More specifically, we swap out an actual attribute a from the training example for a randomly selected one a , and construct another triplet loss term to account for the new composition:</p><formula xml:id="formula_4">f (x ) := M a M ?1 a f (x) L inv = max (0, d(f (x ), M a o) ? d(f (x ), M a o) + m) ,<label>(4)</label></formula><p>where the triplet loss notation is in the same form as Eq 2.</p><p>Here M a M ?1 a represents the removal of attribute a to arrive at the "prototype object" description of an image, and then the application of attribute a to imbue the object with a new attribute. As a result, f (x ) represents a pseudo-instance with a new attribute-object pair, helping the model generalize better.</p><p>The pseudo-instances generated here are inherently noisy, and factoring them in directly (as a new instance) may obstruct training. To mitigate this, we select our negative example to target the more direct, and thus simpler consequence of this swapping. For example, when we swap out "sliced" for "ripe" from a sliced banana to make a ripe banana, we focus on the more obvious fact-that it is no longer "sliced"-by picking the original composition (sliced banana) as the negative, rather than sampling a completely new one.</p><p>Commutative attribute operators. Next we constrain the attributes to respect the commutative property. For example, applying the "sliced" operator after the "ripe" operator is the same as applying "ripe" after "sliced", or in other words a ripe sliced banana is the same as a sliced ripe banana. This commutative loss is expressed as:</p><formula xml:id="formula_5">L comm = a,b?A M a (M b o) ? M b (M a o) 2 .<label>(5)</label></formula><p>This loss forces the attribute transformations to respect the notion of attribute composability we observe in the context of language.</p><p>Antonym consistency. The final linguistic structure of attributes we aim to exploit is antonyms. For example, we hypothesize that the "blunt" operator should undo the effects of the "sharp" operator. To that end, we consider a loss term that operates over pairs of antonym attributes (a, a ):</p><formula xml:id="formula_6">L ant = a,a ?A M a (M a o) ? o 2 .<label>(6)</label></formula><p>For the MIT-States dataset (cf. Sec. 4), we manually identify 30 antonym pairs like ancient/modern, bent/straight, blunt/sharp. <ref type="figure" target="#fig_0">Figure 2b</ref> recaps all the regularizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and inference</head><p>We minimize the combined loss function (L triplet + L aux + L inv + L comm + L ant ) over all the training images, and train our network end to end. The learnable parameters are: the linear layer for f (x), the matrices for every attribute M a , ?a ? A, the object vectors ?o ? O and the two fully-connected layers for the auxiliary classifiers.</p><p>During training, we embed each labeled image x in a semantic space using f (x), and apply its attribute operator g a to its object vector o to get a composed representation g a (o). The triplet loss pushes these two representations close together, while pushing incorrect pair embeddings apart. Our regularizers further make sure compositions are discriminative; attributes obey the commutative property; they undo the effects of their antonyms; and we implicitly synthesize instances with new compositions.</p><p>For inference, we compute and store the embeddings for all candidate pairs, g a (o), ?o ? O and ?a ? A. When a new image q arrives, we sort the pre-computed embeddings by their distance to the image embedding f (q), and identify the compositions with the lowest distances. The distance calculations can be performed quickly on our dataset with a few thousand pairs. Intelligent pruning strategies may be employed to reduce the search space for larger attribute/object vocabularies. We stress that the novel image can be assigned to an unseen composition absent in training images.We evaluate accuracy on the nearest compositionp q = (o q , a q ) as our datasets support instances with single attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments explore the impact of modeling attributes as operators, particularly for recognizing unseen combinations of objects and attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Datasets. We evaluate our method on two datasets:</p><p>-MIT-States <ref type="bibr" target="#b19">[20]</ref>: This dataset has 245 object classes, 115 attribute classes and ?53K images. There is a wide range of objects (e.g., fish, persimmon, room) and attributes (e.g., mossy, deflated, dirty). On average, each object instance is modified by one of the 9 attributes it affords. We use the compositional split described in <ref type="bibr" target="#b32">[33]</ref> for our experiments, resulting in disjoint sets of pairs-about 1.2K pairs in P s for training and 700 pairs in P u for testing.</p><p>-UT-Zappos50k <ref type="bibr" target="#b56">[57]</ref>: This dataset contains 50K images of shoes with attribute labels. We consider the subset of ?33K images that contain annotations for material attributes of shoes (e.g., leather, sheepskin, rubber); see Supp. The object labels are shoe types (e.g., high heel, sandal, sneaker). We split the data randomly into disjoint sets, yielding 83 pairs in P s for training and 33 pairs in P u for testing, over 16 attribute classes and 12 object classes.</p><p>The datasets are complementary. While MIT-States covers a wide array of everyday objects and attributes, UT-Zappos focuses on a fine-grained domain of shoes. In addition, object annotations in MIT-States are very sparse (some classes have just 4 images), while the UT-Zappos subset has at least 200 images per object class.</p><p>Evaluation metrics. We report top-1 accuracy on recognizing pair compositions. We report this accuracy in two forms: (1) Over only the unseen pairs, which we refer to as the closed world setting. During test time, we compute the distance between our image embedding and only the pair embeddings of the unseen pairs P u , and select the nearest one. The closed world setting artificially reduces the pool of allowable labels at test time to only the unseen pairs. This is the setting in which <ref type="bibr" target="#b32">[33]</ref> report their results.</p><p>(2) Over both seen and unseen pairs, which we call the open world setting. During test time, we consider all pair embeddings in P as candidates for recognition. This is more realistic and challenging, since no assumptions are made about the compositions present. We aim for high accuracy in both these settings. We report the harmonic mean of these accuracies given by h-mean = 2 * (open * closed)/(open + closed), as a consolidated metric. Unlike the arithmetic mean, it penalizes large performance discrepancies between settings. The harmonic mean is recommended to handle a similar discrepancy between seen/unseen accuracies in "generalized" zero-shot learning <ref type="bibr" target="#b53">[54]</ref>, and is now widely adopted as an evaluation metric <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Implementation details. For all experiments, we use an ImageNet <ref type="bibr" target="#b40">[41]</ref> pretrained ResNet-18 <ref type="bibr" target="#b14">[15]</ref> for f (x). For fair comparison, we do not finetune this network. We project our images and compositions to a D = 300-dim. embedding space. We initialize our object and attribute embeddings with GloVe <ref type="bibr" target="#b37">[38]</ref> word vectors where applicable, and initialize attribute operators with the identity matrix as this leads to more stable training. All models are implemented in PyTorch. ADAM with learning rate 1e ? 4 and batch size 512 is used. The attribute operators are trained with learning rate 1e ? 5 as they encounter larger changes in gradient values. Our code is available at github.com/attributes-as-operators.</p><p>Baselines and existing methods. We compare to the following methods:</p><p>-VISPROD uses independent classifiers on the image features to predict the attribute and object. It represents methods that do not explicitly model the composition operation. The probability of a pair is simply the product of the probability of each constituent: P (a, o) = P (a)P (o). We report two versions, differing in the choice of the classifier used to generate the aforementioned probabilities: VISPROD(SVM) uses a Linear SVM (as used in <ref type="bibr" target="#b32">[33]</ref>), and VISPROD(NN) uses a single layer softmax regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-ANALOGOUSATTR [5] trains a linear SVM classifier for each seen pair, then uses</head><p>Bayesian Probabilistic Tensor Factorization (BPTF) to infer classifier weights for unseen compositions. We use the same existing code 4 as <ref type="bibr" target="#b4">[5]</ref> to recreate this model. -REDWINE <ref type="bibr" target="#b32">[33]</ref> trains a neural network to transform linear SVMs for the constituent concepts into classifier weights for an unseen combination. Since the authors' code was not available, we implement it ourselves following the paper closely. We train the SVMs with image features consistent with our models. We verify we could reproduce their results with VGG (network they employed), then upgrade its features to ResNet to be more competitive with our approach. -LABELEMBED is like the REDWINE model, except it composes word vector representations rather than classifier weights. We use pretrained GloVe <ref type="bibr" target="#b37">[38]</ref> word embeddings. This is the LabelEmbed baseline designated in <ref type="bibr" target="#b32">[33]</ref>.  <ref type="table">Table 1</ref>: Accuracy (%) on unseen pair detection. Our method outperforms all previous methods in the open world setting. It also is strongest in the consolidated harmonic mean (h-mean) metric that accounts for both the open and closed settings. Our method's gain is significantly wider when we eliminate the pressure caused by scarce object training data, by providing oracle object labels during inference to all methods ("+obj"). The harmonic mean is calculated over the open and closed settings only (it does not factor in +obj).</p><p>To our knowledge <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref> are the most relevant methods for comparison, as they too address recognition of unseen object-attribute pairs. For all methods, we use the same ResNet-18 image features used in our method; this ensures any performance differences can be attributed to the model rather than the CNN architecture. For all neural models, we ensure that the number of parameters and model capacity are similar to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative results: recognizing object-attribute compositions</head><p>Detecting unseen compositions. <ref type="table">Table 1</ref> shows the results. Our method outperforms all previously reported results and baselines on both datasets by a large margin-around 6% on MIT-States and 14% on UT-Zappos in the open world setting-indicating that it learned a strong model for visual composition.</p><p>The absolute accuracies on the two datasets are fairly different. Compared to UT-Zappos, MIT-States is more difficult owing to a larger number of attributes, objects, and unseen pairs. Moreover, it has fewer training examples for primitive object concepts, leading to a lower accuracy overall.</p><p>Indeed, if an oracle provides the true object label on a test instance, the accuracies are much more consistent across both datasets ("+obj" in <ref type="table">Table 1</ref>). This essentially trims the search space down to the attribute afforded by the object in question, and serves as an upper bound for each method's accuracy. On MIT-States, without object labels, the gap between the strongest baseline and our method is about 6%, which widens significantly to about 22% when object labels are provided (to all methods). On UT-Zappos, all methods improve with the object oracle, yet the gap is more consistent with and without (14% vs. 19%). This is consistent with the datasets' disparity in label distribution; the model on UT-Zappos learns a good object representation by itself.</p><p>ANALOGOUSATTR <ref type="bibr" target="#b4">[5]</ref> varies significantly between the two datasets; it relies on having a partially complete set of compositions in the form of a tensor, and uses that information to "fill in the gaps". For UT-Zappos, this tensor is 43% complete, making completion a relatively simpler task compared to MIT-States, where the tensor is only 12 T. Nagarajan and K.  <ref type="table">Table 2</ref>: Ablation study of regularizers used. The auxiliary classifier loss is essential to our method. Adding other regularizers that are consistent with how attributes function also produces boosts in accuracy in most cases, highlighting the merit of thinking of attributes as operators.</p><p>4% complete. We believe that over-fitting due to this extreme sparsity is the reason we observe low accuracies for ANALOGOUSATTR on this dataset.</p><p>In the closed world setting, our method does not perform as well as some of the other baselines. However, this setting is contrived and arguably a weaker indication of model performance. In the closed world, it is easy for a method to produce biased results due to the artificially pruned label space during inference. For example, the attribute "young" occurs in only one unseen composition during test time-"young iguana". Since all images during test time that contain iguanas are of "young iguanas", an attribute-blind model is also perfectly capable of classifying these instances correctly, giving a false sense of accuracy. In practical applications, the separation into seen and unseen pairs arises from natural data scarcity. In that setting, the ability to identify unseen compositions in the presence of known compositions, i.e., the open world, is a critical metric.</p><p>The lower performance in the closed world appears to be a side-effect of preventing overfitting to the subset of closed-world compositions. All models except ours have a large difference between the closed and open world accuracy. Our model operates robustly in both settings, maintaining similar accuracies in each. Our model outperforms the other models in the harmonic mean metric as well by about 3% and 12% on MIT-States and UT-Zappos, respectively. <ref type="table">Table 2</ref> examines the effects of each proposed regularizer on the performance of our model. We see that the auxiliary classification loss stabilizes the learning process significantly, and results in a large increase in accuracy on both datasets. For MIT-States, including the inverse consistency and the commutative operator regularizers provide small boosts and a reasonable increase when used together. For UT-Zappos, the effect of inverse consistency is less pronounced, possibly because the abundance of object training data makes it redundant. The commutative regularizer provides the biggest improvement of 4%. Antonym consistency is not very helpful on MIT-States, perhaps due to the wide visual differences between some antonyms. For example, "ripe" and "unripe" for fruits produce vibrant color changes, and undoing one color change does not directly translate to applying the other i.e., "ripe" may not be the visual inverse of "unripe". <ref type="bibr" target="#b4">5</ref> These ablation experiments show the merits of pushing our model to be consistent with how attributes operate. Overall, the results on two challenging and diverse datasets strongly support our idea to model attributes as operators. Our method consistently outperforms state-ofthe-art methods. Furthermore, we see the promise of injecting novel linguistic/semantic operations into attribute learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of regularizers.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative results: retrieving images for unseen descriptions</head><p>Next, we show examples of our approach at work to recognize unseen compositions.</p><p>Image retrieval for unseen compositions. With a learned composition model in place, our method can retrieve relevant images for textual queries for object-attribute pairs unseen during training. The query itself is in the form of an attribute a and an object o; we embed them, and all the image candidates x, in our semantic space, and select the ones that are nearest to our desired composition. We stress that these compositions are completely new and arise from our model's factored representation of composition. <ref type="figure" target="#fig_1">Figure 3</ref> shows examples. The query is shown in text, and the top 5 nearest images in embedding space are shown alongside. Our method accurately distinguishes between attribute "states" of the same object to retrieve relevant images for the query. The last row shows failure cases. We observe characteristic failures for compositions involving some under-represented object classes in training pairs. For example, compositions involving "hat" are poorly learned as it features in only two training compositions. We also observe common failures involving ambiguous labels (examples of moldy bread are also often sliced in the data).</p><p>Image retrieval for out-of-domain compositions. <ref type="figure" target="#fig_2">Figure 4</ref> takes this task two steps further. First, we perform retrieval on an image database disjoint from training to demonstrate robustness to domain shift in the open world setting. <ref type="figure" target="#fig_2">Figure 4 (left)</ref> shows retrievals from the ImageNet validation set, a set of 50K images disjoint from MIT-States. Even across this dataset, our model can retrieve images with unseen compositions. As  to be expected, there is much more variation. For example, bottle-caps in ImageNet-an object class that is not present in MIT-States-are misconstrued as coins.</p><p>Second, we perform retrieval on the disjoint database and issue queries for compositions that are in neither the training nor test set. For example, the objects barn or cycle are never seen in MIT-States, under any attribute composition. We refer to these compositions as out-of-domain. Our method handles them by applying attribute operators to GloVe object vectors. <ref type="figure" target="#fig_2">Figure 4 (right)</ref> shows examples. This generalization is straightforward with our method, whereas it is prohibited by the existing methods RED-WINE <ref type="bibr" target="#b32">[33]</ref> and ANALOGOUSATTR <ref type="bibr" target="#b4">[5]</ref>. They rely on having pre-trained SVMs for all constituent concepts. In order to allow an out-of-domain composition with a new object category, those methods would need to gather labeled images for that object, train an SVM, and repeat their full training pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a model of attribute-object composition built around the idea of "attributes as operators". We modeled this composition as an attribute-conditioned transformation of an object vector, and incorporated it into an embedding learning model to identify unseen compositions. We introduced several linguistically inspired auxiliary loss terms to regularize training, all of which capitalize on the operator model for attributes. Experiments show considerable gains over existing models. Our method generalizes well to unseen compositions, in open world, closed world, and even out-of-domain settings. In future work we plan to explore extensions to accommodate relative attribute comparisons and to deal with compositions involving multiple attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Supplementary Details</head><p>This section consists of supplementary material to support the main paper text. The contents include: Attribute Affordances: Open vs. Closed world</p><p>As discussed in Section 4.2 of the main paper, recognition in the closed world setting is considerably easier than in the open world setting due to the reduced search space for attribute-object pairs. <ref type="figure" target="#fig_4">Figure 5</ref> highlights this difference for the MIT-States dataset. In the open world setting, each object would have many potential candidates for compositions (red region), but in the closed world case (blue region), this shrinks to a fraction of compositions. Overall this translates to a 2.8? higher chance of randomly picking the correct composition in the closed world. To make things worse, about 14% of the objects occur in the test set compositions that are dominated by a single attribute. For example, "tiger" affords 2 attributes, but one of those occurs in a single image, leaving old tiger as the only relevant composition. A model with poor attribute recognition abilities can still get away with a high accuracy in terms of the "closed world" setting as a result, giving a false sense of performance.</p><p>Previous work has focused only on the closed world setting, and as a result, compromised on the ability to perform well in the open world ( <ref type="table">Table 1</ref> in the main text). Our model performs similarly well in both settings, indicating that it does not become dependent on the (artificially) simpler setting of the closed world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LABELEMBED+ Details</head><p>In Section 4.1 (Baselines) of the main paper, we propose the LABELEMBED+ baseline as an improved baseline model, improving the LABELEMBED baseline presented in the REDWINE paper by Misra et al. We present the details of the architecture of this baseline here. We use a two layer feed-forward network. Specifically, we concatenate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIT-States UT-Zappos closed open h-mean closed open h-mean</head><p>LabelEmbed+ <ref type="formula" target="#formula_1">(1)</ref>   the two primitive input representations of dimension D each, and pass it through a feedforward network with the configuration (linear-relu-linear) and output dimensions 2D and D. Unlike REDWINE, we transform the image representation using a single linear layer, followed by a ReLU non-linearity. We do this to allow some flexibility on the side of the image representation (since we are not finetuning the network responsible for generating the image features themselves).</p><p>Here we report additional experiments where we vary the number of layers for this baseline <ref type="table" target="#tab_5">(Table 3)</ref>. We see that our model outperforms LABELEMBED+ regardless of how many layers are used. This suggests that our improvements are a result of learning a better composition model, and is not related to the network capacity of the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines Variants</head><p>Next we include modifications to the baselines presented in Section 4.  <ref type="table">Table 4</ref>: Baseline variants on MIT-States. The proposed auxiliary loss term, together with allowing attribute and object representations to be optimized during training, can also help the baselines learn a better composition model. Our complete model outperforms these baseline variants as well. <ref type="table">Table 4</ref> shows the results. The first column denotes the models as reported in the main paper, while the second column shows the models with extra components from our own model. Note that our model already includes these modifications, and we simply repeat its results on the "augmented" side for clarity. REDWINE and LABELEMBED are not severely affected because of the way the composition is interpreted-as a set of classifier weights. Extracting the attribute and object identity from classifier weights is less meaningful compared to extracting them from a general composition embedding. The auxiliary loss does however improve the embedding learning models. Our model outperforms all augmented variants of the baselines as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Composition Space</head><p>We visualize the common embedding space described in Section 3.1. <ref type="figure">Figure 6</ref> contains the 2D TSNE projection of the 300D space generated by our method. The black points represent the embeddings of all unseen compositions in MIT-States. Each cluster (squared) represents the span of a single attribute operator-i.e., the points in the vectorspace that it can reach by transforming object vectors. Our composition model maintains a clear separation between several attribute-object compositions, despite many sharing the same object or attribute.</p><p>We also highlight three object superclasses, "fruit", "scenes" and "clothing", and plot all the compositions they are involved in to show which parts of this subspace are shared among different object classes. We see that common attributes like old and new are shared by many objects of each superclass, while more specialized attributes like caramelized for "fruit" are separated in this space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UT-Zappos Subset Selection</head><p>As discussed in Section 4.1 (Datasets) of the main paper, we use a subset of the publicly available UT-Zappos50K dataset in our experiments. The attributes and annotations typically used in this dataset are relative attributes, which are not relevant for our experiments and are not applicable for comparisons to existing work. However, it also contains labels for binary material attributes that are relevant for our experiments. <ref type="figure">Fig. 6</ref>: TSNE visualization of our common embedding space. The span of an attribute operator represents the extent of its application to all objects that afford the attribute. Here each cluster of black points represents a single attribute's span. We see a strong separation between compositions in this semantic space, despite these compositions sharing attributes or objects. We highlight composition clusters for three common object superclasses, "fruit", "scenes" and "clothing". Each colored 'x' corresponds to a single composition involving an object from the respective superclass. For example, a blue 'x' is a composition of the form (attr, fruit) where attr is one of (peeled, diced, etc.) and fruit is one of (apple, banana, etc.).</p><p>Here we describe the process for generating the subset of UT-Zappos50K that we use in our experiments. These images have top level object categories of shoe type (e.g., high heel, sandal, sneaker) as well as finer-grained shoe-type labels (e.g., ankle boots, knee-high boots for the top-level boots category). We merge object categories that have fewer than 200 images per category into a single class (e.g., all slippers are considered as one class), and discard the sub-classes that do not meet this threshold amount. We then discard all the images that do not have annotations for material attributes of shoes (e.g., leather, sheepskin, rubber), which leaves us with ?33K images. We randomly split this set of images into training and testing sets based on their attribute-object compositions. Our subset contains 116 compositions, over 16 attribute classes and 12 object classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Training Details</head><p>We provide additional details to accompany Section 4.1 (Implementation Details) in the main paper. For our combined loss function, we take a weighted sum of all the losses, and select the weights using a validation set. We create this set for both our datasets by holding out a disjoint subset of 20% of the training pairs.</p><p>-For MIT-States, we train all models for 800 epochs. We set the weight of the auxiliary loss L aux to 1000 for our model. -For UT-Zappos, we train all models for 1000 epochs. We weight all regularizers equally. <ref type="bibr" target="#b21">22</ref> T. Nagarajan and K. Grauman The weight for L aux is substantially higher for MIT-States, which may be necessitated by the low volume of training data per composition. On both datasets, we train our models with a learning rate of 1e ? 4 and a batch size of 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Qualitative Examples</head><p>Next we show additional qualitative examples from Section 4.3 for the unseen compositions. <ref type="figure" target="#fig_5">Figure 7</ref> shows retrieval results on a diverse set of images from ImageNet, where object and attribute categories do not directly align with MIT-States. These examples are computed and displayed in the same manner as <ref type="figure" target="#fig_2">Figure 4</ref> in the main paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of proposed approach. Best viewed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Top retrieval results for unseen compositions. Unseen compositions are posed as textual queries on MIT-States (left) and UT-Zappos (right). These attribute-object pairs are completely unseen during training; the representation for them is generated using our factored composition model. We highlight correctly retrieved instances with a green border, and incorrect ones with red. Last row shows failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Top retrieval results in the out-of-domain setting. Images are retrieved from an unseen domain, ImageNet. Left: Our method can successfully retrieve unseen compositions from images in the wild. Right: Retrievals on out-of-domain compositions. Compositions involving objects that are not even present in our dataset (like lock and barn) can be retrieved using our model's factorized representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>-</head><label></label><figDesc>Further analysis of the problems with the closed world setting (from Section 4.2 in the main paper) in the context of the MIT-States dataset. -Architecture details of our LABELEMBED+ baseline model proposed in Section. 4.1 (Baselines) of the main paper. -Variants of baseline models that add our proposed auxiliary regularizer (from Section 3.3). -TSNE visualization of the joint semantic space described in Section 3.1 learned by our method. -Our procedure to obtain the subset of UT-Zappos50K described in Section 4.1 (Datasets) of the main paper. -Additional training details including hyper-parameter selection for all experiments in Section 4.2 of the main paper. -Additional qualitative examples for retrieval on ImageNet from Section 4.3 of the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Attribute affordances for objects. The closed world setting is easier overall due to the reduced number of attribute choices per object. In addition, about 14% of the objects are dominated by a single attribute affordance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Retrieval results on ImageNet images. Text queries of unseen compositions with top-10 image retrievals shown alongside. Note that the compositions are learned from a disjoint set of compositions on a disjoint dataset (MIT-States), then used to issue queries for images in Ima-geNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>-LABELEMBED+ is an improved version of LABELEMBED where (1) We embed both the constituent inputs and the image features using feed-forward networks into a semantic embedding space of dimension D, and (2) We allow the input representations to be optimized during training. See Supp. for details.</figDesc><table><row><cell></cell><cell></cell><cell>MIT-States</cell><cell></cell><cell></cell><cell cols="2">UT-Zappos</cell><cell></cell></row><row><cell></cell><cell cols="7">closed open +obj h-mean closed open +obj h-mean</cell></row><row><cell>CHANCE</cell><cell>0.1</cell><cell>0.05 0.9</cell><cell>0.1</cell><cell>3.0</cell><cell>0.9</cell><cell>6.3</cell><cell>1.3</cell></row><row><cell>VISPROD(SVM)</cell><cell>11.1</cell><cell>2.4 21.6</cell><cell cols="2">3.9 46.8</cell><cell cols="2">4.1 17.8</cell><cell>7.5</cell></row><row><cell>VISPROD(NN)</cell><cell>13.9</cell><cell>2.8 22.6</cell><cell cols="2">4.7 49.9</cell><cell cols="2">4.8 18.1</cell><cell>8.8</cell></row><row><cell cols="2">ANALOGOUSATTR [5] 1.4</cell><cell>0.2 22.4</cell><cell cols="2">0.4 18.3</cell><cell cols="2">3.5 16.9</cell><cell>5.9</cell></row><row><cell>REDWINE [33]</cell><cell>12.5</cell><cell>3.1 18.3</cell><cell cols="2">5.0 40.3</cell><cell cols="2">2.1 10.5</cell><cell>4.0</cell></row><row><cell>LABELEMBED</cell><cell>13.4</cell><cell>3.3 18.8</cell><cell cols="2">5.3 25.8</cell><cell cols="2">5.2 11.1</cell><cell>8.7</cell></row><row><cell>LABELEMBED+</cell><cell>14.8</cell><cell>5.7 27.2</cell><cell cols="2">8.2 37.4</cell><cell cols="3">9.4 19.4 15.0</cell></row><row><cell>OURS</cell><cell cols="7">12.0 11.4 49.3 11.7 33.2 23.4 38.3 27.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Model capacity of baseline methods. The LABELEMBED+ baseline model with increasing model capacity (number of layers shown in brackets). Our model outperforms this baseline regardless of how many layers are involved, suggesting that model capacity is not the limiting factor.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1 of the main paper that are inspired by components of our own model. Specifically, we allow trainable inputs and include the proposed auxiliary regularizer from Section 3.3.</figDesc><table><row><cell>20</cell><cell cols="2">T. Nagarajan and K. Grauman</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Original</cell><cell></cell><cell cols="2">Augmented Baselines</cell></row><row><cell></cell><cell></cell><cell cols="5">closed open h-mean closed open h-mean</cell></row><row><cell></cell><cell>REDWINE</cell><cell>12.5</cell><cell>3.1</cell><cell cols="2">5.0 12.7</cell><cell>3.2</cell><cell>5.1</cell></row><row><cell></cell><cell cols="2">LABELEMBED 13.4</cell><cell>3.3</cell><cell cols="2">5.3 12.2</cell><cell>3.1</cell><cell>4.9</cell></row><row><cell></cell><cell cols="2">LABELEMBED+ 14.9</cell><cell>5.7</cell><cell>8.2</cell><cell>8.1</cell><cell>7.4</cell><cell>7.7</cell></row><row><cell></cell><cell>OURS</cell><cell cols="5">12.0 11.4 11.7 12.0 11.4 11.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We stress that this differs from traditional zero-shot object recognition<ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1]</ref>, where an unseen object is defined by its (previously learned and class-agnostic) attributes. In our case, we have unseen compositions of objects and attributes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.cs.cmu.edu/?lxiong/bptf/bptf.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Attributes for UT-Zappos are centered around materials of shoes (leather, cotton) and so lack antonyms, preventing us from experimenting with that regularizer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This research is supported in part by ONR PECASE N00014-15-1-2291 and an Amazon AWS Machine Learning Research Award. We gratefully acknowledge Facebook for a GPU donation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attributes as Operators</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering the missing link: Predicting classattribute associations for unsupervised zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint discovery of object states and manipulating actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inferring analogous attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2014) 3, 4</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Zero-shot visual recognition using semantics-preserving adversarial embedding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Face generation for low-shot learning using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural algebra of classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Aga: Attribute-guided augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Modeling actions through state changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A regression model of adjective-noun compositionality in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guevara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL Workshop on GEometrical Models of Natural Language Semantics</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>SIMBAD</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning the latent look: Unsupervised discovery of a stylecoherent embedding from fashion images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cross-domain image retrieval with a dual attributeaware ranking network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning hypergraph-regularized attribute predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Discovering states and transformations in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Decorrelating semantic visual attributes by resisting the urge to share</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Whittlesearch: Image search with relative attribute feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facetracer: A search engine for large collections of images with faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transient attributes for high-level understanding and editing of outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Laffont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Zero-shot learning by generating pseudo feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06389</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From red wine to red tomato: Composition with context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2017) 3, 4, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACL: HLT</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coloring objects: adjective-noun visual semantic compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL Workshop on Vision and Language</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Building a bagpipe with a bag and a pipe: Exploring conceptual combination in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL Workshop on Vision and Language</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Basic objects in natural categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Mervis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boyes-Braem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image ranking and retrieval based on multi-attribute queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">End-to-end localization and ranking for relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Generalized zero-shot learning via synthesized examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Walk and learn: Facial attribute representation learning from egocentric video and contextual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schmidt Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Alternative semantic representations for zero-shot human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<title level="m">Actions?transformations. In: CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Fine-grained visual comparisons with local learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Semantic jitter: Dense supervision for visual comparisons via synthetic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning temporal transformations from time-lapse videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Imagine it for me: Generative adversarial approach for zero-shot learning from noisy texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
