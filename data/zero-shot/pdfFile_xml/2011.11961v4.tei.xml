<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Sun</surname></persName>
							<email>jiayusun5-c@my.cityu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
							<email>yanqiong@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
							<email>rynson.lau@cityu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing portrait matting methods either require auxiliary inputs that are costly to obtain or involve multiple stages that are computationally expensive, making them less suitable for real-time applications. In this work, we present a lightweight matting objective decomposition network (MODNet) for portrait matting in real-time with a single input image. The key idea behind our efficient design is by optimizing a series of sub-objectives simultaneously via explicit constraints. In addition, MODNet includes two novel techniques for improving model efficiency and robustness. First, an Efficient Atrous Spatial Pyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for semantic estimation. Second, a self-supervised sub-objectives consistency (SOC) strategy is proposed to adapt MOD-Net to real-world data to address the domain shift problem common to trimap-free methods. MODNet is easy to be trained in an end-to-end manner. It is much faster than contemporaneous methods and runs at 67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms prior trimap-free methods by a large margin on both Adobe Matting Dataset and a carefully designed photographic portrait matting (PPM-100) benchmark proposed by us. Further, MODNet achieves remarkable results on daily photos and videos. Our code and models are available at: https://github.com/ZHKKKe/MODNet, and the PPM-100 benchmark is released at: https://github.com/ZHKKKe/PPM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Portrait matting aims to predict a precise alpha matte that can be used to extract the persons in a given image or video. It has a wide variety of applications, such as photo editing and movie re-creation. Most existing matting methods use a pre-defined trimap as a priori to produce an alpha matte <ref type="bibr" target="#b3">(Cai et al. 2019;</ref><ref type="bibr" target="#b14">Hou and Liu 2019;</ref><ref type="bibr" target="#b20">Li and Lu 2020;</ref><ref type="bibr" target="#b22">Lu et al. 2019;</ref><ref type="bibr" target="#b31">Tang et al. 2019;</ref><ref type="bibr" target="#b35">Xu et al. 2017)</ref>. However, trimaps are costly to annotate. Although a depth sensor <ref type="bibr" target="#b11">(Foix, Aleny?, and Torras 2011)</ref> can ease the task, the resulting trimaps may suffer from low precision. Some recent trimap-free methods attempt to eliminate the model dependence on the trimap. For example, background matting <ref type="bibr" target="#b27">(Sengupta et al. 2020)</ref> replaces the trimap by a separate background image. Other Copyright ? 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. methods <ref type="bibr" target="#b21">Liu et al. 2020;</ref><ref type="bibr" target="#b28">Shen et al. 2016)</ref> include multiple stages (i.e., with several independent models that are optimized sequentially) to first generate a pseudo trimap or semantic mask, which is then used to serve as the priori for alpha matte prediction. Nonetheless, using the background image as input has to take and align two photos, while having multiple stages would significantly increase the inference time. These drawbacks make all aforementioned matting methods not suitable for real-time applications, such as camera preview. Besides, limited by insufficient amount of labeled training data, existing trimap-free methods often suffer from the domain shift problem <ref type="bibr" target="#b29">(Sun, Feng, and Saenko 2016)</ref> in practice, i.e., the models cannot generalize well to real-world data.</p><p>In this work, we present MODNet, a light-weight model for real-time trimap-free portrait matting. As illustrated in <ref type="figure">Fig. 1</ref>, unlike prior methods which require auxiliary inputs or consist of multiple stages, MODNet inputs a single RGB image and applies explicit constraints to solve matting subobjectives simultaneously in one stage. There are two insights behind our design. First, applying explicit constraints to different parts of the model can address portrait matting effectively under a single input image. In contrast, to obtain comparable results, auxiliary inputs would be necessary for the model trained by only one matting constraint. Second, optimizing sub-objectives simultaneously can further exploit the model capability by sharing intermediate representations. In contrast, training multiple stages sequentially will accumulate the errors from the early stages and magnify them in subsequent stages. We further propose two novel techniques to improve MODNet's efficiency and robustness, including (1) an Efficient Atrous Spatial Pyramid Pooling (e-ASPP) module for fast multi-scale feature fusion in portrait semantic estimation, and (2) a self-supervised strategy based on sub-objective consistency (SOC) to alleviate the domain shift problem in real-world data.</p><p>MODNet has several advantages over previous trimapfree methods. First, MODNet is much faster, running at 67 frames per second (f ps) on a GTX 1080Ti GPU with an input size of 512 ? 512 (including data loading  <ref type="figure">Figure 1</ref>: Different Trimap-free Matting Approaches. Existing trimap-free matting methods either (a) require auxiliary inputs to address the complex matting objective directly or (b) consist of multiple stages to address the matting sub-objectives sequentially. Both of them are less suitable for real-time applications. Instead, (c) our MODNet solves the matting sub-objectives simultaneously with only a single input image, which is more efficient and effective.</p><p>pipeline. Finally, MODNet has better generalization ability, due to our SOC strategy. Since open-source portrait matting datasets <ref type="bibr" target="#b28">(Shen et al. 2016;</ref><ref type="bibr" target="#b35">Xu et al. 2017)</ref> are typically small and have limited precision, prior works train and validate their models on private datasets of diverse quality and difficulty levels. As a result, it is difficult to conduct a fair evaluation. In this work, we evaluate existing trimap-free methods under the same environment, i.e., all models are trained on the same dataset and validated on the portrait images from Adobe Matting Dataset <ref type="bibr" target="#b35">(Xu et al. 2017</ref>) and our newly proposed benchmark. Our benchmark is labelled in high quality, and is more diverse than those used in previous works.</p><p>In summary, we present a light-weight network architecture, named MODNet, for real-time trimap-free portrait matting. MODNet includes two novel techniques, an e-ASPP module for efficient semantic feature fusion and a self-supervised SOC strategy to generalize MODNet to new data domain. In addition, we have also designed a new validation benchmark for portrait matting. Our code, pre-trained model, and benchmark are released at https://github.com/ZHKKKe/MODNet and https://github.com/ZHKKKe/PPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Matting</head><p>The purpose of the image matting task is to extract the desired foreground F from a given image I. This task predicts an alpha matte with a precise foreground probability value ? for each pixel i as:</p><formula xml:id="formula_0">I i = ? i F i + (1 ? ? i ) B i ,<label>(1)</label></formula><p>where B is the background of I. This problem is ill-posed since all variables on the right hand side are unknown. Most existing matting methods take a pre-defined trimap as an auxiliary input, which is a mask containing three regions: absolute foreground (? = 1), absolute background (? = 0), and unknown area (? = 0.5). They aim to estimate the foreground probability inside the unknown area only, based on the priori from the other two regions.</p><p>Traditional matting algorithms heavily rely on low-level features, e.g., color cues, to determine the alpha matte through sampling <ref type="bibr">(Chuang et al. 2001;</ref><ref type="bibr" target="#b10">Feng, Liang, and Zhang 2016;</ref><ref type="bibr" target="#b12">Gastal and Oliveira 2010;</ref><ref type="bibr">He et al. 2011;</ref><ref type="bibr" target="#b15">Johnson et al. 2016;</ref><ref type="bibr" target="#b16">Karacan, Erdem, and Erdem 2015;</ref><ref type="bibr" target="#b24">Ruzon and Tomasi 2000;</ref><ref type="bibr" target="#b36">Yang et al. 2018)</ref> or propagation (Aksoy, Aydin, and Pollefeys 2017; <ref type="bibr" target="#b1">Aksoy et al. 2018;</ref><ref type="bibr" target="#b2">Bai and Sapiro 2007;</ref><ref type="bibr" target="#b6">Chen, Li, and Tang 2013;</ref><ref type="bibr" target="#b13">Grady et al. 2005;</ref><ref type="bibr" target="#b18">Levin, Lischinski, and Weiss 2007;</ref><ref type="bibr" target="#b19">Levin, Rav-Acha, and Lischinski 2008;</ref><ref type="bibr" target="#b30">Sun et al. 2004)</ref>, which often fail in complex scenes. With the tremendous progress of deep learning, many methods based on CNNs have been proposed with notable successes. Cho et al. <ref type="bibr" target="#b7">(Cho, Tai, and Kweon 2016)</ref> and <ref type="bibr" target="#b28">Shen et al. (Shen et al. 2016</ref>) combined the classic algorithms with CNNs for alpha matte refinement. Xu et al. <ref type="bibr" target="#b35">(Xu et al. 2017)</ref> proposed an auto-encoder architecture to predict an alpha matte from a RGB image and a trimap. Some works <ref type="bibr" target="#b20">(Li and Lu 2020;</ref><ref type="bibr" target="#b22">Lu et al. 2019</ref>) used attention mechanisms to help improve matting performances. <ref type="bibr" target="#b3">Cai et al. (Cai et al. 2019</ref>) suggested a trimap refinement process before matting and showed the advantages of using the refined trimaps.</p><p>Since creating trimaps requires users' additional efforts and the quality of the resulting mattes depends on users' experiences, some recent methods (including our MODNet) attempt to avoid them, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trimap-Free Portrait Matting</head><p>Image matting is extremely difficult if trimaps are not available, as a semantic estimation step will then be needed to locate the foreground, before an alpha matte can be predicted. To constrain the problem, most trimap-free methods focus on just one type of foreground objects, e.g., portraits. Nevertheless, feeding RGB images to a single network often produce unsatisfactory alpha mattes. Sengupta Given an input image I, MODNet predicts portrait semantics s p , boundary details d p , and final alpha matte ? p through three interdependent branches, S, D, and F , which are constrained by explicit supervisions generated from the ground truth matte ? g . Since the decomposed sub-objectives are correlated and help strengthen each other, we can optimize MODNet end-to-end. a fusion network to combine the predicted foreground and background. Liu et al. ) concatenated three networks to utilize coarse labeled data in matting. The main problem of all these methods is that they cannot be used in interactive applications as: (1) the background images may change across frames, and (2) computationally expensive due to having multiple stages in the pipeline. Compared to the above methods, MODNet is light-weight in terms of pipeline complexity. It takes only one RGB image as input and uses a single model for real-time matting with better performances, by optimizing a series of sub-objectives simultaneously with explicit constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other Techniques</head><p>We briefly discuss two techniques that are relevant to the design and optimization of our method. Atrous Spatial Pyramid Pooling (ASPP). ASPP ) has been widely explored and proved to boost the performance notably in segmentation-based tasks. Although ASPP has a huge number of parameters and a high computational overhead, some matting models <ref type="bibr" target="#b23">(Qiao et al. 2020;</ref><ref type="bibr" target="#b20">Li and Lu 2020)</ref> still used it for better results. In MODNet, we design an efficient variant of ASPP that gives comparable performances with a much lower overhead. Consistency Constraint. Consistency is an important assumptions behind many semi-/self-supervised <ref type="bibr" target="#b26">(Schmarje et al. 2020)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODNet</head><p>In MODNet, we divide the trimap-free matting objective into three parts: semantic estimation, detail prediction, and semantic-detail fusion. We optimize them simultaneously via three branches <ref type="figure" target="#fig_0">(Fig. 2</ref>). In the following subsections, we will delve into the design of each branch and the supervisions used to solve the sub-objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Estimation</head><p>Similar to existing multi-model approaches, the first step of MODNet is to locate the portrait in the input image I. The difference is that we extract high-level semantics only through an encoder, i.e., the low-resolution branch S of MODNet. This has two main advantages. First, semantic estimation becomes more efficient because a separate decoder with huge parameters is no longer required. Second, the high-level representation S(I) is helpful for subsequent branches and joint optimization. An arbitrary CNN backbone can be applied to S. To facilitate real-time interaction, we adopt MobileNetV2 <ref type="bibr" target="#b25">(Sandler et al. 2018)</ref>, which is an ingenious model developed for mobile devices, as S.</p><p>To predict coarse semantic mask s p , we feed S(I) into a convolutional layer activated by the Sigmoid function to reduce its channel number to 1. We supervise s p by a thumbnail of the ground truth matte ? g . Since s p is supposed to be smooth, we use the L2 loss as:</p><formula xml:id="formula_1">L s = 1 2 s p ? G(? g ) 2 ,<label>(2)</label></formula><p>where G stands for 16? downsampling followed by Gaussian blur. It removes the fine structures (such as hair) that are not essential to portrait semantics. Efficient ASPP (e-ASPP). Semantic masks predicted by MobileNetV2 may have holes in some challenging foregrounds or backgrounds. Many prior works showed that ASPP was a feasible solution for improving such erroneous semantics. However, ASPP has a very high computational overhead. To balance between performance and efficiency, we design an efficient ASPP (e-ASPP) module to process S(I), as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. The standard ASPP utilizes atrous convolutions for multiscale feature extraction and applies a standard convolution for multi-scale feature fusion. We modify it to e-ASPP via three steps. First, we split each atrous convolution into a depth-wise atrous convolution and a point-wise convolution. The depth-wise atrous convolution extracts multi-scale features independently on each channel, while the point-wise convolution is appended for inter-channel fusion at each scale. Second, we switch the order of inter-channel fusion and the multi-scale feature fusion. In this way, (1) only one inter-channel fusion is required, and (2) the multi-scale feature fusion is converted to a cheaper depth-wise operation. Third, we compress the number of input channels by 4? for e-ASPP and recover it before the output.</p><p>Compared to the original ASPP, our proposed e-ASPP has only 1% of the parameters and 1% of the computational overhead 1 . In MODNet, our experiments show that e-ASPP can achieve performance comparable to ASPP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detail Prediction</head><p>We process a transition region around the foreground portrait with a high-resolution branch D, which takes I, S(I), and the low-level features from S as inputs. The purpose of reusing the low-level features is to reduce the computational overhead of D. In addition, we further simplify D in the following three aspects: (1) D consists of fewer convolutional layers than S; (2) a small channel number is chosen for the 1 Refer to Appendix A for more details of e-ASPP. convolutional layers in D;</p><p>(3) we do not maintain the original input resolution throughout D. In practice, D consists of 12 convolutional layers, and its maximum channel number is 64. The resolution of the feature maps is reduced to 1/4 of I in the first layer and restored in the last two layers. The impact of the downsampling operation on D is negligible since it contains a skip link.</p><p>We denote the outputs of D as D(I, S(I)), which implies the dependency between sub-objectives -high-level portrait semantics S(I) is a priori for detail prediction. We calculate the boundary detail matte d p from D(I, S(I)) and learn it through the L1 loss as:</p><formula xml:id="formula_2">L d = m d d p ? ? g 1 ,<label>(3)</label></formula><p>where m d is a binary mask to let L d focus on the portrait boundaries. m d is generated through dilation and erosion on ? g . Its values are 1 if the pixels are inside the transition region, and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic-Detail Fusion</head><p>The fusion branch F in MODNet is a straightforward CNN module, combining semantics and details. We first upsample S(I) to match its size with D(I, S(I)). We then concatenate S(I) and D(I, S(I)) to predict the final alpha matte ? p , constrained by:</p><formula xml:id="formula_3">L ? = ? p ? ? g 1 + L c ,<label>(4)</label></formula><p>where L c is the compositional loss from <ref type="bibr" target="#b35">(Xu et al. 2017)</ref>. It measures the absolute difference between input image I and the composited image obtained from ? p , the ground truth foreground, and the ground truth background.</p><p>MODNet is trained end-to-end through the sum of L s , L d , and L ? , as:</p><formula xml:id="formula_4">L = ? s L s + ? d L d + ? ? L ? ,<label>(5)</label></formula><p>where ? s , ? d , and ? ? are hyper-parameters balancing the three losses. The training process is robust to these hyperparameters. We set ? s = ? ? = 1 and ? d = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOC for Real-World Data</head><p>The training data for portrait matting requires excellent labeling in the hair area, which is difficult to do for natural images with complex backgrounds. Currently, most annotated data comes from photography websites. Although these images have monochromatic or blurred backgrounds, the labeling process still needs to be completed by experienced annotators with considerable amount of time. As such, the labeled datasets for portrait matting are usually small. Xu et al. <ref type="bibr" target="#b35">(Xu et al. 2017</ref>) suggested using background replacement as a data augmentation to enlarge the training set, and it has become a common setting in image matting. However, the training samples obtained in such a way exhibit different properties from those of the daily life images. Therefore, existing trimap-free models always tend to overfit the training set and perform poorly on real-world data.</p><p>To address this domain shift problem, we propose to utilize sub-objectives consistency (SOC) to adapt MOD-Net to unseen data distributions. The three sub-objectives in MODNet should have consistent outputs in semantics or details. We take semantic consistency as an example, MODNet outputs portrait semantics S(I) and alpha matte F (S(I), D(S(I))) for input image I. As S(I) is the prior of F (S(I), D(S(I))), they should have consistent portrait semantics. In the labeled source domain, there is good consistency among the predictions of sub-objectives. However, inconsistent predictions occur in the unlabeled target domain, which may cause poor results. Motivated by this observation, our self-supervised SOC strategy imposes the consistency constraints among the predictions of the subobjectives ( <ref type="figure">Fig. 1(b)</ref>) to improve the performance of MOD-Net in the new domain, without ground truth labels. Formally, we denote MODNet as M . As described in Sec. , M has three outputs for an unlabeled image?: s p ,d p ,? p = M (?) .</p><p>(6) We enforce the semantics in? p to be consistent withs p and the details in? p to be consistent withd p by:</p><formula xml:id="formula_5">L cons = 1 2 G(? p ) ?s p 2 +m d ? p ?d p 1 ,<label>(7)</label></formula><p>wherem d indicates the transition region in? p , and G has the same meaning as the one in Eq. 2. However, adding the L2 loss to blurred G(? p ) will smooth the boundaries in the optimized? p . As a result, the consistency between? p and d p will remove the details predicted by the high-resolution branch. To prevent this problem, we duplicate M to M and fix the weights of M before performing SOC. Since the fine boundaries are preserved ind p output by M , we append an extra regularization term to maintain the details in M as: </p><formula xml:id="formula_6">L dd =m d d p ?d p 1 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we first introduce our PPM-100 benchmark for portrait matting. We then compare MODNet with existing matting methods on both Adobe Matting Dataset (AMD) <ref type="bibr" target="#b35">(Xu et al. 2017</ref>) and our PPM-100. We further conduct ablation experiments to evaluate various components of MOD-Net. Finally, we demonstrate the effectiveness of SOC in adapting MODNet to real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Photographic Portrait Matting Benchmark</head><p>Existing works constructed their validation benchmarks from a small amount of labeled data through image synthesis. Their benchmarks are relatively easy due to unnatural fusion or mismatched semantics between the foreground and the background <ref type="figure" target="#fig_3">(Fig. 4(a)</ref>). Hence, trimap-free models may have comparable performances to the trimap-based models on these benchmarks, but unsatisfactory performances on natural images, i.e., images without background replacement. This indicates that the performances of trimap-free methods have not been accurately assessed.</p><p>In contrast, we propose a Photographic Portrait Matting benchmark (PPM-100), which contains 100 finely annotated portrait images with various backgrounds. To guarantee sample diversity, we consider several factors in order to balance the sample types in PPM-100, including: (1) whether the whole portrait body is included; (2) whether the image background is blurred; and (3) whether the person is holding additional objects. We regard small objects held by a foreground person as a part of the foreground, which is more in line with practical applications. As shown in <ref type="figure" target="#fig_3">Fig. 4(b)(c)(d)</ref>, the samples in PPM-100 have more natural backgrounds and richer postures. Hence, PPM-100 can be considered as a more comprehensive benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on AMD and PPM-100 2</head><p>We compare MODNet with trimap-free FDMPA <ref type="bibr" target="#b38">(Zhu et al. 2017)</ref>, LFM , SHM , BSHM , and HAtt <ref type="bibr" target="#b23">(Qiao et al. 2020)</ref>. We use DIM <ref type="bibr" target="#b35">(Xu et al. 2017)</ref> and IndexMatter <ref type="bibr" target="#b22">(Lu et al. 2019</ref>  as the trimap-based baselines. For methods without publicly available codes, we follow their papers to reproduce them. For a fair comparison, we train all models on the same dataset, which contains nearly 3, 000 annotated foregrounds. Background replacement <ref type="bibr" target="#b35">(Xu et al. 2017</ref>) is applied to extend our training set. All images in our training set are collected from Flickr and are annotated by Photoshop. The training set contains ?2, 600 half-body and ?400 full-body portraits. For each labeled foreground, we generate 5 samples by random cropping and 10 samples by compositing with the images from the OpenImage dataset <ref type="bibr" target="#b17">(Kuznetsova et al. 2018</ref>) (as the background). We use MobileNetV2 pretrained on the Supervisely Person Segmentation (SPS) (supervise.ly 2018) dataset as the backbone of all trimap-free models. For the compared methods, we explore the optimal hyper-parameters through grid search. For MODNet, we train it by SGD for 40 epochs. With a batch size of 16, the  <ref type="table">Table 2</ref>: Quantitative Results on AMD. We pick the portrait foregrounds from AMD for validation. A ' ?' indicates that the models pre-trained on SPS.</p><p>initial learning rate is set to 0.01 and is multiplied by 0.1 after every 10 epochs. We use Mean Square Error (MSE) and Mean Absolute Difference (MAD) as quantitative metrics. <ref type="table" target="#tab_3">Table 1</ref> shows the results on PPM-100. MODNet outperforms other trimap-free methods on both MSE and MAD. However, it is unable to outperform trimap-based methods, as PPM-100 contains samples with very challenging poses and costumes. When taking a trimap as input during both training and testing stages, i.e., regarding MODNet as a trimap-based methods, it outperforms the compared trimapbased methods. This demonstrates the superiority of the proposed architecture. <ref type="figure">Fig. 5</ref> shows visual comparison 3 . <ref type="table">Table 2</ref> shows the results on AMD <ref type="bibr" target="#b35">(Xu et al. 2017)</ref>. We pick the portrait foregrounds from AMD and composite 10  test samples for each foreground with diverse backgrounds. We validate all trained models on this synthetic benchmark. Unlike the results on PPM-100, the performance gap between trimap-free and trimap-based models is much smaller.</p><p>The results show that trimap-free models can achieve results comparable to trimap-based models only on the synthetic benchmarks that have unnatural fusion or mismatched semantics between foreground and background. We further evaluate MODNet on model size and execution efficiency. A small model facilitates deployment on mobile/handheld devices, while high execution efficiency is necessary for real-time applications. We measure the model size by the total number of parameters, and we reflect the execution efficiency by the average inference time over PPM-100 on an NVIDIA GTX 1080Ti GPU (all input images are resized to 512 ? 512). Note that fewer parameters do not imply faster inference speed due to large feature maps or timeconsuming mechanisms, e.g., attention, that the model may use. <ref type="figure" target="#fig_4">Fig. 6</ref> summarizes the results. The inference time of MODNet is 14.9 ms (67 f ps), which is twice the f ps of the fastest method, FDMPA (31 f ps). In addition, our MODNet has the smallest number of parameters among the trimapfree methods.</p><p>We have also conducted ablation experiments for MOD-Net on PPM-100, as shown in <ref type="table" target="#tab_5">Table 3</ref>. Applying L s and L d to constrain portrait semantics and boundary details bring considerable performance improvements. The results also show that the effectiveness of e-ASPP in fusing multilevel feature maps. Although SPS pre-training is optional to MODNet, it plays a vital role in other trimap-free methods.</p><p>From <ref type="table" target="#tab_3">Table 1</ref>, we can see that trimap-free DIM without pretraining performs far worse than the one with pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Real-World Data</head><p>To adapt MODNet to real-world data, we capture ?400 video clips (divided into about 50,000 frames) as the unlabeled data for self-supervised SOC domain adaptation. In this stage, we freeze the BatchNorm layers within MODNet and finetune the convolutional layers by Adam at a learning rate of 0.0001. The total number of fine-tuning epochs are 15. Here, we only provide visual results, as ground truth mattes are not available. In <ref type="figure" target="#fig_5">Fig. 7(b)(c)</ref>, we composite the foreground over a green screen to emphasize that SOC is vital for generalizing MODNet to real-world data.</p><p>For video data, we also propose here a simple but effective One-Frame Delay (OFD) trick to reduce the flickers in the predicted alpha matte sequence. The idea behind OFD is that we can utilize the preceding and the following frames to fix the flickering pixels, because the corresponding pixels in adjacent frames are likely to be correct. Suppose that we have three consecutive frames, and their corresponding alpha mattes are ? t?1 , ? t , and ? t+1 , where t is the frame index. We regard ? i t as a flickering pixel if the values of ? i t?1 and ? i t+1 are close, and ? i t is very different from the values of both ? i t?1 and ? i t+1 . When ? i t is a flickering pixel, we replace its value by averaging ? i t?1 and ? i t+1 . As illustrated in <ref type="figure" target="#fig_5">Fig. 7(c)(d)</ref>, OFD can further removes flickers along the boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper has presented a simple, fast, and effective model, MODNet, for portrait matting. By taking only an RGB image as input, our method enables the prediction of a highquality alpha matte in real time, which is benefited from objective decomposition and concurrent optimization with explicit supervisions. Besides, we have introduced (1) an e-ASPP module to speed up the multi-scale feature fusion process, and (2) a self-supervised sub-objectives consistency (SOC) strategy to allow MODNet to handle the domain shift problem. Extensive experiments show that MODNet outperforms existing trimap-free methods on the AMD benchmark, the proposed PPM-100 benchmark, and a variety of real-world data. Our method does have limitations. The main one is that it may fail to handle videos with strong motion blurs due to the lack of temporal information. One possible future work is to address the video matting problem under motion blurs through additional sub-objectives, such as optical flow estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>We thank Yurou Zhou, Qiuhua Wu, and Xiangyu Mao from SenseTime Research for their discussions and help in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Analysis of e-ASPP</head><p>Here we compare the proposed Efficient ASPP (e-ASPP) with the standard ASPP in terms of the number of parameters and computational overhead. For a convolutional layer, the number of its parameters P can be calculated by:</p><formula xml:id="formula_7">P = C out ? C in ? K ? K,<label>(9)</label></formula><p>where C out is the number of output channels, C in is the number of input channels, and K is the kernel size. We can use FLOPs to measure the computational overhead O of a convolutional layer as: <ref type="formula" target="#formula_0">(10)</ref> where H out and W out are the height and the width of output feature maps, respectively.</p><formula xml:id="formula_8">O = C in ? 2 ? K ? K ? H out ? W out ? C out ,</formula><p>Following, we represent the size of the input feature maps by <ref type="figure">(c, h, w)</ref>, where c is the number of channels, h is the height of the input feature maps, and w is the width of the input feature maps. We represent the number of atrous convolutional layers (with a kernel size of k) in both ASPP and e-ASPP by m.</p><p>Standard ASPP (ASPP). In ASPP, (1) all atrous convolutional layers are independently applied to the input features maps to extract multi-scale features. These multi-scale features are then (2) concatenated and processed by a pointwise convolutional layer (with a kernel size of 1). We have:</p><formula xml:id="formula_9">P ASPP =m ? (c ? c ? k ? k) + c ? (m ? c) ? 1 ? 1 =m ? c 2 ? (k 2 + 1),<label>(11)</label></formula><formula xml:id="formula_10">O ASP P =m ? (c ? 2 ? k ? k ? h ? w ? c) + (m ? c) ? 2 ? 1 ? 1 ? h ? w ? c =((2 ? k 2 + 2) ? m ? c) ? (h ? w ? c).<label>(12)</label></formula><p>Efficient ASPP (e-ASPP). As shown in <ref type="figure" target="#fig_2">Fig. 3</ref> (in the paper), e-ASPP consists of four operations, including (1) Channel Reduction, (2) Multi-Scale Feature Extraction, (3) Multi-Scale Feature Fusion, and (4) Inter-Channel Feature Fusion. The total number of parameters and the total FLOPs are the sum of these four operations. We have:</p><formula xml:id="formula_11">P e?ASP P = c 4 ? c ? 1 ? 1 + c 4 ? m ? (1 ? 1 ? k ? k) + c 4 ? (1 ? m ? 1 ? 1) + c ? c 4 ? 1 ? 1 = 2 ? c 2 + (k 2 + 1) ? m ? c 4 ,<label>(13)</label></formula><formula xml:id="formula_12">O e?ASP P =c ? 2 ? 1 ? 1 ? h ? w ? c 4 + c 4 ? m ? (1 ? 2 ? k ? k ? h ? w ? 1) + c 4 ? (m ? 2 ? 1 ? 1 ? h ? w ? 1) + c 4 ? 2 ? 1 ? 1 ? h ? w ? c =(c + (k 2 + 1) ? m 2 ) ? (h ? w ? c).<label>(14)</label></formula><p>Following the standard ASPP, we set k = 3 and m = 5. Usually, c ? 256 is applied in most networks. Therefore, we have: P e?ASP P P ASP P ? 0.01,</p><p>O e?ASP P O ASP P ? 0.01.</p><p>It means that compared to the standard ASPP, our proposed e-ASPP has only 1% of the parameters and 1% of the computational overhead. In MODNet, our experiments show that e-ASPP can achieve performance comparable to ASPP. Note that when the Channel Reduction operation in e-ASPP is disabled, e-ASPP still has only 2% of the parameters and 2% of the computational overhead compared to ASPP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Results on CRGNN-R and D646</head><p>In <ref type="table" target="#tab_7">Table 4</ref>, we provide the quantitative results on a video matting dataset proposed by <ref type="bibr" target="#b33">(Wang et al. 2021)</ref> to show the effectiveness of the proposed SOC strategy. In <ref type="table" target="#tab_8">Table 5</ref>, we compare MODNet with previous SOTA methods on the D646 dataset proposed by <ref type="bibr" target="#b23">(Qiao et al. 2020)</ref>.  . Note that DIM here does not take trimaps as the input but is pre-trained on the SPS (supervise.ly 2018) dataset. Zoom in for the best visualization.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D: Comparison with BM</head><p>We compare MODNet against the background matting (BM) proposed by <ref type="bibr" target="#b27">(Sengupta et al. 2020</ref>). Since BM does not support dynamic backgrounds, we conduct validations in the fixed-camera scenes from <ref type="bibr" target="#b27">(Sengupta et al. 2020)</ref>. BM relies on a static background image, which implicitly assumes that all pixels whose value changes across frames belong to the foreground. As shown in <ref type="figure" target="#fig_7">Fig. 9</ref>, when a moving object suddenly appears in the background, the result of BM will be affected, but MODNet is robust to such disturbances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>et al. (Sengupta et al. 2020) proposed to capture a less expensive background image as a pseudo green screen to alleviate this problem. Other works designed their pipelines with multiple stages. For example, Shen et al. (Chen et al. 2018) assembled a trimap generation network before the matting network. Zhang et al. (Zhang et al. 2019) applied Architecture of MODNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and domain adaptation<ref type="bibr" target="#b34">(Wilson and Cook 2020)</ref> algorithms. For example,<ref type="bibr" target="#b16">Ke et al. (Ke et al. 2020</ref>) designed a consistency-based framework that could be used for semisupervised matting. Toldo et al.<ref type="bibr" target="#b32">(Toldo et al. 2020</ref>) presented a consistency-based domain adaptation strategy for semantic segmentation. However, these methods consist of multiple models and constrain the consistency among their pre-dictions. In contrast, MODNet imposes consistency among various sub-objectives within a model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>of atrous conv C = channel number of input features Illustration of e-ASPP. Our e-ASPP is efficient since it extracts and fuses multi-scale features depth-wise, followed by an inter-channel fusion. The tuple under convolution are (output channel, input channel, kernel size). The dotted lines indicate the same structure as the solid line in the center branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Benchmark Comparison. (a) Validation benchmarks used in<ref type="bibr" target="#b21">Liu et al. 2020;</ref><ref type="bibr" target="#b37">Zhang et al. 2019)</ref>. Images are synthesized by replacing the original backgrounds with new ones. Instead, our PPM-100 contains original image backgrounds and has a higher diversity in the foregrounds, e.g., (b) with fine hairs, (c) with additional objects, and (d) without bokeh or with full-body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison on Model Size and Execution Efficiency. f ps can be obtained by dividing 1, 000 with the inference time.Ls L d e-ASPP SPS MSE ? MAD ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Results on a Real-World Video. We show three consecutive video frames from left to right. From top to bottom: (a) Input, (b) MODNet, (c) MODNet + SOC, and (d) MODNet + SOC + OFD. The blue region in frame t ? 1 shows the effectiveness of SOC, while the red region in frame t highlights the flickers eliminated by OFD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>More Visual Comparisons of Trimap-free Methods on PHM-100. We compare our MODNet with DIM (Xu et al. 2017), FDMPA (Zhu et al. 2017), LFM (Zhang et al. 2019), SHM (Chen et al. 2018), HAtt (Qiao et al. 2020), and BSHM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>MODNet versus BM with a fixed camera position. MODNet outperforms BM<ref type="bibr" target="#b27">(Sengupta et al. 2020</ref>) when a car is entering the background (red region).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The sum of L cons and L dd is optimized during SOC.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Visual Comparison of Trimap-free Methods on PPM-100. MODNet performs better in hollow structures (the 1st row) and hair details (the 2nd row). However, it may still make mistakes in challenging poses or costumes (the 3rd row). DIM<ref type="bibr" target="#b35">(Xu et al. 2017)</ref> here does not take trimaps as input, but is pre-trained on the SPS (supervise.ly 2018) dataset.</figDesc><table><row><cell>Input</cell><cell>DIM</cell><cell>FDMPA</cell><cell>LFM</cell><cell>SHM</cell><cell>HAtt</cell><cell>BSHM</cell><cell>Our</cell><cell>GT</cell></row><row><cell>Figure 5: Method</cell><cell></cell><cell cols="2">Trimap MSE ? MAD ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DIM (Xu et al. 2017)</cell><cell>0.0016</cell><cell>0.0067</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">IndexMatter (Lu et al. 2019)</cell><cell>0.0015</cell><cell>0.0064</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MODNet (Our)</cell><cell></cell><cell>0.0013</cell><cell>0.0054</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DIM (Xu et al. 2017)</cell><cell>0.0221</cell><cell>0.0327</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DIM  ? (Xu et al. 2017)</cell><cell>0.0115</cell><cell>0.0178</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FDMPA  ? (Zhu et al. 2017)</cell><cell>0.0101</cell><cell>0.0160</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LFM  ? (Zhang et al. 2019)</cell><cell>0.0094</cell><cell>0.0158</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SHM  ? (Chen et al. 2018)</cell><cell>0.0072</cell><cell>0.0152</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">HAtt  ? (Qiao et al. 2020)</cell><cell>0.0067</cell><cell>0.0137</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BSHM  ? (Liu et al. 2020)</cell><cell>0.0063</cell><cell>0.0114</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MODNet  ? (Our)</cell><cell></cell><cell>0.0044</cell><cell>0.0086</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Quantitative Results on PPM-100. A ' ?' indicates that the model is pre-trained on SPS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation of MODNet on PPM-100. SPS indicates the model us pre-trained on SPS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results on CRGNN-R (Wang et al. 2021).Fig. 8 provides more visual comparisons of MODNet and the existing trimap-free methods on PHM-100.</figDesc><table><row><cell>Method</cell><cell cols="2">Trimap MSE ? MAD ?</cell></row><row><cell>DIM (Xu et al. 2017)</cell><cell>0.0025</cell><cell>0.0081</cell></row><row><cell>HAtt (Qiao et al. 2020)</cell><cell>0.0054</cell><cell>0.0126</cell></row><row><cell>MODNet (Our)</cell><cell>0.0037</cell><cell>0.0098</cell></row><row><cell>Appendix C: Visual Results on PHM-100</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results on D646<ref type="bibr" target="#b23">(Qiao et al. 2020)</ref>.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Refer to Appendix B for results on more benchmarks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Refer to Appendix C for more visual results.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semantic soft segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A geodesic framework for fast interactive image and video segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disentangled Image Matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">KNN Matting. PAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A cluster sampling method for image matting via sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lock-in Time-of-Flight (ToF) cameras: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Foix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aleny?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors Journal</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Shared sampling for real-time alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S L</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>In Eurographics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random walks for interactive alpha-matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schiwietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>; K.; Rhaemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VIIP. He</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context-aware Image Matting for Simultaneous Foreground and Alpha Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sparse coding for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Varnousfaderani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Guided Collaborative Training for Pixel-wise Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Open Images Dataset V4: Unified image classification, object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>and visual relationship detection at scale. IJCV</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<title level="m">Spectral matting. PAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural image matting via guided contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Boosting Semantic Human Matting With Coarse Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Indices Matter: Learning to Index for Deep Image Matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention-Guided Hierarchical Structure Aggregation for Image Matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Alpha estimation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ruzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey on Semi-, Self-and Unsupervised Learning for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmarje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Santarossa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Schr?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
		<idno>abs/2002.08721</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Background Matting: The World is Your Green Screen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Return of Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Poisson matting. TOG. supervise.ly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Supervisely Person Dataset. supervise.ly</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning-based Sampling for Natural Image Matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Aydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised Domain Adaptation for Mobile Semantic Segmentation based on Cycle Consistency and Feature Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>IMAVIS</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video Matting via Consistency-Regularized Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Survey of Unsupervised Deep Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Image Matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast Deep Matting for Portrait Animation on Mobile Phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
