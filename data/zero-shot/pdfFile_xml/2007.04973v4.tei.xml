<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Code Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paras</forename><surname>Jain</surname></persName>
							<email>parasj@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
							<email>ajayj@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
							<email>tianjunz@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
							<email>pabbeel@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
							<email>jegonzal@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
							<email>istoica@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Code Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like code clone detection, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based RoBERTa model is sensitive to source code edits, even when the edits preserve semantics. We propose Con-traCode: a contrastive pre-training task that learns code functionality, not form. Con-traCode pre-trains a neural network to identify functionally similar variants of a program among many non-equivalent distractors. We scalably generate these variants using an automated source-to-source compiler as a form of data augmentation. Contrastive pretraining outperforms RoBERTa on an adversarial code clone detection benchmark by 39% AUROC. Surprisingly, improved adversarial robustness translates to better accuracy over natural code; ContraCode improves summarization and TypeScript type inference accuracy by 2 to 13 percentage points over competitive baselines. All source is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Programmers increasingly rely on machine-aided programming tools that analyze or transform code automatically to aid software development <ref type="bibr" target="#b33">(Kim et al., 2012)</ref>. Traditionally, code analysis uses hand-written rules, though the wide diversity of programs encountered in practice can limit their generality. Recent work leverages machine learning for richer language understanding, such as learning to detect bugs <ref type="bibr" target="#b47">(Pradel and Sen, 2018)</ref> and predict performance <ref type="bibr" target="#b40">(Mendis et al., 2019)</ref>.</p><p>Still, neural models of source code are susceptible to adversarial attacks. <ref type="bibr" target="#b63">Yefet et al. (2020)</ref> and <ref type="bibr" target="#b52">Schuster et al. (2021)</ref> find accuracy degrades * equal contribution <ref type="figure">Figure 1</ref>: Robust code clone detection: On source code, RoBERTa is not robust to simple label-preserving code edits like renaming variables. Adversarially selecting between possible edits lowers performance below random guessing (dashed line). Contrastive pretraining with ContraCode learns a more robust representation of functionality, consistent across code edits.  <ref type="figure">Figure 2</ref>: For many analyses, programs with the same functionality should have similar representations. Con-traCode learns such representations by pre-training an encoder to retrieve equivalent, transformed programs among many distractors. significantly under adversarial perturbations for both discriminative and generative code models. In our work, we investigate adversarial attacks on code clone detection. Successful adversarial attacks could circumvent malware detectors.</p><p>While self-supervision can improve adversarial robustness <ref type="bibr" target="#b25">(Hendrycks et al., 2019)</ref>, we find that RoBERTa is sensitive to stylistic implementation choices of code inputs. <ref type="figure">Fig. 1</ref> plots the performance of RoBERTa and ContraCode, our proposed method, on a code clone detection task as small label-preserving perturbations are applied to the input code syntax. With just three minor adversarial edits to code syntax, RoBERTa underperforms the random classifier (in gray). In <ref type="figure">Fig. 3</ref>, we show that RoBERTa's representations of code are sensitive to code edits in agreement with prior work <ref type="bibr" target="#b57">(Wang and Christodorescu, 2019;</ref><ref type="bibr" target="#b58">Wang and Su, 2019;</ref><ref type="bibr">Rabin and Alipour, 2020)</ref>.</p><p>To address this issue, we develop ContraCode: a self-supervised representation learning algorithm that captures program semantics. We hypothesize that programs with the same functionality should have similar underlying representations for downstream code understanding tasks.</p><p>ContraCode generates syntactically diverse but functionally equivalent programs using source-tosource compiler transformation techniques (e.g., dead code elimination, obfuscation and constant folding). It uses these programs in a challenging discriminative pretext task that requires the model to identify similar programs out of a large dataset of distractors <ref type="figure">(Fig. 2</ref>). To solve this task, the model must embed code semantics rather than syntax. ContraCode improves adversarial robustness in <ref type="figure">Fig. 1</ref>. Surprisingly, adversarial robustness transfers to better natural code understanding.</p><p>Our novel contributions include:</p><p>1. the novel use of compiler-based transformations as data augmentations for code, 2. the concept of program representation learning based on functional equivalence, and 3. a detailed analysis of architectures, code transforms and pre-training strategies, showing ContraCode improves type inference top-1 accuracy by 9%, learned inference by 2%-13%, summarization F1 score by up to 8% and clone detection AUROC by 2%-46%.</p><p>2 Related work Self-supervised learning (SSL) is a learning strategy where some attributes of a datapoint are predicted from remaining parts. BERT <ref type="bibr" target="#b15">(Devlin et al., 2019</ref>) is a SSL method for NLP that reconstructs masked tokens as a pretext task. RoBERTa  further tunes BERT. Contrastive approaches minimize distance between learned representations of similar examples (positives) and maximize distance between dissimilar negatives <ref type="bibr" target="#b21">(Hadsell et al., 2006)</ref>. <ref type="bibr">CPC (Oord et al., 2018;</ref><ref type="bibr" target="#b24">H?naff, 2020</ref>) encodes segments of sequential data to predict future segments. SimCLR <ref type="bibr" target="#b13">(Chen et al., 2020a)</ref> and MoCo <ref type="bibr" target="#b22">(He et al., 2020;</ref><ref type="bibr" target="#b14">Chen et al., 2020b</ref>) use many negatives for dense loss signal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code representation learning</head><p>We address clone detection <ref type="bibr" target="#b60">(White et al., 2016)</ref>, type inference <ref type="bibr" target="#b23">(Hellendoorn et al., 2018)</ref>, and summarization <ref type="bibr" target="#b5">(Alon et al., 2019a)</ref>. Others explored summarization <ref type="bibr" target="#b42">(Movshovitz-Attias and Cohen, 2013;</ref><ref type="bibr" target="#b4">Allamanis et al., 2016;</ref><ref type="bibr" target="#b30">Iyer et al., 2016;</ref><ref type="bibr" target="#b0">Ahmad et al., 2020)</ref> and types <ref type="bibr" target="#b46">(Pradel et al., 2019;</ref><ref type="bibr" target="#b44">Pandi et al., 2020;</ref><ref type="bibr" target="#b3">Allamanis et al., 2020;</ref><ref type="bibr" target="#b12">Bielik and Vechev, 2020;</ref> for various languages. Inst2vec <ref type="bibr" target="#b10">(Ben-Nun et al., 2018)</ref> embeds statements in LLVM IR by processing a flow graph with a context prediction objective <ref type="bibr" target="#b41">(Mikolov et al., 2013)</ref>. Code2seq <ref type="bibr" target="#b5">(Alon et al., 2019a)</ref> embeds AST paths with an attentional encoder for seq2seq tasks. <ref type="bibr" target="#b32">Kanade et al. (2020)</ref> and <ref type="bibr">Feng et al. (2020)</ref> pre-train a Transformer on code using the masked language modeling (MLM) objective <ref type="bibr" target="#b15">(Devlin et al., 2019;</ref><ref type="bibr" target="#b55">Taylor, 1953)</ref>. <ref type="bibr" target="#b62">Yefet et al. (2019)</ref> find code models are highly sensitive to adversarial code edits in a discrimative setting. <ref type="bibr" target="#b51">Schuster and Paliwal (1997)</ref> discovers in-the-wild attacks on code autocompletion tools. Compared to language models, code models may be more vulnerable to adversarial attacks due to synthetic labels <ref type="bibr" target="#b19">(Ferenc et al., 2018;</ref><ref type="bibr" target="#b47">Pradel and Sen, 2018;</ref><ref type="bibr" target="#b11">Benton et al., 2019)</ref> and duplication <ref type="bibr" target="#b1">(Allamanis, 2019)</ref>   <ref type="figure">Figure 4</ref>: A JavaScript method from our unlabeled training set with two automatically generated semantically-equivalent programs. The method is from the StackEdit Markdown editor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial attacks on code models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our core insight is to use compiler transforms as data augmentations, generating a dataset of equivalent functions ( ?3.1, 3.2). We then use a contrastive objective to learn a representation invariant to these transforms ( ?3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Compilation as data augmentation</head><p>Modern programming languages afford great flexibility to software developers, allowing them to implement the same function in different ways. Yet, crowdsourcing equivalent programs from GitHub is difficult as verifying equivalence is undecidable <ref type="bibr" target="#b31">(Joshi et al., 2002;</ref><ref type="bibr" target="#b9">Bansal and Aiken, 2006)</ref> and approximate verification is costly and runs untrusted code <ref type="bibr" target="#b38">(Massalin, 1987)</ref>. Instead of searching for equivalences, we propose correct-by-construction data augmentation. We apply compiler transforms to unlabeled code to generates many variants with equivalent functionality, i.e. operational semantics. For example, dead-code elimination (DCE) is an optimization that removes operations that do not change function output. While DCE preserves functionality, <ref type="bibr" target="#b57">Wang and Christodorescu (2019)</ref> find that up to 12.7% of the predictions of current supervised algorithm classification models change after DCE.</p><p>We parse a particular source code sequence, e.g. W*x + b into a tree-structured representation (+ (* W x) b) called an Abstract Syntax Tree (AST). We then transform the AST with automated traversal passes. A rich body of prior programming language work explores parsing and transforming ASTs to optimize a program. If source code is emitted by the compiler rather than machine code, this is called source-to-source transformation or transpilation. Transpilation is common for optimizing and obfuscating dynamic languages like JavaScript. Further, if each trans-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code compression</head><p>Identifier modification  form preserves code semantics, then any composition also preserves semantics. We implement our transpiler with the Babel and Terser compiler infrastructures <ref type="bibr" target="#b39">(McKenzie et al., 2020;</ref><ref type="bibr" target="#b49">Santos et al., 2020)</ref> for the JavaScript programming language. In future work, a language-agnostic compiler <ref type="bibr" target="#b34">(Koppel et al., 2018)</ref> could be used to extend ContraCode to other languages. Each compiler transformation is a function ? : P ? P, where the space of programs P is composed of the set of valid ASTs and the set of programs in tokenized source form. <ref type="figure">Fig. 4</ref> shows variants of an example program. <ref type="table" target="#tab_3">Table 1</ref> and Appendix A list program transformations in detail, but we broadly group them into three categories:</p><p>? Code compression changes the syntactic structure of code and performs correctby-construction transforms such as precomputing constant expressions.</p><p>? Identifier modifications substitute method and variable names with random tokens, masking some human-readable information in a program but preserving functionality.</p><p>? Finally, Regularizing transforms improve model generalization by reducing the number of trivial positive pairs with high text overlap. The line subsampling pass in this group Algorithm 1 Transform dropout for stochastic program augmentation. 1: Input: Program source x, transformation functions ?1, . . . ? k , transform probabilities p1, . . . p k , count N 2: Returns: N variants of x 3: V ? {x}, a set of augmented program variants 4: for SAMPLE i ? 1 . . . N ? 1 do 5:</p><p>x ? x 6:</p><p>for transform t ? 1 . . . k do 7:</p><p>Sample yt ? Bernoulli(pt) 8:</p><p>if yt = 1 then 9:</p><p>if REQUIRESAST(?t(?)) and ?ISAST(x ) then x ? PARSETOAST(x ) 10:</p><p>else if ?REQUIRESAST(?t(?)) and ISAST(x ) then x ? LOWERTOSOURCE(x ) 11:</p><p>x ? ?t(x ) 12:</p><p>end if 13: end for 14:</p><p>if ISAST(x ) then x ? LOWERTOSOURCE(x ) 15:</p><p>V ? V ? {x } 16: end for 17: return V potentially modifies program semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Diversity through transform dropout</head><p>Stochastic augmentations in other modalities like random crops generate diverse outputs, but most of our compiler-based transformations are deterministic. To produce a diverse set of transformed programs, we randomly apply a subset of available compiler passes in a pre-specified order, applying transform ? i with probability p i . Intermediate programs are converted between AST and source form as needed for the compiler. Algorithm 1 details our transform dropout procedure. <ref type="figure" target="#fig_1">Figure 5</ref> measures the resulting diversity in programs. We precompute up to 20 augmentations of 1.8M JavaScript methods from GitHub. Algorithm 1 deduplicates method variants before pretraining since some transforms will leave the program unchanged. 89% of the methods have more than one alternative after applying 20 random sequences of transformations. The remaining methods without syntactically distinct alternatives include one-line functions that are obfuscated. We apply subword regularization <ref type="bibr" target="#b35">(Kudo, 2018)</ref> as a final transformation to derive different tokenizations every batch, so pairs derived from the same original method will still differ. All transformations are fast; our compiler transforms 300 functions per second on a single CPU core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contrastive pre-training</head><p>We extend the Momentum Contrast (MoCo) methodology <ref type="bibr" target="#b22">(He et al., 2020)</ref> that was designed for contrastive image representation learning. In our case, we learn a program encoder f q that maps a sequence of program tokens to a single, fixed dimensional embedding. We organize programs into functionally similar positive pairs and dissimilar negative pairs. Generating two augmentations of the same GitHub program yields a positive pair (x q , x k + ), and an augmentation of a different program yields a negative x k ? . The program x q is called a "query" used to retrieve the corresponding "key" x k + during contrastive pre-training. We use these to shape representation space, drawing positives together and pushing away from negatives. Negatives are important to prevent the encoder f q from mapping all programs to the same, trivial representation <ref type="bibr" target="#b50">(Saunshi et al., 2019)</ref>.</p><p>Pre-training objective Like <ref type="bibr" target="#b22">He et al. (2020)</ref>, we use the InfoNCE loss <ref type="bibr" target="#b43">(Oord et al., 2018)</ref>, a tractable objective that frames contrastive learning as a classification task: can the positives be identified among negatives? InfoNCE computes the probability of selecting the positive by taking the softmax of projected embedding similarities across a batch and a queue of negatives. Eq. (1) shows the InfoNCE loss, a function whose value is low when q is similar to the positive key embedding k + and dissimilar to negative key embeddings k ? . t is a temperature hyperparameter proposed by <ref type="bibr" target="#b61">Wu et al. (2018)</ref>.</p><formula xml:id="formula_0">? log exp(q ? k + /t) exp(q ? k + /t) + k ? exp(q ? k ? /t)<label>(1)</label></formula><p>The query representation q = f q (x q ) is computed by the encoder network f q , and x q is a query program. Likewise, k = f k (x k ) using a separate key encoder f k . The summation k ? in the normalizing denominator is taken over the queue of precomputed negatives in the batch. Following <ref type="bibr" target="#b22">He et al. (2020)</ref>, to reduce memory consumption during pre-training, we cache embedded programs from past batches in a queue containing negative samples, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>.</p><p>The query encoder f q is trained via gradient descent while the key encoder f k is trained slowly via an exponential moving average (EMA) of the query encoder parameters. The EMA update stabilizes the pre-computed key embeddings across training iterations. Since keys are only embedded once per epoch, we use a very large set of negatives, over 100K, with minimal additional computational cost and no explicit hard negative mining.   ContraCode is agnostic to the architecture of the program encoder f q . We evaluate contrastive pretraining of 6-layer Transformer <ref type="bibr" target="#b56">(Vaswani et al., 2017)</ref> and 2-layer BiLSTM <ref type="bibr" target="#b51">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b27">Huang et al., 2015)</ref> architectures ( ?4).</p><formula xml:id="formula_1">Z i T C K H W V x V p L P S K O U 8 Y z n u q w = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 U g C C W p g i 6 L b l x W s A 9 o Y p h M J + 3 Q y Y O Z G 6 G G 4 q + 4 c a G I W / / D n X / j t M 1 C W w 9 c O J x z L / f e E 6 S C K 7 D t b 2 N p e W V 1 b b 2 0 Y W 5 u b e / s W n v 7 L Z V k k r I m T U Q i O w F R T P C Y N Y G D Y J 1 U M h I F g r W D 4 f X E b z 8 w q X g S 3 8 E o Z V 5 E + j E P O S W g J d 8 6 r L g 5 d o F k 2 B 2 b l d A f m s P 7 U 9 8 q 2 1 V 7 C r x I n I K U U Y G G b 3 2 5 v Y R m E Y u B C q J U 1 7 F T 8 H I i g V P B x q a b K Z Y S O i R 9 1 t U 0 J h F T X j 6 9 f o w r W u n h M J G 6 Y s B T 9 f d E T i K l R l G g O y M C A z X v T c T / v G 4 G 4 a W X 8 z j N g M V 0 t i j M B I Y E T 6 L A P S 4 Z B T H S h F D J 9</formula><formula xml:id="formula_2">v F l Z Z f W / S t h O G V + o 9 9 m j h V x 5 Y = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r x o / G v X o Z b E U P J W k C n o s e v F Y w b Z C E 8 p m u 2 m X b j Z x d y L U 0 F / i x Y M i X v 0 p 3 v w 3 b t s c t P X B w O O 9 G W b m h a n g G l z 3 2 y q t r W 9 s b p W 3 7 Z 3 d v f 2 K c 3 D Y 0 U m m K G v T R C T q P i S a C S 5 Z G z g I d p 8 q R u J Q s G 4 4 v p 7 5 3 U e m N E / k H U x S F s R k K H n E K Q E j 9 Z 1 K z c + x D y T D / t S O + g 9 9 p + r W 3 T n w K v E K U k U F W n 3 n y x 8 k N I u Z B C q I 1 j 3 P T S H I i Q J O B Z v a f q Z Z S u i Y D F n P U E l i p o N 8 f v g U 1 4 w y w F G i T E n A c / X 3 R E 5 i r S d x a D p j A i O 9 7 M 3 E / 7 x e B t F l k H O Z Z s A k X S y K M o E h w b M U 8 I A r R k F M D C F U c X M r p i O i C A W T l W 1 C 8 J Z f X i W d R t 0 7 q z d u z 6 v N q y K O M j p G J + g U e e g C N d E N a q E 2 o i h D z + g V v V l P 1 o v 1 b n 0 s W k t W M X O E / s D 6 / A F d E p J A &lt; / l</formula><p>Transfer learning After pre-training converges, the encoder f q is transferred to downstream tasks. For code clone detection, we use f q (x) without fine-tuning. For tasks where the output space differs from the encoder, we add a task-specific MLP or Transformer decoder after f q , then fine-tune the resulting network end-to-end on labeled task data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In order to evaluate whether ContraCode defend against adversarial code inputs, we benchmark adversarial code clone detection accuracy <ref type="bibr" target="#b8">(Baker, 1992)</ref>. We evaluate results over natural and adversarial edits. We then evaluate how improvements to adversarial robustness translate to improvements on established in-the-wild code benchmarks. While improvements on adversarial benchmarks would not be expected to translate to real code, we find significant improvements in extreme code summarization <ref type="bibr" target="#b4">(Allamanis et al., 2016)</ref> and type inference <ref type="bibr" target="#b23">(Hellendoorn et al., 2018)</ref> tasks.</p><p>Clone detection experiments show that contrastive and hybrid representations with our compiler-based augmentations are predictive of program functionality in-the-wild, and that contrastive representations are the most robust to adversarial edits ( ?4.1). Contrastive pre-training outperforms baseline supervised and self-supervised methods on all three tasks ( ?4.1-4.3). Finally, ablations suggest it is better to augment unlabeled programs during pre-training rather than augmenting smaller supervised datasets ( ?4.4).</p><p>Experimental setup Models are pre-trained on CodeSearchNet, a large corpus of methods extracted from popular GitHub repositories <ref type="bibr" target="#b28">(Husain et al., 2019)</ref>. CodeSearchNet contains 1,843,099 JavaScript programs. Only 81,487 methods have both a documentation string and a method name. The asymmetry between labeled and unlabeled programs stems from JavaScript coding practices where anonymous functions are widespread. The pre-training dataset described in Section 3.1 is the result of augmenting all 1.8M programs.</p><p>As our approach supports any encoder, we evaluate two architectures: a 2-layer Bidirectional LSTM with 18M parameters, similar to the supervised model used by <ref type="bibr" target="#b23">Hellendoorn et al. (2018)</ref>, and a 6-layer Transformer with 23M parameters. For a baseline self-supervised approach, we pretrain both architectures with the RoBERTa MLM objective, then transfer it to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Robust Zero-shot Code Clone Detection</head><p>ContraCode learns to match variants of programs with similar functionality. While transformations produce highly diverse token sequences (quantified in the supplement), they are artificial and do not change the underlying algorithm. In contrast, human programmers can solve a problem with many data structures, algorithms and programming models. To determine whether pre-  <ref type="table">Table 2</ref>: Zero-shot code clone detection with cosine similarity probe. Contrastive and hybrid representations improve clone detection AUROC on unmodified (natural) HackerRank programs by +8% and +10% AUROC over a heuristic textual similarity probe, respectively, suggesting they are predictive of functionality. Contrastive representations are also the most robust to adversarial code transformations. trained representations are consistent across programs written by different people, we benchmark code clone detection, a binary classification task to detect whether two programs solve the same problem or different ones <ref type="figure" target="#fig_5">(Fig. 7)</ref>. This is useful for deduplicating, refactoring and retrieving code, as well as checking approximate code correctness.</p><p>Benchmarks exist like BigCloneBench (Svajlenko et al., 2014), but to the best of our knowledge, there is no benchmark for the JavaScript. We collected 274 in-the-wild JavaScript programs that correctly solve 33 problems from the Hack-erRank interview preparation website. There are 2065 pairs solving the same problem and 70K pairs solving different problems, which we randomly subsample to 2065 to balance the classes.</p><p>Since we probe zero-shot performance based on pre-trained representations, there is no training set. Instead, we threshold cosine similarity of pooled representations of the programs u and v: u T v/ u v . Many code analysis methods for clone detection measure textual similarity <ref type="bibr" target="#b8">(Baker, 1992)</ref>. As a baseline, we threshold the dissimilarity score, a scaled Levenshtein edit distance between normalized and tokenized programs. <ref type="table">Table 2</ref> reports the area under the ROC curve (AUROC) and average precision (AP, area under Precision-Recall). All learned representations improve over the heuristic on natural code. Selfsupervision through RoBERTa MLM pre-training improves over a randomly initialized network by +1.7% AUROC. Contrastive pre-training achieves +3.4% AUROC over the same baseline. A hybrid objective combining both the contrastive loss and MLM has the best performance with +7.0% AU-ROC (+5.4% over MLM alone). Although MLM is still useful over natural code, ContraCode learns overall stronger representations of functionality.</p><p>However, are these representations robust to code edits? We adversarially edit one program in each pair by applying the loss-maximizing code compression and identifier modification transformation among N samples from Algorithm 1. These transformations preserve program function-ality, so ground-truth labels are unchanged. With only 4 edits, RoBERTa performs worse than the heuristic (-5.8% AUROC) and worse than random guessing (50% AUROC), indicating it is highly sensitive to these kinds of implementation details. ContraCode retains much of its performance (+39% AUROC over RoBERTa) as it explicitly optimizes for invariance to code edits. Surprisingly, the hybrid model is less robust than Contra-Code alone, perhaps indicating that MLM learns non-robust features <ref type="bibr" target="#b29">(Ilyas et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fine-tuning for Type Inference</head><p>JavaScript is a dynamically typed language, where variable types are determined at runtime based on the values they represent. Manually annotating code with types helps tools flag bugs by detecting incompatible types. Annotations also document code, but are tedious to maintain. Type inference tools automatically predict types from context.</p><p>To learn to infer types, we use the annotated dataset of TypeScript programs from DeepTyper <ref type="bibr" target="#b23">(Hellendoorn et al., 2018)</ref>, excluding GitHub repositories that were made private or deleted since publication. The training set contains 15,570 TypeScript files from 187 repositories with 6,902,642 total tokens. Validation and test sets are from held-out repositories. For additional supervision, missing types are inferred by static analysis to augment user-defined types as targets. A 2-layer MLP head predicts types from token embeddings output by the DeepTyper LSTM. We early stop based on validation set top-1 accuracy.</p><p>For the rest of our experiments, baseline RoBERTa models are pre-trained on the same augmented data as ContraCode for fair comparison. Learning representations that transfer from unlabeled JavaScript programs is challenging because TypeScript supports a superset of JavaScript's grammar, with types annotations and other syntactic sugar that need to be learned during fine-tuning. Further, the pre-training data only has methods while DeepTyper's dataset uses entire files (modules). The model is only given source code for a single file, not dependencies.</p><p>In <ref type="table" target="#tab_6">Table 3</ref>, contrastive pre-training outperforms all baseline learned methods. ContraCode is applied in a drop-in fashion to each of the baselines. Pre-training with our contrastive objective and data augmentations yields absolute accuracy improvements of +1.2%, +6.3%, +2.3% top-1 and  +1.8%, +5.7%, +2.8% top-5 over the Transformer, RoBERTa, and DeepTyper, respectively. The RoBERTa baseline may perform poorly since the MLM objective focuses on token reconstruction that is overly sensitive to local syntactic structure, or because sufficient fine-tuning data is available, described as weight "ossification" by <ref type="bibr" target="#b26">Hernandez et al. (2021)</ref>. To combine the approaches, we minimized our loss in addition to MLM as a hybrid local-global objective to pre-training a Transformer, improving accuracy by +6.31% over the RoBERTa Transformer.</p><p>We also evaluate the recent GPT-3 Codex model by OpenAI (an, 2021) using their API. We benchmark the 175B parameter DaVinci model in both a zero-shot as well as a few-shot prompting setup. Although the Codex model was trained over Type-Script programs, it performs poorly as it achieves an accuracy of 26.6% in the zero-shot setup and 30.6% in the few-shot setup. We only evaluate Top-1 accuracy for GPT-3 models as GPT-3 does not reliably output confidence scores.</p><p>Learning outperforms static analysis by a large margin. Overall, our best model has +8.9% higher top-1 accuracy than the built-in Type-Script CheckJS type inference system, showing the promise of learned code analysis. Surfacing multiple candidate types can also be useful to users, while CheckJS only has a single prediction. <ref type="figure" target="#fig_6">Fig. 8</ref> shows two files from held-out repositories. For the first, our model consistently predicts the correct return and parameter types. The   model correctly predicts that the variable message is a string, even though its type is ambiguous without access to the imported write method signature. For the second, ContraCode predicts 4 of 8 types correctly including ViewContainerRef and ChangeDetectorRef from the AngularJS library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Extreme Code Summarization</head><p>The extreme code summarization task asks a model to predict the name of a method given its body <ref type="bibr" target="#b4">(Allamanis et al., 2016)</ref>. These names often summarize the method, such as reverseString(...). Summarization models could help programmers interpret poorly documented code. We create a JavaScript summarization dataset using the 81,487 labeled methods in the CodeSearchNet dataset. The name is masked in the method declaration. A sequence-to-sequence model with an autoregressive decoder is trained to  <ref type="figure">Figure 9</ref>: A held-out JavaScript program from Code-SearchNet and method names generated by a Transformer pre-trained with ContraCode. The correct method name is predicted as the most likely decoding. maximize log likelihood of the ground-truth name, a form of abstractive summarization. All models overfit, so we stop early according to validation loss. As proposed by <ref type="bibr" target="#b4">Allamanis et al. (2016)</ref>, we evaluate model predictions by precision, recall and F1 scores over the set of method name tokens. <ref type="table" target="#tab_8">Table 4</ref> shows results in four settings: (1) supervised training using baseline tree-structured architectures that analyze the AST (code2vec, code2seq), (2) pre-training on all 1.8M programs using MLM followed by fine-tuning on the labeled programs (RoBERTa), (3) training a Transformer from scratch and (4) contrastive pre-training followed by fine-tuning with augmentations.</p><p>Contrastive pre-training outperforms code2seq by +8.2% test precision, +7.3% recall, and +7.9% F1 score. ContraCode outperforms self-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code summarization model F1</head><p>Transformer <ref type="table" target="#tab_8">(Table 4)</ref> 16.86 + augmentations 15.65</p><p>Type inference model Acc@1 Transformer <ref type="table" target="#tab_6">(Table 3)</ref> 45.66 + augmentations 44.14 DeepTyper <ref type="table" target="#tab_6">(Table 3)</ref> 51.73 + augmentations 50.33 supervised pre-training with RoBERTa by +4.8% F1. ContraCode also achieves slightly higher performance than the Transformer learned from scratch. While this improvement is smaller, code summarization challenging as identifier names are not consistent between programmers. <ref type="figure">Figure 9</ref> shows a qualitative example of predictions for the code summarization task. The JavaScript method is not seen during training. A Transformer pre-trained with ContraCode predicts the correct method name through beam search. The next four predictions are reasonable, capturing that the method processes an image. The 2nd and 3rd most likely decodings, getImageItem and createImage, use get and create as synonyms for load, though the final two unlikely decodings include terms not in the method body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Understanding augmentation importance</head><p>We analyze the effect of augmentations on supervised learning and on pre-training.</p><p>Supervised learning with augmentations As a baseline, we re-train models from scratch with compiler transforms during supervised learning rather than pre-training. Data augmentation artificially expands labeled training sets. For sequenceto-sequence summarization, we apply a variety of augmentations (LS, SW, VR, DCI). These all preserve the method name. For type inference, labels are aligned to input tokens, so they must be realigned after transformation. We only apply tokenlevel transforms (LS, SW) as we can track labels. <ref type="table" target="#tab_10">Table 5</ref> shows results. Compiler-based data augmentations degrade supervised models, perhaps by creating a training distribution not reflective of evaluation programs. However, as shown in ?4.1-4.3, augmenting during ContraCode pretraining yields a more accurate model. Our con-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training augmentations</head><p>Acc@1 Acc@5 All augmentations <ref type="table" target="#tab_6">(Table 3)</ref> 52.65% 84.60% w/o identifier modification (-VR, -IM) 51.94% 84.43% w/o line subsampling <ref type="bibr">(-LS)</ref> 51.05% 81.63% w/o code compression <ref type="bibr">(-T,C,DCE,CF)</ref> 50.69% 81.95% trastive learning framework also allows learning over large numbers of unlabeled programs that supervised learning alone cannot leverage. The ablation indicates that augmentations do not suffice, and contrastive learning is important.</p><p>Ablating pre-training augmentations Some data augmentations could be more valuable than others. Empirically, pre-training converges faster with a smaller set of augmentations at the same batch size since the positives are syntactically more similar, but this hurts downstream performance. <ref type="table" target="#tab_11">Table 6</ref> shows that type inference accuracy degrades when different groups of augmentations are removed. Semantics-preserving code compression passes that require code analysis are the most important, improving top-1 accuracy by 1.95% when included. Line subsampling serves as a regularizer, but changes program semantics. LS is relatively less important, but does help accuracy. Identifier modifications preserve semantics, but change useful naming information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Large-scale code repositories like GitHub are a powerful resource for learning machine-aided programming tools. However, most current code representation learning approaches need labels, and popular label-free self-supervised methods like RoBERTa are not robust to adversarial inputs. Instead of reconstructing tokens like BERT, learning what code says, we learn what code does. We propose ContraCode, a contrastive self-supervised algorithm that learns representations invariant to transformations via compiler-based data augmentations. In experiments, ContraCode learns effective representations of functionality, and is robust to adversarial code edits. We find that Contra-Code significantly improves performance on three downstream JavaScript code understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Program transformation details</head><p>We use the Babel compiler infrastructure <ref type="bibr" target="#b39">(McKenzie et al., 2020)</ref> and the terser JavaScript library for AST-based program transformations. We perform variable renaming and dead code insertion (variable declaration insertion) using custom Babel transforms, subword regularization with sentencepiece Python tokenization library, line subsampling using JavaScript string manipulation primatives and other transformations with terser. Terser has two high-level transformation modes, mangling and compression, each with finer grained controls such as formatting, comment and log removal, and dead code elimination. We show an example merge sort with variants in <ref type="figure">Figure 10</ref>.</p><p>Reformatting, beautification, compression (R, B, C): Personal coding conventions do not affect the semantics of code; auto-formatting normalizes according to a style convention.</p><p>Dead-code elimination (DCE): In this pass, all unused code with no side effects are removed. Various statements can be inlined or removed as stale or unneeded functionality.</p><p>Type upconversion (T): In JavaScript, some types are polymorphic &amp; can be converted between each other. As an example, booleans can be represented as true or as 1.</p><p>Constant folding (CF): During constant folding, all expressions that can be pre-computed at compilation time can be inlined. For example, the expression (2 + 3) * 4 is replaced with 20.</p><p>Variable renaming, identifier mangling (VR, IM): Arguments can be renamed with random word sequences and identifiers can be replaced with short tokens to make the model robust to naming choices. Program behavior is preserved despite obfuscation.</p><p>Dead-code insertion (DCI): Commonly used no-ops such as comments and logging are inserted.</p><p>Subword regularization (SW): From Kudo (2018), text is tokenized in several different ways, with a single word (_function) or subtokens (_func tion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Line subsampling (LS):</head><p>We randomly sample (p = 0.9) lines from a method body. While not semantics-preserving, line subsampling serves as a regularizer.</p><p>Original merge sort program // Split the array into halves and merge them recursively function mergeSort (arr) { if (arr.length === 1) { // return once we hit an array with a single item return arr } const middle = Math.floor(arr.length / 2) // get the middle item of the array rounded down const left = arr.slice(0, middle) // items on the left side const right = arr.slice(middle) // items on the right side return merge( mergeSort(left), mergeSort(right) ) } Variable renaming, comment removal, reformatting</p><formula xml:id="formula_3">function mergeSort(e) {</formula><p>if (e.length === 1) { return e; } const t = Math.floor(e.length / 2); const l = e.slice(0, t); const n = e.slice(t); return merge(mergeSort(l), mergeSort(n)); } Combining variable declarations, inlining conditional function mergeSort(e) { if (1 === e.length) return e; const t = Math.floor(e.length / 2), r = e.slice(0, t), n = e.slice(t); return merge(mergeSort(r), mergeSort(n)); } <ref type="figure">Figure 10</ref>: Given a JavaScript code snippet implementing the merge sort algorithm, we apply semanticspreserving transformations to produce functionallyequivalent yet textually distinct code sequences. Variable renaming and identifier mangling passes change variable names. Compression passes eliminate unnecessary characters such as redundant variable declarations and brackets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B How similar are transformed programs?</head><p>To understand the diversity created by program transformations, we compute the Levenshtein minimum edit distance between positive pairs in the precomputed pre-training dataset, i.e. transformed variants of the same source method. For comparison, we also compute the edit distance between negative pairs: transformed variants of different programs.</p><p>The edit distance D(x q , x k ) computes the minimum number of token insertions, deletions or substitutions needed to transform the tokenized query progrm x q into the key program x k . To normalize by sequence length | ? |, let</p><formula xml:id="formula_4">dissimilarity D (x q , x k ) = D(x q , x k ) max(|x q |, |x k |)<label>(2)</label></formula><p>Dissimilarity ranges from 0% for programs with the same sequence of tokens, to 100% for programs without any shared tokens. Note that whitespace transformations do not affect the metric because the tokenizer collapses repeated whitespace. For the positives, we estimate dissimilarity by sampling one pair per source program in the CodeSearchNet dataset (1.6M source programs with at least one pair). We sample the same number of negative pairs. <ref type="figure" target="#fig_7">Fig. 11</ref> shows a histogram of token dissimilarity. Positive pairs have 65% mean dissimilarity, while negatives have 86%. Negatives are more dissimilar on average as source sequences could have different lengths, idioms and functionality. Still, the transformations generated quite different positive sequences, with less than half of their tokens shared. The 25th, median and 75th percentile dissimilarity is 59%, 66% and 73% for positives, and 82%, 87% and 90% for negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental setup</head><p>Architectures The Transformer encoder has 6 layers (23M parameters) in all experiments. For code summarization experiments, we add 4 decoder layers with causal masking to generate the natural language summary. We leverage the default positional embedding function (sin, cos) as used in the original Transformer architecture. The network originally proposed in DeepTyper <ref type="bibr" target="#b23">(Hellendoorn et al., 2018)</ref> had 11M parameters with a 300 dimensional hidden state. We increase the hidden state size to 512 to increase model capacity, so our BiLSTM for type prediction has 17.5M parameters. During fine-tuning, across all experiments, we optimize parameters using Adam with linear learning rate warmup and decay. For the Transformer, the learning rate is linearly increased for 5,000 steps from 0 to a maximum of 10 ?4 . For the bidirectional LSTM, the learning rate is increased for between 2,500 and 10,000 steps to a maximum of 10 ?3 . Type inference hyperparameters are selected by validation top-1 accuracy.</p><p>ContraCode pre-training The InfoNCE objective is minimized with temperature t = 0.07 following <ref type="bibr" target="#b22">He et al. (2020)</ref>. Also following <ref type="bibr" target="#b22">He et al. (2020)</ref>, the key encoder's parameters are computed with the momentum update equation ? k ? m? k + (1 ? m)? q , equivalent to an EMA of the query encoder parameters ? q . To pretrain a Transformer using the ContraCode objective, we first embed each token in the program using the Transformer. However, the InfoNCE objective is defined in terms of a single embedding for the full program. The ContraCode Transformer is pre-trained with a batch size of 96. Our model averages the 512-dimensional token embeddings across the sequence, then applies a two-layer MLP with 512 hidden units and a ReLU activation to extract a 128-dimensional embedding for the loss.</p><p>The DeepTyper bidirectional LSTM architecture has two choices for extracting a global program representation. We aggregate a 1024dimensional representation of the program by concatenating its four terminal hidden states (from two sequence processing directions and two stacked LSTM layers), then apply the same MLP architecture as before to extract a 128-dimensional representation. Alternatively, we can average the hidden state concatenated from each direction across the tokens in the sequence before applying the MLP head. We refer to the hidden-state configuration as a global representation and the sequence averaging configuration as a local representation in Tab. 8. We pre-train the BiLSTM with large batch size of 512 and apply weight decay.</p><p>Code clone detection on HackerRank programs <ref type="figure" target="#fig_5">Figure 7</ref> shows two programs sampled from the HackerRank clone detection dataset. These programs successfully solve the same problem, so they are clones. We report metrics that treat code clone detection as a binary classification task given a pair of programs. 2065 pairs of programs solving the same HackerRank problem and 2065 pairs of programs solving different problems are sampled to construct an evaluation dataset. We use the area under the Receiver Operating Characteristic (AUROC) metric and Average Precision (AP) metrics. The standard error of the AUROC is reported according to the Wilcoxon statistic <ref type="bibr" target="#b20">(Fogarty et al., 2005)</ref>. Average Precision is the area under the Precision-Recall curve. AUROC and AP are both computed using the scikit-learn library <ref type="bibr" target="#b45">(Pedregosa et al., 2011)</ref>.</p><p>A Transformer predicts contextual embeddings of each token in a program, but our thresholded cosine similiarity classifier requires fixed length embeddings of whole programs. To determine if two programs that may differ in length are clones, we pool the token representations across the sequence. We evaluated both mean pooling and max pooling the representation. For the hybrid model pre-trained with both RoBERTa (MLM) and contrastive objectives, mean pooling achieved the best AUROC and AP. For other models, max pooling performed the best.</p><p>Type prediction Following DeepTyper <ref type="bibr" target="#b23">(Hellendoorn et al., 2018)</ref>, our regenerated dataset for type prediction has 187 training projects with 15,570 TypeScript files, totaling 6,902,642 tokens. We tune hyperparameters on a validation set of 23 distinct projects with 1,803 files and 490,335 tokens, and evaluate on a held-out test set of 24 projects with 2,206 files and 958,821. The training set is smaller than originally used in DeepTyper as several projects were made private or deleted from GitHub before May 2020 when we downloaded the data, but we used the same commit hashes for available projects so our splits are a subset of the original. We have released the data with our open-source code to facilitate further work on a stable benchmark as more repositories are deleted over time. We perform early stopping to select the number of training epochs. We train each model for 100 epochs and select the checkpoint with the minimum accuracy@1 metric (all types, including any) on the validation set. Except for the model learned from scratch, the Transformer architectures are pre-trained for 240K steps. Models with the DeepTyper architecture converge faster on the pre-training tasks and are pre-trained for 20K iterations (unless otherwise noted).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extreme code summarization by method name prediction</head><p>We train method prediction models using the labeled subset of CodeSearch-Net. Neither method names nor docstrings are provided as input to the model: the docstring is deleted, and the method name is replaced with the token 'x'. Thus, the task is to predict the method name using the method body and comments alone.</p><p>To decode method names from all models except the code2vec and code2seq baselines which implement their own decoding procedures, we use a beam search with a beam of size 5 and a maximum target sequence length of 20 subword tokens. We detail the cumulative distribution of program lengths in <ref type="figure">Figure 12</ref>. The ContraCode summarization Transformer only needed to be pre-trained for 20K iterations, with substantially faster convergence than RoBERTa (240K iterations). During fine-tuning, we apply the LS,SW,VR,DCI augmentations to ContraCode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Baselines</head><p>Baselines for code summarization and type prediction trained their models on an inconsistent set of programming languages and datasets. In order to normalize the effect of datasets, we selected several diverse state-of-the-art baselines and reimplemented them on the JavaScript dataset.</p><p>AST-based models The authors of code2vec <ref type="bibr">(Alon et al., 2019b)</ref> and code2seq <ref type="bibr" target="#b5">(Alon et al., 2019a)</ref>, AST-based code understanding models, made both data and code available, but train their model on the Java programming language. In order to extend the results in their paper to JavaScript for comparison with our approach, we generated an AST path dataset for the Code-SearchNet dataset. The sensitivity of path-mining embeddings to different datasets is documented in prior work, so published F1 scores are not directly comparable; F1 scores for code2vec <ref type="bibr">(Alon et al., 2019b)</ref> vary between 19 <ref type="bibr" target="#b5">(Alon et al., 2019a)</ref> and 43 <ref type="bibr">(Alon et al., 2019b</ref>) depending on the dataset used. Therefore, we use the same dataset generation code as the authors for fair comparison. We first parse the source functions using the Babel compiler infrastructure. Using the original code on these ASTs, up to 300 token-to-token (leaf-toleaf) paths are extracted from each function's AST as a precomputed dataset. Then, we generate a token and AST node vocabulary using the same author-provided code, and train the models for 20 epochs, using early stopping for code2seq. We observed that code2vec overfits after 20 epochs, and longer training was not beneficial.</p><p>DeepTyper <ref type="bibr" target="#b23">(Hellendoorn et al., 2018)</ref> Deep-Typer uses a two layer GRU with a projection over possible classes, with an embedding size of 300 and hidden dimension of 650. However, we found improved performance by replacing the GRU with a bidirectional LSTM (BiLSTM). We normalize the LSTM parameter count to match our model, and therefore use a hidden dimension size of 512. We also use subword tokenization rather than space delimited tokens according to <ref type="bibr" target="#b35">Kudo (2018)</ref>, as subwords are a key part of state-of-theart models for NLP <ref type="bibr" target="#b53">(Sennrich et al., 2016)</ref>.</p><p>RoBERTa</p><p>We pre-trained an encoder using RoBERTa's masked language modeling loss on our augmented version of CodeSearch-Net, the same data used to pre-train Contra-Code. This model is then fine-tuned on downstream datasets. Unlike the original BERT paper which cuBERT <ref type="bibr" target="#b32">(Kanade et al., 2020)</ref> is based on, hyperparameters from RoBERTa have been found to produce better results during pre-training. RoBERTa pre-trains using a masked language modeling (MLM) objective, where 15% of tokens in a sentence are masked or replaced and are reconstructed by the model. We did not use the BERT Next Sentence Prediction (NSP) loss which RoBERTa finds to be unnecessary. We normalize baseline parameter count by reducing the number of Transformer layers from 24 to 6 for a total of 23M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional results and ablations</head><p>Code clone detection ROC, PR curves <ref type="figure">Fig. 13</ref> plots true postive rate vs false positive rate and precision vs recall for different zero-shot classifiers on the code clone detection downstream tasks. These classifiers threshold a similarity score given by token-level edit distance for the heuristic approach or cosine similarity for the neural network representations. The hybrid self-supervised model combining ContraCode's contrastive objective and MLM achieves better tradeoffs than the other approaches. <ref type="figure">Fig. 14</ref> shows the AUROC and Average Precision of four Transformer models on the same task under adversarial transformations of one input program. Untrained models as well as models pre-trained with RoBERTa's MLM objective are not robust to these code transformations. However, the model pre-trained with ContraCode preserves much of its performance as the adversarial attack is strengthened. Which part of the model should be transferred? SimCLR <ref type="bibr" target="#b13">(Chen et al., 2020a)</ref> proposed using a small MLP head to reduce the dimensionality of the representation used in the InfoNCE loss during pre-training, and did not transfer the MLP to the downstream image-classification task. In contrast, we find it beneficial to transfer part of the contrastive MLP head to type inference, showing a 2% improvement in top-5 accuracy over transferring the encoder only <ref type="table">(Table 7)</ref>. We believe the improvement stems from fine-tuning both the encoder and MLP which allows feature adaptation, while SimCLR trained a linear model on top of frozen features. We only transferred the MLP when contrasting the mean of token embeddings during pre-training, not the terminal hidden states, as the dimensionality of the MLP head differs. These representations are compared next.</p><p>Should we pre-train global or local representations?</p><p>We compare pre-training DeepTyper with two variants of ContraCode. We either use the mean of token hidden states across the program (averaging local features), or the terminal hidden states as input to the MLP used to extract the con- <ref type="table">Table 7</ref>: If local representations are learned, transferring part of the Contrastive MLP head improves type inference. The encoder is a 2-layer BiLSTM (d=512), with a 2-layer MLP head for both pre-training purposes and type inference. The mean hidden state representation is optimized for 10K iterations for the purposes of this ablation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warm-started layers</head><p>Acc@1 Acc@5 BiLSTM 49.32% 80.03% BiLSTM, 1 layer of MLP 49.15% 82.58%</p><p>trastive representation q = f q (x) (global features). Token-level features might capture more syntactic details, but averaging pooling ignores order. Table 8 shows the accuracy of a BiLSTM pre-trained with each strategy. Using the global features for pre-training yields significantly improved performance, +2.38% acc@1 after 10K iterations of pretraining (not converged for the purposes of ablation). The global pre-training strategy achieves our best results.</p><p>Do pre-trained encoders help more with shallow decoders?</p><p>For the sequence-to-sequence code summarization task, ContraCode only pretrains the encoder of the Transformer. In Table 9, we ablate the depth of the decoder to understand how much shallow decoders benefit from contrastive pre-training of the encoder. Similar experiments were performed in a vision context by <ref type="bibr" target="#b16">(Erhan et al., 2010)</ref>, where different numbers of layers of a classifier are pre-trained. After 45k pre-training steps, the 4-layer decoder achieves 0.50% higher precision, 0.64% higher recall and 0.77% higher F1 score than the 1-layer model, so additional decoder depth is helpful for the downstream task. The 1-layer decoder model also benefits significantly from longer pre-training, with a 6.3% increase in F1 from 10k to 45k iterations. This large of an improvement indicates that ContraCode could be more helpful for pretraining when the number of randomly initialized parameters at the start of fine-tuning is small. For larger decoders, more parameters must be optimized during-finetuning, and the value of pretraining is diminished.</p><p>Contrastive representation learning strategies</p><p>In <ref type="figure" target="#fig_1">Figure 15</ref>, we compare two strategies of refreshing the MoCo queue of key embeddings (the dictionary of negative program representations assumed to be non-equivalent to the batch of positives). In the first strategy, we add 8 items out   of the batch to the queue (1?), while in the second we add 96 items (12?). In addition, we use a larger queue (65k versus 125k keys) and a slightly larger batch size (64 versus 96). We observe that for the baseline queue fill rate, the accuracy decreases for the first 8125 iterations as the queue fills. This decrease in accuracy is expected as the task becomes more difficult due to the increasing number of negatives during queue warmup. However, it is surprising that accuracy grows so slowly once the queue is filled. We suspect this is because the key encoder changes significantly over thousands of iterations: with a momentum term m = 0.999, the original key encoder parameters are decayed by a factor of 2.9 ? 10 ?4 by the moving average. If the queue is rapidly refreshed, queue embeddings are predicted by recent key encoders, not old parameters. This also indicates that a large diversity of negative, non-equivalent programs are helpful for rapid convergence of Con-traCode pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t-SNE visualization of representations</head><p>We qualitatively inspect the structure of the learned representation space by visualizing selfsupervised representations of variants of 28 programs using t-SNE <ref type="bibr" target="#b37">(Maaten and Hinton, 2008)</ref> in <ref type="figure" target="#fig_4">Figure 16</ref>. Representations of transformed variants of the same program are plotted with the same color. ContraCode (BiLSTM) clusters variants closely together. Indeed, contrastive learning learns representations that are invariant to a wide class of automated compiler-based transformations. In comparison, the representations learned by masked language modeling (RoBERTa) show more overlap between different programs, and variants do not cleanly cluster. With a hybrid loss combining masked language modeling and contrastive learning, representations of variants of the same program once again cluster.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) { while (i &lt; n) { ... } } function (str, len) { return str.slice(0, len); } function f(n) { return n&lt;2 ? 1 : f(n-1) + f(n-2); } function (arr) { for (i of arr) { ... } } Maximize similarity with equivalent programs Minimize similarity with functionally different programs Given a program,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Histogram of the number of unique transformed variants per JavaScript method during pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " k w v F l Z Z f W / S t h O G V + o 9 9 m j h V x 5 Y = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r x o / G v X o Z b E U P J W k C n o s e v F Y w b Z C E 8 p m u 2 m X b j Z x d y L U 0 F / i x Y M i X v 0 p 3 v w 3 b t s c t P X B w O O 9 G W b m h a n g G l z 3 2 y q t r W 9 s b p W 3 7 Z 3 d v f 2 K c 3 D Y 0 U m m K G v T R C T q P i S a C S 5 Z G z g I d p 8 q R u J Q s G 4 4 v p 7 5 3 U e m N E / k H U x S F s R k K H n E K Q E j 9 Z 1 K z c + x D y T D / t S O + g 9 9 p + r W 3 T n w K v E K U k U F W n 3 n y x 8 k N I u Z B C q I 1 j 3 P T S H I i Q J O B Z v a f q Z Z S u i Y D F n P U E l i p o N 8 f v g U 1 4 w y w F G i T E n A c / X 3 R E 5 i r S d x a D p j A i O 9 7 M 3 E / 7 x e B t F l k H O Z Z s A k X S y K M o E h w b M U8 I A r R k F M D C F U c X M r p i O i C A W T l W 1 C 8 J Z f X i W d R t 0 7 q z d u z 6 v N q y K O M j p G J + g U e e g C N d E N a q E 2 o i h D z + g V v V l P 1 o v 1 b n 0 s W k t W M X O E / s D 6 / A F d E p J A &lt; / l a t e x i t &gt; f k &lt; l at e x i t s h a 1 _ b a s e 6 4 = " / t y r M S r E / Z i F Z K S p z m o b 4 g k 8 E h s = " &gt; A A A B + H i c b V D L S g N B E J y N r 7 g + E v X o Z T A E P I X d K O g x 6 M V j B P O A 7 L L M T m a T I b M P Z n q E u O R L v H h Q x K u f 4 s 2 / c Z L s Q R M L G o q q b r q 7 w k x w B Y 7 z b Z U 2 N r e 2 d 8 q 7 9 t 7 + w W G l e n T c V a m W l H V o K l L Z D 4 l i g i e s A x w E 6 2 e S k T g U r B d O b u d + 7 5 F J x d P k A a Y Z 8 2 M y S n j E K Q E j B d V K 3 c u x B 0 R j b 2 Z H w S S o 1 p y G s w B e J 2 5 B a q h A O 6 h + e c O U 6 p g l Q A V R a u A 6 G f g 5 k c C p Y D P b 0 4 p l h E 7 I i A 0 M T U j M l J 8 v D p / h u l G G O E q l q Q T w Q v 0 9 k Z N Y q W k c m s 6 Y w F i t e n P x P 2 + g I b r 2 c 5 5 k G l h C l 4 s i L T C k e J 4 C H n L J K I i p I Y R K b m 7 F d E w k o W C y s k 0 I 7 u r L 6 6 T b b L g X j e b 9 Z a 1 1 U 8 R R R q f o D J 0 j F 1 2 h F r p D b d R B F G n 0 j F 7 R m / V k v V j v 1 s e y t W Q V M y f o D 6 z P H 1 P 6 k j o = &lt; / l a t e x i t &gt; Embed q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H / j q p S m 0 8 d t f k S I P s z p 3 T h F A j b M = " &gt; A A A B + 3 i c b V B N S 8 N A E J 3 U r x q / Y j 1 6 W S w F T y W p g h 6 L X j x W s K 3 Q h L D Z b t q l m w 9 3 N 2 I J + S t e P C j i 1 T / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k H I m l W 1 / G 5 W 1 9 Y 3 N r e q 2 u b O 7 t 3 9 g H d Z 6 M s k E o V 2 S 8 E T c B 1 h S z m L a V U x x e p 8 K i q O A 0 3 4 w u Z 7 5 / U c q J E v i O z V N q R f h U c x C R r D S k m / V G m 6 O X I U z 5 B Z m I / Q n 5 o N v 1 e 2 m P Q d a J U 5 J 6 l C i 4 1 t f 7 j A h W U R j R T i W c u D Y q f J y L B Q j n B a m m 0 m a Y j L B I z r Q N M Y R l V 4 + v 7 1 A D a 0 M U Z g I X b F C c / X 3 R I 4 j K a d R o D s j r M Z y 2 Z u J / 3 m D T I W X X s 7 i N F M 0 J o t F Y c a R S t A s C D R k g h L F p 5 p g I p i + F Z E x F p g o H Z e p Q 3 C W X 1 4 l v V b T O W u 2 b s / r 7 a s y j i o c w w m c g g M X 0 I Y b 6 E A X C D z B M 7 z C m 1 E Y L 8 a 7 8 b F o r R j l z B H 8 g f H 5 A 8 R e k v g = &lt; / l a t e x i t &gt; k + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t 0 g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>a 2 Y D o g k F H R g p g 7 B m X 9 5 k b R q V e e s W r s 9 L 9 e v i j h K 6 A g d o x P k o A t U R z e o g Z q I o k f 0 j F 7 R m / F k v B j v x s e s d c k o Z g 7 Q H x i f P + D B k 4 8 = &lt; / l a t e x i t &gt; {? } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j 3 Q N h m h l N D y V Q 5 W e 0 b 7 L P M u L a l M = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d u j l 2 g W T Y H Z u V 0 B + a l e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + Z 4 k 5 E = &lt; / l a t e x i t &gt; Enqueue as future negative Maximize Minimize k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " k w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>a t e x i t &gt; MLP MLP ContraCode pre-trains a neural program encoder f q and transfers it to downstream tasks. A-B. Unlabeled programs are transformed C. into augmented variants. D. We pre-train f q by maximizing similarity of projected embeddings of positive program pairs-variants of the same program-and minimizing similarity with a queue of cached negatives. E. ContraCode supports any architecture for f q that produces a global program embedding such as Transformers and LSTMs. f q is then fine-tuned on smaller labeled datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Code clone detection example. These programs solve the same HackerRank coding challenge (reading and summing two integers), but use different coding conventions. The neural code clone detector should classify this pair as a positive, i.e. a clone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>A variant of DeepTyper pre-trained with ContraCode generates type annotations for two held-out programs. The model consistently predicts correct function return types, and often correctly predicts project-specific variable types imported at the top of the file. Metrics are in the top row of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Histogram of pairwise token dissimilarity for contrastive positives (transformed variants of the same method) and negatives (transformed variants of different methods). Code transformations produce positives with dissimilar token sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Character length per method name Figure 12: CodeSearchNet code summarization dataset statistics: (a) The majority of code sequences are under 2000 characters, but there is long tail of programs that span up to 15000 characters long, (b) JavaScript method names are relatively short compared to languages like C and Java.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves for non-adversarial classifiers on the code clone detection task. Equal F1 score curves are shown on right. Adversarial AUROC and Average Precision for four models on the code clone detection task: a randomly initialized transformer, and transformers pretrained on code with the RoBERTa MLM objective, our contrastive objective, or both. Representations learned by the contrastive model transfer robustly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>Pre-training quickly converges if negative programs in the queue are frequently changed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16</head><label>16</label><figDesc>: t-SNE (Maaten and Hinton, 2008) plot of mean pooled program representations learned with masked language modeling (RoBERTa), contrastive learning (ContraCode), and a hybrid loss (RoBERTa + ContraCode). Transformed variants of the same program share the same color. Note that colors may be similar across different programs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 3: A UMAP visualization of JavaScript method representations learned by RoBERTa and ContraCode, in R 2 . Programs with the same functionality share color and number. RoBERTa's embeddings often do not cluster by functionality, suggesting that it is sensitive to implementation details. For example, many different programs overlap, and renaming the variables of Program 19 significantly changes the embedding. In contrast, variants of Program 19 cluster in Contra-Code's embedding space.</figDesc><table><row><cell>RoBERTa embeddings</cell><cell>ContraCode embeddings</cell></row><row><cell>UMAP 2</cell><cell></cell></row><row><cell>UMAP 1</cell><cell>UMAP 1</cell></row><row><cell>Program 19</cell><cell>Program 19</cell></row><row><cell>Variant A</cell><cell>Variant B</cell></row><row><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>that degrade generalization.</figDesc><table><row><cell cols="2">function x(maxLine) {</cell><cell></cell><cell></cell></row><row><cell cols="2">const section = {</cell><cell></cell><cell></cell></row><row><cell>};</cell><cell>text: '', data</cell><cell cols="2">function x(t) { const n = { 'text': '',</cell></row><row><cell cols="2">for (; i &lt; maxLine; i += 1) { section.text += `${lines[i]}\n`; }</cell><cell></cell><cell>'data': data }; for (;i &lt; t; i += 1) { n.text += lines[i] + '\n';</cell><cell>function x(t){const</cell></row><row><cell cols="2">if (section) { parsingCtx.sections.push(section); }</cell><cell>}</cell><cell>} n &amp;&amp; parsingCtx.sections.push(n);</cell><cell>n={'text':'','data':data};for(;i&lt;t;i+= 1)n.text+=lines[i] +'\n';n&amp;&amp;parsingCtx.sections.push(n)}</cell></row><row><cell>}</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Renamed variables, explicit object style,</cell><cell>Mangled source with</cell></row><row><cell cols="2">Original JavaScript method</cell><cell cols="2">explicit concatenation, inline conditional</cell><cell>compressed whitespace</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>We augment programs with 11 automated source-to-source compiler transforms. 10 are correct- by-construction and preserve operational semantics.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Natural code Adversarial (N =4) Adversarial (N =16) AUROC AP AUROC AP AUROC AP Edit distance heuristic 69.55?0.81 73.75 31.63?0.82 42.85 12.11?0.54 32.46 Randomly initialized Transformer 72.31?0.79 75.82 22.72?0.20 37.73 3.09?0.28 30.95 + RoBERTa MLM pre-train 74.04?0.77 77.65 25.83?0.21 39.46 4.51?0.33 31.17 + ContraCode pre-train 75.73?0.75 78.02 64.97?0.24 66.23 58.32?0.88 59.66 + ContraCode + RoBERTa MLM 79.39?0.70 81.47 37.81?0.24 51.42 10.09?0.50 32.52</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Type inference accuracy on TypeScript programs.</figDesc><table><row><cell>As ContraCode does not modify model</cell></row><row><cell>architecture, contrastive pre-training improves both</cell></row><row><cell>BiLSTM and Transformer accuracy (1.5% to 2.28%).</cell></row><row><cell>Compared with TypeScript's built-in type inference,</cell></row><row><cell>we improve accuracy by 8.9%.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>(not our best performing model).</figDesc><table><row><cell>Method</cell><cell>Precision Recall</cell><cell>F1</cell></row><row><cell>code2vec</cell><cell cols="2">10.78% 8.24% 9.34%</cell></row><row><cell>code2seq</cell><cell cols="2">12.17% 7.65% 9.39%</cell></row><row><cell cols="3">RoBERTa MLM 15.13% 11.47% 12.45%</cell></row><row><cell>Transformer</cell><cell cols="2">18.11% 15.78% 16.86%</cell></row><row><cell>+ ContraCode</cell><cell cols="2">20.34% 14.96% 17.24%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Results for different settings of code sum-</cell></row><row><cell>marization: supervised training with 81K functions,</cell></row><row><cell>masked language model pre-training, training from</cell></row><row><cell>scratch and contrastive pre-training with fine-tuning.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Compiler data augmentations degrade performance when training supervised models from scratch.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Ablating compiler transformations used during contrastive pre-training. The DeepTyper BiLSTM is pre-trained with constrastive learning for 20K steps, then fine-tuned for type inference. Augmentations are only used during pre-training. Each transformation contributes to accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Contrasting global, sequence-level representations outperforms contrasting local representations. We compare using the terminal (global) hidden states of the DeepTyper BiLSTM and the mean pooled token-level (local) hidden states. Global InfoNCE with terminal hidden state, 20K steps 52.65% 84.60% InfoNCE with terminal hidden state, 10K steps 51.70% 83.03% Local InfoNCE with mean token rep., 10K steps 49.32% 80.03%</figDesc><table><row><cell>Representation Optimization</cell><cell>Acc@1</cell><cell>Acc@5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Training time and decoder depth ablation on the method name prediction task. Longer pre-training significantly improves downstream performance when a shallow, 1 layer decoder is used.</figDesc><table><row><cell>Decoder</cell><cell cols="2">Pre-training (1.8M programs) (81k programs) Supervision</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>Transformer, 1 layer</cell><cell>MoCo, 10k steps</cell><cell>Original set</cell><cell>11.91%</cell><cell>5.96%</cell><cell>7.49%</cell></row><row><cell>Transformer, 1 layer</cell><cell>MoCo, 45k steps</cell><cell>Original set</cell><cell>17.71%</cell><cell cols="2">12.57% 13.79%</cell></row><row><cell cols="2">Transformer, 4 layers MoCo, 45k steps</cell><cell>Original set</cell><cell>18.21%</cell><cell cols="2">13.21% 14.56%</cell></row><row><cell>Top 5 accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1x queue fill rate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">12x queue fill rate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Lisa Dunlap, Jonathan Ho, Koushik Sen, Rishabh Singh, Aravind Srinivas, Daniel Rothchild, and Justin Wong for helpful feedback. In addition to NSF CISE Expeditions Award CCF-1730628, the NSF GRFP under Grant No. DGE-1752814, and ONR PECASE N000141612723, this research is supported by gifts from Amazon Web Services, Ant Financial, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, NVIDIA, Scotiabank, Splunk and VMware.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A transformer-based approach for source code summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.449</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4998" to="5007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The adverse effects of code duplication in machine learning models of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359591.3359735</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software</title>
		<meeting>the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="143" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Typilus: Neural type hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soline</forename><surname>Ducousso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programming Language Design and Implementation (PLDI)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A convolutional attention network for extreme summarization of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
		<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating sequences from structured representations of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaked</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Omer Levy, and Eran Yahav. 2019b. code2vec: Learning distributed representations of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meital</forename><surname>Zilberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Programming Languages</title>
		<meeting>the ACM on Programming Languages</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Evaluating large language models trained on code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen An</surname></persName>
		</author>
		<idno>abs/2107.03374</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A program for identifying duplicated code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenda</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Science and Statistics</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic generation of peephole superoptimizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorav</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
		<idno type="DOI">10.1145/1168857.1168906</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS XII</title>
		<meeting>the 12th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS XII<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="394" to="403" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural code comprehension: A learnable representation of code semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">Shoshana</forename><surname>Jakobovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="page" from="3589" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Defexts: A curated dataset of reproducible real-world bugs for modern jvm languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ghanbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="47" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial robustness for code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">T</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="896" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>abs/2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<title level="m">Xiaocheng Feng, Ming Gong, Linjun Shou</title>
		<meeting><address><addrLine>Bing Qin</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Code-BERT: A pre-trained model for programming and natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.139</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1536" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A public unified bug dataset for Java</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Ferenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zolt?n</forename><surname>T?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gergely</forename><surname>Lad?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>Siket</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tibor</forename><surname>Gyim?thy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering</title>
		<meeting>the 14th International Conference on Predictive Models and Data Analytics in Software Engineering</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Case studies in the use of roc curve analysis for sensor-based estimates in human computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface 2005, GI &apos;05</title>
		<meeting>Graphics Interface 2005, GI &apos;05<address><addrLine>Waterloo, CAN</addrLine></address></meeting>
		<imprint>
			<publisher>Canadian Human-Computer Communications Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00975</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning type inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allamanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>H?naff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="15637" to="15648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Scaling laws for transfer</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Code-SearchNet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiferet</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/1909.09436</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Summarizing source code using a neural attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1195</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Denali: A goal-directed superoptimizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Randall</surname></persName>
		</author>
		<idno type="DOI">10.1145/512529.512566</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design and Implementation, PLDI &apos;02</title>
		<meeting>the ACM SIGPLAN 2002 Conference on Programming Language Design and Implementation, PLDI &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="304" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pre-trained contextual embedding of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gogul</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/2001.00059</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A field study of refactoring challenges and benefits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miryung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiappan</forename><surname>Nagappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering</title>
		<meeting>the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">One tool, many languages: Language-parametric transformation with incremental parametric syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armando</forename><surname>Varot Premtoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solar-Lezama</surname></persName>
		</author>
		<idno type="DOI">10.1145/3276492</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Program. Lang</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2018" />
			<publisher>OOPSLA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du An</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Superoptimizer: A look at the smallest program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henry Massalin</surname></persName>
		</author>
		<idno type="DOI">10.1145/36206.36194</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Architectual Support for Programming Languages and Operating Systems, ASPLOS II</title>
		<meeting>the Second International Conference on Architectual Support for Programming Languages and Operating Systems, ASPLOS II<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="122" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Babel: compiler for writing next generation javascript</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Mckenzie</surname></persName>
		</author>
		<ptr target="https://github.com/babel/babel" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charith</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4505" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Proceedings of a meeting held</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Natural language models for predicting programming comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Movshovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Opttyper: Probabilistic type inference by optimising logical and natural constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Vlassi Pandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Typewriter: Neural type prediction with search-based validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Gousios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Chandra</surname></persName>
		</author>
		<idno>abs/1912.03768</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepbugs: A learning approach to name-based bug detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">OOPSLA</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Evaluation of generalizability of neural program analyzers under semantic-preserving transformations</title>
		<editor>Md. Rafiqul Islam Rabin and Mohammad Amin Alipour</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Terser: Javascript parser, mangler and compressor toolkit for es6+</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F?bio</forename><surname>Santos</surname></persName>
		</author>
		<ptr target="https://github.com/terser/terser" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5628" to="5637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldip</forename><surname>Paliwal</surname></persName>
		</author>
		<idno type="DOI">10.1109/78.650093</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">You autocomplete me: Poisoning vulnerabilities in neural code completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Tromer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th {USENIX} Security Symposium</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>{USENIX} Security 21</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards a big data curated benchmark of inter-project code clones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Svajlenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">F</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iman</forename><surname>Keivanloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanchal</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Mamun</forename><surname>Mia</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSME.2014.77</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution, ICSME &apos;14</title>
		<meeting>the 2014 IEEE International Conference on Software Maintenance and Evolution, ICSME &apos;14<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="476" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cloze procedure&quot;: A new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Quarterly</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">COSET: A benchmark for evaluating neural program embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Christodorescu</surname></persName>
		</author>
		<idno>abs/1905.11445</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Learning blended, precise semantic program embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Lambdanet: Probabilistic type inference using graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maruth</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>Greg Durrett, and Isil Dillig. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep learning code fragments for code clone detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Vendome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denys</forename><surname>Poshyvanyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00393</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Adversarial examples for models of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Yefet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno>abs/1910.07517</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adversarial examples for models of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Yefet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">OOPSLA</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
