<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diffusion Models for Video Prediction and Infilling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>H?ppe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Mehrjou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didrik</forename><surname>Nielsen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Norwegian Computing Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Dittadi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Technical University of Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kth</forename><surname>Stockholm</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diffusion Models for Video Prediction and Infilling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting and anticipating future outcomes or reasoning about missing information in a sequence are critical skills for agents to be able to make intelligent decisions. This requires strong, temporally coherent generative capabilities. Diffusion models have shown remarkable success in several generative tasks, but have not been extensively explored in the video domain. We present Random-Mask Video Diffusion (RaMViD), which extends image diffusion models to videos using 3D convolutions, and introduces a new conditioning technique during training. By varying the mask we condition on, the model is able to perform video prediction, infilling, and upsampling. Due to our simple conditioning scheme, we can utilize the same architecture as used for unconditional training, which allows us to train the model in a conditional and unconditional fashion at the same time. We evaluate the model on two benchmark datasets for video prediction, on which we achieve state-of-the-art results, and one for video generation. * Equal advising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Videos contain rich information about the world, and a vast amount of diverse video data is available. Training models that understand this data can be crucial for developing agents that interact with the surrounding world effectively. In particular, video prediction plays an increasingly important role: Autonomous driving <ref type="bibr" target="#b18">(Hu et al., 2020)</ref>, anticipating events <ref type="bibr" target="#b59">(Zeng et al., 2017)</ref>, planning  and reinforcement learning <ref type="bibr" target="#b14">(Hafner et al., 2019)</ref> are applications which can benefit from increasing performance of prediction models. On the other hand, video infilling-i.e., observing a part of a video and generating missing frames-can be used for example in planning, estimating trajectories, and video processing. In addition, video models can be valuable for downstream tasks such as action recognition <ref type="bibr" target="#b22">(Kong and Fu, 2018)</ref> and pose estimation <ref type="bibr" target="#b35">(Sahin et al., 2020)</ref>. However, there has not been extensive research on video infilling and most research is focusing on generation or prediction.</p><p>Most recent approaches to video prediction are based on variational autoencoders <ref type="bibr" target="#b3">(Babaeizadeh et al., 2021;</ref><ref type="bibr" target="#b39">Saxena, Ba, and Hafner, 2021)</ref> or GANs <ref type="bibr" target="#b8">(Clark, Donahue, and Simonyan, 2019;</ref><ref type="bibr" target="#b25">Luc et al., 2020)</ref>. Diffusion models <ref type="bibr" target="#b0">(Abstreiter et al., 2021;</ref><ref type="bibr" target="#b11">Dockhorn, Vahdat, and Kreis, 2021;</ref><ref type="bibr" target="#b16">Ho, Jain, and Abbeel, 2020;</ref><ref type="bibr" target="#b27">Mittal et al., 2022;</ref><ref type="bibr" target="#b29">Nichol and Dhariwal, 2021;</ref><ref type="bibr" target="#b40">Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b41">Song et al., 2021)</ref> have recently seen tremendous progress on static visual data, even outperforming GANs in image synthesis , but have not yet been extensively studied for videos. Considering their impressive performance on images, it is reasonable to believe that diffusion models may also be useful for tasks in the video domain.</p><p>In this paper, we extend diffusion models to the video domain via several technical contributions. We use 3D convolutions and a new conditioning procedure incorporating randomness. Our model is not only able to predict future frames of a video but also fill in missing frames at arbitrary positions in the sequence (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Therefore, our Random-Mask Video Diffusion (RaMViD) can be used for several video completion tasks. <ref type="bibr">1</ref> We summarize our technical contributions as follows:</p><p>? A novel diffusion-based architecture for video prediction and infilling.</p><p>? Competitive performance with recent approaches across multiple datasets.</p><p>? Introduce a schedule for the random masking.</p><p>The remainder of this paper is organized as follows: In Section 2, we provide the necessary background on diffusion models and video prediction and outline relevant related work. Section 3 describes Random-Mask Video Diffusion (RaMViD). In Section 4, we present and discuss extensive experiments on several benchmark datasets. We finally conclude with a discussion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and related work</head><p>Diffusion models. Diffusion-based models generally refer to the class of machine learning algorithms that consist of gradually transforming a complex distribution into unstructured noise and learning to reverse this process to recover the data generating distribution. They have attracted a great deal of attention after being successfully applied to a diverse range of tasks such as image generation <ref type="bibr" target="#b30">(Niu et al., 2020;</ref><ref type="bibr" target="#b41">Song et al., 2021)</ref>, audio <ref type="bibr" target="#b7">(Chen et al., 2021)</ref>, graph and shape generation <ref type="bibr" target="#b5">(Cai et al., 2020)</ref>. The essence of these models is two stochastic (diffusion) processes implemented by Stochastic Differential Equations (SDEs), a forward and a backward one. We explain the formulation in the abstract domain here and specialize it later according to the application of this work. Let x 0 ? R d be a sample from the empirical data distribution, i.e., x 0 ? p data (x 0 ) and d be the data dimension. The forward diffusion process takes x 0 as the starting point and creates the random trajectory x [0,T ] from t = 0 to the final time t = T . The forward process is designed such that p(x T | x 0 ) has a simple unstructured distribution. One example of such SDEs is</p><formula xml:id="formula_0">dx t = f (x t , t)dt + g(t)dw := d[? 2 (t)] dt dw,<label>(1)</label></formula><p>where w is the Brownian motion. A desirable property of this process is the fact that the conditional distribution p(x t | x 0 ) takes a simple analytical form:</p><formula xml:id="formula_1">p(x t | x 0 ) = N x t ; x 0 , ? 2 (t) ? ? 2 (0) I .<label>(2)</label></formula><p>Upon learning the gradient of p(x t ) for each t, one can reverse the above process and obtain the complex data distribution from pure noise as</p><formula xml:id="formula_2">dx t = [f (x t , t) ? g 2 (t)? x log p(x t )]dt + g(t)dw ,<label>(3)</label></formula><p>where w is a Brownian motion independent of the one in the forward direction. Hence, generating samples from the data distribution boils down to learning ? x log p(x).</p><p>The original score matching objective <ref type="bibr" target="#b19">(Hyv?rinen and Dayan, 2005)</ref>:</p><formula xml:id="formula_3">E xt s ? (x t , t) ? ? xt log p(x t ) 2 2<label>(4)</label></formula><p>is the most intuitive way to learn the score function, but is unfortunately intractable. Denoising Score Matching (DSM) provides a tractable alternative objective function:</p><formula xml:id="formula_4">J DSM t (?) = E x0 E xt|x0 s ? (x t , t) ? ? xt log p(x t | x 0 ) 2 2<label>(5)</label></formula><p>whose equivalence with the original score matching objective was shown by <ref type="bibr" target="#b48">Vincent (2011)</ref> and used to train energy models in <ref type="bibr" target="#b38">Saremi et al. (2018)</ref>. Similar to many recent works, we use the DSM formulation of score matching in this work to learn the score function.</p><p>Video prediction and infilling. Research in video prediction has gotten more attention in the previous years, as the ability to predict videos can be used for several downstream tasks <ref type="bibr" target="#b31">(Oprea et al., 2020)</ref>. Video prediction can be modeled in a deterministic or stochastic form. Deterministic modeling <ref type="bibr" target="#b43">(Sun et al., 2019;</ref><ref type="bibr" target="#b45">Terwilliger, Brazil, and Liu, 2019;</ref><ref type="bibr" target="#b51">Vondrick and Torralba, 2017;</ref><ref type="bibr" target="#b52">Walker, Gupta, and Hebert, 2015)</ref> tries to predict the most likely future, but this often leads to averaging the future states <ref type="bibr" target="#b24">(Li et al., 2019)</ref>. Due to the stochastic nature of the future, generative models have lately shown to be more successful in capturing the underlying dynamics. For this approach, variational models are often used by modeling the stochastic content in a latent variable <ref type="bibr" target="#b2">(Babaeizadeh et al., 2018;</ref><ref type="bibr" target="#b9">Denton and Fergus, 2018;</ref><ref type="bibr" target="#b39">Saxena, Ba, and Hafner, 2021;</ref><ref type="bibr" target="#b54">Wu et al., 2021a)</ref>. However, this often leads to blurry prediction by underfitting and <ref type="bibr" target="#b3">Babaeizadeh et al. (2021)</ref> have overcome this problem by architectural novelties. The blurry prediction is a less serious problem in GANs and promising results have been achieved especially on large datasets <ref type="bibr" target="#b8">(Clark, Donahue, and Simonyan, 2019;</ref><ref type="bibr" target="#b25">Luc et al., 2020)</ref>. On the other hand, the body of research on video infilling is significantly more scarce, with most works in this area focusing on frame interpolation <ref type="bibr" target="#b20">(Jiang et al., 2018)</ref>. However, <ref type="bibr" target="#b56">Xu et al. (2018)</ref> have shown interesting results in infilling, by modeling the video as a stochastic generation process.</p><p>Concurrent work. <ref type="bibr" target="#b58">Yang, Srivastava, and Mandt (2022)</ref> is the only work so far that has used diffusion models for autoregressive video prediction, by modeling residuals for a predicted frame. However, since their evaluation procedure and datasets are different, a comparison with their work is not possible. A few concurrent works have recently considered diffusion models for video generation. <ref type="bibr" target="#b17">Ho et al. (2022)</ref> focus on unconditional video generation, <ref type="bibr" target="#b15">Harvey et al. (2022)</ref> use diffusion models to predict long videos, <ref type="bibr" target="#b49">and Voleti, Jolicoeur-Martineau, and Pal (2022)</ref>, the most similar to our work, also consider video prediction and infilling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Random-Mask Video Diffusion</head><p>Our method, Random-Mask Video Diffusion (RaMViD), consists of two main features. First, the way we introduce conditional information to the network is different from what has been used so far. Second, by randomizing the mask, we can directly use the same approach for video prediction and video completion (infilling). In the following, we detail each of these aspects of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conditional training</head><p>Let x 0 ? R L,W,H,C be a video with length L. We partition the video x 0 into two parts, the unknown frames x U 0 ? R L?k,W,H,C and the conditioning frames  <ref type="figure">Figure 2</ref>: An unconditionally trained model is used to predict 15 frames given one frame. Even with re-sampling we can see, that objects in the background are not harmonized between the predicted and conditioned frames.</p><formula xml:id="formula_5">x C 0 ? R k,W,</formula><p>indices such that U ? C = ? and U ? C = {0, 1, . . . , L ? 1}. We write x 0 = x U 0 ? x C 0 with the following definition for the ? operator:</p><formula xml:id="formula_6">(a U ? b C ) i := a i if i ? U b i if i ? C (6)</formula><p>where the superscript i indicates tensor indexing and in our case corresponds to selecting a frame from a video. Here, the subscript t indicates the diffusion step, with t = 0 corresponding to the data and t = T to the prior Gaussian distribution.</p><p>If we use an unconditionally trained model, we find that the predicted unknown frames x U 0 do not harmonize well with the conditioning frames x C 0 <ref type="figure">(Fig. 2)</ref>. One solution for this would be re-sampling, as proposed by <ref type="bibr" target="#b26">Lugmayr et al. (2022)</ref>. In re-sampling, we take one step in the learned reversed diffusion (denoising) process and then go back by taking a step in the forward diffusion process (i.e., adding noise again). This is done several times for each diffusion step, to make sure the model harmonizes between the predicted and conditioning frames. However, this becomes computationally too expensive for videos, especially when using very few conditioning frames, as the number of re-sampling steps need to be increased. To mitigate this issue, we propose to train the model conditionally with randomized masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional diffusion models usually optimize</head><formula xml:id="formula_7">E x0 E xt|x0 s ? (x t , x C 0 , t) ? ? xt log p(x t | x 0 ) 2 2<label>(7)</label></formula><p>where x C 0 is typically given as a separate input through an additional layer <ref type="bibr" target="#b7">(Chen et al., 2021)</ref> or it is expanded to the dimension of x t (e.g., via padding) and concatenated with the input <ref type="bibr" target="#b4">(Batzolis et al., 2021;</ref><ref type="bibr" target="#b33">Saharia et al., 2021a;</ref><ref type="bibr" target="#b59">b)</ref>. We, on the other hand, feed the entire sequence to the network s ? but only add noise to the unmasked frames:</p><formula xml:id="formula_8">x U t ? N x U 0 , ? 2 (t) ? ? 2 (0) I .</formula><p>The input to the network is then a video where some frames are noisy and some are clean: <ref type="figure" target="#fig_2">Fig. 3</ref>). The loss is computed only with respect to x U t :</p><formula xml:id="formula_9">x t = x U t ? x C 0 (see</formula><formula xml:id="formula_10">J RaMViD t (?) = E x0 E x U t |x0 s ? (x t , t)(x U t ) ? ? x U t log p(x U t | x 0 ) 2 2 .<label>(8)</label></formula><p>Note that the score function ? x U t log p(x U t | x 0 ) has the same dimension as x U t , whereas in Eq. (7) it had the dimension of the entire video x t . The reversed diffusion process then becomes:</p><formula xml:id="formula_11">dx U t = [f (x U t , t) ? g 2 (t)? x U t log p(x U t |x C 0 )]dt + g(t)dw<label>(9)</label></formula><p>Similarly, <ref type="bibr" target="#b44">Tashiro et al. (2021)</ref> compute the loss only on the unknown input. However, they also use concatenation and zero-padding to bring x C t and x t to the same dimension. For a more detailed schematic, see Appendix A. In our implementation, we used a discrete diffusion process with t ? {0, 1, . . . , T ? 1, T }. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Randomization</head><p>As previously mentioned, the proposed model is able to perform several tasks. We achieve this by sampling C at random. At each training step, we first choose the number of conditioning frames |C| = k ? {1, . . . , K}, where K is a chosen hyperparameter. Then we define C by selecting k random indices from {0, . . . , L ? 1}, and we refrain from applying the diffusion process to the corresponding frames. Since the videos now consist of original and noisy frames in varying positions, the model has to learn to distinguish between them in order to use the frames x C 0 as information for the reversed diffusion process. After training, we can use RaMViD by fixing C to the set of indices of the known frames (C can be any arbitrary subset of {1, . . . , L ? 1}) and generating the unknown frames (those with indices in U).</p><p>Our approach allows us to use the exact same architecture of unconditionally trained models, thus enabling mixed training, where we train the model conditionally and unconditionally at the same time. We set C = ? (i.e., the model does not have any conditional information x C t ) with probability p U , which is a fixed hyperparameter. If C = ?, our objective in Eq. (8) becomes the same as the objective in Eq. (5) used for unconditional training. The pseudocode for RaMViD can be found in Algorithm 1.</p><p>Algorithm 1 RaMViD.</p><p>Initialize model ? s ? T = Number of diffusion steps K = Max number of frames to condition on L = Length of the video while not converged do</p><formula xml:id="formula_12">x 0 ? p data (x 0 ) sample data point t ? Uniform({0, . . . , T }) b ? Bernoulli(p U ) if b then C = ? else k ? Uniform({1, . . . , K}) C ? Uniform ({S ? {0, . . . , L ? 1} : |S| = k}) end if U = {0, . . . , L ? 1} \ C x U t ? N x U t ; x U 0 , ? 2 (t) ? ? 2 (0) I x t = x U t ? x C 0</formula><p>Take a gradient step on:</p><formula xml:id="formula_13">? ? E x0 E x U t |x0 s ? (x t , t)(x U t ) ? ? x U t log p(x U t | x 0 ) 2 2 end while</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Implementation details. Our implementation relies on the official code of <ref type="bibr" target="#b29">Nichol and Dhariwal (2021)</ref>, adapted to video data by using 3D convolutions. 2 Even though most previous work uses the cosine noise schedule, we found that the linear noise schedule works better when training the model conditionally. Therefore, we use a linear diffusion schedule for our experiments. For the architecture, we also use the same as proposed by <ref type="bibr" target="#b29">Nichol and Dhariwal (2021)</ref>: a U-Net with self-attention at the resolutions 16 and 8. We do not encode the time dimension. We use two ResNet blocks per resolution for the BAIR dataset, and three blocks for Kinetics-600 and UCF-101. We set the learning rate for all our experiments to 2e-5, use a batch size of 32 for BAIR and 64 for Kinetics-600 and UCF-101, and fix T = 1000. We found, especially on the more diverse datasets like Kinetics-600 and UCF-101, that larger batch sizes produce better results. Therefore, to increase the batch size, we use gradient accumulation by computing the gradients for micro-batches of size 2 and accumulate for several steps before doing back-propagation.</p><p>Datasets and evaluations. To compare our model to prior work, we train it on the BAIR robot pushing dataset <ref type="bibr" target="#b12">(Ebert et al., 2017)</ref>. The dataset consists of short videos, with 64 ? 64 resolution, of a robot arm manipulating different objects. For evaluation, we use the same setting as <ref type="bibr" target="#b32">Rakhimov et al. (2020)</ref>, which is to predict the next 15 frames given one observed frame. We train on videos of length 20 and choose K = 4.</p><p>Additionally, we evaluate our model on the Kinetics-600 dataset <ref type="bibr" target="#b6">(Carreira et al., 2018)</ref>, which consists of roughly 500,000 10-second YouTube clips, also at 64 ? 64 resolution, from 600 classes. The size and the diversity of this dataset make it a perfect task to investigate if the model captures the underlying real-world dynamics. For downloading and preprocessing we use the dataset's public repository. 3 On Kinetics-600, we compare our model to concurrent work by predicting 11 frames when conditioned on 5 frames <ref type="bibr" target="#b25">(Luc et al., 2020)</ref>. We additionally perform several ablation studies on video completion. We train on 16 frames and choose again K = 4.</p><p>To quantitatively evaluate the unconditional generation performance when using p U &gt; 0, we also train on UCF-101 <ref type="bibr" target="#b42">(Soomro, Zamir, and Shah, 2012)</ref>, a common benchmark for unconditional video generation. It consists of 13,320 videos from 101 human action classes. We also rescale this dataset to 64 ? 64 and train with K = 4.</p><p>To quantitatively evaluate prediction, we use the Fr?chet Video Distance (FVD) <ref type="bibr" target="#b47">(Unterthiner et al., 2018)</ref>, 4 which captures semantic similarity and temporal coherence between videos by comparing statistics in the latent space of a Inflated 3D ConvNet (I3D) trained on Kinetics-400. To evaluate unconditional generation, we use the Inception Score (IS) <ref type="bibr" target="#b37">(Salimans et al., 2016)</ref> to measure the quality and diversity of the generated videos. As we have to adapt the score to videos, we use the public repository from <ref type="bibr" target="#b36">Saito et al. (2020)</ref>. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BAIR</head><p>We train four models on the BAIR dataset with p U ? {0, 0.25, 0.5, 0.75} respectively. The models are trained for 250,000 iterations with a batch size of 32 on 8 GPUs. First, we test our method with the typical evaluation protocol for BAIR (predicting 15 frames, given one conditional frame). With all values of p U , we can achieve state-of-the-art performance, as shown in <ref type="table" target="#tab_0">Table 1</ref>. By using p U &gt; 0, we can even increase the performance of our method. However, it seems that there is a tipping point after which the increasing unconditional rate hurts the prediction performance of the model. Interestingly, we find that also the model trained with p U = 0.75 overcomes the harmonization problem described in Section 3.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method FVD (?)</head><p>Latent Video Transformer <ref type="bibr" target="#b32">(Rakhimov et al., 2020)</ref> 125.8 SAVP <ref type="bibr" target="#b23">(Lee et al., 2018)</ref> 116.4 DVD-GAN-FP <ref type="bibr" target="#b8">(Clark, Donahue, and Simonyan, 2019)</ref> 109.8 TrIVD-GAN-FP <ref type="bibr" target="#b25">(Luc et al., 2020)</ref> 103.3 VideoGPT <ref type="bibr" target="#b57">(Yan et al., 2021)</ref> 103.3 Video Transfomer <ref type="bibr" target="#b53">(Weissenborn, T?ckstr?m, and Uszkoreit, 2020)</ref> 94.0 FitVid <ref type="bibr" target="#b3">(Babaeizadeh et al., 2021)</ref> 93.6 MCVD <ref type="bibr" target="#b49">(Voleti, Jolicoeur-Martineau, and Pal, 2022)</ref> 89.5 N?WA <ref type="bibr" target="#b55">(Wu et al., 2021b)</ref> 86.9</p><p>RaMViD (p U = 0) 86.41 RaMViD (p U = 0.25) 84.20 RaMViD (p U = 0.5) 85.03 RaMViD (p U = 0.75) 86.05</p><p>Since we train with randomized masking, we can also perform video infilling with the same models, without retraining. We condition on the first and last frame (i.e., set C = {0, 15} for sampling) and compute the FVD of the 14 generated frames. Again we find that the performance is very similar for different values of p U (see <ref type="table" target="#tab_1">Table 2</ref>), however, similarly to <ref type="table" target="#tab_0">Table 1</ref>, we observe the best results when using p U = 0.25. Interestingly, RaMViD is also able to perform unconditional generation on BAIR for all considered values of p U , as shown in Appendix B.1.</p><p>So far, we have shown that our method works very well for prediction and infilling. However, since the BAIR dataset is arguably rather simple and not very diverse, we will now evaluate RaMViD on the significantly more complex Kinetics-600 dataset.   <ref type="bibr" target="#b28">Moing, Ponce, and Schmid (2021)</ref> after inquiring about the evaluation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method FVD (?)</head><p>Video Transfomer <ref type="bibr" target="#b53">(Weissenborn, T?ckstr?m, and Uszkoreit, 2020)</ref> 170 ? 5 DVD-GAN-FP <ref type="bibr" target="#b8">(Clark, Donahue, and Simonyan, 2019)</ref> 69 ? 1 CCVS <ref type="bibr" target="#b28">(Moing, Ponce, and Schmid, 2021)</ref> 55 ? 1 TrIVD-GAN-FP <ref type="bibr" target="#b25">(Luc et al., 2020)</ref> 26 ? 1</p><formula xml:id="formula_14">RaMViD (p U = 0) 18.69 RaMViD (p U = 0.25) 16.46 RaMViD (p U = 0.5) 17.61 RaMViD (p U = 0.75)</formula><p>27.64</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Kinetics-600</head><p>For the Kinetics-600 dataset, we increase the batch size to 64 and train for 500,000 iterations on 8 GPUs. First, we evaluate the model on prediction with the setting described in Section 4.1 (predict 11 frames given 5 frames). When comparing our models to concurrent work, we find that RaMViD achieves state-of-the-art results by a significant margin (see <ref type="table" target="#tab_2">Table 3</ref>). In <ref type="figure" target="#fig_3">Fig. 4</ref>, we can see that the model produces temporally coherent outputs and is able to model details, especially in the background, such as clouds and patterns in the water. Nevertheless, it struggles with fast movements: objects moving quickly often get deformed, as can be observed in Appendix B.2. Similar to what we have seen in <ref type="table" target="#tab_0">Table 1</ref>, having an unconditional rate p U &gt; 0 increases the performance up to a tipping point. However, differently from the model trained on BAIR, the FVD score now drops significantly with p U = 0.75. We conjecture that this drop in performance is due to the complexity of the data distribution. In BAIR, the conditional and unconditional distributions are rather similar, while this is not true for Kinetics-600.</p><p>We also evaluate RaMViD on two video completion tasks on Kinetics-600. The first task is to fill in a video given the two first and last frames (i.e., C <ref type="figure" target="#fig_0">= {0, 1, 14, 15})</ref>: the challenge here is to harmonize the observed movement at the beginning with the movement observed at the end. In the second task, the conditioning frames are distributed evenly over the sequence (i.e., C = {0, 5, 10, 15}), hence the model has to infer the movement from the static frames and harmonize them into one realistic video. RaMViD excels on both tasks, as shown quantitatively in <ref type="table" target="#tab_3">Table 4</ref> and qualitatively in Figs. 11 and 12. Especially when setting C = {0, 5, 10, 15} RaMViD is able to fill the missing frames with very high quality and coherence. This setting can be easily applied to upsampling by training a model on high-FPS videos and then sampling a sequence conditioned on a low-FPS video.</p><p>We find that only RaMViD (p U = 0.5) and RaMViD (p U = 0.75) can generate unconditional videos on Kinetics-600. To quantify RaMViD's unconditional generation, we will evaluate these models on the UCF-101 dataset and compare it to other work. RaMViD (p U = 0) 10.68 6.28 RaMViD (p U = 0.25) 10.85 4.91 RaMViD (p U = 0.5) 10.86 5.90 RaMViD (p U = 0.75) 17.33 7.29 <ref type="table">Table 5</ref>: Generative performance of RaMViD on UCF-101. Note that the methods VideoGPT and DVD-GAN in <ref type="table">Table 5</ref> are trained with 128 ? 128 resolution, which gives them a slight advantage, as the IS score is computed with 112 ? 112 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method IS (?)</head><p>VGAN <ref type="bibr" target="#b50">(Vondrick, Pirsiavash, and Torralba, 2016)</ref> 8.31 ? 0.09 MoCoGAN <ref type="bibr" target="#b46">(Tulyakov et al., 2018)</ref> 12.42 ? 0.03 TGAN-F <ref type="bibr" target="#b21">(Kahembwe and Ramamoorthy, 2020)</ref> 13.62 progressive VGAN <ref type="bibr" target="#b1">(Acharya et al., 2018)</ref> 14.56 ? 0.05 VideoGPT <ref type="bibr" target="#b57">(Yan et al., 2021)</ref> 24.69 ? 0.3 TGANv2 <ref type="bibr" target="#b36">(Saito et al., 2020)</ref> 26.60 ? 0.47 DVD-GAN <ref type="bibr" target="#b8">(Clark, Donahue, and Simonyan, 2019)</ref> 32.97 ? 1.7</p><p>Video Diffusion <ref type="bibr" target="#b17">(Ho et al., 2022)</ref> 57 ? 0.62 RaMViD (p U = 0.5) 20.84 ? 0.08 RaMViD (p U = 0.75) 21.71 ? 0.21</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">UCF-101</head><p>We train RaMViD on UCF-101 with the same setting as used for Kinetics-600 but for 450,000 iterations. <ref type="table">Table 5</ref> shows that our model achieves competitive performance on unconditional video generation, although it does not reach state-of-the-art. The trained models can successfully generate scenes with a static background and a human performing an action in the foreground, consistent with the training dataset (see <ref type="figure" target="#fig_0">Figs. 5 and 13</ref>). However, the actions are not always coherent and moving objects can deform over time. Note that UCF-101 is a very small dataset given its complexity. Therefore we do observe some overfitting. Since for each action we only have around 25 different settings, our model does not learn to combine those but generates very similar videos to the training set. Due to the characteristics of this dataset we think with more extensive hyperparameter tuning, one can achieve better results with RaMViD in unconditional generation. But our focus does not lie on this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Autoregressive sampling</head><p>While we train our models only on 16 (Kinetics-600) or 20 (BAIR) frames, it is still possible to sample longer sequences autoregressively. By conditioning on the latest sampled frames, one can sample the next sequence and therefore generate arbitrarily long videos. In <ref type="figure" target="#fig_5">Fig. 6</ref>, we show examples of this autoregressive sampling with RaMViD (p U = 0.25) trained on Kinetics-600. However, we found that this is rather challenging because, at each autoregressive step, the quality of the generated sequence slightly deteriorates. This amplifies over time, often resulting in poor quality after about 30 frames.  <ref type="figure">Figure 5</ref>: Unconditional generation on the UCF-101 dataset. The first generation does not have much movement and is generated very realistically. In the second video, we the background is generated properly, but we see that the fast-moving people are unrealistically deformed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have shown that diffusion models, which have been demonstrated to be remarkably powerful for image generation, can be extended to videos and used for several video completion tasks. The way we introduce conditioning information is novel, simple, and does not require any major modification to the architecture of existing diffusion models, but it is nonetheless surprisingly effective. Although the proposed method targets conditional video generation, we also introduce an alternative masking schedule in an attempt to improve the unconditional generation performance without sacrificing performance on conditional generation tasks.</p><p>Since we have observed varying performance in different tasks using different masking schemes, an interesting direction for future research is to investigate which masking schedules are more suitable for each task. It would also be interesting to explore in future work whether our conditioning technique is also effective for completion on other data domains. Finally, the focus of this work has been on the diffusion-based algorithm for videos rather than on optimizing the quality of each frame. It has been shown in concurrent works that including super-resolution modules helps create high-resolution videos. Adding a super-resolution module to RaMViD would be a relevant direction for future work. As mentioned, we use the linear noise schedule and the score-based ("simple") objective for all experiments. All models are trained with 1000 diffusion steps, for sampling we used 750 steps on BAIR, and 500 on Kinetics-600 and UCF-101.</p><p>As mentioned in Section 4, we use the code base from Nichol and Dhariwal (2021) (MIT license) and their proposed architecture, except that we use 3 ? 3 ? 3 convolution kernels. In the encoder, we downsample only the spatial dimensions down to 8 ? 8 in three steps. We use 128 channels for the first block and increase it by 128 for each downsampling step. As mentioned in Section 4, we use multi-head self-attention at the resolutions 16 and 8, each with 4 attention heads. For sampling, we found it to be more beneficial to sample from the exponential moving average (EMA) of the weights . We set the EMA rate to 0.9999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Additional results</head><p>B.1 Results on BAIR <ref type="figure" target="#fig_6">Fig. 8</ref> shows that all of the models are also able to do unconditional video generation (even RaMViD (p U = 0), we assume that this is due to the low diversity of the dataset). Qualitatively, we can see in <ref type="figure" target="#fig_6">Fig. 8</ref> that videos generated by models with higher p U are better in generating details. While all models can generate the moving robot arm, only the models with p U ? 0.25 can properly generate the different objects in the box. However, we have no quantitative results on unconditional generation on BAIR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Results on Kinetics-600</head><p>Kinetics-600 in practice appears to be the most difficult dataset among those considered here. While our results are state-of-the-art (see <ref type="table" target="#tab_2">Table 3</ref>), we do observe failure cases. One of the most common failure cases is fast movement. In that case we often see a deformation of the moving object (see <ref type="figure">Fig. 9</ref>).</p><p>Fast camera movement can also be a problem for infilling. If an object is placed very different between the first and last frames, the model does not generate a harmonized movement but makes the object disappear in the first and appear in the last frames <ref type="figure" target="#fig_0">Fig. 10</ref>. BAIR robot pushing. The BAIR robot pushing dataset can be used under an MIT license. We use the low resolution dataset (64 ? 64). Since the data is already in the correct size, no prepossessing is necessary. For evaluation we predict one sequence for each of the 256 test videos and compare the FVD to the ground truth. To get a proper evaluation score, we do this 100 times and the final FVD score is the average over all 100 runs. We train on a sequence length of 20.</p><p>Kinetics-600. The Kinetics-600 dataset has a Creative Commons Attribution 4.0 International License. The videos have different resolutions, which is why we reshape and center crop them to a 64 ? 64 resolution. For evaluation we take 50,000 videos from the test set and predict a sequence for each of the videos. We then compute the statistics for the ground truth and the predicted videos to obtain the FVD score. We train on a sequence length of 16.</p><p>UCF-101. We could not find a license for the UCF-101 dataset. The original frames have a resolution of 160 ? 120, therefore we resize and center crop the videos to a 64 ? 64 resolution. We train on the entire dataset of 13,320 videos. To evaluate the generative performance, we sample 10000 videos unconditionally and compute the Inception Score (IS). 6 This is repeated three times.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The first two and last two frames of a video are given and our model does fill in the missing frames very accurate and with much detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>H,C , where U and C are sets of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Example input of the network with C = {0, 8}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Prediction of 11 frames given the first 5 frames on Kinetics-600 with RaMViD (pU = 0.25).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Autoregressive prediction of 45 frames conditioned on 5 frames with RaMViD (pU = 0.25) trained on Kinetics-600.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Unconditional generation on the BAIR dataset sampled from RaMViD for pU = 0 (first row) until pU = 0.75 (last row). Due to the low complexity of the dataset, we can generate reasonable unconditional videos even with pU = 0. However, the quality of details increases with increasing pU .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Prediction of 11 frames given the first 5 frames on Kinetics-600 when conditioning on fast-moving objects. We can see that the background does get preserved well, while the object itself gets unrealistically deformed. Infilling on Kinetics. The conditioned frames are C = {0, 1, 14, 15}. The people in frames 0 and 1 are placed quite differently than in 14 and 15. The model is not able to generate the necessary camera movement and does simply interpolate between the frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Video infilling (C = {0, 1, 14, 15}) on Kinetics-600 with RaMViD (pU = 0.25). Video completion (C = {0, 5, 10, 15}) on Kinetics-600 with RaMViD (pU = 0.25).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Qualitative results of unconditional generation on UCF-101. Scenes with less movement are generated well, but are often close to the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Prediction performance on BAIR. The values are taken from Babaeizadeh et al. (2021) after inquiring about the evaluation procedure.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Infilling performance on BAIR.</figDesc><table><row><cell>Method</cell><cell>FVD (?)</cell></row><row><cell>RaMViD (p U = 0)</cell><cell>85.68</cell></row><row><cell>RaMViD (p U = 0.25)</cell><cell>85.02</cell></row><row><cell>RaMViD (p U = 0.5)</cell><cell>87.04</cell></row><row><cell>RaMViD (p U = 0.75)</cell><cell>87.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Prediction performance on Kinetics-600. Values are taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance of RaMViD on Kinetics-600, when conditioning on different frames. Method C = {0, 1, 14, 15} C = {0, 5, 10, 15}</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">High resolution videos are provided at https://sites.google.com/view/video-diffusion-prediction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/openai/improved-diffusion 3 https://github.com/cvdfoundation/kinetics-dataset 4 https://github.com/google-research/google-research/tree/master/frechet_video_distance 5 https://github.com/pfnet-research/tgan2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/pfnet-research/tgan2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project was enabled by using the Berzelius cluster at the Swedish National Supercomputer Center (NSC).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A. Implementation details <ref type="figure">Fig. 7</ref> presents a sketch of RaMViD's architecture. Thanks to the way we introduce conditioning frames, the architecture does not need to be different from the one in unconditional models. <ref type="figure">Figure 7</ref>: Sketch of our method. In the last step we only compute the loss with respect to the frames that were corrupted with noise. The number of channels c is 128, and l is the video length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC layer SILU FC layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Compute</head><p>Each model is trained on 8 NVIDIA A100 GPUs with 40 GB of memory. The models on BAIR are trained with a batch size of 32 and a micro-batch size of 16 for 250k iterations ( 3 days). All other models on Kinetics-600 and UCF-101 are trained with a batch size of 64 and micro-batch size of 16. The models are trained for 500k iterations on Kinetics-600 ( 10 days) and for 450k iterations on UCF-101 ( 9 days).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Datasets</head><p>The videos in all datasets have more frames than we train on. Therefore we choose random subsequences of the desired length during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Concurrent work</head><p>As mentioned in Section 2, three concurrent works on diffusion models for videos were recently made public. Only <ref type="bibr" target="#b17">Ho et al. (2022)</ref> and <ref type="bibr" target="#b49">Voleti, Jolicoeur-Martineau, and Pal (2022)</ref> consider similar tasks as we do. <ref type="bibr" target="#b17">Ho et al. (2022)</ref> appears to outperform RaMViD on unconditional video generation on UCF-101, which is not surprising, as we train with the mixed method and therefore the models are mostly trained for conditional generation. Voleti, Jolicoeur-Martineau, and Pal (2022) evaluate their method on BAIR with the same procedure we used, and the results reported in their publication suggest that RaMViD outperforms their proposed method.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Diffusion-Based Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korbinian</forename><surname>Abstreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Mehrjou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14257</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards High Resolution Video Generation with Progressive Growing of Sliced Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danda</forename><surname>Pani Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1810.02419</idno>
		<idno>doi: 10.4855 0/ARXIV.1810.02419</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic Variational Video Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">FitVid: Overfitting in pixel-level video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taghi</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13195</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Conditional Image Generation with Score-Based Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Batzolis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Stanczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carola-Bibiane</forename><surname>Sch?nlieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Etmann</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2111.13606</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning Gradient Fields for Shape Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruojin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06520</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Short Note about Kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">WaveGrad: Estimating Gradients for Waveform Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficient Video Generation on Complex Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>CoRR abs/1907.06571</idno>
		<ptr target="http://arxiv.org/abs/1907.06571" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic Video Generation with a Learned Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1174" to="1183" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research. PMLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diffusion Models Beat GANs on Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Score-Based Generative Modeling with Critically-Damped Langevin Diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07068</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-Supervised Visual Planning with Temporal Skip Connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Annual Conference on Robot Learning</title>
		<editor>Sergey Levine, Vincent Vanhoucke, and Ken Goldberg</editor>
		<meeting>the 1st Annual Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="344" to="356" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research. PMLR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep visual foresight for planning robot motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRA.2017.7989324</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2786" to="2793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Latent Dynamics for Planning from Pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2555" to="2565" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research. PMLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Flexible Diffusion Modeling of Long Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Naderiparizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Weilbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2205.11495</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2204.03458</idno>
		<title level="m">Video Diffusion Models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Probabilistic Future Prediction for Video Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergal</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Gurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2003.06409</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Journal of Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Research 6.4.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lower dimensional kernels for video discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subramanian</forename><surname>Ramamoorthy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2020.09.016</idno>
		<idno>issn: 0893-6080</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2020.09.016" />
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="506" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Human Action Recognition and Prediction: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1806.11230</idno>
		<idno>doi: 10.48550 /ARXIV.1806.11230</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stochastic Adversarial Video Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1804.01523</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic Video Generation with Disentangled Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maomao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuobin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="224" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Transformation-based Adversarial Video Prediction on Large-Scale Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albin</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2003.04035</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">RePaint: Inpainting using Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2201.09865</idno>
		<idno>doi: 10.48550 /ARXIV.2201.09865</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From Points to Functions: Infinite-dimensional Representations in Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Mehrjou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Deep Generative Models for Highly Structured Data</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CCVS: Context-aware Controllable Video Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Moing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schmid ; Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Wortman</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research. PMLR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Permutation Invariant Graph Generation via Score-Based Generative Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00638</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A review on deep learning techniques for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Sergiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Martinez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Alejandro</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Castro-Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonis</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Latent Video Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Rakhimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Volkhonskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2006.10704</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2111.05826</idno>
		<idno>doi: 10.48550 /ARXIV.2111.05826</idno>
		<title level="m">Palette: Image-to-Image Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Image Super-Resolution via Iterative Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2104.07636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A Review on Object Pose Recovery: from 3D Bounding Box Detectors to Full 6D Pose Estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juil</forename><surname>Sock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2001.10609</idno>
		<idno>doi: 10.48550 /ARXIV.2001.10609</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-020-01333-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="2586" to="2606" />
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep energy estimator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08306</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Clockwork Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Wortman</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jascha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-Based Generative Modeling through Stochastic Differential Equations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Predicting Future Instance Segmentation with Contextual Pyramid ConvLSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Fang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3350949</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia. MM &apos;19</title>
		<meeting>the 27th ACM International Conference on Multimedia. MM &apos;19<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2043" to="2051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Tashiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano Ermon ;</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recurrent Flow-Guided Semantic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Terwilliger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2019.00186</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1703" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing Motion and Content for Video Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sjoerd</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>CoRR abs/1812.01717</idno>
		<title level="m">Towards Accurate Generative Models of Video: A New Metric &amp; Challenges</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A Connection Between Score Matching and Denoising Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="DOI">10.1162/NECO_a_00142</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Computation 23</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2205.09853</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generating Videos with Scene Dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generating the Future with Adversarial Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.319</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2992" to="3000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dense Optical Flow Prediction from a Static Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.281</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2443" to="2451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scaling Autoregressive Video Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR46437.2021.00235</idno>
		<idno>doi: 10.1109 /CVPR46437.2021.00235</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2318" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">N?WA: Visual Synthesis Pre-training for Neural visUal World creAtion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2111.12417</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Stochastic Dynamics for Video Infilling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1809.00263</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">VideoGPT: Video Generation using VQ-VAE and Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2104.10157</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Diffusion Probabilistic Modeling for Video Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2203.09481</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visual Forecasting by Imitating Dynamics in Natural Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo-</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.326</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
