<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADAPTIVE GRAPH DIFFUSION NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-05">September 5, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
							<email>chuxiongsun@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Big Data</orgName>
								<orgName type="institution" key="instit2">AI Division China Telecom Research Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
							<email>hujie1@chinatelecom.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Big Data and AI Division</orgName>
								<orgName type="department" key="dep2">Telecom Research Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Gu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Big Data and AI Division</orgName>
								<orgName type="department" key="dep2">Telecom Research Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Chen</surname></persName>
							<email>jpchen@bupt.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">School of Software Engineering Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Big Data and AI Division</orgName>
								<orgName type="department" key="dep2">Telecom Research Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADAPTIVE GRAPH DIFFUSION NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-05">September 5, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have received much attention in the graph deep learning domain. However, recent research empirically and theoretically shows that deep GNNs suffer from overfitting and over-smoothing problems. The usual solutions either cannot solve extensive runtime of deep GNNs or restrict graph convolution in the same feature space. We propose the Adaptive Graph Diffusion Networks (AGDNs) which perform multi-layer generalized graph diffusion in different feature spaces with moderate complexity and runtime. Standard graph diffusion methods combine large and dense powers of the transition matrix with predefined weighting coefficients. Instead, AGDNs combine smaller multi-hop node representations with learnable and generalized weighting coefficients. We propose two scalable mechanisms of weighting coefficients to capture multi-hop information: Hop-wise Attention (HA) and Hop-wise Convolution (HC). We evaluate AGDNs on diverse, challenging Open Graph Benchmark (OGB) datasets with semi-supervised node classification and link prediction tasks. Until the date of submission (Aug 26, 2022), AGDNs achieve top-1 performance on the ogbn-arxiv, ogbn-proteins and ogbl-ddi datasets and top-3 performance on the ogbl-citation2 dataset. On the similar Tesla V100 GPU cards, AGDNs outperform Reversible GNNs (RevGNNs) with 13% complexity and 1% training runtime of RevGNNs on the ogbn-proteins dataset. AGDNs also achieve comparable performance to SEAL with 36% training and 0.2% inference runtime of SEAL on the ogbl-citation2 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Graph Neural Networks (GNNs), or Message Passing Neural Networks (MPNNs) <ref type="bibr" target="#b0">[1]</ref>, recently proved effective and became mainstream of graph deep learning in many domains, such as citation networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, social networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, biological graphs <ref type="bibr" target="#b5">[6]</ref>, and traffic networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Based on simple feed-forward networks, they perform message passing in each layer to capture neighborhood information. However, unlike the deep models in the Computer Vision (CV) domain, the deep GNNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> will encounter the over-smoothing problem, resulting in significant performance degradation. The model cannot distinguish the node representations, which become nearly identical after long-range message passing. Deep GNNs may contain many feature transformations since a GNN layer usually couples a graph convolution operator and a feature transformation. Recent research <ref type="bibr" target="#b11">[12]</ref> reveals that the well-known overfitting problem caused by redundant transformations can contribute significantly to the performance degradation of deep GNNs. In addition, many transformations also bring high memory footprints and extended runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2012.15024v2 [cs.LG] 2 Sep 2022</head><p>By emphasizing the shallow information during long-range message passing, several types of residual connections <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> can tackle the over-smoothing and over-fitting problems. However, they do not improve extensive high memory footprints or runtime from deep GNNs. Some techniques <ref type="bibr" target="#b15">[16]</ref> from the CV domain can effectively reduce memory footprints of deep GNNs, but with a longer runtime.</p><p>Residual connections work among pairs of graph convolution and feature transformation. In contrast, the graph diffusion-based methods directly combine sequential graph convolution operators. The shallow information can be preserved with suitable weighting coefficients to alleviate the oversmoothing problem. A graph convolution operator can be described as a matrix multiplication between the weighted adjacency (transition matrix) and node feature matrix. Then a graph diffusion operator replaces the transition matrix with a linear combination of its different powers. On the other hand, since many feature transformations bring both efficiency and performance problems, limiting the number of transformations is also helpful. Thus, employing the graph diffusion with limited feature transformations is reasonable.</p><p>This strategy exists in some decoupled methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> that decouple graph convolution operators and feature transformations. In detail, all graph convolution operators are placed before or after all feature transformations. Thus, the decoupled GNNs can enlarge the receptive field with shallow feature transformations. The graph diffusion or its variants are incorporated to leverage the multi-hop information. However, there are no feature transformations (non-linear hierarchies) among all graph convolution operators. Graph convolution is restricted in the same feature space. This characteristic may limit their model capacity. For example, decoupled GNNs can achieve considerable performance on small citation datasets but residual GNNs outperform them on the larger ogbn-arxiv dataset.</p><p>Directly replacing the graph convolution operator in each GNN layer with the graph diffusion operator can also enlarge the receptive field with shallow feature transformations. Graph Diffusion Convolution (GDC) directly replaces the transition matrix with an explicit diffusion matrix. Efficient approximation and sparsification techniques can reduce the memory cost of the dense and large diffusion matrix. However, GDC lacks flexibility since its weighting coefficients are predefined and fixed for different nodes and feature channels. Moreover, GDC cannot improve link prediction performance. A more efficient but equivalent way of calculating the graph diffusion is to calculate multi-hop node representations iteratively and combine them. Some methods perform this memory-efficient graph diffusion in each layer without calculating the explicit diffusion matrix. In other words, instead of high-power transition matrices, multi-hop representation matrices are calculated and stored. The unified framework of these methods has not been well studied and separated from other graph diffusion-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>. We summarize them as Graph Diffusion Networks (GDNs). However, many existing GDNs still utilize the fixed predefined hop weighting coefficients, which are identical across nodes, channels, and layers.</p><p>Other techniques <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> that do not modify model architecture can also tackle the over-smoothing problem. Many GNNs, including our proposed models, are compatible with them. We will not precisely introduce or compare them.</p><p>In this paper, we refine and propose Graph Diffusion Networks (GDNs). Then we propose Adaptive Graph Diffusion Neural Networks (AGDNs) with generalized graph diffusion associated with two learnable and scalable mechanisms of weighting coefficients: Hop-wise Attention (HA) and Hop-wise Convolution (HC). We show a natural evolution path of GNNs. From MPNNs to GDNs, the receptive field is enlarged without adding extra transformations or decoupling the model architecture. The multi-layer graph diffusion in different feature spaces can contribute to model capacity. From GDNs to AGDNs, a more generalized and flexible paradigm of graph diffusion can bring better performance. HA induces hop-wise and node-wise weighting coefficients. HC can directly learn the hop-wise and channel-wise weighting coefficients. Following the historical development of GNNs, from elegant but inefficient spectral methods to intuitive and effective spatial methods, we generalize the graph diffusion to be more spatial with a loss of spectral analyzability.</p><p>We conduct experiments on diverse Open Graph Benchmark (OGB) <ref type="bibr" target="#b22">[23]</ref> datasets with semi-supervised node classification and link prediction tasks <ref type="bibr" target="#b22">[23]</ref>. The results show that our proposed methods significantly outperform popular GNNs on both tasks. AGDNs achieve top-1 performance on the ogbn-arxiv (Accuracy of 76.37?0.11%), ogbn-proteins (ROC-AUC of 88.65?0.13%) and ogbl-ddi (Hits@20 of 95.38?0.94%) datasets and top-3 performance on the ogbl-citation2 (MRR of 85.49?0.29%) dataset. AGDNs also achieve the SOTA performance among models without using labels as input on the ogbn-products dataset. AGDNs outperform the state-of-the-art (SOTA) RevGNNs with much less complexity and runtime and achieve comparable results to SOTA SEAL with much less runtime on large graphs. In our ablation study on all datasets, AGDNs can significantly outperform the associated GATs. Furthermore, the experiments of MPNNs and AGDNs with different model depths demonstrate that AGDNs can effectively mitigate the over-smoothing effect.</p><p>Our main contributions are 1). We incorporate multi-layer generalized graph diffusion into GNNs with moderate complexity and runtime; 2). We propose two learnable and scalable mechanisms for adaptively capturing multi-hop information. 3). We achieve new SOTA performance on diverse, challenging OGB datasets. We outperform complicated RevGNNs with much less complexity and runtime on large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works 2.1 Residual GNNs</head><p>By employing residual connections, residual GNNs simultaneously alleviate over-fitting and over-smoothing problems. The main concerns of these methods are connection design and memory optimizations. Jumping-Knowledge Network (JKNet) <ref type="bibr" target="#b23">[24]</ref> introduces jumping knowledge connection and adaptively sums intermediate representations. GCN with initial residual and identity mapping (GCNII) <ref type="bibr" target="#b14">[15]</ref> combines two classes of residual connections. DeepGCN <ref type="bibr" target="#b12">[13]</ref> introduces dense and dilated connections from CNNs. DeeperGCN <ref type="bibr" target="#b13">[14]</ref> unifies message aggregation operations with differentiable generalized aggregation functions. It further proposes a novel normalization layer and pre-activation residual GNNs. Reversible GNNs (RevGNNs) <ref type="bibr" target="#b15">[16]</ref> reduce memory footprints by incorporating reversible connections and grouped convolutions. With deep and over-parameterized GNN architecture, RevGNNs can achieve SOTA results on several datasets. However, as a cost of reducing memory footprints, the runtime of deep RevGNNs layers is even longer. This paper will present deep GNNs with shallow feature transformations that outperform SOTA RevGNNs with much less complexity and runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph diffusion</head><p>We describe the graph convolution as a matrix multiplication between the transition matrix and node feature/representation matrix. Then the graph diffusion <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> replaces the transition matrix with a diffusion matrix, which is a linear combination of powers of the transition matrix with weighting coefficients normalized along hops. The weighting coefficients are essential to balance the importance of shallow and deep information. The Personalized PageRank (PPR) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17]</ref> and the Heat Kernel (HK) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> are two popular predefined weighting coefficients. They both follow the prior that more distant neighboring nodes have fewer influences than near neighboring ones. The weighting coefficients can also be analogously trainable parameters using label propagation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Attention walk <ref type="bibr" target="#b29">[30]</ref> jointly optimizes the node embeddings and weighting coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Decoupled GNNs</head><p>To perform deep graph convolutions with shallow feature transformations, decoupled GNNs decouple graph convolution operators and feature transformations. The integration of multi-hop information in several decoupled GNNs can be considered the special cases <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref> or generalized cases <ref type="bibr" target="#b11">[12]</ref> of the graph diffusion. The Approximated Personalized Propagation of Neural Predictions (APPNP) <ref type="bibr" target="#b16">[17]</ref> utilizes the predefined weights exponentially decaying with hops to integrate multi-hop information. The Deep and Adaptive Graph Neural Network (DAGNN) <ref type="bibr" target="#b11">[12]</ref> incorporates learnable weights. They both perform graph convolutions after feature transformations. On the contrary, other decoupled GNNs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b17">18]</ref> perform graph convolutions before feature transformations. The Simplified Graph Convolution network (SGC) <ref type="bibr" target="#b30">[31]</ref> utilizes the last representation and then classifies with a linear layer. SGC is significantly affected by the over-smoothing problem. As an early GNN model, Diffusion Convolution Neural Network (DCNN) <ref type="bibr" target="#b32">[33]</ref> can also be considered a decoupled GNN using the graph diffusion operator with a learnable convolution kernel. DCNN directly performs graph diffusion with learnable hop-wise and channel-wise weighting coefficients (convolution kernel) based on the input node feature matrix and applies a linear layer. Although DCNN finalizes the graph diffusion during model training, it calculates and stores high powers of the transition matrix, which limits its scalability. Based on SGC, the Simple Spectral Graph Convolution network (S 2 GC) sums multi-hop features. The Scalable Inception Graph Neural Network (SIGN) <ref type="bibr" target="#b17">[18]</ref> encodes and concatenates multi-hop representations. However, as a trade-off between efficiency and accuracy, the simple decoupled architecture limits model capacity because all graph convolution operators are restricted in the same feature space. There are no intermediate transformations between graph convolution operators.</p><p>In this paper, we tend to directly replace the graph convolution operator with a graph diffusion operator in each MPNN layer without decoupling the model architecture. We must consider whether to calculate an explicit diffusion matrix during actual implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Explicit diffusion matrix: Graph Diffusion Convolution</head><p>As a preprocessing method to augment input data, the Graph Diffusion Convolution (GDC) <ref type="bibr" target="#b18">[19]</ref> calculates an explicit diffusion matrix. Although GDC controls the sparsity of the final diffusion matrix, the intermediate explicit high-power transition matrices are still maintained, which limits its scalability. There are approximation methods for PPR and HK to alleviate this problem with a loss of information. Moreover, the weighting coefficients are predefined and identical for all nodes, channels, and layers, which may limit the model performance. GDC cannot be calculated in preprocessing for a learnable transition matrix. In addition, GDC cannot improve link prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Implicit diffusion matrix: Graph Diffusion Networks</head><p>The Graph Diffusion Networks (GDNs) perform implicit graph diffusion with right-to-left matrix multiplication starting from the node feature or representation matrix in each layer. First, GDNs sequentially calculate the multi-hop representation matrices. Then, they integrate these matrices without maintaining a high-dimensional diffusion matrix. Such a paradigm is more memory-efficient because high-order aggregated feature matrices are much smaller than high-power transition matrices. Another critical advantage over GDC is that the weighting coefficients in GDNs can be layer-wise. GDNs multiply the receptive field of base MPNNs with the diffusion depth without increasing feature transformations or changing other central model architecture. Thus, GDNs can inherit many essential characteristics from MPNNs, including non-linear hierarchies, attention mechanisms, and residual connections. In contrast, decoupled GNNs lose most of them. Based on an MPNN layer, we can generate its associated GDN layer. Topology Adaptive Graph Convolutional Networks (TAGCNs) <ref type="bibr" target="#b33">[34]</ref> are generated based on GCN and sum multi-hop representations after transformations using uniform weighting coefficients. TAGCNs utilize a generalized version of the graph diffusion since multiple linear transformations are applied to multi-hop features. These additional transformations bring redundant complexity. MAGNA <ref type="bibr" target="#b34">[35]</ref> is generated based on GAT using PPR weighting coefficients. MAGNA further includes residual connections and feed-forward networks for each layer. In addition, GDNs using PPR or HK coefficients with a predefined transition matrix are equivalent to GDC. However, previous works have not precisely studied the unified framework of GDNs. In this paper, we refine the unified framework of GDNs and extend this framework with generalized graph diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Diffusion-like GNNs</head><p>MixHop <ref type="bibr" target="#b35">[36]</ref> and N-GCN <ref type="bibr" target="#b36">[37]</ref> perform diffusion-like operation in each layer, however, they transform node representations with non-linear activations for each hop. Such methods more likely stack MPNN layers into groups by concatenating their outputs without effectively controlling model complexity. They are hard to optimize and have redundant hyperparameters with a complicated tuning strategy, including selecting which hops to use in each layer. Our proposed GDNs and AGDNs follow a more efficient paradigm of graph diffusion without redundant feature transformations or non-linear activations, and AGDNs can adaptively learn the importance of each hop instead of manual selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tricks on GNNs</head><p>The Bag of Tricks on GNNs (BoT) <ref type="bibr" target="#b37">[38]</ref> includes some critical tricks of GNNs applied in many SOTA models on the OGB leaderboard. Firstly, the masked node labels, which are zeros other than sampled training nodes, are used as model input ("label as input"). Secondly, BoT conducts the additional iterative feed-forward passes in each epoch with the predicted soft labels filling up masked zero labels ("label reuse"). Thirdly, BoT proposes a more robust loss function ("Loge loss"). Finally, BoT proposes to adjust the GAT adjacency closer to the GCN adjacency ("norm.adj."). Self-Knowledge Distillation (self-KD) <ref type="bibr" target="#b38">[39]</ref> is another common trick on the OGB leaderboard. Graph Information Aided Node feature exTraction with XR-Transformers (GIANT-XRT) <ref type="bibr" target="#b39">[40]</ref> trains more informative node embedding using raw text data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">GNNs for link prediction</head><p>There exist two main GNN methods applied to link prediction. The first methods follow an encoder-decoder framework <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>, in which GNNs act as the encoder to output node representations. A simple predictor receiving pairs of node representations makes final predictions. Pairwise Learning on Neural Link Prediction (PLNLP) proposes to utilize pairwise AUC loss to improve the quality of ranking metrics. The second method, SEAL <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> applies a GNN on an enclosing subgraph sampled from each pair of nodes and directly output predictions. SEAL also includes an additional labeling trick to enhance structural information. SEAL achieves the SOTA performance of GNNs on many link prediction datasets. However, SEAL requires extensive runtime, which limits its application, especially on large graphs. This paper will show that with the encoder-decoder framework, AGDNs can outperform other encoder-decoder GNNs and even approach or outperform SEAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>Suppose G = (V, E) be a given graph with node set V and edge set E. We denote the number of nodes with N = |V|, the number of edges with E = |E|, and the adjacency matrix with A ? R N ?N . The normalized adjacency matrix is denoted with A ? R N ?N . Considering the common stacking architecture of GNNs, we denote the initial node feature matrix with X ? R N ?d <ref type="bibr">(0)</ref> and the initial edge feature matrix with X E . For the l-th GNN layer, we denote its input node representation with H (l?1) ? R N ?d (l?1) and its output node representation with H (l) ? R N ?d <ref type="bibr">(l)</ref> . We also denote node i's input representation vector with h (l?1) i and its output representation vector with h (l) i . We denote the attention query vector of GAT with a (l) ? R 2?d <ref type="bibr">(l)</ref> and the hop-wise attention query vector of AGDN-HA with a (l) Edge feature vector</p><formula xml:id="formula_0">hw ? R 2?d (l) .</formula><formula xml:id="formula_1">H (l) Node representation matrix h (l) i Node representation vector d (0) Node feature dimension d (l) l-th layer's output dimension d E Edge feature dimension Ni Neighborhood set of node i 3.2 Tasks</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Semi-supervised node classification</head><p>Given a graph G, the node set can be further split into a labeled node set L and an unlabeled node set U. The semisupervised node classification task is to predict unlabeled nodes' labels. The unlabeled nodes' feature and related edges are not accessible at inference for inductive tasks. Nevertheless, they are accessible for transductive tasks. For inductive tasks, unlabeled nodes' all information cannot be accessed during training. Whether GDNs or AGDNs are applicable on inductive datasets ultimately depends on its MPNN base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Link prediction</head><p>Given a graph G and a pair of nodes, the link prediction task is to predict whether an edge exists between two nodes. The edges in a dataset are split into training, validation, and test edges. OGB rules make the training edges accessible during model training and inference. The other edges can only be accessed to compute evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Message Passing Neural Networks</head><p>Let us review the main architecture of MPNNs. This paper focuses on the essential step in MPNNs, neighbor aggregation (message passing). We also consider the following combination with node self feature matrix (usually in the form of residual linear connection). We omit the optional readout operation (only necessary for the graph prediction). The neighbor aggregation with a residual linear connection can be described as a matrix multiplication between weighted adjacency A (transition matrix) and node feature/representation matrix H:</p><formula xml:id="formula_2">H (l) = A (l) H (l?1) W (l) + H (l?1) W (l),r ,<label>(1)</label></formula><p>where W ? R d (l?1) ?d (l) refers to the linear transformation in the l-th layer, and W (l),r ? R d (l?1) ?d (l) refers to its residual linear transformation. A complete MPNN model stacks several MPNN layers with intermediate activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Transition matrix</head><p>The weighted adjacency or transition matrix is used in graph convolution (1-power) and graph diffusion (multiple powers). There are several common transition matrices: row-stochastic adjacency A row = D ?1 A, column-stochastic adjacency A col = AD ?1 and the symmetric normalized adjacency</p><formula xml:id="formula_3">A sym = D ?1/2 AD ?1/2 , where D is the degree matrix (D ii = ? N j=1 A ij )</formula><p>. Different transition matrices are associated with different base MPNN models. GraphSAGE utilizes A row as its weighted adjacency. GCN utilizes A sym modified with adding self-loops. GAT utilizes learnable row-stochastic adjacency, derived from the learnable attention mechanism, which depends on the attributes of source and destination nodes. We can view this adjacency as a learnable row-stochastic adjacency by reviewing the computation of GAT adjacency:</p><formula xml:id="formula_4">e ij = LeakyReLU([h i W ||h j W ] ? a) = LeakyReLU([h i W ] ? a dst + [h j W ] ? a src ),<label>(2)</label></formula><formula xml:id="formula_5">A ij = exp(e ij ) k?Ni exp(e ik ) ,<label>(3)</label></formula><p>where h i is the input representation vector of node i, W is the transformation matrix. The query vector a can be split into source query vector a src and destination query vector a dst .</p><p>For graphs with edge attributes x E ij , BoT <ref type="bibr" target="#b37">[38]</ref> proposes to incorporate edge attributes in attention coefficients:</p><formula xml:id="formula_6">e ij = LeakyReLU([h i W ||h j W ||x E ij W E ] ? a),<label>(4)</label></formula><p>where W E is edge feature transformation matrix.</p><p>Then, we can define the unnormalized GAT adjacency A : A ij = exp(e ij ) and its diagonal in-degree and out-degree matrices are defined as D row and D col , with respectively row summation and column summation as diagonal entries.</p><p>The GAT adjacency may be expressed as A = D ?1 row A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed methods</head><p>In this section, we firstly introduce the frameworks of GDNs and AGDNs. Then, we propose a pseudo-symmetric variant of the GAT transition matrix. Thirdly, we propose two adaptive mechanisms for calculating hop weighting matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Diffusion Networks</head><p>Several existing GNNs using multi-hop information in each layer can be considered GDNs. However, there is a lack of a unified framework for GDNs. For a GDN model, we also stack multiple GDN layers to perform multi-layer graph diffusion. We formulate a GDN layer with the diffusion depth K as below:</p><formula xml:id="formula_7">H (l,0) = H (l?1) W (l) ,<label>(5)</label></formula><formula xml:id="formula_8">H (l,k) = AH (l,k?1) ,<label>(6)</label></formula><formula xml:id="formula_9">H (l) = K k=0 ? (l,k)H (l,k) + H (l?1) W (l),r ,<label>(7)</label></formula><p>where {? (k) } k?{0,1,...,K} is the set of normalized weighting coefficients and K is the diffusion depth. Note that we iteratively calculate the multi-hop representations in a right-to-left matrix multiplication, instead of the explicit multiple powers of the transition matrix (A (l) , (A (l) ) 2 , .., (A (l) ) K ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adaptive Graph Diffusion Networks</head><p>The weighting coefficients in GDC and GDNs are hop-wise. In this paper, we generalize the graph diffusion and make weighting coefficients further node-wise or channel-wise. We suppose that different nodes or channels may require different hop weighting coefficients. We define an AGDN layer as below: where ? refers to the element-wise multiplication and ? (k) in R N ?d (l) is a generalized weighting matrix. GDNs are special cases of AGDNs when all elements of ? (k) are the same. This description in the form of matrices is for global comparison with MPNNs and GDNs.</p><formula xml:id="formula_10">H (l,0) = H (l?1) W (l) ,<label>(8)</label></formula><formula xml:id="formula_11">H (l,k) = AH (l,k?1) ,<label>(9)</label></formula><formula xml:id="formula_12">H (l) = K k=0 ? (k) ?H (l,k) + H (l?1) W (l),r ,<label>(10)</label></formula><p>In detail, we also describe the AGDN layer from a node viewpoint, which matches the actual implementation. We perform sequential graph convolution and sum multi-hop representations with hop-wise (node-wise or channel-wise) weighting coefficients. The generalized graph diffusion at l-th layer for node i is described as below:</p><formula xml:id="formula_13">h (l,0) i = h (l?1) i W (l) ,<label>(11)</label></formula><formula xml:id="formula_14">h (l,k) i = j?Ni A ijh (l,k?1) j ,<label>(12)</label></formula><formula xml:id="formula_15">h (l) ic = K k=0 ? ikch (l,k) ic + d (l?1) c =1 h (l?1) ic W (l),r c c ,<label>(13)</label></formula><p>whereh (l,k) i is the k-hop intermediate representation vector of node i and ? ikc can be viewed extracted from a 3-dimensional (node-wise, hop-wise and channel-wise) tensor ?. The previous weighting matrix ? (k) can be extracted from this tensor by selecting the k-hop. In some cases, to explicitly enhance the position (hop) information, we can add intermediate multi-hop representation vectors with learnable Positional Embedding (PE) row vectors</p><formula xml:id="formula_16">{p (0) , p (1) , ..., p (K) } in R d (l) ?1</formula><p>. We omit this trick since PE marginally improves model performance on certain datasets empirically.</p><p>There exists a trade-off between generalization and spectral analyzability. The eigenvectors of the generalized graph diffusion matrix are generally different from the original transition matrix. From another perspective, AGDNs do not follow the characteristics or limitations of previous diffusion-based methods. Instead, our generalized weighting coefficients can be adaptive across hops and nodes or channels, reflected in the following proposed Hop-wise Attention and Hop-wise Convolution. In addition, we can naturally assign layer-wise weighting coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transition matrix</head><p>We can find that GAT is more concerned about destination nodes' in-degrees. The symmetric normalized adjacency has proven more effective on specific datasets <ref type="bibr" target="#b1">[2]</ref>. It is reasonable to give a variant of GAT adjacency to leverage both source nodes' out-degrees and destination nodes' in-degrees. Thus, motivated by the form of popular symmetric normalized adjacency, we propose a pseudo-symmetric normalized GAT adjacency: In BoT, another version of pseudo-symmetric normalized GAT adjacency is proposed (denoted with "norm.adj.") <ref type="bibr" target="#b37">[38]</ref>:</p><formula xml:id="formula_17">A sym = D ? 1 2 row AD ? 1 2 col .<label>(14)</label></formula><formula xml:id="formula_18">A adj = D 1 2 D ?1 row AD ? 1 2 ,<label>(15)</label></formula><p>where adj refers to "adjustment" since we can view this adjacency as the GAT adjacency adjusted to GCN adjacency. Note that A, A sym and A adj are pseudo-symmetric since they are guaranteed to be symmetric if and only if e ij = e ji , ?i, j ? N , when a src = a dst . As a special case when we set query vectors to zeros, then both A sym and A adj become the standard symmetric normalized adjacency. This characteristic connects GAT and GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Weighting ceofficients</head><p>In this subsection, to simplify the discussion, we omit the subscript (l) and collect all weighting coefficients {? ikc } into a unified weighting tensor with ? ? R N ?(K+1)?d . We can extract ? (k) = ? :k: , where : in the subscript refers to extracting all channels in this dimension. We denote the subscripts for nodes, hops, and channels with i, k, and c. We aim to design 'adaptive' and efficient ways of calculating weighting tensor, which should be variable for nodes or feature channels. It is hard to directly define a unified weighting tensor adaptive for both nodes and feature channels, which results in enormous additional complexity. We propose two efficient ways in the following paragraphs: hop-wise attention (HA) and hop-wise convolution (HC). We denote AGDN variants with AGDN-mean, AGDN-HA, and AGDN-HC, using naive fixed weights, hop-wise attention weights, and hop-wise convolution weights. As a particular case of AGDNs, AGDN-mean can be considered a representative example of GDNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hop-wise Attention</head><p>We suppose that, in many cases, the computation of graph diffusion should be adaptive for different nodes but identical for different feature channels, which manifests as the unified weighting tensor normalized along hops K k=0 ? i,k,c = 1, ?i, c and identical along channels ? i,k,c = ? i,k,1 , ?c ? {1, 2, ..., d}. Then we can simplify this tensor into a 2-dimensional weighting matrix ? HA = [? HA ik ] in R N ?(K+1) , by ignoring the last subscript c. ? can be recovered by adding the third dimension and repeating ? HA for d times in the third dimension. It is still not efficient to define a naive learnable weighting matrix in R N ?(K+1) , which results in redundant complexity and violates the possible inductive setting. Inspired by the attention mechanism in GAT, we propose Hop-wise Attention (HA) using a learnable query vector a hw in R 2d to induce the expected weighting matrix. We need to learn just 2d parameters. Firstly, we calculate ?:</p><formula xml:id="formula_19">? ik = h (l,0) i ||h (l,k) i ? a hw ,<label>(16)</label></formula><p>where ? represents inner product, || represents the concatenation operation and k represents the k-hop representation. Then, as shown in the left part of <ref type="figure" target="#fig_1">Figure 2</ref>, the hop-wise attention scores are calculated as below:</p><formula xml:id="formula_20">? HA ik = exp (? (? ik )) K k=0 exp (? (? ik )) ,<label>(17)</label></formula><p>where ? is an activation function (usually LeakyReLU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hop-wise Convolution</head><p>We consider another simple strategy for integrating multi-hop representations, which performs Hop-wise Convolution (HC). This time, we suppose that, in certain cases, the graph diffusion should be adaptive for different channels. Thus, we directly define learnable weighting tensor, which is identical for all nodes ? i,k,c = ? 1,k,c , ?i ? {1, 2, ..., N }. Then we simplify this tensor into a 2-dimensional convolution kernel matrix ? HC = [? HC kc ] in R (K+1)?d , by ignoring the first subscript i. The complete weighting tensor ? can be recovered by adding the first dimension and repeating ? HC for N times in the first dimension, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We need to learn (K + 1) ? d parameters. Note that we do not require that the tensor is normalized along any dimensions. For each feature channel c ? {1, 2, ..., d}, we conduct individual hop-wise convolution with the associated kernel vector in R K+1 . HC is in the same form as DCNN. However, HC is based on more memory-efficient graph diffusion and calculated with different convolution kernels in different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Complexity</head><p>In complexity analysis, we omit the dimension change between a layer's input and output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model analysis</head><p>We generally lose the elegant spectral analyzability since generalized weighting coefficients can easily change the eigenvectors. However, this also implies that AGDNs are more flexible in the spectral domain.</p><p>For AGDN-HC, the weighting coefficients are identical for all nodes and do not change the eigenvectors. Thus, we can give a preliminary spectral analysis. We will demonstrate that, even without changing eigenvectors, AGDN-HC is still flexible in the spectral domain with a considerable diffusion depth. First, given a feature channel c, we simplify the form of AGDN-HC with an eigendecomposition of the transition matrix A = U ?1 ?U :</p><formula xml:id="formula_21">S c = K k=0 ? kc A k = K k=0 ? kc U ?1 ?U k = U ?1 K k=0 ? kc ? k U ,<label>(18)</label></formula><p>where the row vectors of U refer to the eigenvectors of A and ? is a diagonal matrix whose entries {? 1 , ? 2 , ..., ? N } are eigenvalues of A. The eigenvalues of the transition matrix are bounded by 1 (? i ? [?1, 1], ?i) <ref type="bibr" target="#b42">[43]</ref>. For the i-th eigenvalue ? i of the transition matrix, the associated eigenvalue ? i of the diffusion matrix is:</p><formula xml:id="formula_22">? i = K k=0 ? k,c ? k i .<label>(19)</label></formula><p>This relation can be described as an K-order polynomial function of ? i . With the order increasing, this function becomes more flexible and can approximate more functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we conduct experiments on three OGB node classification datasets and three OGB link prediction datasets. Our proposed AGDNs outperform common MPNNs and SOTA RevGNNs with less complexity and runtime for the semi-supervised node classification datasets. AGDNs outperform other GNN models in link prediction tasks using the same encoder-decoder framework. AGDNs approach SOTA SEAL with much less runtime. AGDNs achieve new SOTA performance on the ogbn-arxiv, ogbn-proteins, and ogbl-ddi datasets. We train all AGDN models on a single V100 card with 16Gb memory. Datasets We utilize three OGB semi-supervised node classification datasets (ogbn-arxiv, ogbn-proteins and ogbnproducts) and three OGB link prediction datasets (ogbl-ppa, ogbl-ddi and ogbl-citation2). We summarize the detailed statistics of these datasets in <ref type="table" target="#tab_1">Table 2</ref>. ogbn-arxiv is a citation network between all Computer Science (CS) arXiv papers, whose data split is based on the publication dates of the papers. ogbn-proteins is a graph between proteins with multi-dimensional edge weights indicating different types of biologically meaningful associations. Its data split is based on the associated species of the proteins. ogbn-products is a co-purchasing network between Amazon products, whose data split is based on the sales ranking. ogbl-ppa is a graph between proteins from 58 species with similar edges to ogbn-proteins, whose edges measured by high-throughput technology are used as training edges, and other edges measured by low-throughput technology are used as validation and testing edges. ogbl-ddi is a drug-drug interaction network with each edge indicating the joint effect of taking the two drugs together. It has data split based on what proteins those drugs target in the body. ogbl-citation2 is a citations graph between a subset of papers from MAG. Its data is split by selecting the most recent papers as source nodes and randomly selecting destination nodes for training/validation/testing sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global settings</head><p>We conduct all experiments on a single Nvidia Tesla V100 with 16 Gb GPU memory. We evaluate our proposed models with 10 runs, fixing random seed 0-9, and report means and standard deviations. Except on the ogbn-products and ogbl-citation2 datasets (evaluated on CPU), we conduct both training and inference of all AGDN models on the same GPU card. All final test scores are from the best model selected based on validation scores. In the tables of this paper, we highlight the results of AGDN with underlined fonts and the best results with bold fonts.</p><p>We utilize AGDN-HC on the ogbn-proteins dataset and AGDN-HA for all other datasets. The unavailable results are indicated by -.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Task 1: Node Classification</head><p>Baselines Several representative GNNs and SOTA GNNs are selected as baselines. For semi-supervised node classification, we utilize GCN <ref type="bibr" target="#b1">[2]</ref>, GraphSAGE <ref type="bibr" target="#b2">[3]</ref>, GAT <ref type="bibr" target="#b43">[44]</ref>, MixHop <ref type="bibr" target="#b35">[36]</ref>, JKNet <ref type="bibr" target="#b23">[24]</ref>, DeeperGCN <ref type="bibr" target="#b13">[14]</ref>, GCNII <ref type="bibr" target="#b14">[15]</ref>, DAGNN <ref type="bibr" target="#b11">[12]</ref>, MAGNA <ref type="bibr" target="#b34">[35]</ref>, UniMP <ref type="bibr" target="#b44">[45]</ref>, GAT+BoT <ref type="bibr" target="#b37">[38]</ref> and RevGNN <ref type="bibr" target="#b15">[16]</ref>.</p><p>Experimental setup For ogbn-arxiv, we utilize 3 AGDN layers with transition matrix of GAT, hidden dimension 256, 3 attention heads, and residual linear connections. For AGDN with BoT, we utilize pseudo-symmetric normalized transition matrix of GAT from BoT. For AGDN with BoT and GIANT-XRT embedding, we utilize our proposed pseudo-symmetric transition matrix of GAT and 2 AGDN layers. For ogbn-proteins, we utilize 6 AGDN layers with the transition matrix of GAT A, hidden dimension 150, 6 attention heads, and residual linear connections. For ogbn-products, we utilize 4 AGDN layers with the transition matrix of GAT, hidden dimension 120, 4 attention heads, and residual linear connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Results on the ogbn-arxiv dataset</head><p>We split the comparison on the ogbn-arxiv dataset into two parts since several critical tricks contribute significantly to the final performance on this leaderboard. We implement AGDN based on the implementation of GAT+BoT. Firstly, for the first part of the comparison <ref type="table" target="#tab_2">(Table 3)</ref>, we disable the options about BoT and compare baselines and AGDN without using labels as input. AGDN outperforms other baselines. Secondly, we adapt BoT, self-KD, and GIANT-XRT for AGDN in the second part of the comparison <ref type="table" target="#tab_3">(Table 4</ref>). We use baselines using labels as input. By progressively applying these tricks, AGDN consistently outperforms GAT and RevGAT. Finally, AGDN+BoT+self-KD+GIANT-XRT achieves a new SOTA performance of 76.37% with a significant margin. Without GIANT-XRT embedding, AGDN is implemented with 3 layers with hidden dimension 256, and RevGAT is implemented with 5 layers with hidden dimension 256. With more complexity, RevGAT can achieve similar performance to AGDN. With GIANT-XRT  embedding, AGDN and RevGAT are implemented with 2 layers with hidden dimension 256. With similar complexity, the margin between RevGAT and AGDN becomes larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Results on the ogbn-proteins and ogbn-products datasets</head><p>Moreover, we evaluate AGDN on larger ogbn-proteins and ogbn-products datasets with the random graph partition technique. For ogbn-proteins, we utilize HC instead of HA. AGDN can achieve a new SOTA result of 88.65%, which even outperforms the much more complex and deeper model RevGNN. We only utilize 6 AGDN layers with hidden dimension 150 with 8.61M parameters. RevGNN-wide includes 448 layers with hidden dimension 224 with 68.47M parameters. Furthermore, the inference of AGDN is also conducted on the same GPU card of training with 16Gb memory. In contrast, the inference of RevGNN is conducted on another GPU card with 48Gb memory. For ogbnproducts, we evaluate AGDN with the random partition. AGDN significantly outperforms other baselines, including RevGNN. AGDN achieves the SOTA performance among models without using labels as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Runtime</head><p>We report training and inference runtime on the ogbn-proteins dataset in <ref type="table" target="#tab_6">Table 7</ref> with the runtime of RevGNNs reported in its paper. This comparison demonstrates that AGDN can outperform RevGNN and cost much less runtime simultaneously. With extended runtime, RevGNN-Deep and RevGNN-Wide cost 2.86Gb and 7.91Gb for training, while AGDN costs 13.67Gb. However, the inference of AGDN is conducted on the same GPU, while the inference of RevGNNs is on another Nvidia RTX A6000 (48Gb) without reporting inference runtime or memory cost.   Based on the naive implementation of the official OGB repository, we compare variants of AGDNs with different base models (GCN, GraphSAGE, GAT), under different diffusion depths K (K = 1, 2, ..., 8), on the ogbn-arxiv dataset. We also evaluate MPNN baselines with related equivalent receptive fields. For example, for K = 4 in each subgraph of <ref type="figure" target="#fig_3">Figure 3</ref> and <ref type="figure" target="#fig_4">Figure 4</ref>, the associated model in MPNN baseline curve has 3 ? 4 layers. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, three MPNN baseline curves, especially for GAT, show a distinct over-smoothing problem. AGDN-mean shows quickly rising but then significantly dropping curves, which is also affected by over-smoothing. AGDN-HA and AGDN-HC show much more stable curves. AGDN-HA has similar optimal results to AGDN-mean. However, AGDN-HC cannot even outperform shallow baseline models. Moreover, we repeat these experiments by adding residual linear connections. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, all models, including MPNNs (with few layers) and AGDNs, can be improved with residual linear connections. MPNNs still have a distinct over-smoothing problem. However, the over-smoothing problem of AGDN-mean is significantly alleviated. Moreover, AGDN-HC is effectively improved. Three variants of AGDN show similar performance. However, AGDN-HA outperforms AGDN-mean, especially at low K. This characteristic is vital in applying complex models on large graphs because we tend to utilize lower K due to the memory limit. This paper selects low K (2 or 3) in other experiments.  Test accuracy </p><formula xml:id="formula_23">A = A gcn 1 8 K A = A sage 1 8 K A = A gat Method MPNN (Baseline) AGDN-Mean AGDN-HA AGDN-HC</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.5">More ablation study</head><p>We report ablation study between GAT and AGDN on the ogbn-arxiv, ogbn-proteins and ogbn-products datasets in <ref type="table">Table 8</ref>. We keep all settings the same for GAT and AGDN. We can confirm the significant improvements of AGDN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Task 2: Link Prediction</head><p>Baselines For link prediction tasks, we utilize DeepWalk <ref type="bibr" target="#b45">[46]</ref>, Matrix Factorization <ref type="bibr" target="#b46">[47]</ref>, Common Neighbor <ref type="bibr" target="#b47">[48]</ref>, Adamic Adar <ref type="bibr" target="#b48">[49]</ref>, Resource Allocation <ref type="bibr" target="#b49">[50]</ref>, GCN <ref type="bibr" target="#b1">[2]</ref>, GraphSAGE <ref type="bibr" target="#b2">[3]</ref>, SEAL <ref type="bibr" target="#b40">[41]</ref> and PLNLP <ref type="bibr" target="#b50">[51]</ref> as baselines. Due to memory limitation, we adapt graph-based sampling techniques including random partition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b44">45]</ref> for ogbn-proteins and ogbn-products, and GraphSAINT <ref type="bibr" target="#b51">[52]</ref> for ogbl-citation2. Some baselines are not implemented for some datasets. Thus we do not report the associated results.</p><p>Experimental Setup For ogbl-ppa, we utilize 2 AGDN layers with the transition matrix of GAT, hidden dimension 128, 1 attention head, and residual linear connections. For ogbl-ddi, we utilize 2 AGDN layers with the transition matrix of GAT, hidden dimension 512, 1 attention head, and residual linear connections. For ogbl-citation2, we utilize 3 AGDN layers with the transition matrix of GAT, hidden dimension 256, 1 attention head, and residual linear connections. In official OGB baselines, naive cross-entropy loss is used regarding link prediction as binary classification. PLNLP proposes to utilize pairwise AUC loss. We adapt AUC loss on the ogbl-ddi dataset. Note that we only utilize this loss on the ogbl-ddi dataset since it does not improve AGDN on other datasets. We adopt the GraphSAINT technique for AGDN <ref type="table">Table 8</ref>: Ablation study on the ogbn-arxiv, ogbn-proteins and ogbn-products datasets. Due to the space limit, we omit the variances of these scores. on the ogbl-citation2 dataset. We utilize learnable node embeddings instead of possible node features on the ogbl-ddi (dimension 512) and ogbl-ppa (dimension 128) datasets. Note that we only manually tune a few hyperparameters based on default settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Training and evaluation We follow the standard training procedure in official OGB baselines, which use an encoderdecoder framework. To emphasize the effect of AGDN, we do not introduce other modifications except pairwise AUC loss. We use the standard data splits and metrics from the official OGB paper for evaluation.     <ref type="table" target="#tab_9">Table 9</ref>, heuristic methods show significant advantages over GNN methods on the ogbl-ppa dataset. As a GNN architecture modified for link prediction, based on a complicated subgraph extracting and labeling tricks, SEAL achieves similar performance to the best heuristic method. AGDN, based on naive official OGB baseline scripts, outperforms GCN, GraphSAGE, and several heuristic methods. AGDN utilizes learnable node embeddings as model input, bringing additional parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>As shown in <ref type="table" target="#tab_0">Table 10</ref>, on the ogbl-ddi dataset, GNN methods act better than heuristic methods. With AUC loss, AGDN can achieve 95.38% Hits@20, the new SOTA result on the ogbl-ddi leaderboard. This dataset is very dense and will make structural patterns meaningless. Thus encoder-decoder GNNs with learnable node embeddings can act much better than SEAL. <ref type="table" target="#tab_0">Table 11</ref>, on the ogbl-citation2 dataset, GNN methods also act better than heuristic methods. GCN, GraphSAGE, and PLNLP are full-batch trained. Due to our GPU memory limitation (16Gb), we train AGDN with a neighbor sampling technique. We can observe significant performance degradation by comparing full-batch GCN (84.74% test MRR) and GCN using GraphSAINT (79.85% test MRR) in the official OGB repository. However, even with GraphSAINT, AGDN still achieves top 3 performance on the whole ogbl-citation2 leaderboard and outperforms full-batch GCN, GraphSAGE, and PLNLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>On the ogbl-ddi dataset, AGDN outperforms SEAL and other encoder-decoder GNNs with a significant margin. On the ogbl-ppa and ogbl-citation2 datasets, the margin between AGDN and SEAL is not enormous (&lt; 8%) and smaller than other encoder-decoder GNNs. We believe that, with more suitable techniques designed for link prediction, AGDN will contribute more to this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Runtime</head><p>We compare training and inference runtime of AGDN and SEAL on the ogbl-ppa, ogbl-ddi, and ogbl-citation2 datasets in <ref type="table" target="#tab_0">Table 13</ref>. With similar Tesla V100 GPU cards, AGDN takes significantly less training and inference runtime than SEAL on the ogbl-ppa and ogbl-citation2 datasets. The model architecture of AGDN is more complicated than SEAL. However, the simple encoder-decoder framework is less expressive but much more efficient than SEAL's time-consuming subgraph sampling and labeling trick. On the small ogbl-ddi dataset, where additional techniques in SEAL work much more efficient, AGDN takes more training runtime but still much less inference runtime than SEAL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Ablation study</head><p>We also conduct an ablation study to demonstrate the effectiveness of AGDN on link prediction tasks. We report the results of both GAT and AGDN (with transition matrix of GAT) on three datasets with the same settings. As shown in <ref type="table" target="#tab_0">Table 12</ref>, AGDN significantly improves the link prediction performance of GAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper proposes a feasible and effective evolution path for GNNs. Firstly, we refine and propose Graph Diffusion Networks (GDNs) by replacing the graph convolution operator with an efficient graph diffusion in each GNN layer. Then, we generalize graph diffusion to propose Adaptive Graph Diffusion Networks (AGDNs). We propose two adaptive and scalable mechanisms of computing hop weighting coefficients/matrices. In the spectral domain, AGDNs are more adaptive and flexible than previous graph diffusion-based methods. We evaluate AGDNs and other popular GNNs on node classification and link prediction tasks. The experimental results show that AGDNs can significantly outperform many popular GNNs and even SOTA GNNs (RevGNNs and SEAL). At the same time, AGDNs have considerable overall advantages of complexity and efficiency over SOTA GNNs. Instead of copying huge models from other domains or using simplified architecture, we enlarge the receptive field with moderate complexity and essential architecture. It is valuable for limited computation hardware and time-critical tasks.</p><p>Limits As a common issue, it is hard to apply the node-wise or layer-wise neighbor sampling techniques to very deep GNNs, including AGDNs. We must employ additional memory-saving techniques from other models if we want to train a very deep/wide AGDN model with a considerable diffusion depth. Fortunately, AGDNs are compatible with most techniques applied to MPNNs. The effect of position embedding in AGDNs has not been precisely studied. We leave potential memory-saving techniques for AGDNs in future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>AGDN layer Architecture: The operator ? represents matrix multiplication, the bold operator ? represents element-wise multiplication, and the operator ? represents summation. The left and right multiplication correspond to the relative position of the multiplicand to the operator ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left: The hop-wise attention is parameterized by a hw , with a LeakyReLU activation function ? and normalized along hops with softmax function; The associated weighting tensor ? can be derived from the N ? (K + 1) matrix ? HA , by repeating d times along the third dimension. Right: The weighting tensor of hop-wise convolution ? is directly derived from (K + 1) ? d kernel matrix ? HC by repeating N times along the first dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The extra time complexity of an AGDN layer over its base MPNN layer comes from K-hop aggregations O(KEd) (by default, we perform feature transformation before aggregation), element-wise multiplication with weighting matrices O(KN d), and hop-wise attention computation O(KN d) if used. Then the extra time complexity of an AGDN layer is O(KEd + KN d). Under the realistic assumption of E N , this extra time complexity becomes O(KEd). The extra space complexity of an AGDN layer is O(KN d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of AGDN-Mean, AGDN-HA, AGDN-HC with different base models and diffusion hops on the ogbn-arxiv dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of AGDN-Mean, AGDN-HA, AGDN-HC with residual linear connections, different base models, and different diffusion hops on the ogbn-arxiv dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Symbol definitions.</figDesc><table><row><cell>Symbols</cell><cell>Definitions</cell><cell>Symbols</cell><cell>Definitions</cell></row><row><cell>G</cell><cell>Graph</cell><cell>K</cell><cell>Diffusion depth</cell></row><row><cell>V</cell><cell>Node set</cell><cell>N</cell><cell>Number of nodes</cell></row><row><cell>E</cell><cell>Edge set</cell><cell>E</cell><cell>Number of edges</cell></row><row><cell>A</cell><cell>Adjacency matrix</cell><cell>A</cell><cell>transition matrix</cell></row><row><cell>X</cell><cell>Node feature matrix</cell><cell>xi</cell><cell>Node feature vector</cell></row><row><cell>X E</cell><cell>Edge feature matrix</cell><cell>x E (i,j)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics</figDesc><table><row><cell>Datasets</cell><cell>#Nodes</cell><cell>#Edges</cell><cell>Metrics</cell></row><row><cell>ogbn-arxiv</cell><cell>169,343</cell><cell>1,166,243</cell><cell>Accuracy</cell></row><row><cell>ogbn-proteins</cell><cell>132,534</cell><cell cols="2">39,561,252 ROC-AUC</cell></row><row><cell cols="4">ogbn-products 2,449,029 61,859,140 Accuracy</cell></row><row><cell>ogbl-ppa</cell><cell>576,289</cell><cell cols="2">30,326,273 Hits@100</cell></row><row><cell>ogbl-ddi</cell><cell>4,267</cell><cell>1,334,889</cell><cell>Hits@20</cell></row><row><cell cols="3">ogbl-citation2 2,927,963 30,561,187</cell><cell>MRR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The first part of experimental results on the ogbn-arxiv dataset. Except for AGDN, other results are from their papers or the OGB leaderboard.</figDesc><table><row><cell>Models</cell><cell cols="2">Accuracy (%) Test Valid</cell><cell>Params</cell></row><row><cell>GCN</cell><cell cols="2">71.74?0.29 73.00?0.17</cell><cell>0.11M</cell></row><row><cell cols="3">GraphSAGE 71.49?0.27 72.77?0.16</cell><cell>0.22M</cell></row><row><cell>DeeperGCN</cell><cell cols="2">71.92?0.16 72.62?0.14</cell><cell>0.49M</cell></row><row><cell>JKNet</cell><cell cols="2">72.19?0.21 73.35?0.07</cell><cell>0.09M</cell></row><row><cell>DAGNN</cell><cell cols="2">72.09?0.25 72.90?0.11</cell><cell>0.04M</cell></row><row><cell>GCNII</cell><cell>72.74?0.16</cell><cell>-</cell><cell>2.15M</cell></row><row><cell>MAGNA</cell><cell>72.76?0.14</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Ours (AGDN) 73.41?0.25 74.23?0.13</cell><cell>1.45M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The second part of experimental results on the ogbn-arxiv dataset. Except for AGDN, other results are from their papers or the OGB leaderboard. x=BoT, y=self-KD, z=GIANT-XRT embedding.</figDesc><table><row><cell>Models</cell><cell>Accuracy(%) Test Valid</cell><cell>Params</cell></row><row><cell>UniMP</cell><cell>73.11?0.20 74.50?0.15</cell><cell>0.18M</cell></row><row><cell>GAT+x</cell><cell>73.91?0.12 75.16?0.08</cell><cell>1.44M</cell></row><row><cell>RevGAT+x</cell><cell>74.02?0.18 75.01?0.10</cell><cell>2.10M</cell></row><row><cell>Ours (AGDN+x)</cell><cell>74.11?0.12 75.25?0.05</cell><cell>1.51M</cell></row><row><cell>GAT+x+y</cell><cell>74.16?0.08 75.14?0.04</cell><cell>1.44M</cell></row><row><cell>RevGAT+x+y</cell><cell>74.26?0.17 74.97?0.08</cell><cell>2.10M</cell></row><row><cell>Ours (AGDN+x+y)</cell><cell>74.31?0.12 75.22?0.09</cell><cell>1.51M</cell></row><row><cell>RevGAT+x+z</cell><cell>75.90?0.19 77.01?0.09</cell><cell>1.30M</cell></row><row><cell>Ours (AGDN+x+z)</cell><cell>76.18?0.16 77.24?0.06</cell><cell>1.31M</cell></row><row><cell>RevGAT+x+y+z</cell><cell>76.15?0.10 77.16?0.09</cell><cell>1.30M</cell></row><row><cell cols="2">Ours (AGDN+x+y+z) 76.37?0.11 77.19?0.08</cell><cell>1.31M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Experimental results on the ogbn-proteins dataset. DeeperGCN, UniMP, RevGNN, and AGDN are implemented with random partition. GAT is implemented with neighbor sampling. AGDN+BoT is based on the implementation of GAT+BoT, however, labels are not used as model input since they empirically bring no improvements. Except for AGDN, other results are from their papers or the OGB leaderboard.</figDesc><table><row><cell>Models</cell><cell>ROC-AUC(%) Test Valid</cell><cell>Params</cell></row><row><cell>GCN</cell><cell>72.51?0.35 79.21?0.18</cell><cell>0.10M</cell></row><row><cell>GraphSAGE</cell><cell>77.68?0.20 83.34?0.13</cell><cell>0.19M</cell></row><row><cell>DeeperGCN</cell><cell>85.80?0.17 91.06?0.16</cell><cell>2.37M</cell></row><row><cell>UniMP</cell><cell>86.42?0.08 91.75?0.06</cell><cell>1.91M</cell></row><row><cell>GAT+BoT</cell><cell>87.65?0.08 92.80?0.08</cell><cell>2.48M</cell></row><row><cell cols="3">RevGNN-deep 87.74?0.13 93.26?0.06 20.03M</cell></row><row><cell cols="3">RevGNN-wide 88.24?0.15 94.50?0.08 68.47M</cell></row><row><cell cols="2">Ours (AGDN) 88.65?0.13 94.18?0.05</cell><cell>8.61M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Experimental results on the ogbn-products dataset. GAT, DeeperGCN, and AGDN are implemented with random partition. GraphSAGE and UniMP are implemented with neighbor sampling. Except for AGDN, all results are from their papers or the OGB leaderboard.</figDesc><table><row><cell>Models</cell><cell cols="2">Accuracy(%) Test Valid</cell><cell>Params</cell></row><row><cell>GCN</cell><cell cols="2">75.64?0.21 92.00?0.03</cell><cell>0.10M</cell></row><row><cell cols="3">GraphSAGE 78.50?0.14 92.24?0.07</cell><cell>0.21M</cell></row><row><cell cols="2">GraphSAINT 80.27?0.26</cell><cell>-</cell><cell>0.33M</cell></row><row><cell>DeeperGCN</cell><cell cols="2">80.98?0.20 92.38?0.09</cell><cell>0.25M</cell></row><row><cell>SIGN</cell><cell cols="2">80.52?0.16 92.99?0.04</cell><cell>3.48M</cell></row><row><cell>UniMP</cell><cell cols="2">82.56?0.31 93.08?0.17</cell><cell>1.48M</cell></row><row><cell cols="3">RevGNN-112 83.07?0.30 92.90?0.07</cell><cell>2.95M</cell></row><row><cell cols="3">Ours (AGDN) 83.34?0.27 92.29?0.10</cell><cell>1.54M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Runtime comparison on the ogbn-proteins dataset with similar Tesla V100 cards.</figDesc><table><row><cell>Model</cell><cell>Training</cell><cell>Inference</cell></row><row><cell cols="2">RevGNN-Deep 13.5d/2000epochs</cell><cell>-</cell></row><row><cell cols="2">RevGNN-Wide 17.1d/2000epochs</cell><cell>-</cell></row><row><cell>AGDN</cell><cell>0.14d/2000epochs</cell><cell>12s</cell></row><row><cell>6.1.4 Over-smoothing and ablation study</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>74.05 88.15 93.85 81.77 91.75 Ours (AGDN) 73.41 74.23 88.65 94.18 83.34 92.29</figDesc><table><row><cell></cell><cell>ogbn-arxiv</cell><cell cols="3">ogbn-proteins ogbn-products</cell></row><row><cell></cell><cell cols="4">Accuracy(%) ROC-AUC(%) Accuracy(%)</cell></row><row><cell></cell><cell cols="2">Test Valid Test</cell><cell>Valid</cell><cell>Test</cell><cell>Valid</cell></row><row><cell>GAT</cell><cell>72.98</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Experimental results on the ogbl-ppa dataset.</figDesc><table><row><cell>Models</cell><cell cols="2">Hits@100(%) Test Valid</cell><cell>Params</cell></row><row><cell>DeepWalk</cell><cell>28.88?1.53</cell><cell>-</cell><cell>150.14M</cell></row><row><cell cols="4">Matrix Factorization 32.29?0.94 32.28?4.28 147.66M</cell></row><row><cell cols="3">Common Neighbor 27.65?0.00 28.23?0.00</cell><cell>0</cell></row><row><cell>Adamic Adar</cell><cell cols="2">32.45?0.00 32.68?0.00</cell><cell>0</cell></row><row><cell cols="3">Resource Allocation 49.33?0.00 47.22?0.00</cell><cell>0</cell></row><row><cell>GCN</cell><cell cols="2">18.67?1.32 18.45?1.40</cell><cell>0.28M</cell></row><row><cell>GraphSAGE</cell><cell cols="2">16.55?2.40 17.24?2.64</cell><cell>0.42M</cell></row><row><cell>SEAL</cell><cell cols="2">48.80?3.16 51.25?2.52</cell><cell>0.71M</cell></row><row><cell>PLNLP</cell><cell>32.38?2.58</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (AGDN)</cell><cell cols="3">41.23?1.59 43.32?0.92 36.90M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Experimental results on the ogbl-ddi dataset.</figDesc><table><row><cell>Models</cell><cell cols="2">Hits@20(%) Test Valid</cell><cell>Params</cell></row><row><cell>DeepWalk</cell><cell>22.46?2.90</cell><cell>-</cell><cell>11.54M</cell></row><row><cell cols="4">Matrix Factorization 13.68?4.75 33.70?2.64 1.22M</cell></row><row><cell cols="3">Common Neighbor 17.73?0.00 9.47?0.00</cell><cell>0</cell></row><row><cell>Adamic Adar</cell><cell cols="2">18.61?0.00 9.66?0.00</cell><cell>0</cell></row><row><cell cols="2">Resource Allocation 6.23?0.00</cell><cell>7.25?0.00</cell><cell>0</cell></row><row><cell>GCN</cell><cell cols="3">37.07?5.07 55.50?2.08 1.29M</cell></row><row><cell>GraphSAGE</cell><cell cols="3">53.90?4.74 62.62?0.37 1.42M</cell></row><row><cell>SEAL</cell><cell cols="3">30.56?3.86 28.49?2.69 0.53M</cell></row><row><cell>PLNLP</cell><cell cols="3">90.88?3.13 82.42?2.53 3.50M</cell></row><row><cell>Ours (AGDN)</cell><cell cols="3">95.38?0.94 89.43?2.81 3.51M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Experimental results on the ogbl-citation2 dataset.</figDesc><table><row><cell>Models</cell><cell>MRR(%) Test Valid</cell><cell>Params</cell></row><row><cell cols="3">Matrix Factorization 51.86?4.43 51.81?4.36 281.11M</cell></row><row><cell cols="2">Common Neighbor 51.47?0.00 51.19?0.00</cell><cell>0</cell></row><row><cell>Adamic Adar</cell><cell>51.89?0.00 51.67?0.00</cell><cell>0</cell></row><row><cell cols="2">Resource Allocation 51.98?0.00 51.77?0.00</cell><cell>0</cell></row><row><cell>GCN</cell><cell>84.74?0.31 84.79?0.23</cell><cell>0.30M</cell></row><row><cell>GraphSAGE</cell><cell>82.60?0.36 82.63?0.33</cell><cell>0.46M</cell></row><row><cell>SEAL</cell><cell>87.67?0.32 87.57?0.31</cell><cell>0.26M</cell></row><row><cell>PLNLP</cell><cell cols="2">84.92?0.29 84.90?0.31 146.51M</cell></row><row><cell>Ours (AGDN)</cell><cell>85.49?0.29 85.56?0.33</cell><cell>0.31M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Ablation study on the ogbl-ppa, ogbl-ddi, ogbl-citation2 datasets. Due to the space limit, we omit the variances of these scores.</figDesc><table><row><cell></cell><cell cols="2">ogbl-ppa</cell><cell>ogbl-ddi</cell><cell>ogbl-citation2</cell></row><row><cell>Models</cell><cell cols="3">Hits@100(%) Hits@10(%)</cell><cell>MRR(%)</cell></row><row><cell></cell><cell>Test</cell><cell>Valid</cell><cell cols="2">Test Valid Test</cell><cell>Valid</cell></row><row><cell>GAT</cell><cell cols="4">37.28 40.64 85.84 77.08 83.07 83.12</cell></row><row><cell cols="5">Ours (AGDN) 41.23 43.32 95.38 89.43 85.49 85.56</cell></row><row><cell>6.2.1 Results</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Runtime comparison on the ogbl-ppa, ogbl-ddi, and ogbl-citation2 datasets with similar Tesla V100 cards.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Training</cell><cell>Inference</cell></row><row><cell>ogbl-ppa</cell><cell>SEAL</cell><cell>20h/20epochs</cell><cell>4h</cell></row><row><cell>ogbl-ppa</cell><cell>AGDN</cell><cell>2.3h/40epochs</cell><cell>0.06h</cell></row><row><cell>ogbl-ddi</cell><cell>SEAL</cell><cell>0.07h/10epochs</cell><cell>0.1h</cell></row><row><cell>ogbl-ddi</cell><cell cols="2">AGDN 0.8h/2000epochs</cell><cell>0.3s</cell></row><row><cell>ogbl-citation2</cell><cell>SEAL</cell><cell>7h/10epochs</cell><cell>28h</cell></row><row><cell cols="3">ogbl-citation2 AGDN 2.5h/2000epochs</cell><cell>0.06h</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational graph auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal graph convolutional network for traffic accident prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leilei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page" from="135" to="147" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Traffic flow prediction over muti-sensor data correlation with graph convolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="page" from="50" to="63" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-stage attention spatialtemporal graph networks for traffic prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinze</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">428</biblScope>
			<biblScope unit="page" from="42" to="53" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deeper insights into graph convolutional networks for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07606</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving graph attention networks with large margin-based constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11945</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepergcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">All you need to train deeper gcns</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6437" to="6449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13354" to="13366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dropedge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10903</idno>
		<title level="m">Towards deep graph convolutional networks on node classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph random neural networks for semi-supervised learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22092" to="22103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diffusion kernels on graphs and other discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Risi Imre Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on machine learning</title>
		<meeting>the 19th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2002</biblScope>
			<biblScope unit="page" from="315" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph convolutional networks using heat kernel for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keting</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16002</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive diffusions for scalable learning over graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Berberidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios B</forename><surname>Nikolakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1307" to="1321" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive graph filtering: Multiresolution classification on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kova?evi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Global Conference on Signal and Information Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="427" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Watch your step: Learning node embeddings via graph attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9180" to="9190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simple spectral graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soummya</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10370</idno>
		<title level="m">Topology adaptive graph convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Direct multi-hop attention based graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14332</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00067</idno>
		<title level="m">Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">N-gcn: Multi-scale graph convolution for semi-supervised node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">uncertainty in artificial intelligence</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="841" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Bag of tricks of semi-supervised classification with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangkun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13355</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3713" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Node feature extraction by self-supervised multi-scale neighborhood prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00064</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Labeling trick: A theory of using graph neural networks for multi-node representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9061" to="9073" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified massage passing model for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Link prediction via matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint european conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Friends and neighbors on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="230" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Predicting missing links via local information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Cheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="630" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litao</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjing</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02936</idno>
		<title level="m">Pairwise learning for neural link prediction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphsaint</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<title level="m">Graph sampling based inductive learning method</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
