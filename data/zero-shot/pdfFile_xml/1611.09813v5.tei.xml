<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MPI for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MPI for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MPI for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MPI for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a CNN-based approach for 3D human body pose estimation from single RGB images that addresses the issue of limited generalizability of models trained solely on the starkly limited publicly available 3D pose data. Using only the existing 3D pose data and 2D pose data, we show state-of-the-art performance on established benchmarks through transfer of learned features, while also generalizing to in-the-wild scenes. We further introduce a new training set for human body pose estimation from monocular images of real humans that has the ground truth captured with a multi-camera marker-less motion capture system. It complements existing corpora with greater diversity in pose, human appearance, clothing, occlusion, and viewpoints, and enables an increased scope of augmentation. We also contribute a new benchmark that covers outdoor and indoor scenes, and demonstrate that our 3D pose dataset shows better in-the-wild performance than existing annotated data, which is further improved in conjunction with transfer learning from 2D pose data. All in all, we argue that the use of transfer learning of representations in tandem with algorithmic and data contributions is crucial for general 3D body pose estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We present an approach to estimate the 3D articulated human body pose from a single image taken in an uncontrolled environment. Unlike marker-less 3D motion capture methods that track articulated human poses from multi-view video sequences, <ref type="bibr" target="#b75">[75,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b12">13]</ref> or use active RGB-D cameras <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b4">5]</ref>, our approach is designed to work from a single low-cost RGB camera.</p><p>Data-driven approaches using Convolutional Neural Networks (CNNs) have shown impressive results for 3D pose regression from monocular RGB, however, in-the-wild This work was funded by the ERC Starting Grant project CapReal (335545). Dan Casas was supported by a Marie Curie Individual Fellow grant (707326), and Helge Rhodin by the Microsoft Research Swiss JRC. We thank The Foundry for license support. scenes and motions remain challenging. Aside from the difficulty of the 3D pose estimation problem, it is further stymied by the lack of suitably large and diverse annotated 3D pose corpora. For 2D joint detection it is feasible to obtain ground truth annotations on in-the-wild data on a large scale through crowd sourcing <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30]</ref>, consequently leading to methods that generalize to in-the-wild scenes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12]</ref>. Some 3D pose estimation approaches take advantage of this generalizability of 2D pose estimation, and propose to lift the 2D keypoints to 3D <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b80">80,</ref><ref type="bibr" target="#b83">83,</ref><ref type="bibr" target="#b79">79,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b13">14]</ref>. This approach however is susceptible to errors from depth ambiguity, and often requires computationally expensive iterative pose optimization. Recent advances in direct CNNbased 3D regression show promise, utilizing different prediction space formulations <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b81">81,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b40">40]</ref> and incorporating additional constraints <ref type="bibr" target="#b81">[81,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b83">83,</ref><ref type="bibr" target="#b78">78]</ref>. However, we show on a new in-the-wild benchmark that existing solutions have a low generalization to in-the-wild conditions. They are far from the accuracy seen for 2D pose prediction in terms of correctly located keypoints.</p><p>Existing 3D pose datasets use marker-based motion capture, MoCap, for 3D annotation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b58">58]</ref>, which restricts recording to skin-tight clothing, or markerless systems in a dome of hundreds of cameras <ref type="bibr" target="#b32">[32]</ref>, which enables diverse clothing but requires an expensive studio setup. Synthetic data can be generated by retargeting MoCap sequences to 3D avatars <ref type="bibr" target="#b14">[15]</ref>, however the results lack realism, and learning based methods pick up on the peculiarities of the rendering leading to poor generalization to real images.</p><p>Our contributions towards accurate in-the-wild pose estimation are twofold. First, in Section 4, we explore the use of transfer learning to leverage the highly relevant mid-and high-level features learned on the readily available in-thewild 2D pose datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">31]</ref> in conjunction with the existing annotated 3D pose datasets. Our experimentally validated mechanism of feature transfer shows better accuracy and generalizability compared to na?ve weight initialization from 2D pose estimation networks and domain adaptation based approaches. With this we show previously unseen levels of accuracy on established benchmarks, as well as generalizability to in-the-wild scenes, with only the existing 3D pose datasets.</p><p>Second, in Section 5, we introduce the new MPI-INF-3DHP dataset 1 real humans with ground truth 3D annotations from a state-of-the-art markerless motion capture system. It complements existing datasets with everyday clothing appearance, a large range of motions, interactions with objects, and more varied camera viewpoints. The data capture approach eases appearance augmentation to extend the captured variability, complemented with improvements to existing augmentation methods for enhanced foreground texture variation. This gives a further significant boost to the accuracy and generalizability of the learned models.</p><p>The data-side supervision contributions are complemented by CNN architectural supervision contributions in Section 3.2, which are orthogonal to in-the-wild performance improvements.</p><p>Furthermore, we introduce a new test set, including sequences outdoors with accurate annotation, on which we demonstrate the generalization capability of the proposed method and validate the value of our new dataset.</p><p>The components of our method are thoroughly evaluated on existing test datasets, demonstrating both state-ofthe-art results in controlled settings and, more importantly, improvements over existing solutions for in-the-wild sequences thanks to the better generalization of the proposed techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There has been much work on learning-and model-based approaches for human body pose estimation from monocular images, with much of the recent progress coming through CNN based approaches. We review the most relevant approaches, and discuss their relation with our work. 3D pose from 2D estimates. Deep CNN architectures have dramatically improved 2D pose estimation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">42]</ref>, with even real-time solutions <ref type="bibr" target="#b74">[74]</ref>. Graphical models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b0">1]</ref> continue to find use in modeling multi-person relations <ref type="bibr" target="#b45">[45]</ref>. 3D pose can be inferred from 2D pose through geometric and statistical priors <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b64">64]</ref>. Optimization of the projection of a 3D human model to the 2D predictions is computationally expensive and ambiguous, but the ambiguity can be addressed through pose priors and it further allows incorporation of various constraints such as inter-penetration constraints <ref type="bibr" target="#b8">[9]</ref>, sparsity assumptions <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b80">80,</ref><ref type="bibr" target="#b82">82]</ref>, joint limits <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2]</ref>, and temporal constraints <ref type="bibr" target="#b50">[50]</ref>. Simo-Serra et al. <ref type="bibr" target="#b60">[60]</ref> sample noisy 2D predictions to ambiguous 3D shapes, which they disambiguate using kinematic constraints, and improve discriminative 2D detection from likely 3D samples <ref type="bibr" target="#b59">[59]</ref>. Li et al. look up the nearest neighbours in a learned joint embedding of human images and 3D poses <ref type="bibr" target="#b36">[36]</ref> to estimate 3D pose from an image. We choose to use the geometric relations between the predicted 2D and 3D skeleton pose to infer the global subject position. Estimating 3D pose directly. Additional image information, e.g. on the front-back orientation of limbs, can be exploited by regressing 3D pose directly from the input image <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b81">81,</ref><ref type="bibr" target="#b25">26]</ref>. Deep CNNs achieve state-of-the-art results <ref type="bibr" target="#b81">[81,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b44">44]</ref>. While CNNs dominate, regression forests have also been used to derive 3D posebit descriptors efficiently <ref type="bibr" target="#b47">[47]</ref>. The input and output representations are important too. To localize the person, the input image is commonly cropped to the bounding box of the subject before 3D pose estimation <ref type="bibr" target="#b25">[26]</ref>. Video input provides temporal cues, which translate to increased accuracy <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b83">83]</ref>. The downside of conditioning on motion is the increased input dimensionality, and requires motion databases with sufficient motion variation, which are even harder to capture than pose data sets. In controlled conditions, fixed camera placement provides additional height cues <ref type="bibr" target="#b78">[78]</ref>. Since monocular reconstruction is inherently scale-ambiguous, 3D joint positions relative to the pelvis, with normalized subject height are widely used as the output. To explicitly encode dependencies between joints, Tekin et al. <ref type="bibr" target="#b65">[65]</ref> regressing to a highdimensional pose representation, learned by an auto encoder. Li et al. <ref type="bibr" target="#b35">[35]</ref> report that predicting positions relative to the parent joint of the skeleton improves performance, but we show that a pose-dependent combination of absolute and relative positions leads to further improvements. Zhou et al. <ref type="bibr" target="#b81">[81]</ref> regress joint angles of a skeleton from single images, using a kinematic model. annotated dataset corpora. 3D annotation <ref type="bibr" target="#b25">[26]</ref> is harder to obtain than 2D pose annotation. Some approaches treat 3D pose as a hidden variable, and use pose priors and projection to 2D to guide the training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b76">76]</ref>. Rogez et al. render mosaics of in-the-wild human pose images using projected mocap data <ref type="bibr" target="#b52">[52]</ref>. Chen et al. <ref type="bibr" target="#b14">[15]</ref> render textured rigged human models, but still require domain adaptation to inthe-wild images for generalization. Other approaches use the estimated 2D pose to look up a suitable 3D pose from a dictionary <ref type="bibr" target="#b13">[14]</ref>, or use the ground truth 2D pose based dictionary lookup to create 3D annotations for in-the-wild 2D pose data <ref type="bibr" target="#b53">[53]</ref>, but neither address the 2D to 3D ambiguity. Our new dataset complements the existing datasets, through extensive appearance and pose variation, by using marker-less annotation and provides an increased scope for augmentation.</p><p>Transfer Learning <ref type="bibr" target="#b43">[43]</ref> is commonly used in computer vision to leverage features and representations learned on one task to offset data scarcity for a related task. Low and/or mid-level CNN features can be shared also among unrelated tasks <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b77">77]</ref>. Pretraining on ImageNet <ref type="bibr" target="#b54">[54]</ref> is commonly used for weight initialization <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b66">66]</ref> in CNNs. We explore different ways of using the low and mid-level features learned on in-the-wild 2D pose datasets for further improving the generalization of 3D pose prediction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN-based 3D Pose Estimation</head><p>We start by introducing the network architecture, utilized input and output domains, and notation. While the particularities of our architecture are explained in Section 3.2, our main contributions towards in-the-wild conditions are covered in sections 4 and 5.</p><p>Given an RGB image, we estimate the global 3D human pose P <ref type="bibr">[G]</ref> in the camera coordinate system. We estimate the global positions of the joints of the skeleton depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, accounting for the camera viewpoint, which goes beyond only estimating in a root-centered (pelvis) coordinate system, as is common in many previous works. Our algorithm consists of three steps, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>(1) the subject is localized in the frame with a 2D bounding box BB, computed from 2D joint heatmaps H, obtained with a CNN we call 2DPoseNet; (2) the root-centered 3D pose P is regressed from the BB-cropped input with a second CNN termed 3DPoseNet; and (3) global 3D pose coordinates P <ref type="bibr">[G]</ref> and perspective correction are computed in closed form using 3D pose P , 2D joint locations K and known camera calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bounding Box and 2D Pose Computation</head><p>We use our 2DPoseNet to produce 2D joint location heatmaps H. The heat map maxima provide the most likely 2D joint locations K which can also act as a stand-in person bounding-box BB detector. See <ref type="figure" target="#fig_0">Figure 1</ref>. The 2D joint locations K are further used for global pose estimation in Section 3.3. In case of an alternative BB detector, K comes from 3DPoseNet. See 2D Auxiliary Task in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Our 2DPoseNet is fully convolutional and is trained on MPII <ref type="bibr" target="#b3">[4]</ref> and LSP <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b29">30]</ref> datasets. We use a CNN structure based on Resnet-101 <ref type="bibr" target="#b22">[23]</ref>, up to the filter banks at level 4. Striding is removed at level 5, and features in the res5a block are halved and identity skip connections removed from res5b and res5c. For specifics of the network architecture and the training scheme, refer to the supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Pose Regression</head><p>The 3D pose CNN, termed 3DPoseNet, is used to regress root-centered 3D pose P from a cropped RGB image, and makes use of new CNN supervision techniques. <ref type="figure" target="#fig_2">Figure 3</ref> depicts the main components of the method, detailed in the following sections. Network The base network derives from Resnet-101 as well, and is identical to 2DPoseNet up to res5a. We remove the remaining layers from level 5. A 3D prediction stub S comprised of a convolution layer (k 5?5 , s 2 ) with 128 features and a final fully-connected layer that outputs the 3D joint locations is added on top. Additionally we predict 2D heatmaps H as an auxiliary task after res5a and, use intermediate supervision with pose P at res3b3 and res4b22. Refer to the supplementary for specifics of the loss weights for the intermediate and auxiliary tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Multi-level Corrective Skip Connections</head><p>We additionally use a skip connection scheme as a trainingtime regularization architecture. We add skip connections from res3b3 and res4b20 to the main prediction P deep , leading to P sum . In contrast to vanilla skip-connections <ref type="bibr" target="#b38">[38]</ref>, we compare both P sum and P deep to the ground truth, and remove the skip connections after training. We show the improvements due to this approach Section 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multi-modal Pose Fusion</head><p>Formulating joint location prediction relative to a single local or global location is not always optimal. Existing literature <ref type="bibr" target="#b35">[35]</ref> has observed that predicting joint locations relative to their direct kinematic parents (Order 1 parents) improves performance. Our experiments reveal that to not universally hold true. We find that depending on the pose and the visibility of the joints in the input image, the optimal relative joint for each joint's location prediction differs. Hence, we use joint locations P relative to the root, O1 relative to Order 1 parents and O2 relative to Order 2 parents along the kinematic tree as the three modes of prediction, see <ref type="figure" target="#fig_1">Figure  2</ref>, and fuse them with fully-connected layers.</p><p>For the joint set we consider, the kinematic relationships chosen suffice, as it puts at least one reference joint for each joint in the relatively low entropy torso <ref type="bibr" target="#b33">[33]</ref>. We use three identical 3D prediction stubs attached to res5a for predicting the pose as P , O1 and O2, and for each we use corrective skip connections. These predictions are fed into a smaller network with three fully connected layers, to implicitly determine and fuse the better constraints per joint into the final prediction P fused . The network has the flexibility to emphasize different combinations of constraints depending on the pose. This can be viewed as intermediate supervision with auxiliary tasks, yet the separate streams for predicting each mode individually are key to its efficacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Global Pose Computation</head><p>The bounding box cropping normalizes subject size and position, which frees 3D pose regression from having to localize the person in scale and image space, but loses global pose information. We propose a lightweight and efficient way to reconstruct the global 3D pose P [G] = (R|T ) P fused from pelvis-centered pose P fused , camera intrinsics, and K. Perspective correction. The bounding box cropping can be interpreted as using a virtual camera, rotated towards the crop center and its field of view covering the crop area. Since the 3DPoseNet only 'sees' the cropped input, its predictions live in this rotated view, leading to a consistent orientation error in P fused . To compensate, we compute rotation R that rotates the virtual camera to the original view. 3D localization. We seek the global translation T that aligns P fused and K under perspective projection. We assume weak perspective projection, ?, and solve the linear least squares equation i K i ? ?(T + P i fused ) 2 , where i indexes the joints. This assumption yields global position</p><formula xml:id="formula_0">T = i P i [xy] ?P [xy] 2 i K i ?K 2 ? ?K [x] K [y] f ? ? ? ? ?P [x] P [y] 0 ? ? ,<label>(1)</label></formula><p>in terms of distances to the 3D meanP and 2D meanK over all joints. P <ref type="bibr">[xy]</ref> is the x, y part of P fused and single subscripts indicate the respective elements. Please see the supplemental document for the derivation and evaluation. Our solution can be considered a generalization of procrustes analysis for projective alignment. Note that this is different to perspective-n-point 6DOF rigid pose estimation <ref type="bibr" target="#b34">[34]</ref>, structure-from-motion, and from the convex approach of Zhou et al. <ref type="bibr" target="#b80">[80]</ref>, which require iterative optimization. as common for many vision tasks. While this affords a faster convergence while training, there remains room for improved generalization beyond the gains from potential supervision and dataset contributions. Due to the similarity of the tasks, features learned for 2D pose estimation on in-the-wild MPII and LSP training sets can be transferred to 3D pose estimation. We explore different variants of the, thus far, un-utilized method of improving generalization by transferring weights from 2DPoseNet to 3DPoseNet. A na?ve initialization of the weights of 3DPoseNet is inadequate, and there is a tradeoff to be made between the preservation of transferred features and learning new pertinent features. We achieve this through a learning rate discrepancy between the transferred layers and the new layers. We experimentally determine the mechanism for this transfer of features through validation. <ref type="table" target="#tab_0">Table 1</ref> shows the evaluated mechanisms for transfer from 2DPoseNet. Based on the experiments, we choose to scale down the learning rate of the layers till res4b22 by a factor of 1000. Through similar experiments for the transfer of ImageNet features, we choose to scale down the learning rate of layers till res4b22 by 10.</p><p>The same approach can be applied to other network architectures, and our experiments on the learning rate discrepancy serve as a sound starting point for the determination of the transfer learning mechanism. Unlike jointly training with annotated 2D and 3D pose datasets, this approach has the advantage of not requiring the 2D annotations to be consistent between the two datasets, and one can simply use off-the-shelf trained 2D pose networks. In Section 6 we show that our approach outperforms domain adaptation, see <ref type="table" target="#tab_3">Table 5</ref>, first row. Additionally, <ref type="table" target="#tab_0">Table 1</ref> validates that the common fine-tuning of the fully-connected layers (third row) and fine-tuning of the complete network (first row) is much less effective then the proposed scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MPI-INF-3DHP: Human Pose Dataset</head><p>We propose a new dataset captured in a multi-camera studio with ground truth from commercial marker-less mo- tion capture <ref type="bibr" target="#b68">[68]</ref>. No special suits and markers are needed, allowing the capture of motions wearing everyday apparel, including loose clothing. In contrast to existing datasets, we record in green screen studio to allow automatic segmentation and augmentation. We recorded 8 actors (4m+4f), performing 8 activity sets each, ranging from walking and sitting to complex exercise poses and dynamic actions, covering more pose classes than Human3.6m. Each activity set spans roughtly one minute. Each actor features 2 sets of clothing split across the activity sets. One clothing set is casual everyday apparel, and the other is plain-colored to allow augmentation.</p><p>We cover a wide range of viewpoints, with five cameras mounted at chest height with a roughly 15 ? elevation variation similar to the camera orientation jitter in other datasets <ref type="bibr" target="#b14">[15]</ref>. Another five cameras are mounted higher and angled down 45 ? , three more have a top down view, and one camera is at knee height angled up. Overall, from all 14 cameras, we capture &gt;1.3M frames, 500k of which are from the five chest high cameras. We make available both true 3D annotations, and a skeleton compatible with the "universal" skeleton of Human3.6m . Dataset Augmentation. Although our dataset has more clothing variation than other datasets, the appearance variation is still not comparable to in-the-wild images. There have been several approaches proposed to enhance appearance variation. Pishchulin et al. warp human size in images with a parametric body model <ref type="bibr" target="#b46">[46]</ref>. Images can be used to augment background of recorded footage <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>. Rhodin et al. <ref type="bibr" target="#b49">[49]</ref> recolor plain-color shirts while keeping the shading details, using intrinsic image decomposition to separate reflectance and shading <ref type="bibr" target="#b39">[39]</ref>.</p><p>We provide chroma-key masks for the background, a chair/sofa in the scene, as well as upper and lower body segmentation for the plain-colored clothing sets. This provides an increased scope for foreground and background augmentation, in contrast to the marker-less recordings of Joo et al. <ref type="bibr" target="#b32">[32]</ref>. For background augmentation, we use images sampled from the internet. For foreground augmentation, we use a simplified intrinsic decomposition. Since for plain colored clothing the intensity variation is solely due to shading, we use the average pixel intensity as a surrogate for the shading component. We composite cloth like textures with <ref type="bibr">Figure 5</ref>. Representative poses (centroids) of the 20 K-means pose clusters of the Human3.6m test set (subjects S9,S11), visually grouped into three broad pose classes, which are used also to perform per-class evaluation. Upright poses are dominant, with complex poses such as sitting and crouching only accounting for 25% and 8% of the poses respectively. Our multimodal fusion scheme significantly improves the latter two, yielding a 3.5mm improvement for Sit and 5.5mm for Crouch class.</p><p>the pixel intensity of the upper body, lower body and chair marks independently, for a photo-realistic result. <ref type="figure" target="#fig_3">Figure 4</ref> shows example captured and augmented frames. Test Set. We found the existing test sets for (monocular) 3D pose estimation to be restricted to limited settings due to the difficulty of obtaining ground truth labels in general scenes. HumanEva <ref type="bibr" target="#b58">[58]</ref> and Human3.6m <ref type="bibr" target="#b26">[27]</ref> are recorded indoors and test on similar looking scenes as the training set, the Human3D+ <ref type="bibr" target="#b14">[15]</ref> test set was recorded with sensor suits that influence appearance and lacks global alignment, and the MARCoNI set <ref type="bibr" target="#b16">[17]</ref> is markerless through manual annotation, but shows mostly walking motions and multiple actors, which are not supported by most monocular algorithms. We create a new test set with ground truth annotations coming from a multi-view markerless motion capture system. It complements existing test sets with more diverse motions (standing/walking, sitting/reclining, exercise, sports (dynamic poses), on the floor, dancing/miscellaneous), camera view-point variation, larger clothing variation (e.g. dress), and outdoor recordings from Robertini et al. <ref type="bibr" target="#b51">[51]</ref> in unconstrained environments. This makes the test set suitable for testing the generalization of various methods. See <ref type="figure">Figure 6</ref> for a representative sample. We use the "universal" skeleton for evaluation. Alternate Metric. In addition to the Mean Per Joint Position Error (MPJPE) widely used in 3D pose estimation, we concur with <ref type="bibr" target="#b26">[27]</ref> and suggest a 3D extension of the Percentage of Correct Keypoints (PCK) <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b70">70]</ref> metric used for 2D Pose evaluation, as well as the Area Under the Curve (AUC) <ref type="bibr" target="#b24">[25]</ref> computed for a range of PCK thresholds. These metrics are more expressive and robust than MPJPE, revealing individual joint mispredictions more strongly. We pick a threshold of 150mm, corresponding to roughly half of head size, similar what is used in MPII 2D Pose dataset. We propose evaluating on the common set of joints across 2D and 3D approaches (joints 1-14 in <ref type="figure" target="#fig_1">Figure 2</ref>), to ensure evaluation compatibility with existing approaches. Joints are grouped by bilateral symmetry (ankles, wrists, shoulders, etc), and can be evaluated by scene setting or activity class. <ref type="figure">Figure 6</ref>. Representative frames from MPI-INF-3DHP test set. We cover a variety of subjects with a diverse set of clothing and poses in 3 different settings: studio with green screen (right); studio without green screen (left); and outdoors (center). <ref type="figure">Figure 7</ref>. Qualitative evaluation on representative frames of the LSP test set. We succeed in challenging cases (left), with only few failure cases (right). The Dance1 sequence of the PanopticDataset <ref type="bibr" target="#b32">[32]</ref>, is also well reconstructed (bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Evaluation</head><p>We evaluate the contributions proposed in the previous sections using the standard datasets Human3.6m and Hu-manEva, as well as our new MPI-INF-3DHP test set. Additionally, we qualitatively observe the performance on LSP <ref type="bibr" target="#b29">[30]</ref> and the CMU Panoptic <ref type="bibr" target="#b32">[32]</ref> datasets, demonstrating robustness to general scenes. Refer to <ref type="figure">Figure 7</ref>. Also refer to the supplementary video for global 3D pose results.</p><p>We evaluate the impact of training 3DPoseNet on Hu-man3.6m, and unaugmented and augmented variants of MPI-INF-3DHP, both with and without transfer learning from 2DPoseNet. We only use Human3.6m compatible camera views from MPI-INF-3DHP for training. Further details are in the supplemental document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Impact of Supervision Methods</head><p>Multi-level corrective skip connections. In <ref type="table" target="#tab_1">Table 2</ref> we compare a baseline method without any skip connections, a network with vanilla skip connections, and our proposed corrective skip regularization on Human3.6m test set. We observe that networks using vanilla skip connections perform markedly worse than the baseline, while corrective skip connections yield more than 5mm improvement for 7 classes of activities (marked as underlined). We verified that the effect is not due to a higher effective learning rate seen by the core network due to the additional loss term.</p><p>Multimodal prediction and fusion. The multi-modal fusion scheme yields noticeable improvement across all datasets tested in tables 2 and 3. Since upright poses dominate in pose datasets, and the activity classes are often diluted significantly by upright poses, the true extent of improvement by the multi-modal fusion scheme is masked. To show that the fusion scheme indeed improves challenging pose classes, we cluster the Human3.6m test set by pose as shown in <ref type="figure">Figure 5</ref>, which visualizes the centroid of each cluster. Then we group the clusters visually into three pose classes, namely Stand/Walk, Sit and Crouch, going by the cluster representatives. For the Stand/Walk class, adding fusion has minimal effect, going from 88.4mm to 88.8mm. However, for Sit class fusion leads to a 3.5mm improvement, from 118.9mm to 115.4mm. Similarly, Crouch class has the highest improvement of 5.5mm, going from 156mm to 150.5mm. The improvement is not simply due to additional training, and is less pronounced if predicting P , O1 and O2 with a common stub, even with more features in the fully-connected layer. Details in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Transfer Learning</head><p>Our approach of transferring representations from 2DPoseNet to 3DPoseNet yields 64.7% 3DPCK on MPI-INF-3DHP test-set when trained with only Human3.6m data, compared to 63.7% 3DPCK of the model trained on our augmented training set without transfer learning. It also shows state of the art performance on Human3.6m test set with an error of ?74mm, demonstrating the dual advantage of the approach in improving both the accuracy of pose estimation and generalizability to in-the-wild scenes. Combining our dataset and transfer learning leads to the best results at ? 72.5% 3DPCK. See <ref type="table" target="#tab_3">Table 5</ref>.</p><p>In contrast to existing approaches countering data scarcity, transfer learning does not require complex dataset synthesis, yet exceeds the performance of Chen et al. <ref type="bibr" target="#b14">[15]</ref> (with synthetic data and domain adaptation, 28.8% 3DPCK, after procrustes alignment) and our base model trained with  <ref type="figure" target="#fig_2">7% 3DPCK)</ref>. Our approach also performs better than domain adaptation <ref type="bibr" target="#b20">[21]</ref> to in-the-wild data ( <ref type="table" target="#tab_3">Table 5</ref>). Details in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Benefit of MPI-INF-3DHP</head><p>Evaluating on MPI-INF-3DHP test-set, without any transfer learning from 2DPoseNet, we see in <ref type="table" target="#tab_5">Table 3</ref> that our dataset, even without augmentation, leads to a ?9% 3DPCK improvement on outdoor scenes over Human3.6m. However, our augmentation strategy is crucial for improved generalization, as seen from the gains in 3DPCK across scene settings in <ref type="table" target="#tab_5">Table 3</ref>, giving 57.3% 3DPCK overall.</p><p>Even when combined with transfer learning, we see in <ref type="table" target="#tab_3">Table 5</ref> that our dataset (both augmented and unagumented) consistently performs better than Human3.6m. The best performance of 76.5% 3DPCK on MPI-INF-3DHP test set and of 72.88mm on Human3.6m is obtained when the two datasets are combined with transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Other Components</head><p>Bounding box computation. On MPI-INF-3DHP test set, we additionally evaluate our best performing network using bounding boxes computed from 2DPoseNet. As shown in <ref type="table" target="#tab_3">Table 5</ref>, the performance drops to 74.4% 3DPCK from 76.5% 3DPCK due to the additional difficulty. Perspective correction. <ref type="table" target="#tab_3">Table 5</ref> shows that perspective correction also has a significant impact, without which, the performance drops to 73% 3DPCK from 76.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Quantitative Comparison</head><p>Human3.6m. <ref type="table" target="#tab_2">Table 4</ref> shows comparison of our method with existing methods, all trained on Human3.6m. Altogether, with our supervision contributions and transfer learning, we are the state of the art (74.11mm, without scaling), while also generalizing to in-the-wild scenes. Note that the Volumetric coarse to fine approach <ref type="bibr" target="#b44">[44]</ref> requires estimates of the bone lengths to convert their predictions from pixels to 3D space. Complementing Human3.6m with our augmented MPI-INF-3DHP dataset further reduces the error to 72mm. HumanEva. The improvements on Human3.6m are confirmed with a 30.8 and 33.5 MPJPE score on the S1 Box and Walk sequences of HumanEva, after alignment. See supplemental document. MPI-INF-3DHP. We also evaluated some of the existing methods on our test set. Deep Kinematic Pose <ref type="bibr" target="#b81">[81]</ref>, attains 13.8% 3DPCK overall. Our full model attains significantly higher accuracy: without transfer learning and trained on Human3.6m obtains 26% 3DPCK, and 64.7% 3DPCK with transfer learning. The large discrepancy in performance between Human3.6m and our new in-the-wild test set highlights the importance of a new benchmark to test generalization to natural images and motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>Despite the demonstrated competitive results, our method and others have limitations. Most training sets, also <ref type="bibr" target="#b14">[15]</ref>, have a strong bias towards chest height cameras. Thus, estimating 3D pose from starkly different camera views is still a challenge. Our new dataset provides diverse viewpoints, which can support development towards viewpoint invariance in future methods. Similar to related approaches, our per-frame estimation exhibits temporal jitter on video sequences. In future, we will investigate integration with model-based temporal tracking to further increase accuracy and temporal smoothness. At less than 250 ms per frame, our approach is much faster than model based methods which work offline in the order of minutes. There still remains scope for improvement towards real time, through smaller input resolution and shallower networks.</p><p>We also show that joining forces with transfer learning, in conjunction with algorithmic and data contributions, will aide progress in 3D pose estimation in many different directions, such as overall accuracy and generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have presented a fully feedforward CNN-based approach for monocular 3D human pose estimation that attains state-of-the-art on established benchmarks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b58">58]</ref> and quantitatively outperforms existing methods on the introduced in-the-wild benchmark. State of the art is attained with enhanced CNN supervision techniques and improved parent relationships in the kinematic chain. Transfer learning from in-the-wild 2D pose data in tandem with a new dataset that includes a larger variety of real and augmented human appearances, activities and camera views, leads to the significantly improved generalization to in-the-wild images. Our method is also the first to efficiently extract global 3D position in non-cropped images, without time consuming iterative optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Document: Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision</head><p>This document accompanies the main paper, and the supplemental video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Further Discussion of Design Choices Regarding Multi-modal Fusion</head><p>To demonstrate that the improvement seen due to the fusion scheme is not simply a result of fine tuning, we compare the result of fusion with components successively removed. Using P , O1 and O2, we get an MPJPE of 74.49mm on Human3.6m. On removing O2, the error increases to 74.77mm, and on removing both O1 and O2, the error increases to 75.27mm. The comparison here is without any multi-level corrective skip training.</p><p>For P , O1 and O2 to have different modes of mispredictions, the underlying feature set that they are computed from has to be as different as possible, because each is related to the other with a linear transform. We achieve some degree of decorrelation between the three by using 3 different prediction stubs, one each for P , O1 and O2 with a convolutional layer (k 5?5 , s 2 ) with 128 features followed by a fully-connected layer. If we replace these three stubs with a single stub with the convolutional layer having 256 features followed by a fully-connected layer, the resulting MPJPE is 75.30mm after fusion, in contrast to an MPJPE of 74.49mm from fusing the result of 3 prediction stubs. Both of these are without corrective-skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Further Discussion of Multi-level Corrective Skip</head><p>Since our multi-level corrective skip scheme adds an additional loss at the last stage (X deep , where X is P /O1/O2) of the network, it increases the effective learning rate seen by the core network. To verify that the improvements seen due to the proposed scheme are not caused by this difference in the effective learning rate, we trained a version of the Base network with loss weights as the sum of the loss weights for X deep and X sum specified in <ref type="table" target="#tab_2">Table 4</ref>. We find that this network performs worse than the Base network (107.14mm vs 104.32mm MPJPE on Human3.6m), and does not approach the accuracy attained with multilevel corrective skip scheme (101.09mm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Global Pose Computation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D localization</head><p>In this section we describe a simple, yet very efficient, method to compute the global 3D location T of a noisy 3D point set P with unknown global position. We assume known scaling and orientation parameters, obtained from its 2D projection estimate K in a camera with known intrinsics parameters (focal length f ). We further assume that the point cloud spread in depth direction is negligible compared to its distance z 0 to the camera and approximate perspective projection of an object near position (x 0 , y 0 , z 0 ) with weak perspective projection (linearizing the pinhole projection model at z 0 ):</p><formula xml:id="formula_1">u v = ? ? ? x y z ? ? , with ? = f z0 0 0 0 f z0 0</formula><p>. <ref type="formula" target="#formula_0">(1)</ref> Estimates K and P are assumed to be noisy due to estimation errors. We find the optimal global position T in the least squares sense, by minimizing T = arg min (x,y,z) E(x, y, z), with</p><formula xml:id="formula_2">E = i K i ? ? (x, y, z) + P i 2 = i K i ? f z (x, y) + P i [xy] 2 ,<label>(2)</label></formula><p>where P i and K i denote the ith joint position in 3D and 2D, respectively, and P i [xy] the xy component of P i . It has partial derivative</p><formula xml:id="formula_3">?E ?x = 2f z i K i [x] + f z P i [x] ? x ,<label>(3)</label></formula><p>where P <ref type="bibr">[x]</ref> denotes the x part of P , andP the mean of P over all joints. Solving </p><formula xml:id="formula_4">?E ?z = f i (K i ?K) (P i [xy] ?P [xy] ) z 2 + f 2 i P i [xy] ?P [xy] 2 z 3 .<label>(4)</label></formula><p>Finally, solving ?E ?z = 0 gives the depth estimate</p><formula xml:id="formula_5">z = f i P i [xy] ?P [xy] 2 i (K i ?K) (P i [xy] ?P [xy] ) ? f i P i [xy] ?P [xy] 2 i K i ?K 2 ,<label>(5)</label></formula><p>where (K i ?K)(P i ?P ) = K i ?K P i ?P cos(?) is approximated for ? ? 0. This is a valid assumption in our case, since the rotation of 3D and 2D pose is assumed to be matching. <ref type="figure" target="#fig_0">Figure 1</ref>. The predicted pose (red) is inaccurate for positions away from the camera center (left), compared against the ground truth (white). Perspective correction (colored) corrects the orientation (center) and is closer to the ground truth (right). Here tested on the walking sequence of HumanEva S1. Evaluation on HumanEva: In addition to evaluating centered pose P , we evaluate the global 3D pose prediction P [G] on the widely used HumanEva motion capture dataset -Box and Walk sequences of Subject 1 from the validation set. Note that we do not use any data from HumanEva for training. We significantly improve the state of the art for the Box sequence (82.1mm <ref type="bibr" target="#b8">[9]</ref> vs 58.6mm). Results on the Walk sequence are of higher accuracy than Bogo et al. <ref type="bibr" target="#b8">[9]</ref>, but lower than the accuracy of Bo et al. <ref type="bibr" target="#b7">[8]</ref> and Yasin et al. <ref type="bibr" target="#b76">[76]</ref>, who, however train on HumanEva <ref type="bibr" target="#b7">[8]</ref> or use an example database dominated by walking motions <ref type="bibr" target="#b76">[76]</ref>. Our skeletal structure does not match that of HumanEva, e.g. the head prediction has a consistent frontal offset and the hip is too wide. To compensate, we compute a linear map of dimension 14x14 (number of joints) that maps our joint positions as a linear combination to the HumanEva structure. The same mapping is applied at every frame, but is computed only once, jointly on the Box and Walk sequence, to limit the correction to global inconsistencies of the skeleton structure. This fine-tuned result is marked by ? in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Perspective correction</head><p>Our 3DPoseNet predicts pose P in the coordinate system of the bounding box crop, which leads to inaccuracies as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The cropped image appears if as it was taken from a virtual camera with the same origin as the original camera, but with view direction to the crop center, see <ref type="figure" target="#fig_1">Figure 2</ref>. To map the reconstruction from the virtual camera coordinates to the original camera, we rotate P by the rotation R between the virtual and original camera. Since the existing training sets provide chest-height camera placements with the same viewpoint, the bias in vertical direction is already learned by the network. We apply perspective correction only in horizontal direction, where a change in cropping and yaw rotation of the person cannot be distinguished by the network. R is then the rotation around the camera up direction by the angle between the original and the virtual view direction, see <ref type="figure" target="#fig_1">Figure 2</ref>. On our MPI-INF-3DHP test set perspective correction improves the PCK by 3 percent points. On HumanEva the improvement is up to 3 mm MPJPE, see <ref type="table" target="#tab_0">Table 1</ref>. The correction is most pronounced for cameras with a large field of view, e.g. Go-Pro and similar outdoor cameras, and when the subject is located at the border of the view. Using the vector from the camera origin to the centroid of 2D keypoints K as the virtual view direction was most accurate in our experiments. However, the crop center can be used instead. Opposed to the Perspective-n-Point algorithm applied by Zhou et al. <ref type="bibr" target="#b83">[83]</ref>, any regression method that works on cropped images could immediately profit from this perspective correction, without computing 2D keypoint detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CNN Architecture and Training Specifics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">2DPoseNet</head><p>Architecture: The architecture derives from Resnet-101, using the same structure as is until level 4. Since we are interested in predicting heatmaps, we remove striding at level 5. Additionally, the number of features in the res5a module are halved, identity skip connections are removed from res5b and res5c, and the number of features gradually tapered to 15 (heatmaps for 14 joints + root). As shown in <ref type="table" target="#tab_1">Table 2</ref>, for 2DPoseNet, our results on MPII and LSP test sets approach that of the state of the art. Intermediate Supervision: Additionally, we employ intermediate supervision at res4b20 and res5a, treating the first 15 feature maps of the layers as the intermediate joint-location heatmaps. Further, we use a Multilevel Corrective Skip scheme, with skip connections coming from res3b3 and res4b22 through prediction stubs comprised of a 1 ? 1 convolution with 20 feature maps followed by a 3 ? 3 convolution with 15 outputs. Training: For training, we use the Caffe <ref type="bibr" target="#b28">[29]</ref> framework, with the AdaDelta solver with a momentum of 0.9 and weight decay rate of 0.005. We employ a batch size of 7, and use Euclidean Loss everywhere. For the Learning Rate and Loss Weight taper schema, refer to <ref type="table" target="#tab_5">Table 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3DPoseNet</head><p>Architecture: The core network is identical to 2DPoseNet up to res5a. A 3D Prediction stub is attached on top, comprised of a 5?5 convolution layer with a stride of 2 and 128 features, followed by a fully-connected layer.</p><p>Multi-level Corrective Skip: We attach 3D prediction stubs to res3b3 and res4b20, similar to the final prediction stub, but with 96 convolutional features instead of 128. The resulting predictions are added to P deep to get P sum . We add a loss term to P deep in addition to the loss term at P sum . 0.05 0.05 6.6e-4 60k 1.0 0.1 0.005 0.005 0.0001 40k 1.0 0.01 0.001 0.001 2.5e-5 40k 1.0 0.001 0.0001 0.0001 0.0008 60k 1.0 0.0001 0.0001 0.0001 0.0001 40k 1.0 0.0001 0.0001 0.0001 3.3e-5 20k 1.0 0.0001 0.0001 0.0001</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal Fusion:</head><p>We add prediction stubs for O1 and O2, similar to those for P . Note that the predictions for P , O1 and O2 are done with distinct stubs, and this slight decorrelation of predictions is important. These predictions are at a later finetuning step fed into three fully-connected layers, with 2k, 1k and 51 nodes respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intermediate Supervision:</head><p>We use intermediate supervision at 4b5 and res4b20, using prediction stubs comprised of 7 ? 7 convolution with a stride of 3 and 128 features, followed by a fully-connected layer predicting P , O1 and O2 as a single vector. Additionally, we predict joint location heatmaps and part-label maps using a 1 ? 1 convolution layer after res5a as an auxiliary task. We don't use the part-label maps when training with MPI-INF-3DHP dataset.</p><p>Training: For training, the solver settings are similar to 2DPoseNet, and we use Euclidean Loss everywhere. For transfer learning, we scale down the learning rate of the transferred layers by a factor determined by validation. For fine-tuning in the multi-modal fusion case, we similarly downscale the learning rate of the trained network by 10,000 with respect to the three new fully-connected layers. For the learning rate and loss weight taper schema for both the main training and multi-modal fusion fine-tuning stages, refer to <ref type="table" target="#tab_2">Tables 4 and 5</ref>. We use different training durations when using Human3.6m or MPI-INF-3DHP in isolation, versus when using both in conjunction. This is reflected in the aforementioned tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">3D Pose Training Data</head><p>In the various experiments on 3DPoseNet, for the datasets we consider, we select ?37.5k frames for each, yielding ?75k samples after scale augmentation at 2 scales (0.7 and 1.0). Human3.6m: We use the H80k <ref type="bibr" target="#b25">[26]</ref> subset of Human3.6m, and train with the "universal" skeleton, using subjects S1,5,6,7,8 for training and S9,11 for testing. The predicted skeleton is not scaled to the test subject skeletons at test time.</p><p>MPI-INF-3DHP: For our dataset, to maintain compatibil-ity of view with Human3.6m and other datasets, we only pick the 5 chest high cameras for all 8 subjects, sampling frames such that at least one joint has moved by more than 200mm between selected frames. A random subset of these frames is used for training, to match the number of selected Human3.6m frames. MPI-INF-3DHP Augmented: The augmented version uses the same frames as the unaugmented MPI-INF-3DHP above, keeping ?25% frames unaugmented, ?40% with only BG and Chair augmentation, and the rest with full augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Domain Adaptation To In The Wild 2D Pose Data</head><p>We use a domain adaptation stub comprised of conv 3?3,256 , conv 3?3,128 , f c 64 and f c 1 layers, and cross entropy domain classification loss. It uses Ganin et al.'s <ref type="bibr" target="#b20">[21]</ref> gradient inversion approach. The domain adaptation stub is attached after res4b22 in the network. We found that directly starting out with ? = ?1 performs better than gradually increasing the magnitude of ? with increasing iterations. We train on the Human3.6m training set, with 2D heatmap and part label prediction as auxiliary tasks. Images from MPII <ref type="bibr" target="#b3">[4]</ref> and LSP <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">31]</ref> training sets are used without annotations for learning better generalizable features. The generalizability is improved, as evidenced by the 41.4 3DPCK on MPI-INF-3DHP test set, but does not match up with the 64.7 3DPCK attained using transfer learning. Detailed results in main <ref type="table" target="#tab_5">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MPI-INF-3DHP Dataset</head><p>We cover a wide range of poses in our training and test sets, roughly grouped into various activity classes. A detailed description of the dataset is available in Section 4 of the main paper. In addition, <ref type="figure" target="#fig_2">Figure 3</ref> samples the various different activity classes, augmentation and subjects represented in our dataset.</p><p>Similarly for the test set, we show a sample of the activities and the variety of subjects in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The Challenge of Learning Invariance to Viewpoint Elevation</head><p>In this paper, we only consider the cameras in the training set placed at chest-height, in part to be compatible with the existing datasets, and in part because viewpoint elevation invariance is a significantly more challenging problem. Existing benchmarks do not place emphasis on this. We will release an expanded version of our MPI-INF-3DHP testset with multiple camera viewpoint elevations, to complement the training data.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>We infer 3D pose from single image in three stages: (1) extraction of the actor bounding box from 2D detections; (2) direct CNN-based 3D pose regression; and (3) global root position computation in original footage by aligning 3D to 2D pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>3D pose, represented as a vector of 3D joint positions, is expressed variously as 1) P : relative to the root (joint #15), 2) O1 (blue): relative to first order and, 3) O2 (orange): relative to second order parents in the kinematic skeleton hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>3D pose Training overview. The main components are 1) regularization through corrective skip connections, and 2D pose prediction as auxiliary task, 2) Multi-modal 3D pose prediction and fusion, 3) a new marker-less 3D pose database with appearance augmentation, and 4) Transfer learning from features learned for 2D pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>MPI-INF-3DHP dataset. We capture actors using a markerless multi-camera in a green screen studio (left), compute masks for different regions (center left) and augment the captured footage by compositing different textures to the background, chair, upper and lower body areas, independently (center right and right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>?E ?x = 0 gives the unique closedform solutions x =K [x] z f ?P [x] and equivalently y = K [y] z f ?P [y] , for ?E ?y = 0. Substitution of x and y in E and differentiating with respect to z yields</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Sketch of the input image cropping and resulting change of field of view. The corresponding rotation R of the view direction is sketched in 2D on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>A sample of the activities, clothing, subjects as well as augmentation on MPI-INF-3DHP Trainig Set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>A sample of the activities and subjects in the test set of MPI-INF-3DHP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of the mechanisms of transfer learning from 2DPoseNet to 3DPoseNet that were explored in the context of the Base network. The table compares the effect of various learning rate multiplier combinations for different parts of the network. For network details, refer to Section 3.2. Human3.6m, Subjects 1,5,6,7,8 used for training, and every 64 th frame of 9,11 used for testing. * = weights randomly initialized</figDesc><table><row><cell cols="2">Learning Rate Multiplier</cell><cell></cell><cell></cell></row><row><cell cols="4">up to res4b22 res5a 3D Stub S Total MPJPE (mm)</cell></row><row><cell>1</cell><cell>1</cell><cell>1*</cell><cell>118.7</cell></row><row><cell>1/10</cell><cell>1/10</cell><cell>1*</cell><cell>84.6</cell></row><row><cell>1/1000</cell><cell>1/1000</cell><cell>1*</cell><cell>89.2</cell></row><row><cell>1/10</cell><cell>1</cell><cell>1*</cell><cell>90.7</cell></row><row><cell>1/1000</cell><cell>1</cell><cell>1*</cell><cell>80.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Activity-wise results (MPJPE in mm) on Human3.6m<ref type="bibr" target="#b26">[27]</ref>. Adding our model components one-by-one on top of the Base network shows successive improvement of the total accuracy. Significant relative improvements greater than 5mm are underlined. Models are trained on Human3.6m, with network weights initialized from ImageNet, unless specified otherwise.The version marked with MPI-INF-3DHP is trained with Human3.6m and MPI-INF-3DHP. Evaluation with all 17 joints, on every 64 th frame, without rescaling to a person specific skeleton. Regular Skip 113.34 112.26 97.40 110.50 108.63 112.09 105.67 125.97 173.41 109.34 120.87 107.75 97.30 126.05 117.45 115.29 Base 98.98 100.14 86.07 101.83 101.34 96.74 94.89 125.28 158.31 100.21 112.49 99.57 83.39 109.61 95.79 104.32 + Corr. Skip 92.57 99.08 85.46 95.43 96.93 89.56 95.67 123.54 160.98 97.13 107.56 93.86 76.99 110.93 88.73 101.09 + Fusion 93.80 99.17 84.73 95.60 94.48 89.40 93.15 119.94 154.61 95.94 106.09 94.13 77.25 108.82 87.38 99.79 + Transfer 2DPoseNet 59.69 69.74 60.55 68.77 76.36 59.05 75.04 96.19 122.92 70.82 85.42 68.45 54.41 82.03 59.79 74.14 + MPI-INF-3DHP 57.51 68.58 59.56 67.34 78.06 56.86 69.13 99.98 117.53 69.44 82.40 67.96 55.24 76.50 61.40 72.88</figDesc><table><row><cell></cell><cell cols="5">Direct Discuss Eating Greet Phone Posing Purch. Sitting</cell><cell>Sit Smoke Down</cell><cell>Take Wait Walk Photo</cell><cell>Walk Walk Total Dog Pair</cell></row><row><cell cols="6">Base + Table 3. Evaluation by scene-setting of our design choices on MPI-</cell></row><row><cell cols="6">INF-3DHP test set with weight transfer from ImageNet. Training</cell></row><row><cell cols="6">on our markerless dataset improves accuracy significantly, in par-</cell></row><row><cell cols="6">ticular with the proposed augmentation strategy. Fusion yields an</cell></row><row><cell cols="6">additional gain. GS indicates sequences with green screen.</cell></row><row><cell>3D dataset</cell><cell>Network architecture</cell><cell cols="3">Studio Studio Outdoor GS no GS</cell><cell>All</cell></row><row><cell></cell><cell></cell><cell cols="4">3DPCK 3DPCK 3DPCK 3DPCK AUC</cell></row><row><cell>Human3.6m</cell><cell>Base + Corr. Skip</cell><cell>22.2</cell><cell>33.9</cell><cell>18.5</cell><cell>25.1 8.7</cell></row><row><cell></cell><cell cols="2">Base + Corr. Skip + Fusion 22.3</cell><cell>34.2</cell><cell>20.0</cell><cell>26.0 9.5</cell></row><row><cell>Ours Unaug.</cell><cell>Base + Corr. Skip</cell><cell>66.9</cell><cell>38.2</cell><cell>27.9</cell><cell>46.8 20.9</cell></row><row><cell></cell><cell cols="2">Base + Corr. Skip + Fusion 67.6</cell><cell>39.6</cell><cell>28.5</cell><cell>47.8 21.8</cell></row><row><cell>Ours Aug.</cell><cell>Base + Corr. Skip</cell><cell>71.1</cell><cell>51.7</cell><cell>36.1</cell><cell>55.4 26.0</cell></row><row><cell></cell><cell cols="2">Base + Corr. Skip + Fusion 73.5</cell><cell>53.1</cell><cell>37.9</cell><cell>57.3 28.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">Comparison of results on Human3.6m [27] with the state</cell></row><row><cell cols="2">of the art. Human3.6m, Subjects 1,5,6,7,8 used for training, and</cell></row><row><cell>9,11 used for testing. Method</cell><cell>Total MPJPE (mm)</cell></row><row><cell>Deep Kinematic Pose[81] J17,B</cell><cell>107.26</cell></row><row><cell>Sparse. Deep. [83] T,J17,B,10,Act</cell><cell>113.01</cell></row><row><cell>Motion Comp. Seq. [67] T,J17,B</cell><cell>124.97</cell></row><row><cell>LinKDE [27] J17,B,Act</cell><cell>162.14</cell></row><row><cell>Du et al. [78] T,J17,B</cell><cell>126.47</cell></row><row><cell>Rogez et al. [52] (J13),B,64</cell><cell>121.20</cell></row><row><cell>SMPLify [9] J14,B,A,(First cam.)</cell><cell>82.3</cell></row><row><cell>3D=2D+Matching [14] J17,B</cell><cell>114.18</cell></row><row><cell>Distance Matrix [40] J17,B</cell><cell>87.30</cell></row><row><cell>Volumetric Coarse-Fine[44] J17,B,S*</cell><cell>71.90</cell></row><row><cell>LCR-Net [53] J17,B</cell><cell>87.7</cell></row><row><cell>Full model (w/o MPI-INF-3DHP) J17,B</cell><cell>74.11</cell></row><row><cell>Full model (w/o MPI-INF-3DHP) J17,B,S</cell><cell>68.61</cell></row><row><cell>Full model (w/o MPI-INF-3DHP) J14,B,A</cell><cell>54.59</cell></row></table><note>S = Scaled to test subject specific skeleton, computed from T-pose.T = Uses Temporal Information, J14/J17 = Joint set evaluated, A = Uses Best Alignment To GT per frame, Act = Activitywise Training, 1/10/64 = Test Set Frame Sampling</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Evaluation on MPI-INF-3DHP test set with weight transfer from 2DPoseNet, by scene setting. Training our full model on our dataset paired with Human3.6m yields best accuracy over all. GS indicates sequences with green screen background.</figDesc><table><row><cell>3D dataset</cell><cell>Method</cell><cell cols="3">Studio Studio Outdoor GS no GS</cell><cell>All</cell></row><row><cell></cell><cell></cell><cell cols="4">3DPCK 3DPCK 3DPCK 3DPCK AUC</cell></row><row><cell>Human3.6m</cell><cell>Domain adapt.</cell><cell>44.1</cell><cell>42.6</cell><cell>35.2</cell><cell>41.4 17.7</cell></row><row><cell></cell><cell>Ours (full model)</cell><cell>70.8</cell><cell>62.3</cell><cell>58.5</cell><cell>64.7 31.7</cell></row><row><cell cols="2">Ours Aug. Ours (full model)</cell><cell>82.6</cell><cell>66.7</cell><cell>62.0</cell><cell>71.7 36.4</cell></row><row><cell cols="2">Ours Unaug. Ours (full model)</cell><cell>84.1</cell><cell>68.9</cell><cell>59.6</cell><cell>72.5 36.9</cell></row><row><cell cols="3">Ours Aug. Ours, w/o persp. corr. 81.9</cell><cell>68.6</cell><cell>67.4</cell><cell>73.5 37.6</cell></row><row><cell>+</cell><cell>Ours, w/o GT BB</cell><cell>80.4</cell><cell>71.2</cell><cell>69.8</cell><cell>74.4 39.6</cell></row><row><cell cols="2">Human3.6m Ours (full model)</cell><cell>84.6</cell><cell>72.4</cell><cell>69.7</cell><cell>76.5 40.8</cell></row></table><note>the synthetic data of Rogez et al. [52] (21.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation on HumanEva-I<ref type="bibr" target="#b58">[58]</ref>, with different alignment strategies used in the literature. For reference, we also show multi-view existing results. Our models use no data from HumanEva for training, while the other methods listed train/finetune on HumanEva-I. * = Does not use GT Bounding Box information. ? = Translation alignment only. ? = trained or fine-tuned on HumanEva-I.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>S1 Box</cell><cell></cell><cell></cell><cell cols="2">S1 Walk</cell></row><row><cell></cell><cell></cell><cell></cell><cell>P [G]</cell><cell>P</cell><cell>P</cell><cell>P [G]</cell><cell>P</cell><cell>P</cell></row><row><cell></cell><cell></cell><cell cols="2">(global)</cell><cell>(align S,T )</cell><cell>(align R,S,T )</cell><cell>(global)</cell><cell>(align S,T )</cell><cell>(align R,S,T )</cell></row><row><cell></cell><cell cols="2">Our full model*</cell><cell>117.1</cell><cell>80.5</cell><cell>58.6</cell><cell>121.1</cell><cell>81.0</cell><cell>67.2</cell></row><row><cell></cell><cell cols="3">w/o Persp. correct.* 116.1</cell><cell>79.4</cell><cell>58.6</cell><cell>123.9</cell><cell>83.6</cell><cell>67.3</cell></row><row><cell>Monocular</cell><cell cols="2">Our full model*? Zhou et al. [83]? Bo et al. [8]*?</cell><cell>77.9 --</cell><cell>38.4 --</cell><cell>30.8 --</cell><cell>89.1 --</cell><cell>48.2 -54.8 ?</cell><cell>33.5 34.2 -</cell></row><row><cell></cell><cell cols="2">Yasin et al. [76]*?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.2</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Bogo et al. [9]?</cell><cell>-</cell><cell>-</cell><cell>82.1</cell><cell>-</cell><cell>-</cell><cell>73.3</cell></row><row><cell></cell><cell cols="2">Akhter et al. [2]</cell><cell>-</cell><cell>-</cell><cell>165.5</cell><cell>-</cell><cell>-</cell><cell>186.1</cell></row><row><cell></cell><cell cols="2">Ramakris. et al. [48]</cell><cell>-</cell><cell>-</cell><cell>151.0</cell><cell>-</cell><cell>-</cell><cell>161.8</cell></row><row><cell>Mulit-view</cell><cell cols="2">Amin et al. [3] Rhodin et al. [50] Elhayek et al. [18]</cell><cell>47.7 59.7 60.0</cell><cell>---</cell><cell>---</cell><cell>54.5 74.9 66.5</cell><cell>---</cell><cell>---</cell></row><row><cell cols="5">Table 2. Results of our 2DPoseNet on MPII Single Person Pose</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">[4] dataset and LSP [30] 2D Pose datasets. * = Trained/Finetuned</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">only on the corresponding training set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MPII</cell><cell cols="2">LSP</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">PCK h0.5 AUC PCK 0.2 AUC</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Our 2DPoseNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">w Person Locali. 89.7 61.3 91.2 65.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">w/o Person Locali. 89.6 61.5 91.2 65.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Stacked Hourgl.[42] 90.9* 62.9*</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Bulat et al.[11]</cell><cell cols="2">89.7* 59.6* 90.7</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Wei et al.[74]</cell><cell cols="3">88.5 61.4 90.5 65.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DeeperCut [25]</cell><cell cols="3">88.5 60.8 90.1 66.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Gkioxary et al [22]</cell><cell>86.1* 57.3*</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Lifshitz et al. [37]</cell><cell cols="2">85.0 56.8 84.2</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Belagiannis et al.[7] 83.9* 55.5* 85.1</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DeepCut[45]</cell><cell cols="3">82.4 56.5 87.1 63.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Hu&amp;Ramanan [24]</cell><cell>82.4* 51.1*</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Carreira et al. [12]</cell><cell cols="2">81.3* 49.1* 72.5*</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Loss weight and learning rate, LR, taper scheme used for 2DPoseNet. 2DPoseNet also employs Multi-level Corrective Skip connections, and the heatmap Hsum is the sum of Hdeep and the skip connections. Heatmaps H4b20 and H5a are used for intermediate supervision.</figDesc><table><row><cell>Base # Iter LR</cell><cell cols="3">Loss Weights (w ? L(H xx )) H sum H deep H 4b20 H 5a</cell></row><row><cell cols="2">0.050 60k 1.0 0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell cols="2">0.010 60k 1.0 0.4</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell cols="2">0.005 60k 1.0 0.2</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell cols="2">0.001 60k 1.0 0.2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Loss weight and LR taper scheme used for 3DPoseNet. There is a difference in the number of iterations used when training with Human3.6m or MPI-INF-3DHP alone, v.s. when training with the two in conjunction. Part Labels P L are used only when training with H3.6m solely. Multi-level skip connections add up with X deep to yield Xsum, where X is P or O1 O2. Loss weight and LR taper scheme used for fine tuning 3DPoseNet for Multi-modal Fusion scheme.</figDesc><table><row><cell></cell><cell cols="2">H3.6m/Our H3.6m+Our</cell><cell></cell><cell cols="3">Loss Weights (w ? L(A bb ))</cell></row><row><cell cols="3">Base Batch = 5 Batch = 6</cell><cell cols="3">X = P/O1/O2</cell></row><row><cell>LR</cell><cell>#Epochs</cell><cell cols="5">#Epochs X 4b5 X 4b20 X deep X sum H P L*</cell></row><row><cell>0.05</cell><cell>3 (45k)</cell><cell cols="2">2.4 (60k) 50</cell><cell>50</cell><cell>50 100</cell><cell>0.1 0.05</cell></row><row><cell>0.01</cell><cell>1 (15k)</cell><cell cols="2">1.2 (30k) 10</cell><cell>10</cell><cell cols="2">10 100 0.05 0.025</cell></row><row><cell cols="2">0.005 2 (30k)</cell><cell>1.2 (30k)</cell><cell>5</cell><cell>5</cell><cell cols="2">5 100 0.01 0.005</cell></row><row><cell cols="2">0.001 1 (15k)</cell><cell>0.6 (15k)</cell><cell>1</cell><cell>1</cell><cell cols="2">1 100 0.01 0.005</cell></row><row><cell>5e-4</cell><cell>2 (30k)</cell><cell cols="5">1.2 (30k) 0.5 0.5 0.5 100 0.005 0.001</cell></row><row><cell>1e-4</cell><cell>1 (15k)</cell><cell cols="5">0.6 (15k) 0.1 0.1 0.1 100 0.005 0.001</cell></row><row><cell></cell><cell cols="3">H3.6m/Our H3.6m+Our</cell><cell></cell><cell></cell></row><row><cell cols="7">Base Batch = 5 Batch = 6 Loss Weights (w ? L(A bb ))</cell></row><row><cell>LR</cell><cell>#Epochs</cell><cell cols="2">#Epochs</cell><cell></cell><cell>P fused</cell></row><row><cell>0.05</cell><cell>(1k)</cell><cell>(2k)</cell><cell></cell><cell></cell><cell>100</cell></row><row><cell>0.01</cell><cell>1 (15k)</cell><cell cols="2">0.8 (20k)</cell><cell></cell><cell>100</cell></row><row><cell cols="2">0.005 1 (15k)</cell><cell cols="2">0.8 (20k)</cell><cell></cell><cell>100</cell></row><row><cell cols="2">0.001 1 (15k)</cell><cell cols="2">0.8 (20k)</cell><cell></cell><cell>100</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">MPI-INF-3DHP dataset available at gvv.mpi-inf.mpg.de/ 3dhp-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. Transfer LearningWe use the features learned with Resnet-101 from Im-ageNet<ref type="bibr" target="#b54">[54]</ref> to initialize both 2DPoseNet and 3DPoseNet,</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering 3d human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiview pictorial structures for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A data-driven approach for real-time full body pose reconstruction from a depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detailed human shape and pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Haussecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02914</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Twin gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation via Deep Learning from 2D Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance animation from lowdimensional control signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="686" to="696" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MARCOnI -ConvNet-based MARker-less Motion Capture in Outdoor and Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient ConvNet-based marker-less motion capture in general scenes with a low number of cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimization and filtering for human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeep: A deep learning framework using motion features for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
		<meeting>the 22nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<idno type="DOI">10.5244/C.24.12.1</idno>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Nonparametric Bayesian Network Prior of Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Monocular model-based 3D tracking of rigid objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Live intrinsic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recovering 3d human body configurations using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thorm?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3178" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2337" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ego-Cap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">General automatic human shape and motion capture using volumetric contour cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Model-based Outdoor Performance Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3108" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A joint model for 2d and 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3634" to="3641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Single image 3d human pose estimation from noisy observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aleny?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2673" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Covariance scaled sampling for monocular 3d body tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">447</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Model-based multiple view reconstruction of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fast articulated motion tracking using a sums of Gaussians body model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="677" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?rquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05708</idno>
		<title level="m">Fusing 2D Uncertainty and 3D Cues for Monocular Body Pose Estimation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Direct Prediction of 3D Body Poses from Motion Compensated Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">The Captury</title>
		<ptr target="http://www.thecaptury.com/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Monocular 3d tracking of the golf swing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="932" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Convolutional Pose Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pfinder: real-time tracking of the human body</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="780" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A Dual-Source Approach for 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Marker-less 3D Human Motion Capture with Monocular Image Sequence and Height-Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yonghao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Spatio-temporal matching for human detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="62" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">3D shape estimation from 2D landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Geometry Meets Deep Learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04309</idno>
		<title level="m">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
