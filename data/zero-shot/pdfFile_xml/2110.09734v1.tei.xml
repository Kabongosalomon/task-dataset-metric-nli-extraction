<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mask-aware IoU for Anchor Assignment in Real-time Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
							<email>kemal.oksuz@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Can</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cam</forename><forename type="middle">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fehmi</forename><surname>Kahraman</surname></persName>
							<email>fehmi.kahraman_01@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Sonat Baltaci</surname></persName>
							<email>sonat.baltaci@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Kalkan</surname></persName>
							<email>skalkan@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
							<email>eakbas@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering Middle</orgName>
								<orgName type="institution">East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mask-aware IoU for Anchor Assignment in Real-time Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>OKSUZ ET AL.: MASK-AWARE INTERSECTION-OVER-UNION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents Mask-aware Intersection-over-Union (maIoU) for assigning anchor boxes as positives and negatives during training of instance segmentation methods. Unlike conventional IoU or its variants, which only considers the proximity of two boxes; maIoU consistently measures the proximity of an anchor box with not only a ground truth box but also its associated ground truth mask. Thus, additionally considering the mask, which, in fact, represents the shape of the object, maIoU enables a more accurate supervision during training. We present the effectiveness of maIoU on a state-of-the-art (SOTA) assigner, ATSS, by replacing IoU operation by our maIoU and training YOLACT, a SOTA real-time instance segmentation method. Using ATSS with maIoU consistently outperforms (i) ATSS with IoU by ? 1 mask AP, (ii) baseline YOLACT with fixed IoU threshold assigner by ? 2 mask AP over different image sizes and (iii) decreases the inference time by 25% owing to using less anchors. Then, exploiting this efficiency, we devise maYOLACT, a faster and +6 AP more accurate detector than YOLACT. Our best model achieves 37.7 mask AP at 25 fps on COCO test-dev establishing a new state-of-the-art for real-time instance segmentation. Code is available at https://github.com/kemaloksuz/Mask-aware-IoU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Instance segmentation is a visual detection problem which aims to classify and locate each object in an image by pixel-level masks. To be able to handle objects of different numbers, locations and scales; SOTA methods <ref type="bibr" target="#b5">[3,</ref><ref type="bibr" target="#b16">14,</ref><ref type="bibr" target="#b37">35</ref>] employ a dense set of object hypotheses, generally represented by boxes or points, and ensure a maximum coverage of the objects. This coverage necessitates a large number of object hypotheses (? 20k per image in YOLACT <ref type="bibr" target="#b5">[3]</ref> for images of size 550 ? 550) to be assigned to ground truths boxes; generally known as the assignment problem <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b44">42]</ref>.</p><p>This assignment problem is commonly tackled by employing heuristic rules. One common rule to assign object hypotheses represented by boxes (i.e. anchors) is using a "fixed IoU threshold" <ref type="bibr" target="#b5">[3,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b16">14]</ref>, in which an anchor,B, can be assigned with a ground truth (i.e positive), B, when their Intersection-over-Union (IoU), defined as IoU(B, B) = |B ? B|/|B ? B|, exceeds a pre-determined threshold, ?. If anchorB cannot be assigned to any ground-truth Anchors with high IoU but missing significant object parts <ref type="figure">Figure 1</ref>: Sample cases illustrating the need for mask-aware IoU (maIoU). Green boxes denote ground truth, red boxes are real anchors produced during the training. Left panel shows cases where the anchor covers a significant part of the object pixels but IoU is low (i.e. less than positive threshold of 0.50 for YOLACT). maIoU is higher than IoU for these cases, potentially correcting the assignment. Right panel shows cases where the anchor covers only a small part of the object pixels but IoU is high (so, anchors are positive). maIoU is lower than IoU, potentially correcting the assignment. Images are from COCO <ref type="bibr" target="#b21">[19]</ref>.</p><p>B (i.e. IoU(B, B) &lt; ?, ?B), thenB is assumed to be a background (i.e. negative) example. A different set of recent methods showed for object detection <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b44">42]</ref> that assigning the anchors using an "adaptive IoU threshold" determined for each ground truth improves the performance. Still, assignment methods heavily rely on IoU as the de facto proximity measure between ground truths and anchors.</p><p>Despite its popularity, IoU has a certain drawback: The IoU between an anchor box and a ground truth box solely depends on their areas, thereby ignoring the shape of the object, e.g. as provided by a segmentation mask. This may give rise to undesirable assignments due to counter-intuitively lower or higher IoU scores. For example, the IoU might be high, implying a positive anchor, but only a small part of the object is included in the anchor; or the IoU may be low, implying a negative anchor, but a large part of the object is included in the anchor. <ref type="figure">Fig. 1</ref> presents examples for such cases, arising due to objects with unconventional poses, occlusion and objects with articulated or thin parts. We will show (in Section 3.1, <ref type="figure" target="#fig_2">Fig.  3</ref>) that such examples tend to produce larger loss values and adversely affect training.</p><p>In this paper, we introduce mask-aware IoU (maIoU), a novel IoU measure for anchor assignment in instance segmentation by exploiting the ground truth masks of the objects, normally used only for supervision. Specifically, unlike IoU, which equally weights all pixels, maIoU yields a proximity measure between 0 and 1 among an anchor box, a ground truth box and a ground truth mask by promoting the pixels on the masks, thereby providing a more consistent assignment ( <ref type="figure">Fig. 1</ref>). Since a naive computation of maIoU is impractical, we present an efficient algorithm with training time similar to the baseline. YOLACT with maIoU-based ATSS assigner consistently improves ATSS assigner with IoU by ? 1 mask AP and standard YOLACT (i.e. fixed IoU threshold) by ? 2 mask AP, and also decreases inference time of YOLACT. Finally, utilizing this efficiency gap, we build maYOLACT detector reaching 37.7 mask AP at 25 fps and outperforming all real-time counterparts. <ref type="table">Table 1</ref>: IoU Variants, their inputs and primary purposes. IoU variants assign a proximity measure based on the properties (prop.) of two inputs (Input 1 and Input 2). In practice, existing variants compare the inputs wrt. the same properties (i.e. either boxes or masks). Our Mask-aware IoU (maIoU) can uniquely compare a box with a box and a mask. With this, maIoU compares anchors (i.e. only box) with ground truths (box and mask) in order to provide better anchor assignment. *: GIoU is also used as a performance measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IoU Variant</head><p>Input 1 prop. Input 2 prop. Primary purpose as Box Mask Box Mask proposed in the paper Mask IoU <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b21">19]</ref> Performance measure Boundary IoU <ref type="bibr" target="#b9">[7]</ref> Performance measure Generalized IoU <ref type="bibr" target="#b31">[29]</ref> Loss function* Distance IoU <ref type="bibr" target="#b45">[43]</ref> Loss function Complete IoU <ref type="bibr" target="#b46">[44]</ref> Loss function Mask-aware IoU (Ours) Assigner</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep Instance Segmentation. In general, deep instance segmentation methods have followed object detection literature. The pioneering Mask R-CNN model <ref type="bibr" target="#b16">[14]</ref> and its variations <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b28">26]</ref> extended Faster R-CNN <ref type="bibr" target="#b30">[28]</ref> by incorporating a mask prediction branch into the twostage detection pipeline. Similarly, anchor-based one-stage methods were also adapted for instance segmentation by using an additional mask prediction branch, e.g. YOLACT <ref type="bibr" target="#b5">[3]</ref> and YOLACT++ <ref type="bibr" target="#b6">[4]</ref> employed a YOLO-like architecture; and PolarMask <ref type="bibr" target="#b41">[39]</ref> and PolarMask++ <ref type="bibr" target="#b42">[40]</ref>, both anchor-free methods, adapted FCOS <ref type="bibr" target="#b32">[30]</ref> for instance segmentation. Differently, SOLO variants <ref type="bibr" target="#b36">[34,</ref><ref type="bibr" target="#b37">35]</ref> classify the pixels based on location and size of each instance. Anchor Assignment in Instance Segmentation. Deep instance segmentation methods using anchors as object hypotheses label anchors based on "fixed IoU threshold" assignment rule: The anchors with IoU larger than ? + with a ground truth box are assigned as positive; while the anchors whose maximum IoU with ground truths is less than ? ? are assigned as negatives; and the remaining anchors whose maximum IoU is between ? ? and ? + are simply ignored during training. To illustrate, YOLACT variants <ref type="bibr" target="#b5">[3,</ref><ref type="bibr" target="#b6">4]</ref> and RetinaMask <ref type="bibr" target="#b13">[11]</ref> use ? ? = 0.40 and ? + = 0.50; while the first stage of Mask R-CNN (i.e. region proposal network <ref type="bibr" target="#b30">[28]</ref>) sets ? ? = 0.30 and ? + = 0.70; and finally its second stage <ref type="bibr" target="#b16">[14]</ref> uses ? ? = ? + = 0.50. Adaptive Anchor Assignment Methods in Object Detection. Recently, adaptive anchor assignment strategies are shown to perform better than fixed IoU threshold in object detection: ATSS <ref type="bibr" target="#b44">[42]</ref> uses top-k anchors wrt. IoU to determine an adaptive IoU threshold for each ground truth (Section 3.4 provides more details on ATSS.) and PAA <ref type="bibr" target="#b19">[17]</ref> computes a score of each anchor including Generalized IoU and fits the distribution of these scores to a two-dimensional Gaussian Mixture Model to split positives and negatives for each ground truth. Similarly, Ke et al. <ref type="bibr" target="#b18">[16]</ref> and Li et al. <ref type="bibr" target="#b20">[18]</ref> identify positives and negatives by using different scoring functions of the predictions. However, these methods are devised and tested for object detection, and thus, do not utilize object masks.</p><p>Other IoU Variants. Over the years, many IoU variants have been proposed -see <ref type="table">Table  1</ref> for a comparative summary. One of the most related IoU variants is Mask IoU, which is used to measure the detection mask quality with respect to (wrt.) the ground truth mask during evaluation <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b21">19]</ref>. Similarly, Boundary IoU <ref type="bibr" target="#b9">[7]</ref> evaluates detection masks by giving higher weights to the pixels closer to the boundaries. Note that these IoU variants compare only two masks, and unlike our maIoU, they cannot compare a box with another box and its associated mask. The other IoU variants are all devised to measure the proximity of two boxes: Generalized IoU (GIoU) <ref type="bibr" target="#b31">[29]</ref> uses the minimum enclosing box in order to measure the proximity of boxes when boxes do not intersect (i.e. their IoU is 0); Distance IoU <ref type="bibr" target="#b45">[43]</ref> adds a penalty parameter based on the minimum enclosing box and the distance between the centers of boxes; Complete IoU <ref type="bibr" target="#b46">[44]</ref> additionally considers aspect ratio differences of the boxes. These IoU-variants compute the overlap at the box level and neglect object shape; and also, they are mainly used as loss functions, not as a positive-negative assignment criterion. Comparative Summary. By measuring the proximity of an anchor box with a ground truth, our maIoU is designed for anchor-based models, which have been using a fixed IoU threshold for assigning anchors as the dominant approach, thereby ignoring object shape. We first show that ATSS <ref type="bibr" target="#b44">[42]</ref>, an adaptive anchor assigner, yields better performance on YOLACT <ref type="bibr" target="#b5">[3]</ref>. Then, we propose maIoU, as the first IoU variant that can measure the proximity of an anchor box with a ground truth box and ground truth mask <ref type="table">(Table 1)</ref>. Replacing IoU of ATSS by our maIoU improves the performance of this strong baseline. We also investigate GIoU and DIoU for anchor assignment. Since they rely only on boxes, our maIoU provides more discriminative information then these IoU variants. Finally, besides our maIoU, adopting recently proposed improvements into YOLACT detector, we build maYOLACT detector, which outperforms its counterparts while being more efficient as well (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section first presents an analysis on fixed-threshold IoU assigner in Section 3.1. Then, Section 3.2 defines maIoU and Section 3.3 provides an efficient algorithm to compute maIoU. Finally, Section 3.4 incorporates our maIoU into the SOTA ATSS assigner <ref type="bibr" target="#b44">[42]</ref> to label anchors during the training of instance segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Mask-over-box Ratio</head><p>Our analysis is based on an intuitive measure to represent the rate of the ground truth mask in a box, defined as the Mask-over-box (MOB) ratio as follows.</p><p>Definition 1. Mask-over-box (MOB) ratio of a box (i.e. anchor or ground truth),B, on a ground truth mask, M, is the ratio of (i) the area of the intersection of the mask and the box, and (ii) the area ofB itself:</p><formula xml:id="formula_0">MOB(B, M) = |B ? M| |B| .</formula><p>( We now analyse how losses on a trained standard YOLACT model (with ResNet-50 backbone), a SOTA instance segmentation method, and the number of anchors change wrt. MOB ratio <ref type="figure" target="#fig_0">(Fig. 2)</ref> and make the following crucial observations:</p><p>(1) The loss value (i.e. error) of a trained YOLACT for an anchor is related to the amount of mask pixels covered by that anchor (i.e. MOB ratio), which is ignored by the standard fixed IoU threshold assigner. <ref type="figure" target="#fig_0">Fig. 2(a,b)</ref> present that the average and standard deviation of the loss values of anchors that are close to the IoU assignment threshold (i.e. the anchors with IoU between [0.30 ? 0.50] for negatives and [0.50 ? 0.70] for positives; hence, anchors with similar IoUs) increase in all tasks (for negatives, it is only classification; for positives, we look at all tasks -i.e. classification, box regression and segmentation) as MOB ratio decreases/increases (i.e. implying covering less/more on-mask pixels) for positive/negative anchors. Also, the numbers of anchors with larger losses are in the order of thousands in all cases <ref type="figure" target="#fig_0">(Fig. 2(c)</ref>).</p><p>(2) Similar to anchors, MOB ratios of the ground truth boxes also vary significantly. We observe in <ref type="figure" target="#fig_2">Fig. 3(a)</ref> that there exists a significant amount of ground truth boxes with low MOB ratios (i.e. for 30 % of the ground truths, MOB ratio is less than 0.50.</p><p>Our observations suggest that IoU does not discriminate anchors with more on-mask pixels from those with less on-mask pixels, which appears naturally due to varying MOB ratios of the ground truths; and thus using IoU for the assignment of the anchors may not be the best method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mask-aware Intersection-over-Union (maIoU)</head><p>Intuition. The main intuition behind mask-aware IoU (maIoU) is to reweigh the pixels within the ground truth box, B, such that on-mask pixels are promoted (in a way, the contribution of off-mask pixels are reduced) by preserving the total energy of B (i.e. |B|). We simply achieve this by distributing the contributions of the off-mask pixels uniformly over the on-mask pixels in B. Finally, maIoU is computed as an intersection-over-union between B (i.e. anchor box) and B with the new pixel weights in B ( <ref type="figure" target="#fig_2">Fig. 3(b)</ref>). Derivation. To facilitate derivation, we first reformulate intersection I between B andB in a weighted form as follows (w m , w m : the contributions of on-mask and off-mask pixels respectively):</p><formula xml:id="formula_1">I(B,B) = ? i ? B?B w = ? i ?B?M w m + ? i ? (B?B?B?M) w m ,<label>(2)</label></formula><p>which effectively does not make use of the mask M since w = w m = w m = 1 for IoU. In maIoU, we discard the contribution of an off-mask pixel: w m = 0, and in order to preserve the total energy, the reduced contribution from all off-mask pixels, which equals |B| ? |M|, is distributed to the on-mask pixels uniformly. This will increase w m by (|B| ? |M|)/|M|: w m = 1 + (|B| ? |M|)/|M| = 1 + |B|/|M| ? |M|/|M| = |B|/|M| = 1/MOB(B, M). which is equal to the conventional union. This is not surprising since our formulation preserves ground truth area (i.e. |B|). With the updated definitions, mask-aware IoU is simply mask-aware intersection over mask-aware union:</p><formula xml:id="formula_2">maIoU(B, B, M) = maI(B, B, M) maU(B, B, M) = 1 MOB(B, M) |B ? M| |B ? B| ,<label>(5)</label></formula><p>which, in effect, is the ratio of covered on-mask pixels by the anchor (|B ? M|) in the union of boxes (|B ? B|), normalized by on-mask pixel density in B (i.e <ref type="figure">. MOB(B, M)</ref>).</p><p>Interpretation. Similar to IoU, maIoU(B, B, M) ? [0, 1] and a larger value implies a better localisation considering not only the boxes but also the ground truth mask ( <ref type="figure" target="#fig_2">Fig. 3(b)</ref>). We visualize the anchor count distribution (in log-scale) of YOLACT on COCO minival on the space spanned by IoU and maIoU ( <ref type="figure" target="#fig_2">Fig. 3(c)</ref>): While IoU and maIoU are positively correlated, there are quite a number of examples with low-IoU &amp; high-maIoU and vice versa, hence the assignment rules based on maIoU is quite different than those based on IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computation of maIoU</head><p>Compared to IoU, computing maIoU involves two additional terms (Eq. 5): (i) |M|, the total number of mask-pixels (since calculating |B| for MOB(B, M)=|M|/|B| is trivial), and (ii) |B ? M|, the number of mask-pixels in the intersection. While the masks are included in the datasets and computing these two quantities is straightforward; it is impractical to compute them naively (i.e. brute force, see <ref type="table" target="#tab_2">Table 3</ref>) considering the large number of anchors covering the image. For this reason, we employ integral images <ref type="bibr" target="#b11">[9]</ref> on the binary ground truth masks, M. More specifically, for M covering an image of size m ? n, we compute its integral image ? M , an (m + 1) ? (n + 1) matrix that encodes the total number of mask pixels above and to the left of each pixel. Accordingly, denoting the (i, j) th element of ? M by ? M i, j , the last element of ? M stores |M|, i.e. |M| = ? M m+1,n+1 . As for the second term, assumingB is represented by a top-left point (x 1 , y 1 ) and a bottom-right point (x 2 , y 2 ) such that x 2 &gt; x 1 and y 2 &gt; y 1 , |B ? M| involves only four look-up operations, i.e. </p><formula xml:id="formula_3">|B ? M| = ? M x 2 +1,y 2 +1 + ? M x 1 ,y 1 ? ? M x 2 +1,y 1 ? ? M x 1 ,</formula><formula xml:id="formula_4">Set |M| = ? M m+1,n+1 and |B ? M| = ? M x 2 +1,y 2 +1 + ? M x 1 ,y 1 ? ? M x 2 +1,y 1 ? ? M x 1 ,y 2 +1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Compute MOB(B, M) = |M|/|B| for ground truth B</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>return maIoU(B, B, M) (Eq. 5) 6: end procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Incorporating maIoU into ATSS Assigner</head><p>ATSS assigner <ref type="bibr" target="#b44">[42]</ref> is a SOTA assignment method used for object detection, yielding better performance than a fixed-threshold IoU assigner and simplifying the anchor design by using a single anchor per pixel unlike its predecessors with up to nine anchors per pixel <ref type="bibr" target="#b23">[21]</ref>. ATSS assigner comprises three steps: (i) selecting top-k (i.e. conventionally k = 9) anchors (B) wrt. the distance of the centers between B andB for each ground truth (B) on each FPN level as "candidates", (ii) filtering out the candidates using an adaptive IoU threshold, computed based on the statistics of these candidates for each B, and (iii) filtering out the candidates, whose centers lie out of B. The surviving candidates after steps (ii) and (iii) are the positive examples and the remaining examples are the negatives. Using our maIoU with ATSS (or any IoU-based assigner) is straightforward: In step (ii), we just replace IoU-based adaptive thresholding by maIoU-based adaptive thresholding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset. We train all models on the COCO trainval set <ref type="bibr" target="#b21">[19]</ref> (115K images), test them on the COCO minival set (5k images) unless otherwise stated. Performance Measures. We mainly report AP-based performance metrics: COCO-style AP (AP, in short), APs where true positives are validated from IoUs of 0.50 and 0.75 (AP 50 and AP 75 ), and APs for small, medium and large objects (AP S , AP M and AP L respectively). Furthermore, we also exploit the recent optimal Localisation Recall Precision (oLRP) Error <ref type="bibr" target="#b26">[24,</ref><ref type="bibr" target="#b27">25]</ref>. While AP is a higher-is-better measure, oLRP is a lower-is-better metric. Implementation Details. We conduct our experiments on YOLACT <ref type="bibr" target="#b5">[3]</ref>, an anchor-based real-time instance segmentation method, using the mmdetection framework <ref type="bibr">[5]</ref>. Following Zhang et al. <ref type="bibr" target="#b44">[42]</ref>, when we use ATSS assigner, we keep k = 9 (Section 3.4) and simplify the anchor configuration by placing a single anchor on each pixel with an aspect ratio of 1 : 1 and a base scale of 4 unless otherwise stated. Also, with ATSS (with IoU, DIoU <ref type="bibr" target="#b45">[43]</ref>, GIoU <ref type="bibr" target="#b31">[29]</ref> or our maIoU), we keep classification and box regression loss weights as they are (1.0 and 1.5 respectively), and increase mask prediction loss weight from 6.125 to 8. When we replace the assigner, note that it affects the examples in all branches (i.e. classification, box regression, semantic and instance mask predictions). We train all models with 32 images distributed on 4 GPUs (8 images/GPU). The remaining design choices of YOLACT <ref type="bibr" target="#b5">[3]</ref> are kept. We adopt ResNet-50 <ref type="bibr" target="#b15">[13]</ref> as the backbone and resize the images during training and inference to S ? S where S can be either 400, 550 or 700 following Bolya et al. <ref type="bibr" target="#b5">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Experiments</head><p>In this section, we demonstrate that our maIoU improves upon other assigners based on IoU variants consistently and our Alg. 1 makes computation of maIoU feasible during training.</p><p>Using ATSS with IoU. We first replace the fixed IoU threshold assigner of YOLACT by ATSS with (w.) IoU and have a stronger baseline for our maIoU: ATSS w. IoU improves fixed IoU assigner by 0.5 ? 1.1 mask AP over different scales <ref type="table" target="#tab_1">(Table 2)</ref>.</p><p>Replacing IoU of ATSS with maIoU. Replacing IoU in ATSS by our maIoU ( <ref type="table" target="#tab_1">Table 2</ref>) (i) improves fixed IoU assigner by 1.3, 1.9 and 2.1 mask APs for 400, 550 and 700 scales respectively, (ii) outperforms ATSS w. IoU by ? 1.0 mask AP in all scales, (iii) performs also better than other IoU variants (i.e. ATSS w. GIoU and ATSS w. DIoU). We note that the contribution of maIoU (i) on models trained by images with larger scales (700 vs. 400 in <ref type="table" target="#tab_1">Table 2</ref>) and (ii) on larger objects (AP L vs. AP S ) are more significant. This is intuitive since the shape of the object gets more precise as the object gets larger.</p><p>Computing maIoU Efficiently. Computing maIoU for every anchor-ground truth pair during training by brute force is infeasible, i.e. it would take ? 3 months to train a single model with 41.89 sec/iteration <ref type="table" target="#tab_2">(Table 3</ref>). Using Alg. 1, we reduce the average iteration time by  ? 70? to 0.59 sec/iteration, which is similar to other standard assigners ( <ref type="table" target="#tab_2">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">maYOLACT Detector: Faster and Stronger</head><p>Thanks to using fewer number of anchors, YOLACT trained with our ATSS w. maIoU assigner (underlined in <ref type="table" target="#tab_3">Table 4</ref>) is ? 25% faster than baseline YOLACT (33 vs. 27 fps 1 ), pointing out the importance of anchor design for the efficiency of real-time systems as well 2 . Exploiting this run-time gap; our aim in this section is to extend the standard YOLACT using our maIoU and the recent improvements in order to make it competitive with the recent methods also by keeping the resulting detector to process images in real-time <ref type="bibr" target="#b5">3</ref> . To achieve that, we use (i) carafe <ref type="bibr" target="#b34">[32]</ref> as the upsampling operation of FPN <ref type="bibr" target="#b22">[20]</ref>, (ii) deformable convolutions <ref type="bibr" target="#b48">[46]</ref> in the backbone, (ii) two anchors with base scales 4 and 8 on each pixel, and (iv) cosine annealing with an initial learning rate of 0.008 by replacing the step learning rate decay. Effect of these improvements are presented in <ref type="table" target="#tab_3">Table 4</ref> and the resulting detector with these improvements is coined as maYOLACT. Note that our maYOLACT-550 detector is still faster than baseline YOLACT-550 and improves it by +6.3 mask AP and +7.2 box AP reaching 34.8 mask AP and 37.9 box AP <ref type="table" target="#tab_3">(Table 4</ref>). <ref type="table">Table 5</ref> compares our maYOLACT with state-of-the-art methods on COCO test-dev. Comparison with YOLACT variants. Achieving 35.2 mask AP, our maYOLACT-550 outperforms all YOLACT variants including the ones with larger backbones (e.g. YOLACT-550++ with ResNet-101) and larger scales (e.g. YOLACT-700). Besides, different from YOLACT++ <ref type="bibr" target="#b6">[4]</ref>, which is ? 25% slower than YOLACT (see <ref type="table">Table 6</ref> in Bolya et al. <ref type="bibr" target="#b6">[4]</ref>), our maYOLACT-550 is faster than YOLACT-550 <ref type="table" target="#tab_3">(Table 4)</ref>, and still keep 7 mask AP gain also on COCO test-dev reaching 35.2 mask AP ( <ref type="table">Table 5</ref>). Comparison with real-time methods. Without multi-scale training as in Solov2 <ref type="bibr" target="#b37">[35]</ref> or specially designed backbone as in CenterMask <ref type="bibr" target="#b38">[36]</ref>; our maYOLACT-700 reaches 37.7 mask AP at 25fps and outperforms existing real-time counterparts. Besides, our best model <ref type="table">Table 5</ref>: Comparison with SOTA on COCO test-dev. Our maYOLACT-700 establishes a new SOTA for real-time instance segmentation. * implies our implementation for YOLACT with ATSS w.IoU. When a paper does not report a performance measure, N/A is assigned and we reproduce the performance using its repository for completeness (shown by ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-art (SOTA)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Backbone AP AP 50 AP 75 AP S AP M AP L Reference fps &lt; 25</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented maIoU to assign a proximity value for an anchor compared to both a ground truth box and its mask. Using maIoU to assign anchors as positive or negative for training instance segmentation methods, we utilised the shape of objects as provided by the groundtruth segmentation masks. We showed that ATSS with our maIoU also improves throughput of the model. Exploiting this efficiency, we improved the performance further and reached SOTA results in real-time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a,b) Mean and standard deviation of loss values of negative anchors (a) and positive (b) anchors with similar IoUs (IoU between [0.30 ? 0.50] for negatives and [0.50 ? 0.70] for positives) over different MOB ratios for a trained YOLACT on COCO minival. Red lines denote the standard deviation. Note that when the MOB ratio increases, the loss values increase for negatives; however, the loss values of all three sub-tasks (Cls.: classification, Segm.: segmentation, Reg.: regression) tend to decrease for positives. (c) The number of anchors for each MOB ratio is in the order of thousands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1) MOB(B, M) ? [0, 1] and less object pixels from M inB implies a lower MOB ratio. When B is the ground truth box of M (i.e.B = B), |M ? B| = |M|, and thus MOB(B, M) = |M|/|B|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) The distribution of MOB ratios of the ground truths on COCO training set. (b) How IoU and maIoU weights the pixels in ground truth box (see Fig. 1 top-left example for the image of this example). While IoU does not differentiate among on-mask (w m ) and off-mask (w m ) pixels, our maIoU sets w m = 0 and weights each on-mask pixel by inverse MOB ratio considering object mask M. (c) Anchor count distribution (in log-scale) of IoU vs maIoU. While IoU &amp; high-maIoU positively correlate, there are quite a few examples with low-IoU &amp; high-maIoU and vice versa. With these weights, the mask-aware intersection, maI is defined by extending Eq. 2: maI(B, B, M) = ? i ?B?M w m + : 0 ? i ? (B?B?B?M) w m = w m |B ? M| = 1 MOB(B, M) |B ? M|. (3) Extending the definition of union (i.e. |B| + |B| ? |B ?B|) with this intersection concept: maU(B, B, M) = |B| + (|B| ? |B ? B| + maI(B, B, M)) ? maI(B, B, M) = |B ? B|, (4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>y 2 +1 . The overall algorithm to compute maIoU(B, B, M) is presented in Alg. 1. Algorithm 1 The algorithm for efficiently calculating mask-Aware IoU. 1: procedure MASKAWAREIOU(B, B, M) 2: Compute |B|, |B ? B| and ? M as integral image of M such that ? M i, j is (i, j) th element of ? M 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different assigners and IoU variants on YOLACT. Considering the shapes of the objects, our ATSS w. our maIoU consistently outperforms its counterparts. AP 50 ? AP 75 ? AP S ? AP M ? AP L ? oLRP ?</figDesc><table><row><cell cols="3">Scale AP ? 400 Assigner fixed IoU threshold 24.8 42.4 ATSS w. IoU 25.3 43.5 ATSS w. DIoU 25.4 43.6</cell><cell>25.0 25.5 25.2</cell><cell>7.3 6.8 7.2</cell><cell>26.0 27.3 27.1</cell><cell>42.0 43.8 43.4</cell><cell>78.3 77.7 77.7</cell></row><row><cell></cell><cell>ATSS w. GIoU</cell><cell>25.1 42.7</cell><cell>25.3</cell><cell>7.0</cell><cell>26.8</cell><cell>41.8</cell><cell>78.0</cell></row><row><cell></cell><cell cols="2">ATSS w. maIoU (Ours) 26.1 44.3</cell><cell>26.3</cell><cell>7.2</cell><cell>28.0</cell><cell>44.3</cell><cell>77.1</cell></row><row><cell></cell><cell>fixed IoU threshold</cell><cell>28.5 47.9</cell><cell>29.4</cell><cell>11.7</cell><cell>31.8</cell><cell>43.0</cell><cell>75.2</cell></row><row><cell></cell><cell>ATSS w. IoU</cell><cell>29.3 49.2</cell><cell>30.2</cell><cell>11.1</cell><cell>33.0</cell><cell>44.5</cell><cell>74.5</cell></row><row><cell>550</cell><cell>ATSS w. DIoU</cell><cell>29.5 49.5</cell><cell>30.1</cell><cell>11.7</cell><cell>33.2</cell><cell>44.9</cell><cell>74.4</cell></row><row><cell></cell><cell>ATSS w. GIoU</cell><cell>29.1 48.6</cell><cell>30.0</cell><cell>12.0</cell><cell>32.2</cell><cell>43.3</cell><cell>74.7</cell></row><row><cell></cell><cell cols="2">ATSS w. maIoU (Ours) 30.4 50.3</cell><cell>31.4</cell><cell>11.5</cell><cell>33.9</cell><cell>46.3</cell><cell>73.7</cell></row><row><cell></cell><cell>fixed IoU threshold</cell><cell>29.7 50.0</cell><cell>30.4</cell><cell>14.2</cell><cell>32.8</cell><cell>43.7</cell><cell>74.3</cell></row><row><cell></cell><cell>ATSS w. IoU</cell><cell>30.8 51.8</cell><cell>31.2</cell><cell>14.1</cell><cell>35.0</cell><cell>44.0</cell><cell>73.3</cell></row><row><cell>700</cell><cell>ATSS w. DIoU</cell><cell>30.9 51.9</cell><cell>31.7</cell><cell>14.0</cell><cell>35.4</cell><cell>44.0</cell><cell>73.3</cell></row><row><cell></cell><cell>ATSS w. GIoU</cell><cell>30.1 50.7</cell><cell>31.0</cell><cell>14.0</cell><cell>33.8</cell><cell>43.1</cell><cell>74.0</cell></row><row><cell></cell><cell cols="2">ATSS w. maIoU (Ours) 31.8 52.8</cell><cell>32.8</cell><cell>14.7</cell><cell>35.6</cell><cell>45.7</cell><cell>72.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Avg. iteration time (t) of assigners. While brute force maIoU computation is inefficient (Alg. 1 is ); our Alg. 1 decreases t by ? 70? and has similar t with Fixed IoU Thr. and ATSS w. IoU.</figDesc><table><row><cell>Assigner</cell><cell cols="2">Alg. 1 t (sec.)</cell></row><row><cell cols="2">Fixed IoU Thr. N/A</cell><cell>0.51</cell></row><row><cell>ATSS w. IoU</cell><cell>N/A</cell><cell>0.57</cell></row><row><cell>ATSS w. maIoU</cell><cell></cell><cell>41.89</cell></row><row><cell>ATSS w. maIoU</cell><cell></cell><cell>0.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>ATSS w. maIoU (underlined) makes YOLACT more accurate and ? 25% faster mainly owing to less anchors. Thanks to this efficiency, we build maYOLACT-550 with 34.8 AP and still larger fps than YOLACT.</figDesc><table><row><cell></cell><cell>Method</cell><cell>AP AP box fps Anchor #</cell></row><row><cell>maYOLACT-550</cell><cell cols="2">YOLACT-550 + ATSS w. maIoU + Carafe FPN [32] + DCNv2 [46] + more anchors + cosine annealing [22] 34.8 37.9 30 ? 12.8K 28.5 30.7 27 ? 19.2K 30.4 32.5 33 ? 6.4K 31.4 33.3 32 ? 6.4K 33.2 35.8 31 ? 6.4K 33.5 36.3 30 ? 12.8K</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For all models, we follow and report the results on the mmdetection framework[5]  on a single Tesla V100 GPU. Mmdetection's YOLACT is slower than the official implementation by Bolya et al.<ref type="bibr" target="#b5">[3]</ref>, who reported 45fps.<ref type="bibr" target="#b4">2</ref> Note that more efficient models can be obtained by using better anchor design methods [12, 23, 41, 45]<ref type="bibr" target="#b5">3</ref> We use 25fps as the cut-off for "real-time" following the common video signal standards (e.g. PAL<ref type="bibr" target="#b39">[37]</ref> and SECAM<ref type="bibr" target="#b40">[38]</ref>) and existing methods<ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b29">27,</ref><ref type="bibr" target="#b33">31,</ref><ref type="bibr" target="#b35">33]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was supported by the Scientific and Technological Research Council of Turkey (T?B?TAK) (under grants 117E054 and 120E494). We also gratefully acknowledge the computational resources kindly provided by T?B?TAK ULAKBIM High Performance and Grid Computing Center (TRUBA) and Roketsan Missiles Inc. Dr. Kalkan is supported by the BAGEP Award of the Science Academy, Turkey.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>N/A N/A N/A</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>N/A N/A N/A</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">To illustrate, on Tesla V100 GPU, our maYOLACT-700 (i) has ? 2? more throughput with 25fps and nearly 4 mask AP gain</title>
	</analytic>
	<monogr>
		<title level="m">Comparison with other methods. Our maYOLACT is also competitive against slower methods (Table 5): It outperforms PolarMask++</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
	<note>RetinaMask [11], Mask R-CNN [14] and TensorMask [6] while being faster</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="https://github.com/youngwanLEE/CenterMask." />
		<title level="m">PolarMask++ on ResNet-101 with 14 fps test time; and (ii) is ? 8? faster than TensorMask on ResNet-101 (i.e. ? 3fps) with similar performance</title>
		<imprint>
			<date type="published" when="2021-09" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>References [1] Official repository of centermask. Last accessed</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Official repository of solo variants</title>
		<ptr target="https://github.com/WXinlong/SOLO." />
		<imprint>
			<date type="published" when="2021-09" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Last accessed</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Yolact++: Better real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<editor>Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin</editor>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boundary IoU: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Summed-area tables for texture mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="212" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tinier-yolo: A real-time object detection method for constrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiming</forename><surname>Ren</surname></persName>
		</author>
		<idno>doi: 10. 1109/ACCESS.2019.2961959</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive anchor box mechanism to improve the accuracy in the object detection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="27383" to="27402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiple anchor learning for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from noisy anchors for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aabo: Adaptive anchor box optimization for object detection via bayesian sub-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenshuo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingzhong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Localization recall precision (LRP): A new performance metric for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Baris Can Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">One metric to measure them all: Localisation recall precision (lrp) for evaluating visual detection tasks. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Baris Can Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalkan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rank &amp; sort loss for object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Baris Can Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Super-bpd: Super boundary-to-pixel direction for fast image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Carafe: Content-aware reassembly of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00154</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SOLO: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Centermask: Single shot instance segmentation with point representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weik</surname></persName>
		</author>
		<idno type="DOI">10.1007/1-4020-0613-6_13881</idno>
		<ptr target="https://doi.org/10.1007/1-4020-0613-6_13881" />
		<title level="m">Phase Alternation by Line</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1255" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weik</surname></persName>
		</author>
		<idno type="DOI">10.1007/1-4020-0613-6_18872</idno>
		<ptr target="https://doi.org/10.1007/1-4020-0613-6_18872" />
		<title level="m">syst?me electronique couleur avec memoire</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1718" to="1718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Polarmask++: Enhanced polar representation for single-shot instance segmentation and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distance-iou loss: Faster and better learning for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Rongguang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Dongwei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enhancing geometric factors in model learning and inference for object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongguang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Anchor box optimization for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
