<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Fine-grained Image Classification with Generative Adversarial Networks and Facial Landmark Detection*</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Darvish</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Science</orgName>
								<orgName type="institution">University of Mohaghegh Ardabili</orgName>
								<address>
									<settlement>Ardabil</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Pouramini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Science</orgName>
								<orgName type="institution">University of Mohaghegh Ardabili</orgName>
								<address>
									<settlement>Ardabil</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Bahador</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Science</orgName>
								<orgName type="institution">University of Mohaghegh Ardabili</orgName>
								<address>
									<settlement>Ardabil</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Fine-grained Image Classification with Generative Adversarial Networks and Facial Landmark Detection*</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Data augmentation</term>
					<term>Fine-grained classification detection</term>
					<term>Generative adversarial networks</term>
					<term>Landmark detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained classification remains a challenging task because distinguishing categories needs learning complex and local differences. Diversity in the pose, scale, and position of objects in an image makes the problem even more difficult. Although the recent Vision Transformer models achieve high performance, they need an extensive volume of input data. To encounter this problem, we made the best use of GAN-based data augmentation to generate extra dataset instances. Oxford-IIIT Pets was our dataset of choice for this experiment. It consists of 37 breeds of cats and dogs with variations in scale, poses, and lighting, which intensifies the difficulty of the classification task. Furthermore, we enhanced the performance of the recent Generative Adversarial Network (GAN), StyleGAN2-ADA model to generate more realistic images while preventing overfitting to the training set. We did this by training a customized version of MobileNetV2 to predict animal facial landmarks; then, we cropped images accordingly. Lastly, we combined the synthetic images with the original dataset and compared our proposed method with standard GANs augmentation and no augmentation with different subsets of training data. We validated our work by evaluating the accuracy of fine-grained image classification on the recent Vision Transformer (ViT) Model. Code is available at: https://github.com/mahdi-darvish/GAN-augmented-pet-classifier</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, Convolutional Neural Networks and Vision Transformers have grown into the state-of-the-art techniques in computer vision <ref type="bibr" target="#b0">[1]</ref> with remarkable results on famous datasets, namely ImageNet <ref type="bibr" target="#b1">[2]</ref>, JFT-300M <ref type="bibr" target="#b2">[3]</ref>. One of the main factors affecting the performance of mentioned algorithms is a considerable quantity of high-resolution and distinct training instances. On the other hand, collecting such datasets requires expertise in the specialized field also is tedious and costly. Moreover, sometimes it is not feasible to collect a satisfactory number of data; for example, in the case of Fine-Grained Classification <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, which is a sub-field of image recognition <ref type="bibr" target="#b6">[7]</ref>, that tries to differentiate subcategories of a general class. Examples include distinguishing breeds of animals or species of flowers. As a result, several methods have been introduced to tackle these problems. <ref type="bibr">*</ref>This work has been submitted to the IEEE for possible publication. . The generator (G) takes a random noise (Z) as input to build fake images that would look like real ones (X). The discriminator (D) learns to tell the difference between the generator's fake image from actual data and outputs a probability for the image being real.</p><p>Data augmentation techniques have been performed remarkably well as a solution to the problem of limited data <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. These algorithms use various approaches to incorporate new instances from existing training data which are divided into two groups, traditional methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and image generation using Generative Adversarial Networks (GANs) <ref type="bibr" target="#b11">[12]</ref>. Color jittering, flipping, blurring, rotation, and adding a small amount of noise exemplify the former. GANs can produce unseen images with the same statistics as original training data. GANs are capable of training a generative model (automatically finding the patterns from the input data and learning harmonies to produce new examples without supervision) by associating two neural networks, namely, generator and discriminator (see <ref type="figure" target="#fig_0">Figure 1</ref>). The generator model makes a synthetic example from a random vector. The generated images, then, are fed to the discriminator along with authentic images, and the discriminator attempt to identify the fake ones. These two models are trained together until the success rate of the discriminator stabilized at approximately 50%.</p><p>As previously discussed, GANs seem to be an effective method for data augmentation and had exceptional success  on certain datasets, e.g., 100-shot Obama and Panda <ref type="bibr" target="#b9">[10]</ref>. Still, there are difficulties generalizing to other datasets. The mentioned datasets all have similar characteristics, such as objects (faces) are centered and often have limited distribution. However, it is not always the case; most datasets have objects in various poses and sizes, which leads to poor convergence of GAN models <ref type="bibr" target="#b12">[13]</ref>. As a result, the produced images might have issues with abnormal colors and distribution shifts. <ref type="bibr" target="#b13">[14]</ref> In other cases, when the size of the training set is not satisfactory, the loss of the discriminator falls rapidly. Still, the validation loss decreases, showing the network just producing images almost identical to the training set. Consequently, when the suggested images add up to the original dataset for computer vision tasks, in particular image classification, the evaluation metrics barely fluctuate, which indicates the impracticality of these methods <ref type="bibr" target="#b14">[15]</ref>.</p><p>To overcome the stated drawbacks, we took advantage of several methods. For this purpose, we experimented with our work on the Oxford-IIIT Pet dataset <ref type="bibr" target="#b15">[16]</ref>, which consists of different breeds of dogs and cats images. Experimental results demonstrate a noticeable increase in accuracy of highly developed model, Vision Transformer (ViT), <ref type="bibr" target="#b16">[17]</ref> on the task of fine-grained image classification. First, we tried generating new realistic samples to feed the model, but produced images were impractical and had poor quality. To solve this problem, we capitalize on a new version of StyleGAN <ref type="bibr" target="#b17">[18]</ref>, the highly developed StyleGAN2-ADA <ref type="bibr" target="#b18">[19]</ref>, which proposed a flexible discriminator augmentation mechanism that improves performance on limited data regimes. As previously suggested, GANs perform adequately when training objects are not centered, so we trained a customized version of the famous CNN architecture, MobileNetV2 <ref type="bibr" target="#b19">[20]</ref>, to predict facial landmarks <ref type="bibr" target="#b20">[21]</ref> of cats <ref type="bibr" target="#b21">[22]</ref> and dogs <ref type="bibr" target="#b22">[23]</ref> and cropped instances of the Oxford-IIIT Pet dataset according to predicted landmarks. At last, we trained the StyleGan2-ADA using the cropped images, next off by combining the produced images with the original data, created a more prosperous and extensive dataset. Finally, we trained the upgraded dataset on the ViT benchmark with the same hyper-parameters and achieved a better result. This paper commences with introducing some related works in the following section. Next, it explains the approach proposed and presented the experimental setup in sections III and IV, respectively. In section V, the outcomes are reported, and finally, we have a conclusion in section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Dataset size is an essential factor in the performance of image classifiers, and the primary issue is overfitting <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>. When the model is overfitting the training set, it will not generalize well on the test data. Many methods have been proposed over the years to overcome this problem; the most straightforward methods include using regularization <ref type="bibr" target="#b24">[25]</ref> or dropout <ref type="bibr" target="#b25">[26]</ref>. The former adds a penalty to the norm of the weights so that the results will be less fluctuated. Dropout drops certain connections in the network by removing the percentage of neurons randomly. Later techniques such as batch normalization <ref type="bibr" target="#b26">[27]</ref> re-scale and center each layer, thus making the model more stable and accurate. Transfer learning <ref type="bibr" target="#b27">[28]</ref> is another popular strategy that uses the weights of the same model trained on a different dataset instead of randomly initialize them. All of the suggested approaches are orthogonal to our work, and it is possible to use them all together.</p><p>That being the case, it requires specialization and time to tune the parameters to get the best accuracy. Our method generates new images, which data augmentation can apply to and increase the size of the dataset further.</p><p>The most advanced augmentation method is using GANs to generate new never-seen-before samples from current data. Since the invention of GANs <ref type="bibr" target="#b11">[12]</ref>, various models dependent on GAN were introduced <ref type="bibr" target="#b28">[29]</ref>. One issue with these models was the lack of ability to distinguish between different classes of training data and no control over generated images. Consequently, Conditional Generative Adversarial Networks (cGAN) <ref type="bibr" target="#b29">[30]</ref> were proposed in 2014, which allowed image generation to be conditional on a possible class label, enabling generate images on a specified target class. Sanfort et al. proved the potential of GAN-based data augmentation <ref type="bibr" target="#b30">[31]</ref> confirmed that using CycleGAN for augmentation in diverse tasks, e.g., segmentation and classification, can substantially boost performance in these domains.</p><p>In this paper, we used the state-of-the-art model StyleGAN2-ADA, introduced by Karras et al. in 2020 <ref type="bibr" target="#b18">[19]</ref>, which by augmenting the input data perform better than previous ones and better on low data regimes. However, a downside with GAN models is that the objects need to be centered and have similar poses; we study challenging scenarios where mentioned conditions do not apply and overcome this issue by cropping the images from predicted landmarks of samples, in this case, animals, to boost the StyleGAN2-ADA. Furthermore, we showed that these changes not only reduce the Frechet Inception Distance <ref type="bibr" target="#b31">[32]</ref> score of StyleGAN2-ADA but also, generated images can be used for data augmentation in fine-grained images classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Our procedure consists of using GANs to expand the length of the dataset by generating new instances of the training set (see examples in <ref type="figure">Figure 4</ref>). We further improved the generated images' quality by detecting the facial landmarks and cropping images fed to the GANs model. Moreover, we compared the result with original data. Finally, by combining synthetic images with real ones, we trained on the highly developed classifier model to show the effectiveness of our method, The whole pipeline is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fine-Grained Classification Model</head><p>Vision transformers recently achieved exceptional results on various datasets. ViT <ref type="bibr" target="#b16">[17]</ref> and BiT <ref type="bibr" target="#b32">[33]</ref>, two popular transformer models, delivered outstanding performance on the Oxford-IIIT Pet dataset. In this work, we train our augmented dataset plus the original one on a variant of ViT named the R26 + ViT-S/32 model. This hybrid model is a ViT-S/32 on top of a ResNet-26 <ref type="bibr" target="#b33">[34]</ref> backbone; this combination allows us to achieve similar outcomes to larger models with less than half the computational finetuning cost. R26 + ViT-S/32 is pre-trained on ImageNet-21k and has fewer parameters comparing to larger variants of ViT. We chose this model due to limitations in computational power, even though models with more parameters yield better results. We compared the results and our method's capability by training each dataset variation, namely, original, augmented, cropped-augmented on the classifier model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Augmentation via Image Generation</head><p>By employing GANs framework to generate new images, it is possible to use them for the task of augmentation to reduce overfitting and increase the accuracy of the classifier. To keep the data balance, we used a conditional form of StyleGAN2-ADA, which receives an extra parameter as input class = {1, 2, ..., 37} which denotes one of the 37 breeds available in the pets dataset. The conditional model works more efficiently than training each class separately. It consolidates data from different classes while keeping each of them realistic and does not confuse the looks of each breed together.</p><p>StyleGAN2-ADA takes advantage of augmentation to create notable results even with limited data. To experiment with the size of the dataset, we trained both StyleGAN and ViT models using {10%, 50%, 100%} subsets of pets dataset to show the effectiveness of dataset size in the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Facial Landmark Detection</head><p>The position and size of the object in the image ( in this case, cats and dogs) play a crucial role in robust image generation using GANs. Most publicly available datasets do not meet this requirement; thus, GANs are limited to a petite number of datasets, i.e., CelebA <ref type="bibr" target="#b34">[35]</ref>, AFHQ <ref type="bibr" target="#b35">[36]</ref>, etc. In this paper, we demonstrate how preparing the Oxford-IIIT Pet dataset by cropping the images can improve the performance of GANs and therefore generate more realistic images. Due to inconsistent annotations in the pets dataset, instead of using provided bounding boxes, we trained a model to predict the animal landmarks, i.e., ears, nose, and eyes. Furthermore, we cropped the images by taking advantage of predicted landmarks. This method can easily generalize on different datasets, despite the bounding box restrictions.</p><p>We finetuned a MoblieNetV2 model on annotated datasets from Sandler et al. <ref type="bibr" target="#b19">[20]</ref>. The mentioned model was pre-trained on ImageNet, which outputs 14 numbers denoting 7 landmarks on the animal face (see <ref type="figure" target="#fig_2">Figure 3</ref>). All the data is augmented with traditional methods, i.e., mirroring, rotation, and zooming. Furthermore, we used landmark normalization based on the center of the anchor, introduced in <ref type="bibr" target="#b36">[37]</ref>, which slightly boosted the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Oxford IIIT-Pet Dataset To demonstrate the capability of our proposed method, we chose the public Oxford-IIIT Pet <ref type="bibr" target="#b15">[16]</ref> dataset, which includes pictures of animals in different shapes and poses. This diversity suggests a more natural distribution and intensifies our work's performance compared to the benchmark results. This dataset contains a total of 7349 images in 37 different breeds of cats (12 categories) and dogs (25 categories). Every species consists of roughly 200 photos in JPG format and resized to 128 ? 128 for training StyleGAN2-ADA, and fine-tuning ViT.</p><p>We used the official training split according to the original paper for both training the GANs model as well as fine tuning the fine-grained image classifier. To demonstrate the effect of the proposed augmentation technique, we performed all experiments on subsets of the dataset, i.e., 10%, 50%, and 100%, both on cropped and the original data.</p><p>Landmark Detection Datasets We benefit from of two separate datasets to finetune a MobileNet model for the purpose of facial landmark prediction of cats and dogs. By combining the dog face dataset collected by Mougeot et al. <ref type="figure">Fig. 4</ref>: Comparison between synthetic and authentic images. We show (a) the original data,(b) and (c) generated images on the whole dataset, cropped and uncropped, respectively. (d) cropped images on 50%, (e) uncropped images generated on 50% subset and finally (f) and (g), cropped and uncropped images result of training on only 10% of the data. These qualitative visualizations prove the effectiveness and the interpretability of our method. <ref type="bibr" target="#b37">[38]</ref>. with a variation of the cat face dataset from Zhang et al. <ref type="bibr" target="#b21">[22]</ref>, we produced a consolidated dataset for the mentioned task. In addition, we unified and improved the landmarks format. The first dataset contains 1393 classes of dogs, 8363 images, and there are at least two images per dog. Each image is of size 224 ? 224 ? 3 and has JPG format. The second dataset is consists of 10,000 annotated cat images collected from flickr.com. To keep the unification of both datasets, we resized all images from the cat dataset to 224 ? 224. Lastly, we used the official train/validation/test split suggested in the paper.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>MobileNetV2 Settings We used original MobileNetV2 architecture with some modifications for facial landmark prediction. It accepts 224 ? 224 RGB images as input. Global max-pooling is applied to pooling layers. We discarded the Fully Connected Layers (FCL) on top; instead, we used two FCL with 2 7 nodes and Rectified Linear Unit (ReLu) activation function. The model's output is the size of 14, which denotes 7 coordinates of the animal landmark (2 for each ear, 1 for each eye, 1 for nose).</p><p>StyleGAN2-ADA Settings This model has trained six times on different variants of the pets dataset, with the same hyperparameters. All input images are 128 ? 128 ? 3, and the conditional form of the model is used with 37 classes for each species type. ADA target value is set to 0.6, which controls data augmentation intensity and, methods of augmentation are set to default. Learning rates for generator and discriminator are both set to 0.0025.</p><p>R26 + ViT-S/32 Settings To keep the results comparable with the available performance of the ViT model, we used the same hyperparameters as mentioned in the official paper; however, because of the lower resolution of generated images, we resized the final dataset to 128 ? 128 for consistency. The model is pre-trained on ImageNet with 85.99% accuracy, and  dropout is excluded. Additionally, Adam optimizer performs slightly better than Stochastic Gradient Descent (SGD). We split the data and evaluated the model using 3680 images for training and 3669 images for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Details</head><p>Our model is implemented using PyTorch on an Ubuntu 20.04 server. We use a Tesla P100 GPU to accelerate our training process. We finetuned MobileNetV2 model for 15 epochs and 32 batch size. As recommended in StyleGAN2-ADA paper, 5000 kimg training results in realistic images and it is almost converged. We trained all variants from scratch up to 5120 kimg, each of them took approximately 60 hours. Finally, ViT model pre-trained for 300 epochs, then fined tuned for 16 more on each variant of pets dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metrics</head><p>We used three independent metrics to evaluate the performances of our approaches and tasks. For the fine-grained image classification task, we considered the Accuracy metric since there are pretty equal samples belonging to each class. Accuracy has the following definition where Ra is the total   In order to assess the quality of the images that are generated, Frechet Inception Distance (FID) is used. The lower amount of FID displays better quality. In FID, first, a pre-trained InceptionV3 model is loaded and removes the features from an intermediate layer. Then the output is taken as the activations from the last pooling layer. A multivariate Gaussian distribution is used to model the data arrangement using covariance ? and mean ?. The FID interpolated the authentic images x and produced images g is calculated as follows, and T r sums up all the diagonal elements:</p><formula xml:id="formula_1">F ID(x, g) = ||? x ? ? g || 2 2 + T r(? x + ? g ? 2 ? x ? ? g )</formula><p>For measuring facial landmarking accuracy, we used the Root Mean Square Error (RMSE) <ref type="bibr" target="#b38">[39]</ref>, which is a metric that shows the average distance between the predicted values from the model and the actual values in the dataset. We used the following formula to measure the average length between every N anticipated landmarks (x p i , y p i ) and the corresponding 'ground truth' (x t i , y t i ) on a per landmark basis. The predicted landmarks that aren't good enough will be placed far-off from their related provided annotations positions and cause to raise the value of RMSE.</p><formula xml:id="formula_2">RM SE = 1 N N i=1 (x p i ? x t i ) 2 + (y p i ? y t i ) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Analysis</head><p>To evaluate the effectiveness of our landmark predictor, we evaluated RMSE of the trained MobileNetV2 model with and without landmark normalization. <ref type="table" target="#tab_2">Table I</ref> shows that normalization decreases RMSE on both validation and test set, suggesting increased accuracy and quality in predicted landmarks. Since we detect only one object per image, in some cases where multiple instances of dogs or cats were visible, predicting the landmark for the wrong instance caused the introduction of outliers. We removed these outliers to keep the RMSE more legitimate.</p><p>The reported results in <ref type="table" target="#tab_2">Table II</ref> show that both centering the objects and manipulating the size of the dataset have a discernible effect on the quality of generated outputs, measured by their FID. Cropping the images decreased FID substantially on all three variants of the dataset. On the 100% subset of the cropped dataset, FID reached as low as 15.9, which is 32% less than the uncropped version. Limiting the amount of data fed into the GANs model had a denying impact on the condition of generated images that are shown in <ref type="figure">Figure 4</ref> for variants of the dataset. Regarding <ref type="figure" target="#fig_3">Figure 5</ref>, it is noticeable that FID is still slightly decreasing and have not converged yet, but due to computational constraints, we stopped at approximately 5000 kimg.</p><p>As shown in <ref type="table" target="#tab_2">Table II</ref>, the size of the database has a direct impact on the accuracy of experimented ViT model. It is clear that data augmentation has a negligible influence on the results when the dataset is not prepared and even reduced accuracy on the 10% subset due to the low quality of generated images and lack of detail. Our method generated higher quality instances and dealt with the problem of abstraction in images. Consequently, increased accuracy in all three subsets of the dataset and improved the available benchmark by roughly 2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we gained improvement in fine-grained image classification by generating new samples with the use of GANs. New data samples were obtained from StyleGAN2-ADA. We finetuned a custom MobileNetV2 model to predict animal facial landmarks, then cropped the Oxford-IIIT Pet dataset images accordingly. The newly generated images from the cropped dataset caused to enhancement in quality and diversity. An increase in the number and diversification of sample images impacts increasing the accuracy of the state-of-the-art ViT model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The architecture of Generative Adversarial Network (GAN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The complete classification pipeline of our method for GAN-based data augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Examples of different cat and dog breeds and their associated landmarks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>We evaluate the accuracy of the used model and FID for three different dataset conditions (Original, augmented, and cropped-augmented ) in data regimes of 10, 50, and 100 percent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2109.00891v1 [cs.CV] 28 Aug 2021</figDesc><table><row><cell>Images from Datatset</cell><cell>Predict Facial Landmarks</cell><cell></cell><cell>Crop Images according to Landmarks</cell></row><row><cell>ViT Classifier</cell><cell>Combined Dataset (Real + Synthetic)</cell><cell>Generated Images</cell><cell>Conditional StyleGAN2-ADA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>We evaluate the RMSE once for the MobileNetV2 model and once for MobileNetV2 that is added with Normalization. The RMSE is calculated for both validation and test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>We evaluate the accuracy of the used model and FID for three different dataset conditions (Original, augmented, and augmented-cropped ) in data regimes of 10, 50, and 100 percent. number of input samples and R means the number of testing images.</figDesc><table><row><cell>Accuracy =</cell><cell>R a R</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Advances in Two-Dimensional and Three-Dimensional Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mcneill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Helm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="323" to="372" />
			<pubPlace>Berlin, Heidelberg; Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring vision transformers for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Turgutlu</surname></persName>
		</author>
		<idno>abs/2106.10587</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Exploring vision transformers for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Turgutlu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1712.04621</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Using gans to augment data for cloud image segmentation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dev</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning data augmentation strategies for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="566" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Convergence problems with generative adversarial networks (gans)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Barnett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting and simulating artifacts in gan fake images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Workshop on Information Forensics and Security (WIFS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gan-based data augmentation for chest x-ray classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hulkund</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facial landmark detection: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="115" to="142" />
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cat head detection -how to effectively exploit shape and texture features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weiwei Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2008-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dog breed classification via landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5237" to="5241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The problem of overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Hawkins</surname></persName>
		</author>
		<idno type="PMID">14741005</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularization theory and neural networks architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="269" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Recurrent batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aglar G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on deep transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2018</title>
		<editor>V. K?rkov?, Y. Manolopoulos, B. Hammer, L. Iliadis, and I. Maglogiannis</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recent progress on generative adversarial networks (gans): A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="36322" to="36333" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data augmentation using generative adversarial networks (cyclegan) to improve generalizability in ct segmentation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sandfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Pickhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">16884</biblScope>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object-part attention model for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Retinaface: Single-stage dense face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A deep learning approach for dog face verification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mougeot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PRICAI 2019: Trends in Artificial Intelligence</title>
		<editor>A. C. Nayak and A. Sharma</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="418" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A review of image-based automatic facial landmark identification techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
