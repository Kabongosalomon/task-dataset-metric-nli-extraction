<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OPEN-VOCABULARY OBJECT DETECTION VIA VISION AND LANGUAGE KNOWLEDGE DISTILLATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
							<email>xiuyegu@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
							<email>tsungyil@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
							<email>weicheng@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
							<email>yincui@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename></persName>
						</author>
						<title level="a" type="main">OPEN-VOCABULARY OBJECT DETECTION VIA VISION AND LANGUAGE KNOWLEDGE DISTILLATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP r with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP r . The model can directly transfer to other datasets without finetuning, achieving 72.2 AP 50 on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-theart (Zareian et al., 2021) by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/ tree/master/models/official/detection/projects/vild.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Consider <ref type="figure" target="#fig_0">Fig. 1</ref>, can we design object detectors beyond recognizing only base categories (e.g., toy) present in training labels and expand the vocabulary to detect novel categories (e.g., toy elephant)? In this paper, we aim to train an open-vocabulary object detector that detects objects in any novel categories described by text inputs, using only detection annotations in base categories.</p><p>Existing object detection algorithms often learn to detect only the categories present in detection datasets. A common approach to increase the detection vocabulary is by collecting images with more labeled categories. The research community has recently collected new object detection datasets with large vocabularies <ref type="bibr" target="#b13">(Gupta et al., 2019;</ref><ref type="bibr" target="#b22">Kuznetsova et al., 2020)</ref>. LVIS <ref type="bibr" target="#b13">(Gupta et al., 2019</ref>) is a milestone of these efforts by building a dataset with 1,203 categories. With such a rich vocabulary, it becomes quite challenging to collect enough training examples for all categories. By Zipf's law, object categories naturally follow a long-tailed distribution. To find sufficient training examples for rare categories, significantly more data is needed <ref type="bibr" target="#b13">(Gupta et al., 2019)</ref>, which makes it expensive to scale up detection vocabularies.</p><p>On the other hand, paired image-text data are abundant on the Internet. Recently, <ref type="bibr">Radford et al. (2021)</ref> train a joint vision and language model using 400 million image-text pairs and demonstrate impressive results on directly transferring to over 30 datasets. The pretrained text encoder is the key to the zero-shot transfer ability to arbitrary text categories. Despite the great success on learning image-level representations, learning object-level representations for open-vocabulary detection is still challenging. In this work, we consider borrowing the knowledge from a pretrained openvocabulary classification model to enable open-vocabulary detection. We begin with an R-CNN <ref type="bibr" target="#b11">(Girshick et al., 2014)</ref> style approach. We turn open-vocabulary detection into two sub-problems: 1) generalized object proposal and 2) open-vocabulary image classification. We train a region proposal model using examples from the base categories. Then we use the pretrained open-vocabulary image classification model to classify cropped object proposals, which can contain both base and novel categories. We benchmark on LVIS <ref type="bibr" target="#b13">(Gupta et al., 2019)</ref> by holding out all rare categories as novel categories and treat others as base categories. To our surprise, the performance on the novel categories already surpasses its supervised counterpart. However, this approach is very slow for inference, because it feeds object proposals one-by-one into the classification model.</p><p>To address the above issue, we propose ViLD (Vision and Language knowledge Distillation) for training two-stage open-vocabulary detectors. ViLD consists of two components: learning with text embeddings (ViLD-text) and image embeddings <ref type="bibr">(ViLD-image)</ref> inferred by an open-vocabulary image classification model, e.g., CLIP. In ViLD-text, we obtain the text embeddings by feeding category names into the pretrained text encoder. Then the inferred text embeddings are used to classify detected regions. Similar approaches have been used in prior detection works <ref type="bibr" target="#b1">(Bansal et al., 2018;</ref><ref type="bibr" target="#b29">Rahman et al., 2018;</ref><ref type="bibr">Zareian et al., 2021)</ref>. We find text embeddings learned jointly with visual data can better encode the visual similarity between concepts, compared to text embeddings learned from a language corpus, e.g., GloVe <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref>. Using CLIP text embeddings achieves 10.1 AP r (AP of novel categories) on LVIS, significantly outperforming the 3.0 AP r of using GloVe. In ViLD-image, we obtain the image embeddings by feeding the object proposals into the pretrained image encoder. Then we train a Mask R-CNN whose region embeddings of detected boxes are aligned with these image embeddings. In contrast to ViLD-text, ViLD-image distills knowledge from both base and novel categories since the proposal network may detect regions containing novel objects, while ViLD-text only learns from base categories. Distillation enables ViLD to be general in choosing teacher and student architectures. ViLD is also energy-efficient as it works with off-the-shelf open-vocabulary image classifiers. We experiment with the CLIP and ALIGN <ref type="bibr">(Jia et al., 2021)</ref> teacher models with different architectures (ViT and EfficientNet).</p><p>We show that ViLD achieves 16.1 AP for novel categories on LVIS, surpassing the supervised counterpart by 3.8. We further use ALIGN as a stronger teacher model to push the performance to 26.3 novel AP, which is close (only 3.7 worse) to the 2020 LVIS Challenge winner <ref type="bibr" target="#b36">(Tan et al., 2020)</ref> that is fully-supervised. We directly transfer ViLD trained on LVIS to other detection datasets without finetuning, and obtain strong performance of 72.2 AP 50 on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. We also outperform the previous state-of-the-art open-vocabulary detector on COCO <ref type="bibr">(Zareian et al., 2021)</ref> by 4.8 novel AP and 11.4 overall AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Increasing vocabulary in visual recognition: Recognizing objects using a large vocabulary is a long-standing research problem in computer vision. One focus is zero-shot recognition, aiming at recognizing categories not present in the training set. Early works <ref type="bibr" target="#b9">(Farhadi et al., 2009;</ref><ref type="bibr" target="#b34">Rohrbach et al., 2011;</ref><ref type="bibr" target="#b18">Jayaraman &amp; Grauman, 2014)</ref> use visual attributes to create a binary codebook representing categories, which is used to transfer learned knowledge to unseen categories. In this direction, researchers have also explored class hierarchy, class similarity, and object parts as discriminative features to aid the knowledge transfer <ref type="bibr" target="#b34">(Rohrbach et al., 2011;</ref><ref type="bibr" target="#b0">Akata et al., 2016;</ref><ref type="bibr" target="#b44">Zhao et al., 2017;</ref><ref type="bibr" target="#b7">Elhoseiny et al., 2017;</ref><ref type="bibr" target="#b19">Ji et al., 2018;</ref><ref type="bibr" target="#b3">Cacheux et al., 2019;</ref><ref type="bibr" target="#b41">Xie et al., 2020)</ref>. Another focus is learning to align latent image-text embeddings, which allows to classify images using arbitrary texts. <ref type="bibr" target="#b10">Frome et al. (2013)</ref> and <ref type="bibr" target="#b26">Norouzi et al. (2014)</ref> are pioneering works that learn a visual-semantic embedding space using deep learning. <ref type="bibr" target="#b40">Wang et al. (2018)</ref>  from both word embeddings and knowledge graphs. Recent work CLIP <ref type="bibr">(Radford et al., 2021)</ref> and ALIGN <ref type="bibr">(Jia et al., 2021)</ref> push the limit by collecting million-scale image-text pairs and then training joint image-text models using contrastive learning. These models can directly transfer to a suite of classification datasets and achieve impressive performances. While these work focus on image-level open-vocabulary recognition, we focus on detecting objects using arbitrary text inputs.</p><p>Increasing vocabulary in object detection: It's expensive to scale up the data collection for large vocabulary object detection.  and <ref type="bibr" target="#b47">Zhou et al. (2021)</ref> unify the label space from multiple datasets. Joseph et al. (2021) incrementally learn identified unknown categories. Zero-shot detection (ZSD) offers another direction. Most ZSD methods align region features to pretrained text embeddings in base categories <ref type="bibr" target="#b1">(Bansal et al., 2018;</ref><ref type="bibr" target="#b4">Demirel et al., 2018;</ref><ref type="bibr" target="#b30">Rahman et al., 2019;</ref><ref type="bibr" target="#b14">Hayat et al., 2020;</ref>. However, there is a large performance gap to supervised counterparts. To address this issue, <ref type="bibr">Zareian et al. (2021)</ref> pretrain the backbone model using image captions and finetune the pretrained model with detection datasets. In contrast, we use an image-text pretrained model as a teacher model to supervise student object detectors. All previous methods are only evaluated on tens of categories, while we are the first to evaluate on more than 1,000 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Notations: We divide categories in a detection dataset into the base and novel subsets, and denote them by C B and C N . Only annotations in C B are used for training. We use T (?) to denote the text encoder and V(?) to denote the image encoder in the pretrained open-vocabulary image classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LOCALIZATION FOR NOVEL CATEGORIES</head><p>The first challenge for open-vocabulary detection is to localize novel objects. We modify a standard two-stage object detector, e.g., Mask R-CNN <ref type="bibr" target="#b16">(He et al., 2017)</ref>, for this purpose. We replace its classspecific localization modules, i.e., the second-stage bounding box regression and mask prediction layers, with class-agnostic modules for general object proposals. For each region of interest, these modules only predict a single bounding box and a single mask for all categories, instead of one prediction per category. The class-agnostic modules can generalize to novel objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OPEN-VOCABULARY DETECTION WITH CROPPED REGIONS</head><p>Once object candidates are localized, we propose to reuse a pretrained open-vocabulary image classifier to classify each region for detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image embeddings:</head><p>We train a proposal network on base categories C B and extract the region proposalsr ? P offline. We crop and resize the proposals, and feed them into the pretrained image encoder V to compute image embeddings V(crop(I,r)), where I is the image.</p><p>We ensemble the image embeddings from 1? and 1.5? crops, as the 1.5? crop provides more context cues. The ensembled embedding is then renormalized to unit norm:</p><formula xml:id="formula_0">V(crop(I,r {1?,1.5?} )) = v v , where v = V(crop(I,r 1? )) + V(crop(I,r 1.5? )).<label>(1)</label></formula><p>Text embeddings: We generate the text embeddings offline by feeding the category texts with prompt templates, e.g., "a photo of {category} in the scene", into the text encoder T . We ensemble multiple prompt templates and the synonyms if provided.</p><p>Then, we compute cosine similarities between the image and text embeddings. A softmax activation is applied, followed by a per-class NMS to obtain final detections. The inference is slow since every cropped region is fed into V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VILD: VISION AND LANGUAGE KNOWLEDGE DISTILLATION.</head><p>We propose ViLD to address the slow inference speed of the above method. ViLD learns region embeddings in a two-stage detector to represent each proposal r. We denote region embeddings by R(?(I), r), where ?(?) is a backbone model and R(?) is a lightweight head that generates region embeddings. Specifically, we take outputs before the classification layer as region embeddings.</p><p>Replacing classifier with text embeddings: We first introduce ViLD-text. Our goal is to train the region embeddings such that they can be classified by text embeddings. <ref type="figure" target="#fig_1">Fig. 3(b)</ref> shows the architecture and training objective. ViLD-text replaces the learnable classifier in <ref type="figure" target="#fig_1">Fig. 3(a)</ref> with the text embeddings introduced in Sec. 3.2. Only T (C B ), the text embeddings of C B , are used for training. For the proposals that do not match any groundtruth in C B , they are assigned to the background category. Since the text "background" does not well represent these unmatched proposals, we allow the background category to learn its own embedding e bg . We compute the cosine similarity between each region embedding R(?(I), r) and all category embeddings, including T (C B ) and e bg . Then we apply softmax activation with a temperature ? to compute the cross entropy loss. To train the first-stage region proposal network of the two-stage detector, we extract region proposals r ? P online, and train the detector with ViLD-text from scratch. The loss for ViLD-text can be written as:</p><formula xml:id="formula_1">e r = R(?(I), r)</formula><p>z(r) = sim(e r , e bg ), sim(e r , t 1 ), ? ? ? , sim(e r , t |C B | )</p><formula xml:id="formula_2">L ViLD-text = 1 N r?P L CE sof tmax z(r)/? , y r ,<label>(2)</label></formula><p>where sim(a, b) = a b/( a b ), t i denotes elements in T (C B ), y r denotes the class label of region r, N is the number of proposals per image (|P |), and L CE is the cross entropy loss.</p><p>During inference, we include novel categories (C N ) and generate T (</p><formula xml:id="formula_3">C B ? C N ) (sometimes T (C N )</formula><p>only) for open-vocabulary detection <ref type="figure">(Fig. 2)</ref>. Our hope is that the model learned from annotations in C B can generalize to novel categories C N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distilling image embeddings:</head><p>We then introduce ViLD-image, which aims to distill the knowledge from the teacher image encoder V into the student detector. Specifically, we align region embeddings R(?(I),r) to image embeddings V(crop(I,r)) introduced in Sec. 3.2.</p><p>To make the training more efficient, we extract M proposalsr ?P offline for each training image, and precompute the M image embeddings. These proposals can contain objects in both C B and C N , as the network can generalize. In contrast, ViLD-text can only learn from C B . We apply an L 1 loss between the region and image embeddings to minimize their distance. The ensembled image embeddings in Sec. 3.2 are used for distillation: The total training loss of ViLD is simply a weighted sum of both objectives:</p><formula xml:id="formula_4">L ViLD-image = 1 M r? P V(crop(I,r {1?,1.5?} )) ? R(?(I),r) 1 .<label>(3)</label></formula><formula xml:id="formula_5">L ViLD = L ViLD-text + w ? L ViLD-image ,<label>(4)</label></formula><p>where w is a hyperparameter weight for distilling the image embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MODEL ENSEMBLING</head><p>In this section, we explore model ensembling for the best detection performance over base and novel categories. First, we combine the predictions of a ViLD-text detector with the open-vocabulary image classification model. The intuition is that ViLD-image learns to approximate the predictions of its teacher model, and therefore, we assume using the teacher model directly may improve performance. We use a trained ViLD-text detector to obtain top k candidate regions and their confidence scores. Let p i,ViLD-text denote the confidence score of proposalr belonging to category i. We then feed crop(I,r) to the open-vocabulary classification model to obtain the teacher's confidence score p i,cls . Since we know the two models have different performance on base and novel categories, we introduce a weighted geometric average for the ensemble:</p><formula xml:id="formula_6">p i,ensemble = p ? i,ViLD-text ? p (1??) i,cls , if i ? C B p (1??) i,ViLD-text ? p ? i,cls . if i ? C N<label>(5)</label></formula><p>? is set to 2/3, which weighs the prediction of ViLD-text more on base categories and vice versa. Note this approach has a similar slow inference speed as the method in Sec. 3.2.</p><p>Next, we introduce a different ensembling approach to mitigate the above inference speed issue. Besides, in ViLD, the cross entropy loss of ViLD-text and the L 1 distillation loss of ViLD-image is applied to the same set of region embeddings, which may cause contentions. Here, instead, we learn two sets of embeddings for ViLD-text (Eq. 2) and ViLD-image (Eq. 3) respectively, with two separate heads of identical architectures. Text embeddings are applied to these two regions embeddings to obtain confidence scores p i,ViLD-text and p i,ViLD-image , which are then ensembled in the same way as Eq. 5, with p i,ViLD-image replacing p i,cls . We name this approach ViLD-ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Implementation details: We benchmark on the Mask R-CNN <ref type="bibr" target="#b16">(He et al., 2017)</ref> with ResNet <ref type="bibr" target="#b15">(He et al., 2016)</ref> FPN <ref type="bibr" target="#b24">(Lin et al., 2017)</ref> backbone and use the same settings for all models unless explicitly specified. The models use 1024?1024 as input image size, large-scale jittering augmentation of range [0.1, 2.0], synchronized batch normalization <ref type="bibr" target="#b17">(Ioffe &amp; Szegedy, 2015;</ref> of batch size 256, weight decay of 4e-5, and an initial learning rate of 0.32. We train the model from scratch for 180,000 iterations, and divide the learning rate by 10 at 0.9?, 0.95?, and 0.975? of total iterations. We use the publicly available pretrained CLIP model 1 as the open-vocabulary classification model, with an input size of 224?224. The temperature ? is set to 0.01, and the maximum number of detections per image is 300. We refer the readers to Appendix D for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BENCHMARK SETTINGS</head><p>We mainly evaluate on LVIS <ref type="bibr" target="#b13">(Gupta et al., 2019)</ref> with our new setting. To compare with previous methods, we also use the setting in <ref type="bibr">Zareian et al. (2021)</ref>, which is adopted in many zero-shot detection works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LVIS:</head><p>We benchmark on LVIS v1. LVIS contains a large and diverse set of vocabulary (1,203 categories) that is more suitable for open-vocabulary detection. We take its 866 frequent and common categories as the base categories C B , and hold out the 337 rare categories as the novel categories C N . AP r , the AP of rare categories, is the main metric.</p><p>COCO: <ref type="bibr" target="#b1">Bansal et al. (2018)</ref> divide <ref type="bibr">COCO-2017</ref><ref type="bibr" target="#b23">(Lin et al., 2014</ref> into 48 base categories and 17 novel categories, removing 15 categories without a synset in the WordNet hierarchy. We follow previous works and do not compute instance masks. We evaluate on the generalized setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LEARNING GENERALIZABLE OBJECT PROPOSALS</head><p>We first study whether a detector can localize novel categories when only trained on base categories. We evaluate the region proposal networks in Mask R-CNN with a ResNet-50 backbone. <ref type="table" target="#tab_1">Table 1</ref> shows the average recall (AR) <ref type="bibr" target="#b23">(Lin et al., 2014)</ref> on novel categories. Training with only base categories performs slightly worse by ? 2 AR at 100, 300, and 1000 proposals, compared to using both base and novel categories. This experiment demonstrates that, without seeing novel categories during training, region proposal networks can generalize to novel categories, only suffering a small performance drop. We believe better proposal networks focusing on unseen category generalization should further improve the performance, and leave this for future research. In <ref type="table" target="#tab_2">Table 2</ref>, we evaluate the approach in Sec. 3.2, i.e., using an open-vocabulary classifier to classify cropped region proposals. We use CLIP in this experiment and find it tends to output confidence scores regardless of the localization quality (Appendix B). Given that, we ensemble the CLIP confidence score with a proposal objectness score by geometric mean. Results show it improves both base and novel APs. We compare with supervised baselines trained on base/base+novel categories, as well as Supervised-RFS <ref type="bibr" target="#b25">(Mahajan et al., 2018;</ref><ref type="bibr" target="#b13">Gupta et al., 2019)</ref> that uses category frequency for balanced sampling. CLIP on cropped regions already outperforms supervised baselines on AP r by a large margin, without accessing detection annotations in novel categories. However, the performances of AP c and AP f are still trailing behind. This experiment shows that a strong openvocabulary classification model can be a powerful teacher model for detecting novel objects, yet there is still much improvement space for inference speed and overall AP. We apply CLIP to classify cropped region proposals, with or without ensembling objectness scores, and report the mask average precision (AP). The performance on novel categories (APr) is far beyond supervised learning approaches. However, the overall performance is still behind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>APr  We evaluate the performance of ViLD and its variants (ViLD-text, ViLD-image, and ViLDensemble), which are significantly faster compared to the method in Sec. 4.3. Finally, we use stronger teacher models to demonstrate our best performance. <ref type="table" target="#tab_3">Table 3</ref> summarizes the results.</p><p>Text embeddings as classifiers (ViLD-text): We evaluate ViLD-text using text embeddings generated by CLIP, and compare it with GloVe text embeddings <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref> pretrained on a large-scale text-only corpus. <ref type="table" target="#tab_3">Table 3</ref> shows ViLD-text achieves 10.1 AP r , which is significantly better than 3.0 AP r using GloVe. This demonstrates the importance of using text embeddings that are jointly trained with images. ViLD-text achieves much higher AP c and AP f compared to CLIP on cropped regions (Sec. 4.3), because ViLD-text uses annotations in C B to align region embeddings with text embeddings. The AP r is worse, showing that using only 866 base categories in LVIS does not generalize as well as CLIP to novel categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distilling image embeddings (ViLD-image):</head><p>We evaluate ViLD-image, which distills from the image embeddings of cropped region proposals, inferred by CLIP's image encoder, with a distillation weight of 1.0. Experiments show that ensembling with objectness scores doesn't help with other ViLD variants, so we only apply it to ViLD-image. Without training with any object category labels, ViLD-image achieves 11.2 AP r and 11.2 overall AP. This demonstrates that visual distillation works for open-vocabulary detection but the performance is not as good as CLIP on cropped regions.</p><p>Text+visual embeddings (ViLD): ViLD shows the benefits of combining distillation loss (ViLDimage) with classification loss using text embeddings (ViLD-text). We explore different hyperparameter settings in Appendix <ref type="table" target="#tab_9">Table 7</ref> and observe a consistent trade-off between AP r and AP c,f , which suggests there is a competition between ViLD-text and ViLD-image. In <ref type="table" target="#tab_3">Table 3</ref>, we compare ViLD with other methods. Its AP r is 6.0 higher than ViLD-text and 4.9 higher than ViLD-image, indicating combining the two learning objectives boosts the performance on novel categories. ViLD outperforms Supervised-RFS by 3.8 AP r , showing our open-vocabulary detection approach is better than supervised models on rare categories. Model ensembling: We study methods discussed in Sec. 3.4 to reconcile the conflict of joint training with ViLD-text and ViLD-image. We use two ensembling approaches: 1) ensembling ViLD-text with CLIP (ViLD-text+CLIP); 2) ensembling ViLD-text and ViLD-image using separate heads (ViLD-ensemble). As shown in <ref type="table" target="#tab_3">Table 3</ref>, ViLD-ensemble improves performance over ViLD, mainly on AP c and AP r . This shows ensembling reduces the competition. ViLD-text+CLIP obtains much higher AP r , outperforming ViLD by 6.5, and maintains good AP c,f . Note that it is slow and impractical for real world applications. This experiment is designed for showing the potential of using open-vocabulary classification models for open-vocabulary detection.</p><p>Stronger teacher model: We use CLIP ViT-L/14 and ALIGN <ref type="bibr">(Jia et al., 2021)</ref> to explore the performance gain with a stronger teacher model (details in Appendix D). As shown in <ref type="table" target="#tab_3">Table 3</ref>, both models achieve superior results compared with R50 ViLD w/ CLIP. The detector distilled from ALIGN is only trailing to the fully-supervised 2020 Challenge winner <ref type="bibr" target="#b36">(Tan et al., 2020)</ref>  To compare with them, we train and evaluate ViLD variants following the benchmark setup in <ref type="bibr">Zareian et al. (2021)</ref> and report box AP with an IoU threshold of 0.5. We use the ResNet-50 backbone, shorten the training schedule to 45,000 iterations, and keep other settings the same as our experiments on LVIS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">TRANSFER TO OTHER DATASETS</head><p>Trained ViLD models can be transferred to other detection datasets, by simply switching the classifier to the category text embeddings of the new datasets. For simplicity, we keep the background embedding trained on LVIS. We evaluate the transferability of ViLD on PASCAL VOC <ref type="bibr" target="#b8">(Everingham et al., 2010)</ref>, COCO <ref type="bibr" target="#b23">(Lin et al., 2014)</ref>, and Objects365 <ref type="bibr" target="#b35">(Shao et al., 2019)</ref>. Since the three datasets have much smaller vocabularies, category overlap is unavoidable and images can be shared among datasets, e.g., COCO and LVIS. As shown in <ref type="table" target="#tab_6">Table 5</ref>, ViLD achieves better transfer performance than ViLD-text. In PASCAL and COCO, the gap is large. This improvement should be credited to visual distillation, which better aligns region embeddings with the text classifier. We also compare with supervised learning and finetuning the classification layer. Although across datasets, ViLD has 3-6 AP gaps compared to the finetuning method and larger gaps compared to the supervised method, it is the first time we can directly transfer a trained detector to different datasets using language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">QUALITATIVE RESULTS</head><p>Qualitative examples: In <ref type="figure">Fig. 4</ref>, we visualize ViLD's detection results. It illustrates ViLD is able to detect objects of both novel and base categories, with high-quality mask predictions on novel objects, e.g., it well separates banana slices from the crepes (novel category). We also show qualitative results on COCO and Objects365, and find ViLD generalizes well. We show more qualitative results, e.g., interactive detection and systematic expansion, in Appendix A.</p><p>On-the-fly interactive object detection: We tap the potential of ViLD by using arbitrary text to interactively recognize fine-grained categories and attributes. We extract the region embedding and compute its cosine similarity with a small set of on-the-fly arbitrary texts describing attributes and/or fine-grained categories; we apply softmax with temperature ? on top of the similarities. To our surprise, though never trained on fine-grained dog breeds <ref type="figure">(Fig. 5)</ref>, it correctly distinguishes husky from shiba inu. It also works well on identifying object colors <ref type="figure" target="#fig_0">(Fig. 1)</ref>. The results demonstrate knowledge distillation from an open-vocabulary image classification model helps ViLD to gain understanding of concepts not present in the detection training. Of course, ViLD does not work all the time, e.g., it fails to recognize poses of animals.</p><p>(a) Fine-grained breeds and colors.</p><p>(b) Colors of body parts. <ref type="figure">Figure 5</ref>: On-the-fly interactive object detection. One application of ViLD is using on-the-fly arbitrary texts to further recognize more details of the detected objects, e.g., fine-grained categories and color attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systematic expansion of dataset vocabulary:</head><p>In addition, we propose to systematically expand the dataset vocabulary (v = {v 1 , ..., v p }) with a set of attributes (a = {a 1 , ..., a q }) as follows:</p><formula xml:id="formula_7">Pr(v i , a j | e r ) = Pr(v i | e r ) ? Pr(a j | v i , e r ) = Pr(v i | e r ) ? Pr(a j | e r ),<label>(6)</label></formula><p>where e r denotes the region embedding. We assume v i ? ? a j | e r , i.e., given e r the event the object belongs to category v i is conditionally independent to the event it has attribute a j .</p><p>Let ? denote the temperature used for softmax and T denote the text encoder as in Eq. 2. Then</p><p>Pr(v i | e r ) = sof tmax i (sim(e r , T (v))/? ), (7) Pr(a j | e r ) = sof tmax j (sim(e r , T (a))/? ).</p><p>In this way, we are able to expand p vocabularies into a new set of p ? q vocabularies with attributes. The conditional probability approach is similar to YOLO9000 <ref type="bibr" target="#b32">(Redmon &amp; Farhadi, 2017)</ref>. We show a qualitative example of this approach in <ref type="figure">Fig. 6</ref>, where we use a color attribute set as a. Our open-vocabulary detector successfully detects fruits with color attributes.</p><p>We further expand the detection vocabulary to fine-grained bird categories by using all 200 species from CUB-200-2011 <ref type="bibr" target="#b39">(Wah et al., 2011)</ref>. <ref type="figure" target="#fig_8">Fig. 7</ref> shows successful and failure examples of our openvocabulary fine-grained detection on CUB-200-2011 images. In general, our model is able to detect visually distinctive species, but fails at other ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We present ViLD, an open-vocabulary object detection method by distilling knowledge from openvocabulary image classification models. ViLD is the first open-vocabulary detection method evaluated on the challenging LVIS dataset. It attains 16.1 AP for novel cateogires on LVIS with a ResNet50 backbone, which surpasses its supervised counterpart at the same inference speed. With a stronger teacher model (ALIGN), the performance can be further improved to 26.3 novel AP. We demonstrate that the detector learned from LVIS can be directly transferred to 3 other detection datasets. We hope that the simple design and strong performance make ViLD a scalable alternative approach for detecting long-tailed categories, instead of collecting expensive detection annotations.  <ref type="figure">Figure 6</ref>: Systematic expansion of dataset vocabulary with colors. We add 11 color attributes (red orange, dark orange, light orange, yellow, green, cyan, blue, purple, black, brown, white) to LVIS categories, which expand the vocabulary size by 11?. Above we show an example of detection results. Our open-vocabulary detector is able to assign the correct color to each fruit. A class-agnostic NMS with threshold 0.9 is applied. Each figure shows top 15 predictions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>Our paper studies open-vocabulary object detection, a sub-field in computer vision. Our method is based on knowledge distillation, a machine learning technique that has been extensively used in computer vision, natural language processing, etc. All of our experiments were conducted on public datasets with pretrained models that are either publicly available or introduced in published papers. The method proposed in our paper is a principled method for open-vocabulary object detection that can be used in a wide range of applications. Therefore, the ethical impact of our work would primarily depends on the specific applications. We foresee positive impacts if our method is applied to object detection problems where the data collection is difficult to scale, such as detecting rare objects for self-driving cars. But the method can also be applied to other sensitive applications that could raise ethical concerns, such as video surveillance systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A ADDITIONAL QUALITATIVE RESULTS</head><p>Transfer to PASCAL VOC: In <ref type="figure">Fig. 8</ref>, we show qualitative results of transferring an openvocabulary detector trained on LVIS <ref type="bibr" target="#b13">(Gupta et al., 2019)</ref> to PASCAL VOC Detection (2007 test set) <ref type="bibr" target="#b8">(Everingham et al., 2010)</ref>, without finetuning (Sec. 4.6 in the main paper). Results demonstrate that the transferring works well. <ref type="figure">Figure 8</ref>: Transfer to PASCAL VOC. ViLD correctly detects objects when transferred to PASCAL VOC, where images usually have lower resolution than LVIS (our training set). In the third picture, our detector is able to find tiny bottles, though it fails to detect the person.</p><p>Failure cases: In <ref type="figure" target="#fig_9">Fig. 9</ref>, we show two failure cases of ViLD. The most common failure cases are the missed detection. A less common mistake is misclassifying the object category. We show a failure case of mask prediction on PASCAL VOC in <ref type="figure" target="#fig_0">Fig. 10</ref>. It seems that the mask prediction is sometimes based on low-level appearance rather than semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ANALYSIS OF CLIP ON CROPPED REGIONS</head><p>In this section, we analyze some common failure cases of CLIP on cropped regions and discuss possible ways to mitigate these problems.</p><p>Visual similarity: This confusion is common for any classifiers and detectors, especially on large vocabularies. In <ref type="figure" target="#fig_0">Fig. 11(a)</ref>, we show two failure examples due to visual similarity. Since we only use a relatively small ViT-B/32 CLIP model, potentially we can improve the performance with a higher-capacity pretrained model. In <ref type="table" target="#tab_7">Table 6</ref>, when replacing this CLIP model with an EfficientNet-l2 ALIGN model, we see an increase on AP. <ref type="figure" target="#fig_0">Figure 10</ref>: An example of ViLD on PASCAL VOC showing a mask of poor quality. The class-agnostic mask prediction head occasionally predicts masks based on low-level appearance rather than semantics, and thus fails to obtain a complete instance mask. Aspect ratio: This issue is introduced by the pre-processing of inputs in CLIP. We use the ViT-B/32 CLIP with a fixed input resolution of 224 ? 224. It resizes the shorter edge of the image to 224, and then uses a center crop. However, since region proposals can have more extreme aspect ratios than the training images for CLIP, and some proposals are tiny, we directly resize the proposals to that resolution, which might cause some issues. For example, the thin structure in <ref type="figure" target="#fig_0">Fig. 11(b)</ref> right will be highly distorted with the pre-processing. And the oven and fridge can be confusing with the distorted aspect ratio. There might be some simple remedies for this, e.g., pasting the cropped region with original aspect ratio on a black background. We tried this simple approach with both CLIP and ALIGN. Preliminary results show that it works well on the fully convolutional ALIGN, while doesn't work well on the transformer-based CLIP, probably because CLIP is never trained with black image patches.</p><p>Multiple objects in a bounding box: Multiple objects in a region interfere CLIP's classification results, see <ref type="figure" target="#fig_0">Fig. 11(c)</ref>, where a corner of an aquarium dominates the prediction. This is due to CLIP pretraining, which pairs an entire image with its caption. The caption is usually about salient objects in the image. It's hard to mitigate this issue at the open-vocabulary classification model's end. On the other hand, a supervised detector are trained to recognize the object tightly surrounded by the bounding box. So when distilling knowledge from an open-vocabulary image classification model, keeping training a supervised detector on base categories could help, as can be seen from the improvement of ViLD over .</p><p>Confidence scores predicted by CLIP do not reflect the localization quality: For example, in <ref type="figure" target="#fig_0">Fig. 12(a)</ref>, CLIP correctly classifies the object, but gives highest scores to partial detection boxes. CLIP is not trained to measure the quality of bounding boxes. Nonetheless, in object detection, it is important for the higher-quality boxes to have higher scores. In <ref type="figure" target="#fig_0">Fig. 12(c)</ref>, we simply re-score by taking the geometric mean of the CLIP confidence score and the objectness score from the proposal model, which yields much better top predictions. In <ref type="figure" target="#fig_0">Fig. 12(b</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL QUANTITATIVE RESULTS</head><p>Hyperparameter sweep for visual distillation: <ref type="table" target="#tab_9">Table 7</ref> shows the parameter sweep of different distillation weights using L 1 and L 2 losses. Compared with no distillation, additionally learning from image embeddings generally yields better performance on novel categories. We find L 1 loss    We take the geometric mean of CLIP classification score and objectiveness score, and use it to rescore (a). In this way, a high-quality box as well as the correct category rank first.</p><p>can better improve the AP r performance with the trade-off against AP c and AP f . This suggests there is a competition between ViLD-text and ViLD-image.   <ref type="figure" target="#fig_0">Figure 13</ref>: Model architecture and training objectives for ViLD-ensemble. The learning objectives are similar to ViLD. Different from ViLD, we use two separate heads of identical architecture in order to reduce the competition between ViLD-text and ViLD-image objetvies. During inference, the results from the two heads are ensembled as described in Sec. 3.4. Please refer to <ref type="figure" target="#fig_1">Fig. 3</ref> for comparison with other ViLD variants.</p><p>Model used for qualitative results: For all qualitative results, we use a ViLD model with ResNet-152 backbone, whose performance is shown in <ref type="table" target="#tab_10">Table 8</ref>.</p><p>Details for supervised baselines: For a fair comparison, we train the second stage box/mask prediction heads of Supervised and Supervised-RFS baselines in the class-agnostic manner introduced in Sec. 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details for R-CNN style experiments:</head><p>We provide more details here for the R-CNN style experiments: CLIP on cropped regions in Sec. 4.2 and ViLD-text+CLIP in Sec. 4.3. 1) Generalized object proposal: We use the standard Mask R-CNN R50-FPN model. To report mask AP and compare with other methods, we treat the second-stage refined boxes as proposals and use the corresponding masks.</p><p>We apply a class-agnostic NMS with 0.9 threshold, and output a maximum of 1000 proposals. The objectness score is one minus the background score. 2) Open-vocabulary classification on cropped regions: After obtaining CLIP confidence scores for the 1000 proposals, we apply a class-specific NMS with a threshold of 0.6, and output the top 300 detections as the final results.</p><p>Additional details for ViLD variants: Different from the R-CNN style experiments, for all ViLD variants (Sec. 3.3, Sec. 3.4), we use the standard two-stage Mask R-CNN with the class-agnostic localization modules introduced in Sec. 3.1. Both the M offline proposals and N online proposals are obtained from the first-stage <ref type="bibr">RPN (Ren et al., 2015)</ref>. In general, the R-CNN style methods and ViLD variants share the same concept of class-agnostic object proposals. We use the second-stage outputs in R-CNN style experiments only because we want to obtain the Mask AP, the main metric, to compare with other methods. For ViLD variants, we remove the unnecessary complexities and show that using a simple one-stage RPN works well.</p><p>Architecture for open-vocabulary image classification models: Popular open-vocabulary image classification models <ref type="bibr">(Radford et al., 2021;</ref><ref type="bibr">Jia et al., 2021)</ref> perform contrastive pre-training on a large number of image-text pairs. Given a batch of paired images and texts, the model learns to maximize the cosine similarity between the embeddings of the corresponding image and text pairs, while minimizing the cosine similarity between other pairs. Specifically, for CLIP <ref type="bibr">(Radford et al., 2021)</ref>, we use the version where the image encoder adopts the Vision Transformer (Dosovitskiy et al., 2020) architecture and the text encoder is a Transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>. For ALIGN <ref type="bibr">(Jia et al., 2021)</ref>, its image encoder is an EfficientNet <ref type="bibr" target="#b37">(Tan &amp; Le, 2019)</ref> and its text encoder is a BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>.</p><p>Details for ViLD with stronger teacher models: In both experiments with CLIP ViT-L/14 and ALIGN, we use EfficientNet-b7 as the backbone and ViLD-ensemble for better performance. We also crop the RoI features from only FPN level P 3 in the feature pyramid. The large-scale jittering range is reduced to [0.5, 2.0]. For CLIP ViT-L/14, since its image/text embeddings have 768 dimensions, we increase the FC dimension of the Faster R-CNN heads to 1,024, and the FPN dimension to 512. For ViLD w/ ALIGN, we use the ALIGN model with an EfficientNet-l2 image encoder and a BERT-large text encoder as the teacher model. We modify several places in the Mask R-CNN architecture to better distill the knowledge from the teacher. We equip the ViLD-image head in ViLD-ensemble with the MBConvBlocks in EfficientNet. Since the MBConvBlocks are fullyconvolutional, we apply a global average pooling to obtain the image embeddings, following the teacher. The ViLD-text head keeps the same Faster R-CNN head architecture as in Mask R-CNN. Since ALIGN image/text embeddings have 1,376 dimensions (2.7? CLIP embedding dimension), we increase the number of units in the fully connected layers of the ViLD-text head to 2,048, and the FPN dimension to 1,024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text prompts:</head><p>Since the open-vocabulary classification model is trained on full sentences, we feed the category names into a prompt template first, and use an ensemble of various prompts. Following <ref type="bibr">Radford et al. (2021)</ref>, we curate a list of 63 prompt templates. We specially include several prompts containing the phrase "in the scene" to better suit object detection, e.g., "There is {article} {category} in the scene".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of our open-vocabulary detector with arbitrary texts. After training on base categories (purple), we can detect novel categories (pink) that are not present in the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Model architecture and training objectives. (a) The classification head of a vanilla two-stage detector, e.g., Mask R-CNN. (b) ViLD-text replaces the classifier with fixed text embeddings and a learnable background embedding. The projection layer is introduced to adjust the dimension of region embeddings to be compatible with the text embeddings. (c) ViLD-image distills from the precomputed image embeddings of proposals with an L1 loss. (d) ViLD combines ViLD-text and ViLD-image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>(c) shows the architecture. Zhu et al. (2019) use a similar approach to make Faster R-CNN features mimic R-CNN features, however, the details and goals are different: They reduce redundant context to improve supervised detection; while ViLD-image is to enable open-vocabulary detection on novel categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 3(d)shows the model architecture and training objectives. ViLD-image distillation only happens in training time. During inference, ViLD-image, ViLD-text and ViLD employ the same set of text embeddings as the detection classifier, and use the same architecture for open-vocabulary detection(Fig. 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>by 3.7 AP r , which employs two-stage training, self-training, and multi-scale testing etc. The results demonstrate ViLD scales well with the teacher model, and is a promising open-vocabulary detection approach. 4.5 PERFORMANCE COMPARISON ON COCO DATASET Several related works in zero-shot detection and open-vocabulary detection are evaluated on COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>" orange: 0.16 (b) After expanding vocabulary with color attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Systematic expansion of dataset vocabulary with fine-grained categories. We use the systematic expansion method to detect 200 fine-grained bird species in CUB-200-2011. (a): Our open-vocabulary detector is able to perform fine-grained detection (bottom) using the detector trained on LVIS (top). (b): It fails at recognizing visually non-distinctive species. It incorrectly assigns "Western Gull" to "Horned Puffin" due to visual similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Failure cases on LVIS novel categories. The red bounding boxes indicate the groundtruths of the failed detections. (a) A common failure type where the novel objects are missing, e.g., the elevator car is not detected. (b) A less common failure where (part of) the novel objects are misclassified, e.g., half of the waffle iron is detected as a calculator due to visual similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Other objects in the bounding box</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Typical errors of CLIP on cropped regions. (a): The prediction and the groundtruth have high visual similarity. (b): Directly resizing the cropped regions changes the aspect ratios, which may cause troubles. (c): CLIP's predictions are sometimes affected by other objects appearing in the region, rather than predicting what the entire bounding box is.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>The prediction scores of CLIP do not reflect the quality of bounding box localization. (a): Top predictions of CLIP on cropped region. Boxes of poor qualities receive high scores, though the classification is correct. (b): Top predictions of a vanilla Mask R-CNN model. Box qualities are good while the classification is wrong. (c):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>distills information</figDesc><table><row><cell>Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Cropped Regions</cell><cell>Pre-trained Image Encoder</cell><cell>I1</cell><cell>I2</cell><cell cols="5">Knowledge Distillation</cell><cell></cell><cell>L1 loss</cell></row><row><cell></cell><cell>Backbone</cell><cell></cell><cell>RoIAlign</cell><cell>conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+ RPN</cell><cell></cell><cell></cell><cell>conv</cell><cell></cell><cell>R2</cell><cell>R2?B1</cell><cell>R2?B2</cell><cell>...</cell><cell></cell><cell></cell><cell>Cross entropy loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>B1</cell><cell>B2</cell><cell>...</cell><cell>Bn</cell><cell></cell></row><row><cell></cell><cell>stop sign</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Base Categories Novel Categories</cell><cell>car ??? lapponian dice ???</cell><cell>A photo of a {category} in the scene</cell><cell>Pre-trained Text Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>text embeddings image embeddings region embeddings</cell></row><row><cell></cell><cell>herder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inference</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>B1</cell><cell>B2</cell><cell>...</cell><cell>Bn</cell><cell>N1</cell><cell>...</cell><cell>Nk</cell></row><row><cell></cell><cell>Backbone + RPN</cell><cell></cell><cell>RoIAlign</cell><cell>conv</cell><cell></cell><cell>R1</cell><cell>R1?B1</cell><cell>R1?B2</cell><cell>...</cell><cell>R1?Bn</cell><cell>R1?N1</cell><cell>...</cell><cell>R1?Nk</cell><cell>dice</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>conv</cell><cell></cell><cell>R2</cell><cell>R2?B1</cell><cell>R2?B2</cell><cell>...</cell><cell>R2?Bn</cell><cell>R2?N1</cell><cell>...</cell><cell>R2?Nk</cell><cell>herder lapponian</cell></row></table><note>R2?Bn R1Figure 2: An overview of using ViLD for open-vocabulary object detection. ViLD distills the knowledge from a pretrained open-vocabulary image classification model. First, the category text embeddings and the im- age embeddings of cropped object proposals are computed, using the text and image encoders in the pretrained classification model. Then, ViLD employs the text embeddings as the region classifier (ViLD-text) and mini- mizes the distance between the region embedding and the image embedding for each proposal (ViLD-image). During inference, text embeddings of novel categories are used to enable open-vocabulary detection.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Training with only base categories achieves comparable average recall (AR) for novel categories on LVIS. We compare RPN trained with base only vs. base+novel categories and report the bounding box AR.</figDesc><table><row><cell>Supervision</cell><cell cols="3">ARr@100 ARr@300 ARr@1000</cell></row><row><cell>base</cell><cell>39.3</cell><cell>48.3</cell><cell>55.6</cell></row><row><cell>base + novel</cell><cell>41.1</cell><cell>50.9</cell><cell>57.0</cell></row><row><cell cols="3">4.3 OPEN-VOCABULARY CLASSIFIER ON CROPPED REGIONS</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Using CLIP for open-vocabulary detection achieves high detection performance on novel categories.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of ViLD and its variants. ViLD outperforms the supervised counterpart on novel categories. Using ALIGN as the teacher model achieves the best performance without bells and whistles. All results are mask AP. We average over 3 runs for R50 experiments. ? : methods with R-CNN style; runtime is 630? of Mask R-CNN style. ? : for reference, fully-supervised learning with additional tricks.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell>APr APc AP f</cell><cell>AP</cell></row><row><cell>ResNet-50+ViT-B/32</cell><cell>CLIP on cropped regions  ? ViLD-text+CLIP  ?</cell><cell cols="2">18.9 18.8 16.0 17.7 22.6 24.8 29.2 26.1</cell></row><row><cell></cell><cell>Supervised-RFS (base+novel)</cell><cell cols="2">12.3 24.3 32.4 25.4</cell></row><row><cell></cell><cell>GloVe baseline</cell><cell cols="2">3.0 20.1 30.4 21.2</cell></row><row><cell>ResNet-50</cell><cell>ViLD-text ViLD-image</cell><cell cols="2">10.1 23.9 32.5 24.9 11.2 11.3 11.1 11.2</cell></row><row><cell></cell><cell>ViLD (w=0.5)</cell><cell cols="2">16.1 20.0 28.3 22.5</cell></row><row><cell></cell><cell>ViLD-ensemble (w=0.5)</cell><cell cols="2">16.6 24.6 30.3 25.5</cell></row><row><cell>EfficientNet-b7</cell><cell>ViLD-ensemble w/ ViT-L/14 (w=1.0) ViLD-ensemble w/ ALIGN (w=1.0)</cell><cell cols="2">21.7 29.1 33.6 29.6 26.3 27.2 32.9 29.3</cell></row><row><cell>ResNeSt269+HTC</cell><cell cols="3">2020 Challenge winner (Tan et al., 2020)  ? 30.0 41.9 46.0 41.5</cell></row><row><cell cols="2">4.4 VISION AND LANGUAGE KNOWLEDGE DISTILLATION</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance on COCO dataset compared with existing methods. ViLD outperforms all the other methods in the table trained with various sources by a large margin, on both novel and base categories.</figDesc><table><row><cell>Method</cell><cell>Training source</cell><cell cols="3">Novel AP Base AP Overall AP</cell></row><row><cell>Bilen &amp; Vedaldi (2016) Ye et al. (2019)</cell><cell>image-level labels in CB ? CN</cell><cell>19.7 20.3</cell><cell>19.6 20.1</cell><cell>19.6 20.1</cell></row><row><cell>Bansal et al. (2018)</cell><cell></cell><cell>0.31</cell><cell>29.2</cell><cell>24.9</cell></row><row><cell>Zhu et al. (2020)</cell><cell>instance-level labels in CB</cell><cell>3.41</cell><cell>13.8</cell><cell>13.0</cell></row><row><cell>Rahman et al. (2020)</cell><cell></cell><cell>4.12</cell><cell>35.9</cell><cell>27.9</cell></row><row><cell>Zareian et al. (2021)</cell><cell>image captions in CB ? CN instance-level labels in CB</cell><cell>22.8</cell><cell>46.0</cell><cell>39.9</cell></row><row><cell>CLIP on cropped regions ViLD-text ViLD-image ViLD (w = 0.5)</cell><cell>image-text pairs from Internet (may contain CB ? CN ) instance-level labels in CB</cell><cell>26.3 5.9 24.1 27.6</cell><cell>28.3 61.8 34.2 59.5</cell><cell>27.8 47.2 31.6 51.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>summarizes the results. ViLD outperforms Zareian et al. (2021) by 4.8 Novel AP and 13.5 Base AP. Different from Zareian et al. (2021), we do not have a pretraining phase tailored for detection. Instead, we use an off-the-shelf classification model. The performance of ViLD-text is low because only 48 base categories are available, which makes generalization to novel categories challenging. In contrast, ViLD-image and ViLD, which can distill image features of novel categories, outperform all existing methods (not apple-to-apple comparison though, given different methods use different settings).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Generalization ability of ViLD. We evaluate the LVIS-trained model with ResNet-50 backbone on PASCAL VOC 2007 test set, COCO validation set, and Objects365 v1 validation set. Simply replacing the text embeddings, our approaches are able to transfer to various detection datasets. The supervised baselines of COCO and Objects365 are trained from scratch. ? : the supervised baseline of PASCAL VOC is initialized with an ImageNet-pretrained checkpoint. All results are box APs. Qualitative results on LVIS, COCO, and Objects365. First row: ViLD is able to correctly localize and recognize objects in novel categories. For clarity, we only show the detected novel objects. Second row: The detected objects on base+novel categories. The performance on base categories is not degraded with ViLD.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">PASCAL VOC  ? AP50 AP75</cell><cell>AP</cell><cell cols="2">COCO AP50 AP75</cell><cell>AP</cell><cell cols="2">Objects365 AP50 AP75</cell></row><row><cell></cell><cell>ViLD-text</cell><cell>40.5</cell><cell>31.6</cell><cell>28.8</cell><cell>43.4</cell><cell>31.4</cell><cell>10.4</cell><cell>15.8</cell><cell>11.1</cell></row><row><cell></cell><cell>ViLD</cell><cell>72.2</cell><cell>56.7</cell><cell>36.6</cell><cell>55.6</cell><cell>39.8</cell><cell>11.8</cell><cell>18.2</cell><cell>12.6</cell></row><row><cell></cell><cell>Finetuning</cell><cell>78.9</cell><cell>60.3</cell><cell>39.1</cell><cell>59.8</cell><cell>42.4</cell><cell>15.2</cell><cell>23.9</cell><cell>16.2</cell></row><row><cell></cell><cell>Supervised</cell><cell>78.5</cell><cell>49.0</cell><cell>46.5</cell><cell>67.6</cell><cell>50.9</cell><cell>25.6</cell><cell>38.6</cell><cell>28.0</cell></row><row><cell>LVIS</cell><cell>novel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.08</cell><cell></cell><cell></cell></row><row><cell>LVIS</cell><cell>base + novel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transfer to</cell><cell>COCO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transfer to</cell><cell>Objects365</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Last two rows: ViLD can directly transfer to COCO and Objects365 without further finetuning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>ALIGN on cropped regions achieves superior APr, and overall very good performance.</figDesc><table><row><cell>It shows</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>), we show top predictions of the Mask R-CNN model. Its top predictions have good bounding boxes, while the predicted categories are wrong. This experiment shows that it's important to have both an open-vocabulary classification model for better recognition, as well as supervision from detection dataset for better localization.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter sweep for visual distillation in ViLD. L1 loss is better than L2 loss. For L1 loss, there is a trend that APr increases as the weight increases, while AP f,c decrease. For all parameter combinations, ViLD outperforms ViLD-text on APr. We use ResNet-50 backbone and shorter training iterations (84,375 iters), and report mask AP in this table.Distill loss Distill weight w APr APc AP f</figDesc><table><row><cell>AP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Performance of ViLD variants. This table shows additional box APs for models inTable 3and ResNet-152 results. AP c AP f AP AP r AP c AP f AP</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell>AP r</cell><cell>Box</cell><cell>Mask</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/openai/CLIP, ViT-B/32.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>We provide detailed descriptions of the proposed method in Sec. 3. Details about experiment settings, hyper-parameters and implementations are presented in Sec. 4, Appendix C and Appendix D. We release our code and pretrained models at https://github.com/tensorflow/tpu/ tree/master/models/official/detection/projects/vild to facilitate the reproducibility of our work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-cue zero-shot learning with strong supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling inter and intra-class relations in the triplet loss for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Le Cacheux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><forename type="middle">Le</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Crucianu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ramazan Gokberk Cinbis, and Nazli Ikizler-Cinbis. Zero-shot object detection by hybrid region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkan</forename><surname>Demirel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding. NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Link the head to the &quot;beak&quot;: Zero shot learning from noisy text description at part precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Piotr Doll?r, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Syed Waqas Zamir, and Fahad Shahbaz Khan. Synthesizing the unseen for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Zero shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked semanticsguided attention model for fine-grained zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><forename type="middle">Mark</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards open world object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Kj Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transductive learning for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved visual-semantic alignment for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluating knowledge transfer and zero-shot learning in a large-scale setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanming</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<title level="m">Joint COCO and LVIS workshop at ECCV 2020: LVIS Challenge Track</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Region graph embedding network for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Sen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cap2det: Learning to amplify weak caption supervision for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Berent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection using captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Dela</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Hao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Open vocabulary scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object detection with a unified label space from multiple datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Background learnable cascade for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13086</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Simple multi-dataset detection. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Don&apos;t even look once: Synthesizing features for zero-shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Supervised-Rfs</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>base+novel</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Supervised-Rfs</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>base+novel</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">In general, box AP is slightly higher than mask AP. In addition, we include the results of ViLD variants with the ResNet-152 backbone. The deeper backbone improves all metrics. The trend/relative performance is consistent for box and mask APs, as well as for different backbones</title>
	</analytic>
	<monogr>
		<title level="m">Box APs and ResNet-152 backbone: Table 8 shows the corresponding box AP of Table 3 in the main paper</title>
		<imprint/>
	</monogr>
	<note>ViLD-ensemble achieves the best box and mask AP r</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">We compare the text embeddings ensembled over synonyms and 63 prompt templates (listed in Appendix D) with a non-ensembled version: Using the single prompt template &quot;a photo of {article} {category}</title>
	</analytic>
	<monogr>
		<title level="m">Ablation study on prompt engineering: We conduct an ablation study on prompt engineering</title>
		<imprint/>
	</monogr>
	<note>Table 9 illustrates that ensembling multiple prompts slightly improves the performance by 0.4 AP r</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Ablation study on prompt engineering. Results indicate ensembling multiple prompt templates slightly improves APr. ViLD w/ multiple prompts is the same ViLD model in Table 3, and ViLD w/ single</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Details</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">a close-up photo of {article} {category}.&apos; &apos;a close-up photo of the {category}.&apos; &apos;a jpeg corrupted photo of {article} {category}</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vild-Ensemble Architecture</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fig. 13, we show the detailed architecture and learning objectives for ViLD-ensemble, the ensembling technique introduced in Sec. 3.4</title>
		<imprint/>
	</monogr>
	<note>the plastic {category}.&apos; &apos;a toy {category}.&apos; &apos;the toy {category}</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
