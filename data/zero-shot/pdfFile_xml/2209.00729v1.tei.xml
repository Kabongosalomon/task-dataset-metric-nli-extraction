<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HistoSeg : Quick attention with multi-loss function for multi-structure segmentation in digital histology images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saad</forename><surname>Wazir</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Sciences and Technology (NUST)</orgName>
								<address>
									<settlement>Islamabad</settlement>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Moazam Fraz</surname></persName>
							<email>moazam.fraz@seecs.edu.pk</email>
							<affiliation key="aff1">
								<orgName type="department">National University of Sciences and Technology (NUST)</orgName>
								<address>
									<settlement>Islamabad</settlement>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HistoSeg : Quick attention with multi-loss function for multi-structure segmentation in digital histology images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Computational Pathology</term>
					<term>Semantic Segmenta- tion</term>
					<term>Attention in Neural Networks</term>
					<term>Histology Images</term>
					<term>Medical Image Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image segmentation assists in computeraided diagnosis, surgeries, and treatment. Digitize tissue slide images are used to analyze and segment glands, nuclei, and other biomarkers which are further used in computer-aided medical applications. To this end, many researchers developed different neural networks to perform segmentation on histological images, mostly these networks are based on encoder-decoder architecture and also utilize complex attention modules or transformers. However, these networks are less accurate to capture relevant local and global features with accurate boundary detection at multiple scales, therefore, we proposed an Encoder-Decoder Network, Quick Attention Module and a Multi Loss Function (combination of Binary Cross Entropy (BCE) Loss, Focal Loss Dice Loss). We evaluate the generalization capability of our proposed network on two publicly available datasets for medical image segmentation MoNuSeg and GlaS and outperform the state-of-the-art networks with 1.99% improvement on the MoNuSeg dataset and 7.15% improvement on the GlaS dataset. Implementation Code is available at this link: https://bit.ly/HistoSeg</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In computer-aided diagnosis, a key role is played by segmentation of histological images in the number of applications, Detection, Segmentation, and Staging of objects in histological images <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref> like Glands, Benin, and Malignant nuclei or other biomarkers helps in monitoring, diagnosis, and treatment of patients. It also helps in surgery which is dependent on image-guided principles. Therefore, in the modern era, a variety of methods like creating state-of-the-art and intelligent methods for the segmentation of histological images are very popular. In the past few years, Deep Convolution Neural Networks (DCNN) took place of the most accurate and intelligent methods for the segmentation of histological images, and also the performance and accuracy of histological image segmentation is greatly increased.DCNN models which are specially designed for histological image segmentation like U-Net <ref type="bibr" target="#b15">[16]</ref>, MSRF-Net <ref type="bibr" target="#b21">[22]</ref>, U-Net++ <ref type="bibr" target="#b26">[27]</ref>, and MedT <ref type="bibr" target="#b8">[9]</ref> achieves greater performance and accuracy on very difficult datasets and prove that DCNN are very successful to learn the features to segment organs, nuclei, and other objects in histological images.</p><p>In segmentation, Encoder-Decoder networks show massive performance boosts like U-Net and DeepLabv3+ <ref type="bibr" target="#b2">[3]</ref>. Their Encoder-Decoder networks contain convolutions, pooling, and atrous convolutions with skip connections which make them simple and powerful as compared to more complicated networks. Many of the above mention networks lack the ability to model global features as compared to local features. But in recent years by the introduction of transformers which are adopted in computer vision applications, the ability to learn global and local features of these Encoder-Decoder networks greatly increase due to the use of attention mechanisms used in these transformer-based networks.</p><p>In recent years, feature learning techniques like feature pyramids, skip connections, atrous convolutions, attention mechanism and transformers are used to enhance the feature learning process and global feature representation. Furthermore, specific or customized loss functions are used to penalize segmentation task error for histological image segmentation. However, by combining different techniques like incorporating improved attention units and specific loss function, there is room for improvement in DCNN for segmentation of histological images. To this end, we propose an Encoder-Decoder network which includes Quick Attention Unit in encoder as well as in decoder branch with a residual connection between them, and we proposed a custom loss function which is a combination of fixed focal loss, binary cross-entropy, and dice loss. In summary, our main contributions are as follows </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Segmentation of histological images is a task performed on complicated and small datasets for this purpose U-Net was proposed which is specially designed for segmenting tumors in the brain and lungs. authors of U-Net proposed an architecture which is an Encoder-Decoder based model in which input is downsampled using convolution layers which are performed in encoder branch and then upsampled using learnable convolution layers which are performed in the decoder branch, they also incorporated skip connections which flow the information from the encoder part to the decoder part of the network, This ability lacks in Fully Convolutional Network(FCN) <ref type="bibr" target="#b11">[12]</ref>, which improves the overall results. DCNN which are specifically designed for segmentation of histological images <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> perform very well on medical images datasets as compared to natural images datasets. Zongwei et al proposed a U-Net++ which is variant of U-Net with modification in skip connections which merge the convolution layer in encoder with its corresponding upsampling layer in a decoder, they also incorporate dense skip connections. In MSRF-Net, authors proposed a fusion block that fuses features of multiple scales of different receptive fields, the encoder branch of MSRF-Net consists of squeeze and excitation module and the decoder branch has a triple attention mechanism that uses both channel and spatial wise attention with gated convolutions. With the advancements in image feature representation techniques, the networks are getting deeper and losing the capability of capturing relevant information. Complex and controlled attention modules are introduced to capture rich global and local contextual information, increase the receptive field of encoders, and ability to discard irrelevant information. like Prajit et al <ref type="bibr" target="#b12">[13]</ref> incorporates self-attention units in vision models to learn feature representations, Ashish et al <ref type="bibr" target="#b19">[20]</ref> proposed a Multi-scale self-guided attention for segmentation task in histological images.</p><p>The most recent development in computer vision are Transformers <ref type="bibr" target="#b9">[10]</ref>, they are introduced for natural language processing with more complicated attention units but these transformers can also be used for image processing tasks like in TransUNet <ref type="bibr" target="#b1">[2]</ref>, authors proposed a U shaped Encoder-Decoder network in which transformer is used as an encoder for feature extraction, the input image convert into patches and then gets into the transformer as the input sequence and a U-Net based network upsample the feature map. Jeya et al proposed a Medical Transformer <ref type="bibr" target="#b22">[23]</ref> which incorporates gated axial attention for medical image segmentation it's a transformer-based network which have self-attention modules with control mechanism in encoder due to this strategy the dependency of pre-trained weights is removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>Our proposed model is Encoder-Decoder network with Quick Attention Units and Multi Loss Function. We manage to perform semantic segmentation on histological images. The input of the model is RGB H&amp;E stain image and the model predicts the binary mask in the form of a binary image as output. The whole pipeline details are discussed below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quick Attention Unit</head><p>With the introduction of attention units in DCNN, it focuses on more relevant features of the object, so more and more complicated attention units are incorporated in DCNN which increases the computation cost so we introduced a relatively less computational expensive attention unit which is called Quick Attention. It is represented as</p><formula xml:id="formula_0">QA (x) = ? ( f (x) ) + x<label>(1)</label></formula><p>Here x is an input feature map, f() is a 1x1 convolution with stride 1 and the same number of filters as the input feature map, and ? is a sigmoid function. Quick Attention takes in the feature map as an input WxHxC (Width x Height x Channels) and creates two instances of the input feature map then it performs the 1x1xC convolution on the first instance and calculates the sigmoid activations after that it is added with the second instance to generate the final attention map as output which is of same dimensions as of input.Visual representation of our proposed attention unit is in <ref type="figure">Fig 1.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder-Decoder</head><p>The encoder consists of layers from input to global average pooling (GAP) as shown in <ref type="figure">Fig 1 part</ref> a. After some refinements on the input layer which has a dimension of 256x256x3, using 3x3 convolution, batch normalization, and relu activations the activation map goes through a series of expanded convolution blocks to capture dense features rather than sparse features in the case of simple convolution and max-pooling operation. Each expanded convolution block consists of 1x1 convolution, batch normalization, relu activation, depthwise convolution with a residual connection between every expanded convolution as shown in <ref type="figure">Fig 1 part c</ref>. The depthwise convolution capture features at different receptive fields by specifying dilation rate parameter. After expanded convolution 6, we introduced quick attention which allows a network to capture more relevant features and from now onwards the spatial dimension of feature maps remains the same 32x32xC before global average pooling, after expanded convolutions, Atrous Spatial Pyramid Pooling (ASPP) capture features at multiple scales, it performs 1x1 convolution, 3x3 dilated convolutions with the rate 6,12,18 and image pooling after that it concatenates all the features maps and performs 1x1 convolution as shown in <ref type="figure">Fig 1 part d</ref>. After ASPP block global average pooling is performed. The decoder of the network performs upsampling and creates a 32x32x256 feature map which is then concatenated with the feature map from ASPP block, 1x1 convolution is used to match the channels of the feature map. then we again add quick attention to the decoder and create a residual connection between encoder and decoder quick attention modules, then sigmoid activation is performed followed by upsampling layer which uses bilinear interpolation by a factor of 4 and generates the maks of size 256x256x1.  <ref type="figure">Fig. 1</ref>: Overall architecture of our proposed network, Expanded Convolutions from 1 to 6 have dilation rate = 1, from 7 to 13 have dilation rate = 2, and from 14 to 16 have dilation rate = 4, the dilation rate in expanded convolutions is used for depthwise convolution operations. Expanded Convolutions which have the same dimensions are grouped together. The overall pipeline to train the model is that we create patches of histology images using the sliding window technique and feed them into the model for training and validation. After training, we evaluate the model without creating patches full-size image is fed into the model for predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi Loss Function</head><p>Our proposed loss function is a combination of BCE Loss, Focal Loss, and Dice loss. Each one of them contributes individually to improve performance further details of loss functions are mentioned below, (1) BCE Loss calculates probabilities and compares each actual class output with predicted probabilities which can be either 0 or 1, it is based on Bernoulli distribution loss, it is mostly used when there are only two classes are available in our case there are exactly two classes are available one is background and other is foreground. In a proposed method it is used for pixel-level classification. BCE Loss represent as</p><formula xml:id="formula_1">BCELoss (y, y) = ? (ylog (y) + (1 ? y) log (1 ? y)) (2)</formula><p>Here y is the actual value and ? is the predicted value </p><formula xml:id="formula_2">FocalLoss (p ) = ?? (1 ? p ) ? log (p )<label>(3)</label></formula><p>Here ? is always greater than zero but when ? is equal to 1 then it works like a Cross Entropy function, the range of ? is between 0 to 1, it is treated as a hyperparameter.</p><p>(3) Dice Loss is inspired by the Dice Coefficient Score which is an evaluation metric used to evaluate the results of image segmentation tasks. Dice Coefficient is convex in nature so it has been changed, so it can be more traceable. It is used to calculate the similarity between two images, Dice Loss represent as DiceLoss (y, p) = 1 ? (2yp + 1) ? (y + p + 1)</p><p>Here 1 is added to ensure that function is not become undefined in edge case such as y = p = 0</p><p>We proposed a Loss function which is a combination of all three above mention loss functions to benefit from all, BCE is used for pixel-wise classification, Focal Loss is used for learning hard examples, we use 0.25 as the value for alpha and 2.0 as the value of gamma. Dice Loss is used for learning better boundary representation, our proposed loss function represent as  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Preprocessing</head><p>We have first created patches of each image using the sliding window technique with a 256 step size to avoid overlap. For the MoNuSeg dataset, we have generated patches from training data of 30 RGB images having a size of 1000x1000x3 (Height x Width x Channels) and their corresponding masks having a size of 1000x1000x1. For test data, we didn't apply any augmentation or create patches test data consist of 14 RGB images and their corresponding masks. We split the training data into training and validation sets, the overall dataset stats are mentioned in <ref type="table" target="#tab_1">Table I</ref> For the GlaS dataset, we have generated patches from training data of 85 RGB images having multiple image sizes and their corresponding masks. For test data, we resize 80 images and their corresponding masks. We split the training data into training and validation sets, the overall dataset stats are mentioned in <ref type="table" target="#tab_1">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION METRICS</head><p>To evaluate our proposed model, we employed three evaluation metrics, details are given below (1) F1 Score is used to measure the detection accuracy of objects. When the object is segmented and if it intersects a minimum of fifty percent with its ground truth then it is considered as true positive otherwise it is a false positive. The difference between the number of objects from ground truth and the number of true positives is considered as false negatives. Given that the F1 Score represent as 2 Loss = (BCELoss + FocalLoss) + DiceLoss (5) F 1(A, B) = |A|/|A ? B| + |B|/|A ? B| (6)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASETS</head><p>We have performed experiments on two datasets Multiorgan Nucleus Segmentation (MoNuSeg) <ref type="bibr" target="#b10">[11]</ref> and Gland Segmentation in Colon Histology Images (GlaS) <ref type="bibr" target="#b20">[21]</ref>, both are publicly available datasets and used for histological image segmentation task, MoNuSeg Dataset consists of 44 Hema-Here A and B are binary vectors and the backslash is the set minus.</p><p>(2) Dice Score measures the overlap between predicted output and ground truth, so basically, it's a measure of similarity of objects. Dice Score is defined as toxylin and Eosin (H&amp;E) stained tissue images scanned at 40x magnification. This dataset has seven organ types with Dice(A, B) = 2|A ? B| |A| + |B| (3) Intersection over Union (IoU) is also known as the Jaccard index, it measures the percent overlap between predicted output and ground truth. The average IoU over all classes is known as Mean IoU (mIoU). The IoU is defined as</p><formula xml:id="formula_5">IoU(A, B) = |A ? B| = |A| (8) | max(A, B)| |A| + |B| ? |A ? B|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTATION AND RESULTS</head><p>We performed our experiments on Google Colab. The datasets are converted into NumPy arrays after preprocessing. Train, Val, and Test splits are loaded into the main memory and fed into the network. Our proposed model is trained in end-toend fashion for 200 epochs on MoNuSeg and for 100 epochs on GlaS, batch size of 8 was set. We use Adam optimizer with 0.01 learning rate, pre-trained mobilenetv2 <ref type="bibr" target="#b16">[17]</ref> as a feature extractor with an output stride of 8, sigmoid activations instead of softmax at the last layer, and 0.1 dropout. Quantitative results on GlaS and MoNuSeg Datasets are reported in <ref type="table" target="#tab_1">Table  III</ref> and results of comparison with other networks are reported in <ref type="table" target="#tab_1">Table IV</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ABLATION STUDY</head><p>For the ablation study, we incorporate different attention methods in our proposed network. The self Attention method is implemented in the encoder branch then it produces effective results but when it is implemented in the decoder branch it gives a negative F1 score and validation loss is not decreasing. Residual Channel Attention, Residual Spatial Attention, and Residual Mixed Attention (combination of Channel Attention and Spatial Attention) methods are implemented in the decoder branch and replaced by some convolution layers then they perform better otherwise when they are implemented in an encoder branch the validation loss is not decreasing and results are not improving after a few epochs. The quantitative results of different attention methods on both MoNuSeg and GlaS datasets are shown in <ref type="table">Table V</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION AND FUTURE WORK</head><p>Accurate segmentation of multi-structural objects at multiple scales in histological images is very crucial because these results are later used for further biological analysis and prognosis. To this end, we proposed a neural network which utilized Encoder-Decoder structure, Quick Attention Units,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE V: Ablation Study</head><p>Attention Method MoNuSeg GlaS Self Attention <ref type="bibr" target="#b12">[13]</ref> 0.56 IoU 0.61 IoU Residual Channel Attention <ref type="bibr" target="#b23">[24]</ref> 0.37 IoU 0.31 IoU 0.37 IoU 0.35 IoU Residual Spatial Attention <ref type="bibr" target="#b23">[24]</ref> Residual Mixed Attention <ref type="bibr" target="#b23">[24]</ref> 0.40 IoU 0.45 IoU Loss Function produces more accurate and improved results over multiple datasets which shows the improved generalization capability of our proposed model. In future we extend the model capability to capture multi class and multi structure features in a single pipeline, include more datasets to evaluate the generalization capability of model, build a pipeline to use our proposed approach in instance segmentation and panoptic segmentation area and introduce model inference optimization by quantization of weights and skip some layers so we can decrease the model inference time and use it on edge devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 2 )</head><label>2</label><figDesc>Focal Loss is a variant of BCE, it enables the model to focus on learning hard examples by decreasing the wights of easy examples it works well when the data is highly imbalanced, Focal Loss represent as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. Training graphs as shown in Fig 3 reveals that validation loss is decreasing after every epoch and the model does not overfit. Our proposed model outperforms the base models which shows a 1.99 % improvement on the MoNuSeg dataset and 7.15 % improvement on the GlaS dataset.The Qualitative results are shown in Fig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>Qualitative results of our proposed method.First two rows shows the results from MoNuSeg Dataset and last two rows shows the results from GlaS Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>Training and Validation Loss graphs of our proposed network and Multi Loss Function. The proposed attention units are much simpler and effective as compared to other attention units. The attention units in our proposed method are used in both encoder and decoder with residual connection to ensure that model focuses on more relevant global and local features at multiple scales. Quick Attention Units combined with Multi</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Quick Attention Unit for better global and local feature representation by avoiding irrelevant features. Multi Loss Function which is a combination of fixed focal loss, binary cross-entropy loss, and dice loss which focus on learning hard negative examples and crisp boundary detection.</figDesc><table /><note>? Proposes a? Introduces a? Proposes an Encoder-Decoder network which capture dense multi scale features.? Successfully improves the accuracy and performance over the different state of the art models on two different datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Distribution of MoNuSeg dataset</figDesc><table><row><cell>Distribution</cell><cell cols="2">No of images Image Size</cell><cell>Mask Size</cell></row><row><cell>Training Set</cell><cell>1470</cell><cell>256x256x3</cell><cell>256x256x1</cell></row><row><cell cols="2">Validation Set 686</cell><cell>256x256x3</cell><cell>256x256x1</cell></row><row><cell>Test Set</cell><cell>14</cell><cell cols="2">1000x1000x3 1000x1000x1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Distribution of GlaS dataset</figDesc><table><row><cell>Distribution</cell><cell cols="3">No of images Image Size Mask Size</cell></row><row><cell>Training Set</cell><cell>1644</cell><cell>256x256x3</cell><cell>256x256x1</cell></row><row><cell cols="2">Validation Set 703</cell><cell>256x256x3</cell><cell>256x256x1</cell></row><row><cell>Test Set</cell><cell>80</cell><cell>400x400x3</cell><cell>400x400x1</cell></row><row><cell cols="4">benign and malignant tissue samples from multiple patients.</cell></row><row><cell cols="4">GlaS Dataset consists of 165 H&amp;E stained images with benign</cell></row><row><cell cols="4">and malignant tissue samples from multiple patients.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Quantitative results of our proposed network</figDesc><table><row><cell></cell><cell>MoNuSeg</cell><cell></cell><cell></cell><cell>GlaS</cell><cell></cell></row><row><cell>F1</cell><cell>IoU</cell><cell>Dice</cell><cell>F1</cell><cell>IoU</cell><cell>Dice</cell></row><row><cell cols="6">75.08 71.06 95.20 98.07 76.73 99.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Quantitative comparison of the proposed method with other networks Net [25] 76.83 62.49 76.26 63.03 MedT [9] 79.55 66.17 81.02 69.61 Proposed Network 75.08 71.06 98.07 76.73</figDesc><table><row><cell>Network</cell><cell cols="2">MoNuSeg</cell><cell>GlaS</cell></row><row><cell></cell><cell>F1</cell><cell>IoU</cell><cell>F1</cell><cell>IoU</cell></row><row><cell>FCN [12]</cell><cell cols="4">28.84 28.71 66.61 50.84</cell></row><row><cell>U-Net [16]</cell><cell cols="4">79.43 65.99 77.78 65.34</cell></row><row><cell>U-Net++ [27]</cell><cell cols="4">79.49 66.04 78.03 65.55</cell></row><row><cell>Res-UNet [26]</cell><cell cols="4">79.49 66.07 78.83 65.95</cell></row><row><cell>Deeplabv3+ [3]</cell><cell cols="4">72.16 66.56 76.01 67.04</cell></row><row><cell>Axial Attention U-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks for Automatic Polyp Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Awadelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahmed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FABnet: feature attention-based network for simultaneous segmentation of microvessels and nerves in routine histology images of oral cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mm Fraz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uncertainty driven pooling network for microvessel segmentation in routine histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mm Fraz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational pathology and ophthalmic medical image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MISSFormer: An Effective Medical Image Segmentation Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07162</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cellular community detection for tissue phenotyping in colorectal cancer histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajid</forename><surname>Javed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">101696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Doubleu-net: A deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 33rd International symposium on computerbased medical systems (CBMS). IEEE. 2020</title>
		<imprint>
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Medical transformer: gated axialattention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Transformers in Vision: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<idno>CoRR abs/2101.01169</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiscale Dilated UNet for Segmentation of Multi-Organ Nuclei in Digital Histology Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Conference on Smart Communities: Improving Quality of Life Using ICT, IoT and AI (HONET). 2020</title>
		<imprint>
			<biblScope unit="page" from="68" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiscale Unified Network for Simultaneous Segmentation of Nerves and Micro-vessels in Histology Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afia</forename><surname>Rasool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Moazam</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajid</forename><surname>Javed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2). 2021</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<idno>CoRR abs/1801.04381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel digital score for abundance of tumour infiltrating lymphocytes predicts disease free survival in oral squamous cell carcinoma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Shaban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific reports</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-aware convolutional neural network for grading of colorectal cancer histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Shaban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale self-guided attention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gland segmentation in colon histology images: The glas challenge contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korsuk</forename><surname>Sirinukunwattana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1361" to="8415" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07451</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Medical Transformer: Gated Axial-Attention for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeya Maria Jose</forename><surname>Valanarasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021</title>
		<editor>Marleen de Bruijne et al. Cham</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axialattention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weighted Res-UNet for High-Quality Retina Vessel Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Information Technology in Medicine and Education (ITME)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
