<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Zero-Shot Action Recognition with Prioritised Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
							<email>xun.xu@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
							<email>s.gong@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Task Zero-Shot Action Recognition with Prioritised Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zero-Shot Learning (ZSL) promises to scale visual recognition by bypassing the conventional model training requirement of annotated examples for every category. This is achieved by establishing a mapping connecting low-level features and a semantic description of the label space, referred as visual-semantic mapping, on auxiliary data. Reusing the learned mapping to project target videos into an embedding space thus allows novel-classes to be recognised by nearest neighbour inference. However, existing ZSL methods suffer from auxiliary-target domain shift intrinsically induced by assuming the same mapping for the disjoint auxiliary and target classes. This compromises the generalisation accuracy of ZSL recognition on the target data. In this work, we improve the ability of ZSL to generalise across this domain shift in both model-and data-centric ways by formulating a visual-semantic mapping with better generalisation properties and a dynamic data re-weighting method to prioritise auxiliary data that are relevant to the target classes. Specifically: (1) We introduce a multi-task visual-semantic mapping to improve generalisation by constraining the semantic mapping parameters to lie on a low-dimensional manifold, (2) We explore prioritised data augmentation by expanding the pool of auxiliary data with additional instances weighted by relevance to the target domain. The proposed new model is applied to the challenging zero-shot action recognition problem to demonstrate its advantages over existing ZSL models.</p><p>1 Target and testing all refer to categories (e.g. action classes) to be recognised without labelled examples. 2 Auxiliary and training all refer to categories (e.g. action classes) with labelled data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Action recognition has long been a central topic in computer vision <ref type="bibr" target="#b0">[1]</ref>. A major thrust in action recognition is scaling methods to a wider and finer range of categories <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. The traditional approach to dealing with a growing number of categories is to collect labeled training examples of each new category. This is not scalable, particularly in the case of actions, due to the temporally extended nature of videos compared to images, making annotation (segmentation in both space and time) more onerous than for images. In contrast, the Zero-Shot Learning (ZSL) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> paradigm is gaining significant interest by providing an alternative to classic supervised learning which does not require an ever increasing amount of annotation. Instead of collecting training data for the target arXiv:1611.08663v1 [cs.CV] 26 Nov 2016 categories 1 to be recognised, a classifier is constructed by re-using a visual to semantic space mapping pre-learned on a training/auxiliary set 2 of totally independent (disjoint) categories. Specifically training class labels are represented in a vector space such as attribute <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> or word-vectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. Such vector representations of class-labels are referred to as semantic label embeddings <ref type="bibr" target="#b6">[7]</ref>. A mapping (e.g. regression <ref type="bibr" target="#b8">[9]</ref> or bilinear model <ref type="bibr" target="#b6">[7]</ref>) is learned between low-level visual features and their semantic embeddings. This mapping is assumed to generalise and be re-used to project visual features of target classes into semantic embedding space and matched against target class embeddings.</p><p>A fundamental challenge for ZSL is that in the context of supervised learning of the visual-semantic mapping, the ZSL setting violates the traditional assumption of supervised learning <ref type="bibr" target="#b9">[10]</ref> -that training and testing data are drawn from the same distribution. Thus its efficacy is reduced by domain shift <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. For example, when a regressor is used to map visual features to semantic embedding, the disjoint training and testing classes in ZSL intrinsically require the regressor to generalise out-of-bounds. This inherently limits the accuracy of ZSL recognition. In this work, we address the issue of the generalisation capability of a ZSL mapping regressor from both the model-and data-centric perspectives: (1) by proposing a more robust regression model with better generalisation properties, and (2) improving model learning by augmenting auxiliary data with a re-weighted additional dataset according to the relevance to the target problem.</p><p>Multi-Task Embedding When establishing the mapping between visual features and semantic embeddings, most ZSL methods learn each dimension of this mapping independently -whether semantic embedding is discrete as in the case of attributes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, or continuous as in the case of word vectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. This strategy is likely to overfit to the training classes because it treats each dimension of the label in semantic embedding independently despite the labels living on a non-uniform manifold <ref type="bibr" target="#b13">[14]</ref> and many independent mappings result in a large number of parameters to be learned. We denote this conventional approach as Single-Task Learning (STL) due to the independent learning of mappings for each attribute/word dimension. In contrast, we advocate a Multi-Task Learning (MTL) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10]</ref> regression approach to mapping visual features and their semantic embeddings. By constraining the mapping parameters of each learning task to lie closely on a low-dimensional manifold, we gain two advantages: <ref type="bibr" target="#b0">(1)</ref> Exploiting the relation between the response variables (dimensions of the label embedding), (2) reducing the total number of parameters to fit. The resulting visual-semantic mapping is more robust to the domain shift between ZSL training and testing classes. As a helpful byproduct, the MTL mapping, provides a lower dimensional latent space in which the nearest neighbour (NN) matching required by ZSL can be better performed <ref type="bibr" target="#b16">[17]</ref> compared to the usual higher dimensional label semantic embedding space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prioritised Auxiliary Data Augmentation for Domain Adaptation</head><p>From a data-, rather than model-centric perspective, studies have also attempted to improve the generalisation of ZSL methods by augmenting 3 the auxiliary dataset with additional datasets containing a wider array of classes and instances <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. The idea is that including a broader additional set should provide better coverage of the visual feature and label embedding spaces, therefore helping to learn a visual-semantic mapping that better generalises to target classes, and thus improves performance when representing and recognising target classes. However, existing studies on exploring this idea have been rather crude, e.g. simply expanding the training dataset by blindly concatenating auxiliary set with additional data <ref type="bibr" target="#b8">[9]</ref>. This is not only inefficient but also dangerous, because it does not take into account the (dis)similarity between the extra incorporated data and the target classes for recognition, thus risking negative transfer <ref type="bibr" target="#b9">[10]</ref>. In this work, we address the issue that auxiliary and target data/categories will have different marginal distributions <ref type="figure">(Fig 1)</ref>. We selectively re-weight those relevant instances/classes from the auxiliary data that are expected to improve the the visual-semantic mapping in the context of the specific target classes to be recognised (target domain). We formulate this prioritised data augmentation as a domain adaptation problem by minimizing the discrepancy between the marginal distributions of the auxiliary and target domains. To achieve this, we propose an importance weighting strategy to re-weight each auxiliary instance in order to minimise the discrepancy. Specifically we generalise the classic Kullback-Leibler Importance Estimation Procedure (KLIEP) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> to the zero-shot learning problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Zero-Shot Learning Zero-shot Learning (ZSL) <ref type="bibr" target="#b4">[5]</ref> aims to generalize existing knowledge to recognize new categories without training examples by re-using a mapping learned from visual features to their semantic embeddings. Commonly used label embeddings are semantic attributes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11]</ref> and word-vectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>. The latter has the advantage of being learned from data without requiring manual annotation. Commonly used visual-semantic mappings include linear <ref type="bibr" target="#b11">[12]</ref> and non-linear regression <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>, classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>, and bilinear ranking <ref type="bibr" target="#b6">[7]</ref>. Existing ZSL methods suffer from weak generalisation due to the domainshift induced by disjoint auxiliary-target classes, an issue that has recently been highlighted explicitly in the literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Attempts to address this so far include post-processing heuristics <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, sparse coding regularisation <ref type="bibr" target="#b7">[8]</ref>, and simple blind enlarging of the training set with auxiliary data <ref type="bibr" target="#b8">[9]</ref>. In contrast to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, we focus on: (1) Building a visual-semantic mapping with intrinsically better generalisation properties, and (2) re-weighting the auxiliary set to prioritise auxiliary instances most relevant to the target instances and classes. Our method is complementary to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> and can benefit from these heuristics. Zero-Shot Action Recognition Among many ZSL tasks in computer vision, zero-shot action recognition <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> is of particular interest because of the lesser availability of labelled video compared to image data and videos are more difficult to label than static images due to extended temporal duration and more complex ontology. ZSL action recognition is much less studied than still image recognition, and existing video-ZSL methods suffer from the same domain-shift drawbacks highlighted above. Multi-Task Regression Learning Multi-Task Learning (MTL) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref> aims to improve generalisation in a set of supervised learning tasks by modelling and exploiting shared knowledge across the tasks. An early study <ref type="bibr" target="#b14">[15]</ref> proposed to model the weight vector for each task t as a sum of a shared global task w 0 and task specific parameter vector w t . However, the assumption of a globally shared underlying task is too strong, and risks inducing negative transfer <ref type="bibr" target="#b9">[10]</ref>. This motivates the Grouping and Overlapping Multi-Task Learning (GOMTL) <ref type="bibr" target="#b15">[16]</ref> framework which instead assumes that each task's weight vector is a task-specific combination of a small set of latent basis tasks. This constrains the parameters of all tasks to lie on a low dimensional manifold.</p><p>MTL methods have been studied for action recognition <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. However, all of these studies focus on improving standard supervised action recognition with multi-task sharing. For example, considering each of multiple views <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, feature modalities <ref type="bibr" target="#b26">[27]</ref>, or -most obviously -action categories <ref type="bibr" target="#b25">[26]</ref> as different tasks. Multi-view/multi-feature recognition is orthogonal to our work, while the later ones are concerned with supervised recognition, and cannot be generalised to the ZSL scenario. In contrast, we take a very different approach and treat each dimension of the visual-semantic mapping as a task, in order to leverage MTL to improve auxiliary-target generalisation across the disjoint target categories. Finally, we note that the use of MTL to learn the visual semantic mapping provides a further benefit of a lower-dimensional space in which zero- Visual feature matrix for N instances; column representing the i-th instance Y ? {0, 1} nc?nx ; yi Binary class labels for N instances 1-of-nc encoding; column representing the i-th instance V ? R dz ?nc ;</p><p>Semantic label embedding for nc categories; Z ? R dz ?nx ; zi Semantic label embedding for nx instances; column representing the i-th instance W ? R dz ?dx ; wd STL regression coefficient matrix; row representing the regressor for the d-th dimension A ? R T ?dx ; at MTL regression coefficient matrix; row representing the regressor for the t-th latent task S ? R dz ?T ; sd MTL linear combination matrix; row representing linear combination vector for the d-th output</p><formula xml:id="formula_0">L ? R T ?nx ; li Latent space embedding for visual instances; column is ith instance ? ? R nx?1</formula><p>weighting vector for auxiliary data f : X ? Z Visual to semantic mapping function shot recognition can be better performed due to being more meaningful for NN matching <ref type="bibr" target="#b16">[17]</ref>. Importance Weighting for Domain Adaptation Domain shift is a widely studied problem in transfer learning <ref type="bibr" target="#b9">[10]</ref>, although it is usually induced by sampling bias <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> or sensor change <ref type="bibr" target="#b31">[32]</ref> rather than the disjoint categories in ZSL. Importance weighting (IW) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref> has been one of the main adaptation techniques to address this issue. The prior work in this area is designed for the standard domain transfer problem in a supervised learning setting <ref type="bibr" target="#b32">[33]</ref>, while we are the first to generalise it to the zero-shot learning scenario. The IW technique we generalise is related to another domain adaptation approach based on discovering a feature mapping to minimise the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> between distributions. However MMD, is less appropriate for us due to focus on feature mapping rather than instance reweighing, and our expectation is that only subsets of auxiliary instances will be relevant to the target rather than the holistic auxiliary set.</p><p>Contributions This paper contributes both model-and data-centric strategies to improve ZSL action recognition: (1) We formulate learning a more generalisable visual-semantic mapping in ZSL as a multi-task learning problem with a lower-dimensional latent semantic embedding space for more effective matching. <ref type="formula" target="#formula_3">(2)</ref> We improve visual-semantic regression generalisation by prioritised data augmentation using importance weighting of auxiliary instances relevant to the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual-Semantic Mapping with Multi-Task Regression</head><p>In ZSL, we aim to recognise action categories Y given visual features X where training/auxiliary and testing/target categories do not overlap Y tr ? Y te = ?.</p><p>The key method by which ZSL is achieved is to embed each category label in Y into a semantic label embedding space Z which provide a vector representation of any nameable category. <ref type="table" target="#tab_1">Table 1</ref> summarises the notation used in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training a Visual Semantic Mapping</head><p>We first introduce briefly the conventional single task learning using regression for visual-semantic mapping <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Single-Task Regression Given a matrix V describing the embedded action names 4 , and per-video binary labels Y, we firstly obtain the label embedding of any action label for a video clip as z i = Vy i . We then learn a visual-semantic mapping function f : X ? Z on the training categories. Given a loss function l(?, ?), we learn the mapping f by optimising Eq <ref type="formula" target="#formula_1">(1)</ref> where ?(f ) denotes regularization on the mapping:</p><formula xml:id="formula_1">min f 1 n tr x n tr x i=1 l (f (x i ), z i ) + ?(f ).<label>(1)</label></formula><p>The most straightforward choice of mapping f and loss l is linear f (x) = Wx, and square error respectively, which results in a regularized linear (ridge) regression problem:</p><formula xml:id="formula_2">l (f (x i ), z i ) = ||z i ? Wx i || 2 2 . A closed-form solution to W can then be obtained by W = ZX T XX T + ?n tr x I ?1 . Each row w d of regressor W maps visual feature x i to dth dimension of response variable z i . Since regres- sors {w d } d=1???dz</formula><p>are learned independently from each other this is referred as single-task learning (STL) with each w d defining one distinct 'task'. From Single to Multi-Task Regression In the conventional ridge-regression solution to Eq. (1), each task w d is effectively learned separately, ignoring any relationship between tasks. We wish to model this relationship by discovering a latent basis of predictors such that tasks w d are constructed as linear combinations of T latent tasks {a t } t=1???T . So the dth regression predictor is now modelled as w d = t s dt a t = s T d A, where s d is the combination coefficient for d-th task. Denoting multi-task regression prediction as f (x i , S, A), we now optimise:</p><formula xml:id="formula_3">min S,A 1 n tr x n tr x i=1 l(f (x i , S, A), z i ) + ??(S) + ?? (A).<label>(2)</label></formula><p>Grouping and Overlap Multi-Task Learning An effective method following the MTL design pattern above is GOMTL <ref type="bibr" target="#b15">[16]</ref>. GOMTL uses a W = SA task parameter matrix factorisation, where the number of latent tasks T (typically T &lt; d z ) is a free parameter. Requiring the combination coefficients s t to be sparse, via a 1 regulariser, the loss is written as</p><formula xml:id="formula_4">min {st},A T t=1 1 n tr x n tr x i=1 ||z t,i ? s t Ax i || + ? T t=1 ||s t || 1 + ?||A|| 2 F<label>(3)</label></formula><p>This can be solved by iteratively updating A and S. When A is fixed, loss function reduces to a standard L1 regularized (LASSO) regression problem that can be efficiently solved by Alternating Direction Method of Multipliers (ADMM) <ref type="bibr" target="#b35">[36]</ref>. When S is fixed, we can efficiently solve A by gradient descent. Regularized Multi-Task Learning (RMTL) The classic RMTL method <ref type="bibr" target="#b14">[15]</ref> models task parameters as the sum of a globally shared and task specific parameter vector: w t = a 0 + a t . It can be seen that this corresponds to a special case of GOMTL's W = SA predictor matrix factorisation <ref type="bibr" target="#b24">[25]</ref>. Here there are T = d z +1 latent tasks, a fixed task combination vector s t = [1 1(t = 1) 1(t = 2) ? ? ? 1(t = d z )] T where 1(?) is the indicator function and A = a T 0 a T 1 ? ? ? a T dz T .</p><p>Explicit Multi-Task Embedding (MTE) In GOMTL Eq <ref type="formula" target="#formula_4">(3)</ref>, it can be seen that the label embedding z i is approximated from the data by the mapping s t Ax i , and this approximation is reached by combination via the latent representation Ax i . While GOMTL defines this space implicitly via the learned A, we propose to model it explicitly as l i ? Ax i . This is so the actual projections l i in this latent space can be regularised explicitly, in order to learn a latent space which generalises better to test data, and hence improves ZSL matching later. Specifically, we split the GOMTL loss ||z i ? SAx i || 2 2 into two parts: ||l i ? Ax i || 2 2 and ||z i ? Sl i || 2 2 to learn the mapping to the latent space, and from the latent space to the label embedding respectively. This allows us to place additional regularization on l i to avoid extreme values in the latent space and thus later improve neighbour matching (Section 3.2). Given the large and high dimensional video datasets, we apply Frobenius norm on S in contrast to GOMTL's 1 .</p><formula xml:id="formula_5">min {st},A,{li} T t=1 1 n tr x n tr x i=1 ||z t,i ? s t l i || 2 2 + ||l i ? Ax i || 2 2 + ? S T t=1 ||s t || 2 2 + ? A ||A|| 2 F + ? L n tr x i=1 ||l i || 2 2<label>(4)</label></formula><p>Our explicit multi-task embedding has similarities to <ref type="bibr" target="#b17">[18]</ref>, but our purpose is multi-task regression for ZSL, rather than embedding for video descriptions. To solve our explicit embedding model we iteratively solve L,A and S while fixing the other two. With the 2 norm on S, this has a convenient closed-form solution to each parameter:</p><formula xml:id="formula_6">L = (S T S + (? L n tr x + 1)I) ?1 (S T Z + AX) S = ZL T (LL T + ? S n tr x I) ?1 A = LX T (XX T + ? A n tr x I) ?1<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Zero-Shot Action Recognition</head><p>We consider two alternative NN matching methods for zero-shot action prediction that use the MTL mappings described above.</p><p>Distributed Space Matching Given a trained visual-semantic regression f , we project testing set visual feature x te into the semantic label embedding space. The standard strategy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref>] is then to employ NN matching in this space for zero-shot recognition. Specifically, given the matrix of label embeddings for each target category name V te , and using cosine distance norm, the testing video x te are classified by: y * = arg min y * ||V te y * ? f (x te )|| <ref type="bibr" target="#b5">(6)</ref> where f (x te ) = Wx te for STL and f (x te ) = SAx te for MTL. Latent Space Matching MTL methods provide an alternative to matching in label space: Matching in the latent space. The representation of testing data in this space is the output of latent regressors l te = Ax te <ref type="figure">(Eq. (4)</ref>). To get the representation of testing categories in the latent space we invert the combination matrix S to project target category names V te into latent space. Specifically we classify by Eq. <ref type="formula" target="#formula_7">(7)</ref>, where (S T S) ?1 S T is the Moore-Penrose pseudoinverse.</p><formula xml:id="formula_7">y * = arg min y * ||(S T S) ?1 S T V te y * ? AX te ||<label>(7)</label></formula><p>NN matching in the latent space is better than in semantic label space because: (i) the dimension is lower T &lt; d z , and (ii) we have explicitly regularised the latent space to be well behaved (Eq. <ref type="formula" target="#formula_5">(4)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Importance Weighting</head><p>Augmenting auxiliary data with additional examples from other datasets has been proved to benefit learning the visual-semantic mapping <ref type="bibr" target="#b8">[9]</ref>. However, simply aggregating auxiliary and additional datasets is not ideal as including irrelevant data risks 'negative transfer'. Therefore we are motivated to develop methodology to prioritise augmented auxiliary data that is useful for a particular ZSL recognition scenario. Specifically, we learn a per-instance weighting ?(x) on the auxiliary dataset X tr to adjust each instance's contribution according to relevance to the target domain. Because Importance Weighting (IW) adapts auxiliary data to the target domain, we assume a transductive setting with access to testing data X te . Kullback-Leibler Importance Estimation Procedure (KLIEP) We first introduce the way to estimate a per-instance auxiliary-data weight given the distribution of target data X te . This is based on the idea <ref type="bibr" target="#b18">[19]</ref> of minimizing the KL-divergence (D KL ) between training p tr (x) and testing data distribution p te (x) via learning a weighting function ?(x). This is formalised in Eq. <ref type="formula" target="#formula_8">(8)</ref>:</p><formula xml:id="formula_8">min ? D KL (p te (x)|?(x)p tr (x)) = p te (x) log p te (x) ?(x)p tr (x) dx min ? p te (x) log p te (x) p tr (x) dx ? p te (x) log ?(x)dx<label>(8)</label></formula><p>The first term is fixed w.r.t. ?(x) so the objective to optimise is:</p><formula xml:id="formula_9">min ? ? p te (x) log ?(x)dx ? ? 1 n te x n te x i=1 log ?(x i )<label>(9)</label></formula><p>Aligning Both Visual Features and Labels KLIEP is conventionally used for domain adaptation by reweighting instances <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>. In the case of transductive ZSL, we have the target data X te and category labels Z te respectively, although not instance-label association which is to be predicted. In this case we can further improve ZSL by extending KLIEP to align training and testing sets in both visual feature and category sense 5 . Specifically, we minimise the kullback-leibler divergence between the target and auxiliary in terms of both the visual and category distributions:</p><formula xml:id="formula_10">min ?x,?z D KL (p te (X)||? x (X)p tr (X)) + D KL (p te (Z)||? z (Z)p tr (Z)) min ?x,?z ? 1 n te x log ? x (x te i ) ? 1 n te x log ? z (z te i )<label>(10)</label></formula><p>Given both X te and Z te , we construct the weighting functions as a combination of Gaussian kernels centered at the testing data and categories. Specifically</p><formula xml:id="formula_11">we define ?(x, z) = ? x (x) + ? z (z)</formula><p>where ? x (x) and ? z (z) are calculated as in Eq. <ref type="bibr" target="#b10">(11)</ref>. Here ?(x, z) extends the previous notation ?(x) to indicate giving a weight to each training instance given visual feature x and class name embedding z. So if there are n tr x instances, ?(x, z) returns a weight vector of length n tr x .</p><formula xml:id="formula_12">? x (x) = n te x i=1 ? i ?(x, x te i ), ? z (z) = n te x i=1 ? j ?(z, z te i ), ?(x, x te i ) = exp ? ||x ? x te i || 2 2? 2 (11)</formula><p>For ease of formulation, we denote a = [? 1 ? ? ? ? n te</p><formula xml:id="formula_13">x ] T , b = [? 1 ? ? ? ? n te x ] T , ? a (x) = [?(x, x te 1 ) ? ? ? ?(x, x te n te x )] T and ? b (z) = [?(z, z te 1 ) ? ? ? ?(z, z te n te x )] T .</formula><p>The optimization can be thus written as</p><formula xml:id="formula_14">min a,b ? 1 n te x n te x i=1 log a T ? a (x te i ) ? 1 n te x n te x i=1 log b T ? b (z te i ), s.t. 1 n tr x n tr x i=1 ?(x tr i , z tr i ) = 1<label>(12)</label></formula><p>The above constrained optimization problem is convex w.r.t. both a and b. It can be solved by interior point methods using the derivatives in Eq. (13):</p><formula xml:id="formula_15">?a = ? 1 n te x n te x i=1 1 a T ? a (x te i ) ? a (x te i ), ?b = ? 1 n te x n te x i=1 1 b T ? b (z te i ) ? b (z te i ) (13)</formula><p>Weighted Visual-Semantic Regression Given per-instance weights ? estimated above, we can rewrite the loss function for both single-task ridge regression and multi-task regression in Sec 3.1 as ? i l(f (x i , A), z i ) and ? i l(f (x i , S, A), z i ) respectively. All our loss functions have quadratic form, so the weight can be expressed inside the quadratic loss e.g.</p><formula xml:id="formula_16">? i ||z i ? Wx i || 2 2 = ||z i ? ? i ? Wx i ? ? i || 2 2 .</formula><p>Thus to incorporate the weight information we simply replace the original semantic embedding matrix withz i = z i ? ? i and data matrix withx i = x i ? ? i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Datasets and Settings We evaluated our contributions on three human action recognition datasets, HMDB51 <ref type="bibr" target="#b2">[3]</ref>, UCF101 <ref type="bibr" target="#b3">[4]</ref> and Olympic Sports <ref type="bibr" target="#b36">[37]</ref>. They contain 6766, 13320, 783 videos and 51, 101, 16 categories respectively. For all datasets we extract improved trajectory feature (ITF) <ref type="bibr" target="#b37">[38]</ref>, a state-ofthe-art space-time feature representation for action recognition. We use Fisher Vectors (FV) <ref type="bibr" target="#b38">[39]</ref> to encode three raw descriptors (HOG, HOF and MBH). Each descriptor is reduced to half of its original dimension by PCA, resulting in a 198 dim representation. Then we randomly sample 256,000 descriptors from all videos and learn a Gaussian Mixture with 128 components to obtain the FVs. The final dimension of FV encoded feature is 2 ? 128 ? 198 = 50688 dimensions. For the label-embedding, we use 300-dimensional word2vec <ref type="bibr" target="#b39">[40]</ref>. We use T = n tr c latent tasks, and cross-validation to determine regularisation strength hyper-parameters for the models 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visual-semantic Mappings for Zero-Shot Action Recognition</head><p>Evaluation Criteria To evaluate zero-shot action recognition, we divide each dataset evenly into training and testing parts with 5 random splits. Using classification accuracy for HMDB51 and UCF101 and average precision for Olympic Sports as the evaluation metric, the average and standard deviation over the 5 splits are reported for each dataset. Compared Methods We study the efficacy of our contributions by evaluating the different visual-semantic mappings presented in Sec 3.1. We compare MTLregression methods with conventional STL Ridge Regression (denoted RR) for ZSL. For RR/STL, nearest neighbour matching is used to recognise target categories. Note that the RR+NN method here corresponds to the core strategy used by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref>]. The multi-task models we explore include: RMTL [15]: assumes each task's predictor is the sum of a global latent vector and a task-specific vector. GOMTL <ref type="bibr" target="#b15">[16]</ref>: Uses a predictor-matrix factorisation assumption in which tasks' predictors lie on a low-dimensional subspace. Multi-Task Embedding (MTE): Our model differs from GOMTL in that it explicitly models and regularises a lower dimensional latent space. For the multi-task methods, we also compare the ZSL matching strategies introduced in Section 3.2: Distributed: Standard NN matching (Eq. (6)), and Latent: our proposed latent-space matching (Eq. <ref type="formula" target="#formula_7">(7)</ref>).</p><p>Results: The comparison of single task ridge regression with our multi-task methods is presented in <ref type="table" target="#tab_2">Table 2</ref>. From these results we make the following observations: (i) Overall our multi-task methods improve on the corresponding single-task baseline of RR. MTL regression (RMTL, GOMTL and MTE) improves single-task ridge regression by 5 ? 10% in relative terms, with the biggest margins visible on the Olympic Sports dataset. (ii) Within multi-task models, the GOMTL with sparse 1 regularization outperforms RMTL. This suggests learning the task combination S from data is better than fixing it as in RMTL.</p><p>(iii) Our MTE generally outperforms other multi-task methods supporting the explicit modelling and regularisation of the latent space. (iv) In most cases, NN matching in the latent space improve zero-shot performance. This is likely due to the lower dimension of the latent space compared to the dimension of the original word vector embedding, making NN matching more meaningful <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Importance Weighted Data Augmentation</head><p>We next evaluate the impact of importance weighting in data augmentation for zero-shot action recognition. We perform the same 5 random split benchmark for each dataset. For data augmentation, we augment each dataset's training split with the data from all other datasets. For instance, for ZSL on HMDB51 we augment the training data with all videos from UCF101 and Olympic Sports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Methods</head><p>We study the impact of the data augmentation methods: Naive DA: Naive Data Augmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41]</ref> simply assigns equal weight to each auxiliary training sample. Visual KLIEP: The auxiliary data is aligned with the testing sample distribution X te (Eq. <ref type="formula" target="#formula_8">(8)</ref>). Category KLIEP: The auxiliary categories are aligned with testing category distribution Z te . This is achieved by the same prodcedure in Eq. (8) by replacing x with z. Full KLIEP: The distribution of both samples X te and categories Z te is used to reweight the auxiliary data (Eq. (12)).</p><p>Results: From the results in <ref type="table" target="#tab_3">Table 3</ref>, we draw the conclusions: (i) Both the baseline single task learning (STL) method and our Multi-Task Embedding (MTE) improve with Naive DA (compare unaugmented results in <ref type="table" target="#tab_2">Table 2</ref>), (ii) The Visual, Category, and Full visual+category-based weightings all improve on Naive DA in the case of STL RR. (iii) We see that our MTE with Full KLIEP augmentation performs the best overall. The ability of KLIEP to improve on Naive DA suggests that the auxiliary data is indeed of variable relevance to the target data, and selectively re-weighing the auxiliary data is important. (iv) For KLIEP-based DA, either Visual or Category DA provides most of the improvement, with relatively less improvement obtained by using both together. Alternative Models We also compare against previous state-of-the-art methods including those driven by both attributes and word-vector category embeddings. DAP/IAP <ref type="bibr" target="#b4">[5]</ref>: Direct/Indirect attribute prediction are classic attributebased zero-shot recognition models based on training SVM classifiers independently for each attribute, and using a probabilistic model to match attribute predictions with target classes. HAA: We implement a simplified version of the Human Actions by Attributes model <ref type="bibr" target="#b20">[21]</ref>: We first train attribute detection SVMs, and test samples are assigned to categories based on cosine distance between their vector of attribute predictions and the target classes' attribute vectors. SVE <ref type="bibr" target="#b8">[9]</ref>: Support vector regression was adopted to learn the visual to semantic mapping. ESZSL <ref type="bibr" target="#b41">[42]</ref>: Embarrassingly Simple Zero-Shot Learning defines the loss function as the mean square error on label prediction in contrast to the regression loss defined in other baseline models. SJE: Structured Joint Embedding <ref type="bibr" target="#b6">[7]</ref> employed a triplet hinge loss. The objective is to enforce relevant labels having higher projection values from visual features than those of nonrelevant labels. UDA: The Unsupervised Domain Adaptation model <ref type="bibr" target="#b21">[22]</ref> learns dictionary on auxiliary data and adapts it to the target data as a constraint on the target dictionary rather than blindly using the same dictionary. This work combines both attribute and word vector embeddings. Comparison Versus State of the Art: <ref type="table" target="#tab_4">Table 4</ref> compares our models with various contemporary and state-of-the-art models. For clear comparison, we indicate for each method which embedding ((W)ordvector / (A)ttribute) and feature (our FV, or BoW) are used, as well as whether it has a transductive dependency on the test data (TD) or exploits additional augmenting data (Aug). From these results we conclude that: (i) Although data augmentation has a big impact, our non-transductive and no data augmentation method (MTE) generally outperforms prior alternatives due to learning an effective latent matching space robust to the train/test class shift; (ii) The performance of our MTE with word-vector embedding is strong when compared with DAP/IAP/HAA/ESZSL even with attribute embedding. Given the same attribute embedding, MTE outperforms all state-of-the-art models due to the discovery of latent attributes from the original attribute space; (iii) Moreover, given importance weighting on auxiliary data, our method (MTE + Full KLIEP) with word-vector embedding performs the best overall -including against <ref type="bibr" target="#b8">[9]</ref> which also exploits data augmentation; (iv) Finally, our method is synergistic to the post processing self-training approach <ref type="bibr" target="#b10">[11]</ref> as well as the hubness strategies <ref type="bibr" target="#b11">[12]</ref>, which further explains the advantages of our approach (MTE + Full KLIEP + PP) over other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Results and Further Analysis</head><p>Importance Weighting: To visualise the impact of our IW, we randomly select 4 / 16 classes as target / auxiliary sets respectively. We then estimate the weight on the 16 auxiliary video classes according to the Full KLIEP (Section 4). Examples of the auxiliary video weightings are presented in <ref type="figure" target="#fig_0">Fig 2.</ref> We observe that auxiliary classes semantically related to the targets are given higher weight e.g. HandstandPushups?Cartwheel in first sample, SalsaSpin?Hug and Sword Exercise ? Fencing in the second sample. While the visually and semantically less relevant auxiliary videos are given much lower weights.</p><p>Multi-task Embedding: We next qualitatively illustrate single versus multitask visual-semantic mappings. Specifically we take 5 classes to be recognized and visualise their data after visual-semantic projection by tSNE <ref type="bibr" target="#b42">[43]</ref>. A comparison between the representations generated by single-task (RR) and multi-task (MTE) mappings is given in <ref type="figure">Fig 3.</ref> The multi-task embedding discovers data in a  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ridge Regression Embedding Multi-Task Embedding</head><p>basketball layup discus throw pole vault diving springboard tennis serve <ref type="figure">Fig. 3</ref>. Qualitative comparison between single-task ridge regression (RR) and multitask embedding (MTE).</p><p>lower dimension latent space where NN classification becomes more meaningful. The improved representation is illustrated by computing the ROC curve for each target category, as seen in <ref type="figure">Fig 3.</ref> MTE provides improved detection over RR, demonstrating the better generalisation of this representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we focused on zero-shot action recognition from the perspective of improving generalisation of the visual-semantic mapping across the disjoint train/test class gap. We propose both model-and data-centric improvements to a traditional regression-based pipeline by respectively, multi-task embeddingto minimise overfit of the train data and to build a lower dimensional latent matching space; and prioritising data augmentation by importance weightingto best exploit auxiliary data for the recognition of target categories. Our experiments on a set of contemporary action-recognition benchmarks demonstrate the impact of both our contributions and show state-of-the-art results overall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Visualisation of Full KLIEP auxiliary data weighting. Left: 4 target videos with category names. Right: 16 auxiliary videos with bars indicating the estimated weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Two strategies to improve generalisation of visual-semantic mapping in ZSL.</figDesc><table><row><cell cols="3">( ) p X Auxiliary tr Distribution</cell><cell>( ) p X te</cell><cell cols="2">Target Distribution</cell><cell cols="3">Higher Weight ? X Importance ( ) Weight</cell><cell>Z</cell><cell>te</cell><cell>Auxiliary Category Embedding Target Category Embedding</cell></row><row><cell></cell><cell></cell><cell>( ) p X te</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>( ) p X te</cell></row><row><cell></cell><cell>run</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>run</cell><cell></cell><cell>1 l</cell></row><row><cell></cell><cell cols="2">walk</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>walk</cell><cell>x</cell><cell>l</cell><cell>2</cell><cell>z</cell><cell>run</cell></row><row><cell>Handstand</cell><cell>Cartwheel</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Handstand</cell><cell cols="2">Cartwheel</cell><cell>A</cell><cell>??</cell><cell>S</cell><cell>walk</cell><cell>Cartwheel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>l</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>T</cell></row><row><cell>Smile</cell><cell>Haircut</cell><cell cols="3">( ) p X tr Importance Weighting</cell><cell>Smile</cell><cell cols="2">Weight Lower</cell><cell cols="2">( ) ( ) tr p X ? X Multi-Task Embedding Haircut</cell><cell>Handstand</cell></row><row><cell>Fig. 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>tr ZLeft: Importance weighting to prioritise auxiliary data relevant to the target domain. Right: Learning the mapping from visual features X to semantic embedding Z by MTL reduces overfitting, and also provides a latent lower dimensional representation {lt} to benefit nearest neighbour matching.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Notation Summary</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>n tr c ; n te c</cell><cell>Number of training categories ; testing categories</cell></row><row><cell>n tr x ; n te</cell><cell></cell></row></table><note>x Number of all training instances; all testing instances X ? R dx?nx ; xi</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Visual-semantic mappings for zero-shot action recognition: MTL ( ) versus STL (X). Latent matching ( ) versus distributed (X) matching</figDesc><table><row><cell cols="5">ZSL Model MTL Latent Matching HMDB51 UCF101 Olympic Sports</cell></row><row><cell>RR</cell><cell>X</cell><cell>NA</cell><cell>18.3 ? 2.1 14.5 ? 0.9</cell><cell>40.9 ? 10.1</cell></row><row><cell>RMTL [15]</cell><cell></cell><cell>X</cell><cell>18.5 ? 2.1 14.6 ? 1.1</cell><cell>41.1 ? 10.0</cell></row><row><cell>RMTL [15]</cell><cell></cell><cell></cell><cell>18.7 ? 1.7 14.7 ? 1.0</cell><cell>41.1 ? 10.0</cell></row><row><cell>GOMTL [16]</cell><cell></cell><cell>X</cell><cell>18.5 ? 2.2 13.1 ? 1.5</cell><cell>43.5 ? 8.8</cell></row><row><cell>GOMTL [16]</cell><cell></cell><cell></cell><cell>18.9 ? 1.0 14.9 ? 1.5</cell><cell>44.5 ? 8.5</cell></row><row><cell>MTE</cell><cell></cell><cell>X</cell><cell>18.7 ? 2.2 14.2 ? 1.3</cell><cell>44.5 ? 8.2</cell></row><row><cell>MTE</cell><cell></cell><cell></cell><cell>19.7 ? 1.6 15.8 ? 1.3</cell><cell>44.3 ? 8.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Data augmentation and importance weighting for ZSL action recognition.</figDesc><table><row><cell cols="4">ZSL Model Weighting Model HMDB51 UCF101 OlympicSports</cell></row><row><cell>RR</cell><cell>Naive DA</cell><cell>21.9 ? 2.4 19.4 ? 1.7</cell><cell>46.5 ? 9.4</cell></row><row><cell>MTE</cell><cell>Naive DA</cell><cell cols="2">23.4 ? 3.4 20.9 ? 1.5 49.4 ? 8.8</cell></row><row><cell>RR</cell><cell cols="2">Visual KLIEP 23.2 ? 2.7 20.3 ? 1.6</cell><cell>47.2 ? 9.3</cell></row><row><cell>RR</cell><cell cols="2">Category KLIEP 23.0 ? 2.1 20.2 ? 1.6</cell><cell>51.8 ? 8.7</cell></row><row><cell>RR</cell><cell>Full KLIEP</cell><cell>23.7 ? 2.7 20.7 ? 1.4</cell><cell>51.3 ? 9.0</cell></row><row><cell>MTE</cell><cell cols="2">Visual KLIEP 23.4 ? 2.8 20.8 ? 2.0</cell><cell>51.4 ? 9.2</cell></row><row><cell>MTE</cell><cell cols="2">Category KLIEP 23.3 ? 2.4 20.9 ? 1.7</cell><cell>50.9 ? 8.3</cell></row><row><cell>MTE</cell><cell>Full KLIEP</cell><cell cols="2">23.9 ? 3.0 21.9 ? 2.7 52.3 ? 8.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison versus state of the art. Embed: Label embedding, Feat: Visual feature used, Aug: Data augmentation required? TD: Transductive Requirement? Method Embed Feat TD Aug HMDB51 UCF101 Olympic Sports MTE W FV X X 19.7 ? 1.6 15.8 ? 1.3 44.3 ? 8.1 MTE + Full KLIEP W FV 23.9 ? 3.0 21.9 ? 2.7 52.3 ? 8.1 MTE + Full KLIEP + PP W FV 24.8 ? 2.2 22.9 ? 3.3 56.6 ? 7.7</figDesc><table><row><cell>MTE</cell><cell>A</cell><cell cols="2">FV X X</cell><cell>N/A</cell><cell>18.3 ? 1.7</cell><cell>55.6 ? 11.3</cell></row><row><cell>DAP [5] -CVPR 2009</cell><cell>A</cell><cell cols="2">FV X X</cell><cell>N/A</cell><cell>15.9 ? 1.2</cell><cell>45.4 ? 12.8</cell></row><row><cell>IAP [5] -CVPR 2009</cell><cell>A</cell><cell cols="2">FV X X</cell><cell>N/A</cell><cell>16.7 ? 1.1</cell><cell>42.3 ? 12.5</cell></row><row><cell>HAA [21] -CVPR 2011</cell><cell>A</cell><cell cols="2">FV X X</cell><cell>N/A</cell><cell>14.9 ? 0.8</cell><cell>46.1 ? 12.4</cell></row><row><cell>SVE [9] -ICIP 2015</cell><cell cols="5">W BoW X X 14.9 ? 1.8 12.0 ? 1.4</cell><cell>N/A</cell></row><row><cell>SVE [9] -ICIP 2015</cell><cell cols="2">W BoW</cell><cell cols="3">X 15.6 ? 0.7 16.5 ? 2.4</cell><cell>N/A</cell></row><row><cell>SVE [9] -ICIP 2015</cell><cell cols="2">W BoW X</cell><cell></cell><cell cols="2">19.3 ? 4.0 13.1 ? 2.0</cell><cell>N/A</cell></row><row><cell>SVE [9] -ICIP 2015</cell><cell cols="2">W BoW</cell><cell></cell><cell cols="2">22.8 ? 2.6 18.4 ? 1.4</cell><cell>N/A</cell></row><row><cell>ESZSL [42] -ICML 2015</cell><cell>W</cell><cell cols="4">FV X X 18.5 ? 2.0 15.0 ? 1.3</cell><cell>39.6 ? 9.6</cell></row><row><cell>ESZSL [42] -ICML 2015</cell><cell>W</cell><cell>FV X</cell><cell></cell><cell cols="2">22.7 ? 3.5 18.7 ? 1.6</cell><cell>51.4 ? 8.3</cell></row><row><cell>ESZSL [42] -ICML 2015</cell><cell>A</cell><cell cols="2">FV X X</cell><cell>N/A</cell><cell>17.1 ? 1.2</cell><cell>53.9 ? 10.8</cell></row><row><cell>SJE [7] -CVPR 2015</cell><cell>W</cell><cell cols="4">FV X X 13.3 ? 2.4 9.9 ? 1.4</cell><cell>28.6 ? 4.9</cell></row><row><cell>SJE [7] -CVPR 2015</cell><cell>A</cell><cell cols="2">FV X X</cell><cell>N/A</cell><cell>12.0 ? 1.2</cell><cell>47.5 ? 14.8</cell></row><row><cell>UDA [22] -ICCV 2015</cell><cell>A</cell><cell>FV</cell><cell>X</cell><cell>N/A</cell><cell>13.2 ? 1.9</cell><cell>N/A</cell></row><row><cell cols="3">UDA [22] -ICCV 2015 A+W FV</cell><cell>X</cell><cell>N/A</cell><cell>14.0 ? 1.8</cell><cell>N/A</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In this work, data augmentation means exploiting additional data in a wider context from multiple data sources, in contrast to synthesising more artificial variations of one dataset as in deep learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To deal with multi-word compound action category names, e.g. "Apply Eye Makeup", we apply a simple average, summing the component word vectors<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">KLEIP with labels was studied by<ref type="bibr" target="#b19">[20]</ref>, but they assumed the target joint distribution of X and Z is known. So<ref type="bibr" target="#b19">[20]</ref> is only suitable for traditional supervised learning with labeled target examples of zi and xi in correspondence. In our case we have the videos to classify and the zero-shot category names, but the assignment of names to videos is our task rather than prior knowledge.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Ridge Regression (RR) has 15M (300?50688) parameters, whilst for HMDB51 where T = 25, GOMTL and MTE have 1.27M (50688?25+25?300) parameters.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local SVM approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sch?ldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Evaluation of Output Embeddings for Fine-Grained Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Zero-Shot Object Recognition by Semantic Manifold Distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic embedding space for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Survey on Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transductive Multi-view Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hubness and pollution: Delving into crossspace mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<title level="m">Reasoning about linguistic regularities in word embeddings using matrix manifolds</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
		<title level="m">Learning Task Grouping and Overlap in Multitask Learning. In: ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shaft</surname></persName>
		</author>
		<title level="m">When is nearest neighbor meaningful? Database Theory</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VideoStory : A New Multimedia Embedding for Few-Example Recognition and Translation of Events</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Von B?nau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Importance Weighted Inductive Transfer Learning for Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vanck</surname></persName>
		</author>
		<editor>ECMLPKDD.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised Domain Adaptation for Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing an action using its name: A knowledge-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dynamic concept composition for zero-example event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A Unified Perspective on Multi-domain and Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to share latent tasks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-task sparse learning with beta process prior for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single/multi-view human action recognition via regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Latent multitask learning for view-invariant action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Correcting Sample Selection Bias by Unlabeled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Boosting for Regression Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pardoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>In: NIPS.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A robust and efficient video representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Zero-shot action recognition by word-vector embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04458</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
