<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Rethinking the Competition between Detection and ReID in Multi-Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE.</roleName><forename type="first">Weiming</forename><forename type="middle">Hu</forename></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Rethinking the Competition between Detection and ReID in Multi-Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Multi-object Tracking</term>
					<term>Reciprocal Representa- tion Learning</term>
					<term>Scale-aware Attention</term>
					<term>One-shot</term>
					<term>ID embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to balanced accuracy and speed, one-shot models which jointly learn detection and identification embeddings, have drawn great attention in multi-object tracking (MOT). However, the inherent differences and relations between detection and re-identification (ReID) are unconsciously overlooked because of treating them as two isolated tasks in the one-shot tracking paradigm. This leads to inferior performance compared with existing two-stage methods. In this paper, we first dissect the reasoning process for these two tasks, which reveals that the competition between them inevitably would destroy taskdependent representations learning. To tackle this problem, we propose a novel reciprocal network (REN) with a self-relation and cross-relation design so that to impel each branch to better learn task-dependent representations. The proposed model aims to alleviate the deleterious tasks competition, meanwhile improve the cooperation between detection and ReID. Furthermore, we introduce a scale-aware attention network (SAAN) that prevents semantic level misalignment to improve the association capability of ID embeddings. By integrating the two delicately designed networks into a one-shot online MOT system, we construct a strong MOT tracker, namely CSTrack. Our tracker achieves the state-of-the-art performance on MOT16, MOT17 and MOT20 datasets, without other bells and whistles. Moreover, CSTrack is efficient and runs at 16.4 FPS on a single modern GPU, and its lightweight version even runs at 34.6 FPS. The complete code has been released at https://github.com/JudasDie/SOTS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Multi-object tracking (MOT), aiming to estimate the locations and scales of multiple targets in a video sequence, is one of the most fundamental yet challenging tasks in computer vision <ref type="bibr" target="#b0">[1]</ref>. It may be applied to many practical scenarios, such as intelligent driving, human-computer interaction, and pedestrian behavior analysis.</p><p>Convolutional Neural Network (CNN) based object detection and re-identification (ReID) have demonstrated excellent whereas ReID tends to focus on the specific pedestrian. From this point of view, they are contradictory to each other. <ref type="bibr">(b)</ref> represents that different resolution focuses on different scale of targets in FPN-based model, where the arrow indicates the output resolution from high to low. This arrangement can help detector to detect pedestrians with different sizes, but not suitable for semantic matching of pedestrians with different sizes in ReID task.</p><p>performance in multi-object tracking in the past few years. The related ReID-based trackers can be divided into two classes, i.e., two-stage and one-shot structures. The two-stage models follow the tracking-by-detection paradigm (or more precisely, tracking-after-detection) <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>, which divides MOT into two separate tasks, i.e., detection and association. It first obtains bounding boxes of objects in each frame through an off-theshelf detector, and then associates the candidate boxes with existing tracklets by matching the extracted identification (ID) embedding of each bounding box across frames. Although effective, two-stage methods suffer from massive computational cost, since the ID embedding extraction model (e.g., ReID network <ref type="bibr" target="#b5">[6]</ref>) needs to perform forward inference for each individual bounding box. Alternatively, the one-shot methods <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref> integrate detection and ID embedding extraction into a unified framework. By redesigning the prediction head of the detector, the one-shot tracker can simultaneously yield arXiv:2010.12138v3 [cs.CV] 24 May 2022 detection results and ID embeddings, which may bring an increase in speed but at the same time a decrease in accuracy.</p><p>In this paper, we first dissect the reasoning process of oneshot tracker and analyze why the performance is lower than the two-stage methods. Our analysis reveals that the performance degradation primarily derives from excessive competition between detection and ReID tasks, which can be summarized into two aspects: 1) Competition of Object Representation Learning: In one-shot methods, object class confidence, target scale and ID information are obtained by processing a shared feature tensor. Although one-shot methods are efficient, the inherent differences between different tasks are ignored, as shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>. Specifically, object detection requires that objects belonging to the same category (e.g., pedestrian) have similar semantics, while ReID tends to distinguish two different pedestrian, which is contradictory to the purpose of detection task. The competition between the above two tasks may lead to ambiguous learning, which means that the pursuit of high performance in one task may result in performance degradation or stagnation in the other. 2) Competition of Semantic Level Assignment: The one-shot trackers <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref> usually convert detector to tracker by adding a parallel branch to yield ID embeddings. Although it is simple, it ignores that semantic level assignment of detection task is not suitable for ReID task. Specifically, modern object detectors always introduce feature pyramid networks (FPNs) <ref type="bibr" target="#b10">[11]</ref> to improve localization accuracy for various target sizes. In this framework, objects of different scales will be assigned to different resolution features, as shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>. However, we observe that this assignment is not suitable for ReID in the one-shot tracker since it will lead to a semantic level misalignment, whether for different targets or the same target across frames. Existing one-shot trackers <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> usually extract ID embedding from a certain feature resolution depending on detection results, which may lead to a semantic level gap when matching targets with drastic scale changes. Furthermore, it will impair the performance of subsequent data association (Please see the ablation experiments in Sec. IV-B for detailed discussion and verification).</p><p>To tackle the above problems, we fully consider the essential differences of two tasks in one-shot tracker, and design two sub-networks to solve the above problems. Specifically, we propose a novel reciprocal network (REN) to conduct collaborative learning among different tasks. The reciprocal network separates the feature maps of object detection and ID embedding extraction into two different task-driven branches, by which the task-dependent representations can be learned. Specifically, given shared features, a novel structure combining self-relation and cross-relation is designed to enhance the feature representation. The former impels hidden nodes to learn task-dependent features, and in parallel the latter aims to improve the collaborative learning of these two tasks. Meanwhile, we design a scale-aware attention network (SAAN) to improve the alignment of ID embedding extraction, and enhance the model resilience to scale changes. In this work, we develop an architecture that combines spatial and channel attention to achieve our goal, and this architecture enhances the influence of object-related regions and channels. The output of SAAN is a feature tensor that includes all the targets with rich semantics in all resolutions. Following the above design idea, aggregation feature-based ID embedding is helpful to prevent semantic misalignment during matching.</p><p>The main contributions of our work are three-folds:</p><p>? We propose a novel reciprocal network to learn taskdependent representations. It not only effectively mitigates the deleterious competition, but also improves the capability of collaborative learning between detection and ReID tasks in one-shot MOT methods. ? We introduce a scale-aware attention network to apply spatial and channel attention to feature maps at different resolutions. By fusing these features and extracting ID embeddings from a consistent representation, it effectively prevents semantic level misalignment, and improves the resilience to objects with different scales during matching. ? The extensive experiments demonstrate that our method effectively improves the performance of the one-shot MOT method, especially for association ability of reidentification features, which is competitive with the twostage methods. The organization of this paper is as follows: Section II reviews related work in the literature. Section III presents our proposed approach. The experimental results and analysis are provided in Section IV. Section V concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>We firstly review the related MOT methods, including twostage and one-shot structures. After that, we briefly review the related works for joint detection and tracking without ReID in MOT task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Two-stage MOT Methods</head><p>With the development of object detectors, many MOT trackers follow the tracking-by-detection paradigm (or more precisely, tracking-after-detection). In particular, these trackers [2]- <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> divide MOT into two separate tasks, i.e., detection and association. The object bounding boxes are firstly predicted by high-performance detectors such as Faster R-CNN <ref type="bibr" target="#b13">[14]</ref>, SDP <ref type="bibr" target="#b14">[15]</ref> and DPM <ref type="bibr" target="#b15">[16]</ref>. Then, the candidate boxes are linked into tracklets across frames by an association network. Since the candidate boxes can be directly yielded by off-the-shelf detector, two-stage MOT methods focus on improving association performance. In the early works, Sort <ref type="bibr" target="#b11">[12]</ref> firstly performs Kalman filtering to predict candidate boxes in the next frame and uses Hungarian method to associate across frames by measuring bounding box overlap. IOU-Tracker <ref type="bibr" target="#b12">[13]</ref> directly associates the last frame bounding box with candidate boxes in the current frame using IOU matching. Although simple and efficient, both Sort and IOU-Tracker may fail in challenging scenarios such as incomplete detections or crowded scenes since they rely on the nearest neighbor hypothesis (i.e., only the candidate box closest to the target has the same ID). Therefore, some works [2]- <ref type="bibr" target="#b4">[5]</ref> attempt to apply ReID network to yield more robust predictions. In a nutshell, each target is cropped from the image and fed into an additional network to extract ID appearance embedding, which is employed to associate with existing tracklets by measuring the cosine distance across frame. Moreover, some works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> introduce graph neural networks to further improve matching capacity. For example, MPN <ref type="bibr" target="#b17">[18]</ref> replaces the Hungarian algorithm with graph neural networks to dissect the information and associate them with the edge classification.</p><p>Although the above methods achieve impressive performance, they still suffer from massive computation costs since the ID information extraction network needs to perform forward inference for each bounding box. In contrast, one-shot trackers consider constructing an entire model to simultaneously generate detection boxes and ID embeddings, which exhibits attractive efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. One-shot MOT Methods</head><p>Due to balanced accuracy and speed, the one-shot paradigm <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>, which integrates detection and ID embedding extraction into a unified network, offer a new solution for MOT. Tong et al. <ref type="bibr" target="#b6">[7]</ref> first modify a detector (i.e., Faster RCNN <ref type="bibr" target="#b13">[14]</ref>) to handle both detection and ReID tasks . By introducing extra fully-connected layers (FC), the detector obtains the ability to yield ID embeddings. Recently-proposed JDE <ref type="bibr" target="#b7">[8]</ref> and RetinaTrack <ref type="bibr" target="#b8">[9]</ref> convert a FPN-based detector ( i.e., Yolo-v3 <ref type="bibr" target="#b18">[19]</ref> and RetinaNet <ref type="bibr" target="#b19">[20]</ref>) to a one-shot tracker by redesigning the prediction head. Although those simple modifications help generate detection and ID embeddings with a small overhead and achieve impressive performance, they ignore the inherent differences between detection and ReID. This induces their performance not so good as the two-stage methods, especially evaluated by the IDF1 score. Recent released method, FairMOT <ref type="bibr" target="#b9">[10]</ref>, uses the anchor-free method <ref type="bibr" target="#b20">[21]</ref> to reduce the ambiguity of anchors. Despite alleviating the scale-aware competition by aggregating multilayer features to yield ID embeddings, it fails to handle the competition between object detection and ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Joint Detection and Tracking without ReID</head><p>Besides the one-shot MOT methods, several methods <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref> attempt to train joint detection and tracking models by integrating other association ways instead of classical ReID. For instance, CTracker <ref type="bibr" target="#b21">[22]</ref> uses adjacent frame pairs as input and generates a pair of bounding boxes for the same target by a chaining structure. TubeTK <ref type="bibr" target="#b22">[23]</ref> utilizes tubes to encode the target's temporal-spatial position and local moving trail. Tracktor <ref type="bibr" target="#b23">[24]</ref> converts existing object detectors (i.e., Faster-RCNN <ref type="bibr" target="#b13">[14]</ref>) to trackers by exploiting the regression head of detector to perform the regression of the object boxes from last frame to current image. The following work, CenterTrack <ref type="bibr" target="#b24">[25]</ref>, which shares a similar spirit with Tracktor <ref type="bibr" target="#b23">[24]</ref>, predicts the location of the existing objects by learning the offset of heat points (the center point of object boxes). However, their association performances are also inferior to two-stage methods due to the poor capability of the direct bounding boxes transduction compared with ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>Our proposed framework, named CSTrack, consists of two main components, i.e., reciprocal network designed in Sec. III-B and scale-aware attention network described in Sec. III-C. Before shedding light on the two essential parts, we first give a brief overview of the whole framework in Sec. III-A. Moreover, we describe the details of training and online inference in Sec, III-D and Sec, III-E, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Before describing our model, we review the popular JDE architecture <ref type="bibr" target="#b7">[8]</ref> (Joint Detection and Embeddings), which is the baseline of our method, and explain why the vanilla model is not suitable for mitigating the competition between detection and ReID tasks. And then, we introduce the CSTrack framework, which strikes for a better balance between detection and ReID tasks by integrating our proposed sub-networks.</p><p>Preliminaries. JDE <ref type="bibr" target="#b7">[8]</ref> redesigns the prediction head of an object detector to conduct object detection and ID embedding extraction simultaneously in a one-shot model. Specifically, given a frame x, it is first processed by a feature extractor ? (including Backbone and Neck), which yields multi-resolution features F i | i=1,2,3 (see <ref type="figure" target="#fig_4">Fig. 2</ref> (a)),</p><formula xml:id="formula_0">F i | i=1,2,3 = ?(x).<label>(1)</label></formula><p>Then, F i | i=1,2,3 is fed into the Head network (i.e., including the parallel object detection and ReID branches) to predict detection result D and raw ID embedding maps E, as illustrated in <ref type="figure" target="#fig_4">Fig. 2</ref> (b). The detection result will be post-processed by non maximum suppression (NMS) <ref type="bibr" target="#b13">[14]</ref> to yield candidate boxes. Each candidate box extracts its corresponding ID embedding from the row ID embedding maps E (following the same feature resolution level as the candidate box) to link with the existing tracklets. In this framework, the shared features are directly fed into two independent task-driven branches, which generate the output directly applying a 1 ? 1 convolution layer, ignoring the inherent differences between these two tasks. It may lead to ambiguous learning during training. Moreover, ReID branch extracts ID embeddings from probable arbitrary feature level guided by detection task, which may induce semantic level misalignment during matching.</p><p>CSTrack. We build our framework based on JDE <ref type="bibr" target="#b7">[8]</ref>, as shown in <ref type="figure" target="#fig_4">Fig. 2 (c)</ref>. To alleviate the competition of object representation learning and enhance task-dependent representations, we propose a novel reciprocal network (REN) to decouple the feature F t before feeding it into different task branches and then exchange semantic information of different tasks with a cross-relation layer. The idea of designing REN is inspired by recent self-attention <ref type="bibr" target="#b25">[26]</ref> and multi-task decoding mechanisms <ref type="bibr" target="#b26">[27]</ref>, which can enhance representations for each task with only small overheads. Note that, REN not only focuses on the specificities of features for different tasks, but also learns commonalities to encode the feature by constructing the cross-relation weight maps. For ReID branch, instead of applying a 1 ? 1 convolution layer on each feature resolution, we design a scale-aware attention network (SAAN, </p><formula xml:id="formula_1">(b) (c) Detection Detection REN REN ReID (SAAN) ReID (SAAN) ID Embeddings 1 F 2 F 3 F 1 F 2 F 3 F Detection Detection ReID ReID 1 F 2 F 3 F Backbone 1 8 1 16 1 32 Neck (a) 1 F 2 F 3 F Frame x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Embeddings</head><p>... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Embeddings</head><p>...   <ref type="figure">Fig. 3</ref>: Diagram of reciprocal network (REN). For the original feature map F i , we construct the self-relation and crossrelation maps to impel the generation of task-dependent features F T1 i and F T2 i . detailed in <ref type="figure" target="#fig_5">Fig. 4</ref>) to fuse features from different feature resolutions. Meanwhile, both spatial and channel-wise attention modules <ref type="bibr" target="#b27">[28]</ref> are adopted to suppress noisy background and learn object-related representations.</p><formula xml:id="formula_2">i F W H ? avgpool C C C C W H ? ? ? i F? 1 M 2 M W H C ? ? ? transpose 1 T W softmax transpose softmax 2 T W 2 S W 1 S W W H ? 1 T i F 2 T i F 1 W 2 W Matrix Multiplication Element-wise Addition Element-wise Addition 1x1Conv+Reshape Reshape Reshape W H C ? ? ? C W H ? C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reciprocal Network</head><p>In this section, we propose a reciprocal network to learn both specificities (task-dependent) and commonalities (collaboration) of features for detection and ReID tasks. For specificities learning, self-relation reflecting correlations between different feature channels are learned to enhance feature representation for each task. For commonalities learning, a cross-relation layer is elaborately designed to exchange the semantic information of different tasks.</p><p>The structure of our reciprocal network is illustrated in <ref type="figure">Fig. 3</ref>, where the features outputted from feature extractor are denoted as F i | i=1,2,3 ? R C?H?W . First, we pass F i through an avg-pooling layer to obtain the statistical information F i ? R C?H ?W . Second, tensors M 1 and M 2 for detection and ReID are respectively generated by passing F i through different convolution layers and reshape them to C?N , where N = H ? W . Third, we perform matrix multiplication on M 1 /M 2 (/ indicates "or") and its corresponding transpose tensor. A row softmax layer is followed to calculate the selfrelation weight maps {W T1 , W T2 } ? R C?C for each task, and the calculation is as follows:</p><formula xml:id="formula_3">w ij T k = exp m i k ? m j k C j=1 exp m i k ? m j k , k ? {1, 2}<label>(2)</label></formula><p>where ? denotes dot product operation. m i k and m j k indicate the i th and j th row of M 1 /M 2 , respectively. w ij T k denotes the value at location of (i, j) on W T k , which represents the relation of the i th and j th channel in a tensor. Then, we perform matrix multiplication between M 1/2 and the transpose of M 2/1 to learn commonalities between different tasks, and then a row softmax layer is followed to generate cross-relation weight maps {W S1 , W S2 } ? R C?C ,</p><formula xml:id="formula_4">w ij S k = exp m i k ? m j h C j=1 exp m i k ? m j h , (k, h) ? {(1, 2), (2, 1)} (3)</formula><p>where w ij S k denotes the effect of the i th feature channel of task 1/2 on the j th feature channel of task 2/1. Finally, the self-relation and cross-relation weights are finally fused by a trainable parameter ?, obtaining</p><formula xml:id="formula_5">{W 1 , W 2 } ? R C?C W k = ? k ? W T k + (1 ? ? k ) ? W S k , k ? {1, 2} .<label>(4)</label></formula><p>The original feature map F i is processed by one 1 ? 1 convolution layer and rearranged to the shape of R C?N , where N = H ?W . Then, we perform matrix multiplication between the reshaped feature and the learned weight maps W 1/2 to obtain an enhanced representation for each task. The enhanced representation is rearranged to the same shape as the original feature map F i (i.e., R C?H?W ) and fused with original F i by residual attention to prevent information loss. The feature tensors F Ti i and F T2 i will be sent to different task branch for further processing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Scale-aware Attention Network</head><p>We build a scale-aware attention network (SAAN), as illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref> to aggregate features from different resolutions, which guarantees semantic level alignment of ID embeddings from objects with different sizes. In this network, we introduce the spacial attention module (SAM) <ref type="bibr" target="#b27">[28]</ref> and channel attention module (CAM) to learn 'where' and 'what' feature is more important for yielding discriminative ID embeddings. In particular, the features from the scale of 1/16 and 1/32 (compared to the size of input image) are upsampled to 1/8 firstly. Then, a 3 ? 3 convolutional layers are followed to encode the upsampled feature maps. For better 'awaring' useful information for each target in different resolutions, we firstly arrange SAM to enhance the target-related features and suppress background noise. Specially, we perform avg-pooling and max-pooling on channel dimension to yield two 2D maps with size 1 ? H ? W . These maps are concatenated and processed by a 7 ? 7 convolutional layer and a Sigmoid layer sequentially to generate a spatial attention map. The obtained spatial attention map learned individually in each resolution and is fused with the original feature by element-wise multiplication and residual attention. Then, we concatenate the feature maps from different scales and pass them through the channel attention module. The channel attention module comprises global avg-pooling and max-pooling layers, which learn different statistical information of the different resolution features. The outputs of pooling layers are first processed by the shared network consisting of a 1D convolutional layer and a fully-connected layer. Then, the feature maps will be fused by element-wise addition and normalized by a Sigmoid layer to generate an attention map with only one channel, which is applied to the vanilla features similar to the spacial attention map. For the arrangement of the attention module, the experimental result shows that our design is helpful to improve association ability of ID embeddings, as discussed in Sec. IV-B.</p><p>Finally, we use a 3 ? 3 convolution layer to map features to 512 channels, as E ? R 512?W ?H . Each ID embedding e xy ? R 512?1?1 denotes the identity information of the object at location (x, y), which can be extracted according to detection results for the subsequent ReID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training details</head><p>For jointly optimizing the object detection and ReID tasks, we minimize a weighted sum of detection losses (consisting of classification loss and box regression loss) and ReID loss. Specifically, the ground-truth annotation for an input image is denoted as</p><formula xml:id="formula_6">{B i }, where B i = (x (i) , y (i) , w (i) , h (i) , c (i) ).</formula><p>Here, (x (i) , y (i) ) indicate the coordinates of the center of bounding box and (w (i) , h (i) ) indicate the size of bounding box. c (i) is the ID index that the object belongs to the i th pedestrian. C is the number of ID in training dataset, which denotes the number of pedestrians. For each location (x, y) on the detection result maps of different resolution, we represent it as a 5D vector t = (x * , y * , w * , h * , p), following the standard anchor-based protocol <ref type="bibr" target="#b18">[19]</ref>. Here (x * , y * , w * , h * ) are the bounding box predictions and p indicates the foreground probability of the bounding box. For foreground and background classification, we define the anchor points in the</p><formula xml:id="formula_7">location (x, y) = ( x (i) r , y (i) r )</formula><p>as positive samples (r is the downsample ratio, i.e., 8, 16, 32, and ? indicates rounding down to an integer). Therefore, the classification loss <ref type="bibr" target="#b19">[20]</ref> can be formulated as:</p><formula xml:id="formula_8">L cls (p t ) = ?? (1 ? p t ) ? log (p t ) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_9">p t = p if (x, y) = ( x (i) r , y (i) r ) 1 ? p otherwise .<label>(6)</label></formula><p>We set ? = 0.25 and ? = 0 following YOLO-v5. For bounding box regression, we employ the CIOU loss <ref type="bibr" target="#b28">[29]</ref>, which can be defined as,</p><formula xml:id="formula_10">L reg (b x,y ) = 1 ? C(b i ,b x,y ) if (x, y) = ( x (i) r , y (i) r ) 0 otherwise .<label>(7)</label></formula><p>where C represents the CIOU operation. b i indicates the ground-truth box andb x,y is the box prediction at location (x, y) of feature maps. We define our detection loss as follows,</p><formula xml:id="formula_11">L det = 1 N pos M i x,y L cls p i x,y + ?L reg b i x,y ,<label>(8)</label></formula><p>where M is the number of the resolutions (i.e., 3 in our model) and N pos denotes the number of positive samples. ? indicates the loss weight. We set ? = 0.05 as that in YOLO-v5. The definition and training of ReID loss follow the baseline tracker JDE <ref type="bibr" target="#b7">[8]</ref>, which uses the cross-entropy loss as the objective for ID embedding learning. In this work, we model this task as a classification task, and use a FC layer to map ID embeddings to a class distribution vector P = {p(c), c ? [1, 2, . . . , C]}. By comparing with the one-hot representation of the GT class label Y i (c) ? R C?1?1 , we compute the ReID loss as</p><formula xml:id="formula_12">L id = ? 1 N N i=1 C c=1 Y i (c) log(p(c)),<label>(9)</label></formula><p>where N denotes the number of objects in the current image. The joint loss can be written as a weighted linear sum of detection loss L det and ReID loss L id , as</p><formula xml:id="formula_13">L total = L det + ?L id ,<label>(10)</label></formula><p>where ? is experimentally set to 0.02 to balance object detection and ReID tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Online Tracking</head><p>In this section, we present how to link the detected candidate boxes with existing tracklets. For a fair comparison, we follow the cascade matching design in JDE <ref type="bibr" target="#b7">[8]</ref>, as shown in <ref type="figure">Fig. 5</ref>. Specifically, we first calculate the similarity of ID embeddings between candidate boxes and templates (i.e., the ID embeddings of the previous tracklets) with the cosine metric. Then a Kalman filter is used to provide distance restriction, which removes unreasonable matches between candidate boxes and existing tracklets. After that, the Hungarian algorithm is employed to find the optimal bipartite matching. In the second matching stage, the unmatched tracklets will be re-checked again by an IOU matching, as in Sort <ref type="bibr" target="#b11">[12]</ref>. For the successful matched tracklets, the templates will be updated to adapt to appearance variations, as</p><formula xml:id="formula_14">t t i = ?t t?1 i + (1 ? ?)e t i ,<label>(11)</label></formula><p>where e t i is an ID embedding of candidate box and i indicates the ID index of target. t t?1 i and t t i represent the corresponding templates before and after update, respectively. ? is a weight term, which is set as 0.9, following JDE <ref type="bibr" target="#b7">[8]</ref>. The unmatched candidate boxes will be initialized as new tracklets, where the corresponding ID embeddings are simultaneously initialized as the template of new tracklets. The unmatched tracklets are set as the 'inactive' state. If a tracklet cannot find its matched box for 30 frames, it will be terminated. On the contrary, the unmatched tracklet will be restored to'active' state when it is successfully matched with a candidate box before terminating. The successful matched tracklets, the new tracklets, and the 'inactive' tracklets will be used to link with the candidate boxes in the next frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To demonstrate the advantages of the proposed CSTrack, we firstly study the effectiveness of each component in our tracking framework in Sec. IV-B. Then we compare our method with state-of-the-art approaches in Sec. IV-C. After that, we further analyze the data association ability of our framework in Sec. IV-D. Finally, we visualize the qualitative results of CSTrack and analyze the failure cases.  <ref type="figure">Fig. 5</ref>: The details of online tracking. For the detected candidate boxes, we will link them with existing tracklets by a cascade matching design following JDE <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Training. For a fair comparison, we use the same training datasets as the baseline tracker JDE <ref type="bibr" target="#b7">[8]</ref>, consisting of ETH <ref type="bibr" target="#b29">[30]</ref>, CityPerson <ref type="bibr" target="#b30">[31]</ref>, CalTech <ref type="bibr" target="#b31">[32]</ref>, MOT17 <ref type="bibr" target="#b32">[33]</ref>, CUDK-SYSU <ref type="bibr" target="#b33">[34]</ref> and PRW <ref type="bibr" target="#b34">[35]</ref>. Wherein, ETH and CityPerson only provide box annotations, so we only use them to train the detection branch. The other four datasets provide both detection and ID annotations, allowing us to train both object detection and ReID tasks. We chop off the videos in ETH <ref type="bibr" target="#b29">[30]</ref> that are overlapped with the testing benchmark MOT16 <ref type="bibr" target="#b32">[33]</ref>. For further improving performance, we introduce CrowdHuman <ref type="bibr" target="#b35">[36]</ref> into training. We train our network with the SGD optimizer for 30 epochs on a single RTX 2080 Ti GPU. The batch size is set to 8. The initial learning rate is 5 ? 10 ?4 , and we decay the learning rate to 5 ? 10 ?5 at the 20th epoch. The hyperparameters of the object detector are set the same as Yolo-v5 and others follow IDE <ref type="bibr" target="#b7">[8]</ref>. The Backbone and Neck networks are initialized with the parameters pre-trained on COCO <ref type="bibr" target="#b36">[37]</ref>. For the reciprocal network, we set the feature size after avg-pooling layer as (H , W ) = (6, 10) and initialize the trainable parameters ? 1/2 = 0.5.</p><p>Testing and Evaluation Metrics. We evaluate our network on MOT16 <ref type="bibr" target="#b32">[33]</ref>, MOT17 <ref type="bibr" target="#b32">[33]</ref> and the recently-released MOT20 <ref type="bibr" target="#b37">[38]</ref>. Specifically, MOT16 <ref type="bibr" target="#b32">[33]</ref> and MOT17 <ref type="bibr" target="#b32">[33]</ref> contain the same 7 testing videos, yet some videos are reannotated. MOT20 <ref type="bibr" target="#b37">[38]</ref> consists of 4 testing videos under extremely crowded scenes. Following the common practices in MOT Challenge 1 , we adopt the CLEAR metric <ref type="bibr" target="#b38">[39]</ref>, particularly Multiple Object Tracking Accuracy (MOTA) to evaluate the overall performance. ID F1 Score (IDF1) <ref type="bibr" target="#b39">[40]</ref> is employed to evaluate the association performance. We also report other metrics including the Most Tracked ratio (MT) for the ratio of most tracked (&gt; 80% time) objects, Most Lost ratio (ML) for most lost (&lt; 20% time) objects, the number of Identity Switch (ID Sw.) <ref type="bibr" target="#b40">[41]</ref> and FPS for measuring frame rate of the overall system.</p><p>Our tracker is implemented using Python 3.7 and PyTorch 1.6.0. The experiments are conducted on a single RTX 2080Ti GPU and Xeon Gold 5218 2.30GHz CPU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies 2</head><p>Component-wise Analysis. In this section, we verify the effectiveness of our proposed reciprocal network (REN in Sec. III-B) and scale-aware attention network (SAAN in Sec. III-C). For a fair comparison with JDE, all experiments are trained using the six datasets mentioned above and validated in the MOT16 testing set <ref type="bibr" target="#b32">[33]</ref>. The comparison experimental results are presented in Tab. I. We first replace the object detector in JDE <ref type="bibr" target="#b7">[8]</ref> from YOLO-v3 <ref type="bibr" target="#b18">[19]</ref> to YOLO-v5, which ensures a better performance and faster inference speed (x vs. y). The YOLO-v5 replacement provides a strong baseline for our following design. When equipped with the proposed reciprocal network (REN), it achieves 1.9 points gains on MOTA, 2.4 points gains on IDF1 and the IDs decrease from 1798 to 1365 (y vs. z). This demonstrates the effectiveness of our REN component in learning taskdependent representations to improve both detection and association performance. The introduced SAAN module aims to avoid the semantic gap when matching ID embeddings, and it brings 8.6 points gains on IDF1 (z vs. |). Compared with the vanilla JDE <ref type="bibr" target="#b7">[8]</ref>, our tracker significantly improves tracking performance (x vs. |), i.e., MOTA +8.5%, IDF1 +15.8% and ID Sw. decreased from 1544 to 1121.</p><p>Self-relation and Cross-relation Analysis. To verify the effectiveness of self-relation and cross-relation in our tracking framework, we conduct experiments with different settings and present the comparison results in Tab. II. We train all the models on half of the training set and validate on another half. Compared with the model without REN (x vs. |), employing self-relation to learn specificities can bring    Impact of Attention Module Arrangement. In order to understand the impact of the attention module arrangement in SAAN (see Sec. III-C), we conduct ablation studies on MOT17 validation set <ref type="bibr" target="#b32">[33]</ref>. As shown in Tab. III, compared with one-shot tracker equipped with the basic ReID head, our model achieves better MOTA and IDF1 (x?~vs. ). This verifies the effectiveness of our designed SAAN network in learning discriminative embeddings. As our core motivation is to robustly aggregate information from different resolutions, we validate our model with different attention module arrangements. Firstly, we configure different combinations of spatial attention modules (SAM) and channel attention modules (CAM) at each resolution. The results show that introducing spatial attention outperforms the basic model by +1.9% in MOTA, +1.0% in IDF1 and ID Sw. decreased from 723 to 633, respectively (see x?{). This demonstrates that SAM module is useful to enhance the target-related features and suppress background noise. Considering results from | to~, we observe that jointly applying channel attention in aggregated feature can achieve even better results, further decreasing ID Sw. from 633 to 535.</p><p>Training Dataset Setting. We conduct another ablation study to evaluate the impact of different training dataset setting, and the corresponding results are presented in Tab. IV. According to the results of Tab. IV, when only using the MOT17 dataset <ref type="bibr" target="#b32">[33]</ref> for training (see y), our tracker achieves 71.3 MOTA and 68.6 IDF1, which outperforms many existing trackers. For a fair comparison, we employ the same setting as our baseline tracker JDE <ref type="bibr" target="#b7">[8]</ref>, i.e., training on the six datasets mentioned in Sec. IV-A. From the result presented in Tab. IV (x vs. z), our model surpasses JDE by 8.5 point in MOTA and 15.8 point in IDF1, and the ID Sw. decreased form 1544 to 1050. It further verifies that our design is simple and effective to improve tracking performance by alleviating the competition between object detection and ReID in the one-shot tracker. Moreover, we further add CrowdHuman <ref type="bibr" target="#b35">[36]</ref> images into training, and observe that it brings gains of 2.7 point on TABLE VI: Comparison with state-of-the-art online MOT trackers on the MOT16, MOT17 and MOT20 benchmarks. # indicates one-shot method. * indicates other joint detection and tracking methods which adopt non-ReID methods for data association. Others without special sign indicate two-stage methods. FP and FN mean false positive and false negative, respectively. FPS denotes frame rates for the whole tracking framework. "CSTrack-S" indicates our lightweight version (detailed in Sec. IV). MOTA and 1.7 points on IDF1 (z vs. {). One possible reason is that training with CrowdHuman dataset can improve model tracking performance in crowded scene. Impact of ID Loss Weight. To jointly optimize object detection and target representation (i.e. ID embedding) learning, we balance their loss with a handcrafted weight ?. To study the impact of ? for ID embedding learning, an ablation experiment is conducted and the results are shown in Tab. V. x?| demonstrate that a large ID loss weight ? will degrade detection performance (see MOTA score), while a small ID loss weight degrades matching performance (see IDF1 and ID Sw. score). We set ? to 0.02 considering better scoring across different evaluation criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons with State-of-the-art Trackers</head><p>We compare our multi-object tracker CSTrack with other start-of-the-art MOT tracking methods on MOT16 <ref type="bibr" target="#b32">[33]</ref>, MOT17 <ref type="bibr" target="#b32">[33]</ref> and MOT20 <ref type="bibr" target="#b37">[38]</ref> benchmarks. The compared methods can be divided into three categories. The first one is two-stage method including SORT POI <ref type="bibr" target="#b11">[12]</ref>, POI <ref type="bibr" target="#b2">[3]</ref>, DeepSORT-2 <ref type="bibr" target="#b1">[2]</ref>, RAN <ref type="bibr" target="#b4">[5]</ref>, TAP <ref type="bibr" target="#b3">[4]</ref>, CNNMTT <ref type="bibr" target="#b43">[43]</ref>, IAT <ref type="bibr" target="#b41">[42]</ref>, STPP <ref type="bibr" target="#b44">[44]</ref> and TPM <ref type="bibr" target="#b45">[45]</ref>. The second one is the one-shot method including JDE <ref type="bibr" target="#b7">[8]</ref> and FairMOTv2 <ref type="bibr" target="#b9">[10]</ref>. The third one is the other joint detection and tracking method without using ReID for data association, including CTrackerV1 <ref type="bibr" target="#b21">[22]</ref>, TubeTK <ref type="bibr" target="#b22">[23]</ref>, Tracktor++ <ref type="bibr" target="#b23">[24]</ref> and CenterTrack <ref type="bibr" target="#b24">[25]</ref>. The evaluations of all above methods on these benchmarks are performed by the official online server 3 .</p><p>Two-stage Methods. The comparison with two-stage methods follows the protocols in <ref type="bibr" target="#b32">[33]</ref>. All the two-stage methods use the private detector (i.e., FasterRCNN <ref type="bibr" target="#b13">[14]</ref>), which is trained on a large private pedestrian detection dataset. The main differences among them lie in their ID embedding extraction and association strategies. As shown in Tab. VI, our proposed CSTrack outperforms the prior state-of-the-art methods by a significant margin and performs much faster. For example, comparing with the top performance two-stage In the term of data association, it is not feasible to directly compare these methods because of the nonuniform object detectors. Therefore, we further analyze the upper bound of association under the same detections input in Sec. IV-D.</p><p>One-shot Trackers. We present the evaluation results with the comparisons to recently-prevailing one-shot trackers on MOT16 <ref type="bibr" target="#b32">[33]</ref>, MOT17 <ref type="bibr" target="#b32">[33]</ref> and MOT20 <ref type="bibr" target="#b37">[38]</ref>. Our CSTrack achieves MOTA score of 75.6 and IDF1 score of 73.3 on MOT16, which evidently outperforms JDE <ref type="bibr" target="#b7">[8]</ref> on MOTA of +11.2% and IDF1 of +17.5%. Besides, we compare our model with the recently-proposed one-shot FairMOTv2 <ref type="bibr" target="#b9">[10]</ref>, which achieves the second performance of MOTA in all three benchmarks. FairMOTv2 <ref type="bibr" target="#b9">[10]</ref> is trained on the same datasets as CSTrack, which makes the comparison fair. On MOT16 and MOT17, the false negative (FN) and false positive (FP) of our tracker surpass FairMOTv2 for 5.1%?13.3% on FP and 2.1%?2.7% on FN. On MOT20, our tracker outperforms FairMOTv2 by significantly decreasing FP from 103440 to 25404 and ID Sw. from 5243 to 3196. It means that our tracker can achieve a comparable or even superior IDF1 score when we use fewer boxes, which actually verifies that our method authentically improves the performance of data association.</p><p>Other Joint Detection and Tracking Methods. To further evaluate the proposed method, we conduct some comparisons with other joint detection and tracking methods, which adopt non-ReID methods for data association. The evaluation results on MOT16 and MOT17 are presented in Tab. VI. Compared with these methods, our tracker significantly improves tracking performances, especially on data association ability, i.e., IDF1 +11.1%?18.4% in MOT16 and +7.6%?17.2% in MOT17. It verifies that our design effectively use ID information, which makes the data association ability of our tracker outperform the existing joint detection and tracking methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Further Analysis</head><p>HiEve Challenge. For further evaluating the strength of CSTrack, we show a comparison on a more challenging benchmark, i.e., Human in Events dataset (HiEve) <ref type="bibr" target="#b47">[47]</ref>. HiEve In this section, we study the upperbound of the data association ability of our model. As a common wisdom, tracking performance is greatly influenced by the adopted object detectors <ref type="bibr" target="#b38">[39]</ref>. To eliminate the influence of object detectors, we validate on the MOT16 training set by replacing the detection results with ground-truth bounding boxes. For a fair comparison, all of these methods are trained on the same training set, i.e., MOT17 <ref type="bibr" target="#b32">[33]</ref>. As shown in Tab. VIII, compared with JDE <ref type="bibr" target="#b7">[8]</ref>, our method yields substantial improvements on data association reflected by 9 points gains on IDF1 score. Moreover, the IDF1 score of our method surpasses the widely-used two-stage method DeepSORT-2 <ref type="bibr" target="#b1">[2]</ref>, which employs Wide Residual Network (WRN) <ref type="bibr" target="#b48">[48]</ref> as ID embedding extraction model. It further verifies our design can effectively improve association ability of one-shot tracker.</p><p>Discriminative Ability of ID Embeddings. We visualize the discriminative ability of ID embeddings between JDE and our method. As shown in <ref type="figure" target="#fig_6">Fig. 6 (b)</ref> and (d), we observe that ID embeddings of JDE look more similar among targets than CSTrack, not only in the current example frame but also in the temple (i.e., ID embeddings of the previous tracklets). For a more straightforward comparison, we construct the cosine metric matrix between ID embeddings of the current example frame in <ref type="figure" target="#fig_6">Fig. 6</ref> (c) and the cosine metric matrix between the templates in <ref type="figure" target="#fig_6">Fig. 6 (e)</ref>. Here, red color indicates a higher correlation between two ID embeddings, and blue indicates less correlated. The value on the diagonal indicates the similarity between the target and itself. The ideal cosine metric matrixes in <ref type="figure" target="#fig_6">Fig. 6 (c)</ref> and <ref type="figure" target="#fig_6">Fig. 6</ref> (e) are ones with only the diagonals red and the rest blue. Compared with JDE <ref type="bibr" target="#b7">[8]</ref>, we observe that our method can learn more discriminative embeddings to avoid ambiguous matching between targets. It is also efficient for the update of the discriminative template. Moreover, we visualize the cosine metric matrix between ID embeddings of the current frame and previous tracklets template in <ref type="figure" target="#fig_6">Fig. 6 (f)</ref>. During matching, we hope that each row and each column has at most one red color in ideal cosine metric matrix, and the rest are blue. As shown in <ref type="figure" target="#fig_6">Fig. 6</ref> (f), we observe that our method can significantly improve the association ability. The advantage over JDE <ref type="bibr" target="#b7">[8]</ref> is most pronounced on the robustness to occlusion and scale changes (see <ref type="figure" target="#fig_6">Fig. 6 (a)</ref>), which verifies that our proposed SAAN network is efficient to alleviate scaleaware competition and improve the resilience to objects with different scales. Qualitative Results and Failure Cases. In this section, we show the qualitative results in MOT17 testing set <ref type="bibr" target="#b32">[33]</ref> (see <ref type="figure" target="#fig_7">Fig. 7</ref>) and MOT20 testing set <ref type="bibr" target="#b37">[38]</ref> (see <ref type="figure" target="#fig_8">Fig. 8</ref>). We observe that our method is efficient to deal with large-scale variations and keep correct identities (see MOT17-08 and MOT17-12). It mainly attributes to our SAAN module as it focuses on multi-resolution information to learn more discriminative embeddings. Moreover, we observe that our tracker performs well in many challenging scenes. For instance, from the results of MOT17-07 and MOT17-14, we can find that our tracker can predict accurate bounding boxes for small targets. In the visualization of MOT17-03, we find that our tracker performs robustly to occlusion objects, even in the presence of overcrowding (e.g., the scenes in MOT20 testing set).</p><p>Despite the promising results, we also should be aware of the unaddressed cases. We discuss two scenarios in which CSTrack fails in <ref type="figure" target="#fig_9">Fig. 9</ref>. In particular, <ref type="figure" target="#fig_9">Fig. 9 (a)</ref> illustrates the negative influence of image blur, which makes our detector misclassify objects as background at some points. <ref type="figure" target="#fig_9">Fig. 9 (b)</ref> shows another failure circumstance, i.e., over-occlusion. As we can see, only a fraction of the targets have been detected over time, and the same local-visible objects will be missed by the detector. These cases are challenging for MOT because the missed targets will break the temporal consistency of tracklets. Despite being different in nature, these two cases arguably arise from the over-rely on image-based detection, which only focuses on single-frame features from the current frame. When a single-frame feature is not reliable, the tracker will miss the objects. In further work, we will further improve the one-shot tracker by employing temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a new one-shot online model CSTrack for the MOT task. A novel reciprocal network (REN) and scale-aware attention network (SAAN) are introduced to mitigate the competition and improve the collaboration of detection and ReID subtasks in MOT. The experimental results demonstrate the effectiveness and the efficiency of our framework. Compared with the methods on public benchmarks, our model achieves state-of-the-art performance, which outperforms our baseline JDE by +11.2% on MOTA and +17.5% on IDF1. Besides, CSTrack is a nearly real-time MOT tracker and its lightweight version can run at 34.6 FPS with only a minor performance drop, which is more suitable for real applications.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Motivation of CSTrack. (a) visualizes the similarity maps about detection and ReID tasks respectively, where detection expects all the pedestrians have high response values</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>Architecture diagrams. (a) is the feature extractor including backbone and neck (FPN). (b) illustrates the vanilla prediction structure of JDE. (c) illustrates our proposed prediction structure of CSTrack. Different from JDE, CSTrack introduces a reciprocal network (REN) to learn task-dependent representations and a scale-aware attention network to generate discriminative embeddings, which efficiently mitigate the competition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>The details of scale-aware attention network (SAAN). Wherein, (a) is the overall structure of the network, (b) is the diagram of spatial attention module (SAM), and (c) is the diagram of channel attention module (CAM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization about the discriminative ability of ID embeddings: (a) Example Image with ground truth detection results. (b) ID embeddings of the current example frame. (c) Cosine metric matrix between two ID embeddings of the current example frame. (d) Templates (i.e., ID embeddings of the previous tracklets). (e) Cosine metric matrix between two ID embeddings of the template. (f) Cosine metric matrix between ID embeddings of the current example frame and the template. Red color indicates a higher correlation between two features. Best viewed in color and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative results of our CSTrack on MOT17 [33]. Different colored bounding boxes denote different identity. The line under each bounding box indicates the tracklet of each target. The frame number is in the upper left of the figure. Best viewed in color and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative results of our CSTrack on MOT20 [38]. Different colored bounding boxes denote different identity. The line under each bounding box indicates the tracklet of each target. The frame number is in the upper left of the figure. Best viewed in color and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Failure cases: the first row (a) illustrates image blur and the second row (b) illustrates over-occlusion. The red dotted boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Component-wise Analysis of CSTrack on MOT16 testing set.</figDesc><table><row><cell>NUM</cell><cell>Method</cell><cell cols="3">REN SAAN MOTA? IDF1? ID Sw.?</cell></row><row><cell cols="2">x JDE(yolo-v3)</cell><cell>64.4</cell><cell>55.8</cell><cell>1544</cell></row><row><cell cols="2">y JDE(yolo-v5)</cell><cell>68.9</cell><cell>60.7</cell><cell>1798</cell></row><row><cell cols="2">z JDE(yolo-v5)</cell><cell>70.8</cell><cell>63.1</cell><cell>1365</cell></row><row><cell cols="2">{ JDE(yolo-v5)</cell><cell>69.4</cell><cell>69.3</cell><cell>1226</cell></row><row><cell cols="2">| JDE(yolo-v5)</cell><cell>72.9</cell><cell>71.6</cell><cell>1121</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Ablation study of REN on MOT17 validation set. SR represents self-relation, CR represents cross-relation.</figDesc><table><row><cell cols="2">NUM SR CR</cell><cell>? 1</cell><cell>? 2</cell><cell cols="3">MOTA? IDF1? ID Sw.?</cell></row><row><cell>x</cell><cell></cell><cell>?</cell><cell>?</cell><cell>65.3</cell><cell>69.5</cell><cell>713</cell></row><row><cell>y</cell><cell></cell><cell>?</cell><cell>?</cell><cell>64.8</cell><cell>67.2</cell><cell>616</cell></row><row><cell>z</cell><cell></cell><cell>0.5</cell><cell>0.5</cell><cell>64.5</cell><cell>68.9</cell><cell>679</cell></row><row><cell>{</cell><cell></cell><cell cols="2">0.12122 0.31519</cell><cell>66.0</cell><cell>70.7</cell><cell>535</cell></row><row><cell>|</cell><cell></cell><cell>w/o REN</cell><cell></cell><cell>64.6</cell><cell>68.7</cell><cell>739</cell></row><row><cell>}</cell><cell cols="3">replace REN with CNN</cell><cell>64.9</cell><cell>69.0</cell><cell>717</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Impact of attention module arrangement in our tracker, where the module will be arranged serially, as<ref type="bibr" target="#b27">[28]</ref>.</figDesc><table><row><cell>NUM</cell><cell>Each resolution SAM CAM</cell><cell>concatenate SAM CAM</cell><cell cols="2">MOTA? IDF1? ID Sw.?</cell></row><row><cell>x</cell><cell></cell><cell></cell><cell>64.1</cell><cell>66.4</cell><cell>723</cell></row><row><cell>y</cell><cell></cell><cell></cell><cell>66.0</cell><cell>70.4</cell><cell>633</cell></row><row><cell>z</cell><cell></cell><cell></cell><cell>65.4</cell><cell>68.4</cell><cell>659</cell></row><row><cell>{</cell><cell></cell><cell></cell><cell>64.9</cell><cell>69.8</cell><cell>690</cell></row><row><cell>|</cell><cell></cell><cell></cell><cell>64.1</cell><cell>69.3</cell><cell>721</cell></row><row><cell>}</cell><cell></cell><cell></cell><cell>66.0</cell><cell>70.7</cell><cell>535</cell></row><row><cell></cell><cell></cell><cell></cell><cell>64.3</cell><cell>69.9</cell><cell>700</cell></row><row><cell></cell><cell cols="2">Basic ReID Head (w/o SAAN)</cell><cell>63.8</cell><cell>61.3</cell><cell>1047</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Analysis of training dataset settings on MOT16 testing set. "S" indicates only using MOT17 for training. "M" indicates using six datasets mentioned in the manuscript. "B" denotes training on the seven datasets including "M" and Crowdhuman dataset. It verifies that CSTrack can achieve efficient performance, even only using MOT17 for training.</figDesc><table><row><cell>NUM</cell><cell>Settings</cell><cell cols="3">image bbox ID MOTA? IDF1? ID Sw.?</cell></row><row><cell>x</cell><cell>JDE (M)</cell><cell cols="2">54K 270K 8.7K 64.4</cell><cell>55.8 1544</cell></row><row><cell>y</cell><cell>CSTrack (S)</cell><cell>5K</cell><cell>112K 0.5K 71.3</cell><cell>68.6 1356</cell></row><row><cell>z</cell><cell>CSTrack (M)</cell><cell cols="2">54K 270K 8.7K 72.9</cell><cell>71.6 1050</cell></row><row><cell>{</cell><cell>CSTrack (B)</cell><cell cols="2">73K 740K 8.7K 75.6</cell><cell>73.3 1121</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Impact of ID loss weight setting in our tracker. ? indicates the ID loss weight. The reason for this degradation is that only learning commonalities is insufficient to resolve the underlying competition between object detection and ReID. When we merge self-relation and cross-relation with equal weights (i.e., we set ? 1/2 as constant weights 0.5, see z), the performance is inferior to the model only introducing self-relation (x vs. z). But surprisingly, when introducing learning weights to adjust the fusion of self-relation and cross-relation, we obtain promising results with MOTA of 66.0% and IDF1 of 70.7%. Notably, the ID Sw. decreased about 25% (x vs. {), demonstrating the effectiveness of our reciprocal network. It is noted that the learning weights ? 1 and ? 2 converge to 0.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>12122 and 0.31519,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>respectively. This reveals that different tasks have different</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>requirements for features. Meanwhile, it verifies the speci-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ficities and commonalities learning can efficiently learn task-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>dependent representations, which improves the performance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>remarkably. What's more, to further verify the effectiveness of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>our reciprocal network, we replace REN with CNN module</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>which has a nearly similar computational cost (74.6M and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1304 GFlops for</cell></row><row><cell>NUM</cell><cell>?</cell><cell>MOTA?</cell><cell cols="2">IDF1? ID Sw.?</cell></row><row><cell>x</cell><cell>1</cell><cell>21.1</cell><cell>35.2</cell><cell>966</cell></row><row><cell>y</cell><cell>0.8</cell><cell>33.5</cell><cell>45.7</cell><cell>1002</cell></row><row><cell>z</cell><cell>0.6</cell><cell>54.2</cell><cell>60.5</cell><cell>693</cell></row><row><cell>{</cell><cell>0.4</cell><cell>55.0</cell><cell>62.3</cell><cell>683</cell></row><row><cell>|</cell><cell>0.2</cell><cell>64.5</cell><cell>69.0</cell><cell>713</cell></row><row><cell>}</cell><cell>0.1</cell><cell>65.3</cell><cell>69.1</cell><cell>642</cell></row><row><cell></cell><cell>0.05</cell><cell>65.2</cell><cell>70.3</cell><cell>652</cell></row><row><cell></cell><cell>0.02</cell><cell>66.0</cell><cell>70.7</cell><cell>535</cell></row><row><cell></cell><cell>0.01</cell><cell>66.2</cell><cell>68.6</cell><cell>701</cell></row><row><cell></cell><cell>0.005</cell><cell>65.8</cell><cell>67.2</cell><cell>742</cell></row></table><note>0.7% gains on MOTA and 0.8% gains on IDF1, respectively. When only introducing the cross-relation, our method sightly outperforms the model without REN by 0.2% on MOTA, but IDF1 decreases from 68.7% to 67.2% (y vs. |).REN version v.s. 77.2M and 1308 GFLOPs for CNN version). We observe that REN can achieve better results, even with fewer parameters and FLOPs ({ vs. }).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>Comparison with state-of-the-art trackers on the HiEve banchmarks (private detection).</figDesc><table><row><cell>Model</cell><cell cols="4">MOTA? IDF1? MT? ML? FP? FN? ID Sw.?</cell></row><row><cell cols="2">DeepSORT [2] 27.1</cell><cell>28.5</cell><cell>8.4</cell><cell>41.4 5894 42668 3122</cell></row><row><cell>JDE [8]</cell><cell>33.1</cell><cell>36.0</cell><cell cols="2">15.1 24.1 9526 33327 3605</cell></row><row><cell>FairMOT [10]</cell><cell>35.0</cell><cell>46.6</cell><cell cols="2">16.2 44.1 6523 37750 2312</cell></row><row><cell cols="2">CenterTrack [25] 42.4</cell><cell>38.3</cell><cell cols="2">23.9 26.5 5802 29766 2940</cell></row><row><cell cols="2">NewTracker [46] 46.4</cell><cell>43.2</cell><cell cols="2">26.3 30.8 4667 30489 2133</cell></row><row><cell>CSTrack</cell><cell>48.6</cell><cell>51.4</cell><cell cols="2">20.4 33.5 2366 31933 1475</cell></row><row><cell cols="5">tracker (i.e., POI [3]), CSTrack outperforms it by +9.5% on</cell></row><row><cell cols="5">MOTA and +8.2% on IDF1. Considering the inference speed,</cell></row><row><cell cols="5">the proposed CSTrack runs faster than existing two-stage</cell></row><row><cell cols="5">methods, i.e., 16.4 FPS vs. 0.5?8.6 FPS on MOT16 [33].</cell></row><row><cell cols="5">Furthermore, we construct a light version, namely CSTrack-</cell></row><row><cell cols="5">S, which reduces convolution channels and layers of CSTrack</cell></row><row><cell cols="5">to 30% of its original settings. For the lightweight vision, the</cell></row><row><cell cols="5">running speed is clearly sped up to 34.6 FPS with only a small</cell></row><row><cell cols="5">performance drop (MOTA -1.8?2.4% and IDF1 -2.3?3.6%).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII :</head><label>VIII</label><figDesc>To show the potential of CSTrack. IDP and IDR describe the precision and recall of matching target tracklets.</figDesc><table><row><cell>Model</cell><cell cols="4">MOTA? IDF1? IDP? IDR? ID Sw.?</cell></row><row><cell>JDE [8]</cell><cell>97.6</cell><cell>87.6</cell><cell>88.3 86.9</cell><cell>871</cell></row><row><cell>DeepSORT-2 [2]</cell><cell>98.9</cell><cell>95.6</cell><cell>95.9 95.3</cell><cell>93</cell></row><row><cell>CSTrack</cell><cell>98.9</cell><cell>96.6</cell><cell>97.1 96.1</cell><cell>162</cell></row><row><cell cols="5">focuses on human-centric complex events, e.g., fighting, quar-</cell></row><row><cell cols="5">reling, accident, robbery and shopping, including 19 videos</cell></row><row><cell cols="5">for training and 13 videos for testing. For evaluating on the</cell></row><row><cell cols="5">HiEve benchmark, we fine-tune our method on the training</cell></row><row><cell cols="5">set of HiEve following the same training procedure on MOT</cell></row><row><cell cols="5">challenge benchmarks (See Sec. IV-A). All evaluations are</cell></row><row><cell cols="5">performed by the official online server 4 , as shown in Tab. VII.</cell></row><row><cell cols="5">The proposed CSTrack outperforms its baseline JDE [8] on</cell></row><row><cell cols="5">MOTA for +16.5% and IDF1 for +15.4%. Compared with</cell></row><row><cell cols="5">other top-ranked methods, our tracker still shows better track-</cell></row><row><cell cols="4">ing performance on MOTA and IDF1.</cell><cell></cell></row><row><cell cols="2">Upper-bound Analysis.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Chao Liang received a Bachelor's degree from School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China, in 2019. From 2019 to now, he is pursuing his M.S. degree in School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China. His research interests include object detection, object tracking and person reidentification. Zhang received a Bachelor's degree in optics and information engineering from the University of Electronic Science and Technology of China in 2017. From 2017 to now, he is reading a Ph.D. of pattern recognition and intelligent system in the Institute of Automation, Chinese Academy of Sciences. His current research interest is video understanding, including object tracking, object segmentation, and video representation learning. Xue Zhou (Member, IEEE) received the B.S. degree in automatic control from the University of Electronic Science and Technology of China, Chengdu, China, in 2003, and the Ph.D. degree in pattern recognition and intelligent system from the Institute of Automation, Chinese Academy of Sciences, Beijing, China, in 2008. She is currently a Professor with the School of Automation Engineering, University of Electronic Science and Technology of China. Her current research interests are video object perception, including video object segmentation, level-set based object tracking, multiple object tracking and pedestrian reidentification. Bing Li received the Ph.D. degree from the Department of Computer Science and Engineering, Beijing Jiaotong University, Beijing, China, in 2009. He is currently a Professor with the Institute of Automation, Chinese Academy of Sciences, Beijing. His current research interests include video understanding, color constancy, visual saliency, multi-instance learning, and Web content security. Shuyuan Zhu (S'08-A'09-M'13) received the Ph.D. degree from the Hong Kong University of Science and Technology (HKUST), Hong Kong, in 2010. From 2010 to 2012, he worked at HKUST and Hong Kong Applied Science and Technology Research Institute Company Limited, respectively. In 2013, he joined University of Electronic Science and Technology of China and is currently a Professor with School of Information and Communication Engineering. Dr. Zhu's research interests include image/video compression and image processing. He currently serves as an Associate Editor of IEEE Transactions on Circuits and Systems for Video Technology. Weiming Hu received the Ph.D. degree from the Department of Computer Science and Engineering, Zhejiang University, Zhejiang, China, in 1998. From 1998 to 2000, he was a postdoctoral research fellow with the Institute of Computer Science and Technology, Peking University, Beijing. He is currently a professor with the Institute of Automation, Chinese Academy of Sciences(CASIA), Beijing. His research interests are visual motion analysis, recognition of web objectionable information, and network intrusion detection.</figDesc><table><row><cell>Zhipeng</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Due to the limited submissions of the MOT Challenge (totally 4 times per model on each benchmark), some ablation studies are verified on validation set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://motchallenge.net</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://humaninevents.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple object tracking: A literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">103448</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poi: Multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online multi-target tracking with tensor-based high-order graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1809" to="1814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent autoregressive networks for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards real-time multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12605</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Retinatrack: Online single stage joint detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Votel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fairmot: On the fairness of detection and re-identification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2004</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3464" to="3468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">High-speed tracking-bydetection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2129" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial-temporal relation networks for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3988" to="3998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6247" to="6257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="145" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tubetk: Adopting tubes to track multi-object in a one-step training model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6308" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="941" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="474" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distance-iou loss: Faster and better learning for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3213" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mot16: A benchmark for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Mot20: A benchmark for multi object tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09003</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to associate: Hybridboosted multi-target tracker for crowded scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2953" to="2960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Online multi-object tracking with instance-aware tracker and dynamic model refreshment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in 2019 IEEE winter conference on applications of computer vision (WACV</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="161" to="170" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-target tracking using cnn-based features: Cnnmtt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ahadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="7077" to="7096" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatiotemporal point process for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tpm: Multiple object tracking with tracklet-plane matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107480</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Application of multi-object tracking with siamese track-rcnn to the human in events dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4625" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Human in events: A large-scale benchmark for human-centric video analysis in complex events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04490</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
