<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">SenseTime Computer Vision Group</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Su</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Shanghai AI Laboratory</orgName>
								<address>
									<country>Shanghai China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haosen</forename><surname>Yang</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">SenseTime Computer Vision Group</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Shanghai AI Laboratory</orgName>
								<address>
									<country>Shanghai China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Recognition</term>
					<term>Adaptive Inference</term>
					<term>Temporal Sampling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is challenging for artificial intelligence systems to achieve accurate video recognition under the scenario of low computation costs. Adaptive inference based efficient video recognition methods typically preview videos and focus on salient parts to reduce computation costs. Most existing works focus on complex networks learning with video classification based objectives. Taking all frames as positive samples, few of them pay attention to the discrimination between positive samples (salient frames) and negative samples (non-salient frames) in supervisions. To fill this gap, in this paper, we propose a novel Non-saliency Suppression Network (NSNet), which effectively suppresses the responses of non-salient frames. Specifically, on the frame level, effective pseudo labels that can distinguish between salient and non-salient frames are generated to guide the frame saliency learning. On the video level, a temporal attention module is learned under dual video-level supervisions on both the salient and the non-salient representations. Saliency measurements from both two levels are combined for exploitation of multigranularity complementary information. Extensive experiments conducted on four well-known benchmarks verify our NSNet not only achieves the state-of-the-art accuracy-efficiency trade-off but also present a significantly faster (2.4?4.3?) practical inference speed than state-of-the-art methods. Our project page is at https://lawrencexia2008.github.io/ projects/nsnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The prevalence of digital devices exponentially increases the data amount of video content. Meanwhile, the proliferation of videos poses great challenges for  The arrows indicate the moving directions of frame features during training process. In (a), the features of non-salient frames, the 1 st , 2 nd , 5 th frames are forced to cluster near the centroid of the category. In contrast, our approach introduce a non-salient category and those low saliency frames are labeled as non-salient and function as negative samples against the video category during training. In this way, features of the 1 st , 2 nd , 5 th frames in (b) are pushed away to form another cluster of non-salient category.</p><p>existing video analysis systems, and consequently draws more and more attention from the research community. Thanks to the renaissance of deep neural networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b44">45]</ref>, a surge of progress has been made to promote the development of advanced video understanding techniques <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15]</ref>. Although achieving promising performance on some benchmarks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref> with supervised learning or unsupervised learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b5">6]</ref>, many of them apply computationally heavy networks, which hinders their deployment to the practical applications, such as autonomous driving and personalized recommendations. Accordingly, building an efficient video understanding system is a crucial step towards widespread deployment in the real world.</p><p>To achieve efficient video recognition, a rich line of studies have been proposed, which roughly fall into two paradigms: i) lightweight architecture based methods and ii) adaptive inference based methods. The first category of approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b49">50]</ref> devote to reducing the computational cost via designing lightweight networks. By contrast, another series of works propose to achieve efficient recognition by leveraging adaptive inference strategy to flexibly allocate resources according to the the saliency of frames. Specifically, the adaptive inference mechanism has been applied on multiple dimensions of video, including temporal sampling <ref type="bibr" target="#b19">[20]</ref>, spatial sampling <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b26">27]</ref>, etc. Compared to the former, the adaptive inference based methods is easier to be incorporated into the existing advanced recognition backbones. For example, in the pipeline of a adaptive temporal sampling method, a sampler network is first trained based on lightweight feature to sample key frames, and then an off-the-shell computationconsuming recognizer is evoked on sampled frames for final recognition.</p><p>Semantic saliency of each frame is the fundamental basis for the adaptive inference based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>. Nonetheless, it is difficult to obtain explicit supervision of frame-level saliency in the general setting of video recognition. Therefore, existing methods are mainly based on either reinforce learning (RL) or attention mechanism <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10]</ref>, where the agent (resp., attention module) is optimized to take actions (resp., attend) to those salient frames by classification objective based rewards (resp., loss). In this way, the sampler is trained to determine the salient frames by using all frames as the positive samples of the corresponding video category, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. Due to the lack of negative samples in training, it is hard for the sampler to accurately determine the non-salient frames from the salient ones within one video, which may easily overestimate the saliency of the non-salient frames. As a result, it is reasonable to introduce the negative samples for the frame-level video category classification objective based learning, which helps suppress the response of the non-salient frames to the video category during the sampling process.</p><p>To this end, we propose a novel Non-saliency Suppression (NS) mechanism, to provide negative-sample-aware supervision for saliency measurement, which can effectivly suppress the response of non-salient frames in the adaptive inference based framework. Specifically, the key principle is that the salient frames should belong to the corresponding video category while the non-salient frames should fall into a special category, which is distinguishable from all video categories. We term this special category as non-salient category, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. As the video categories are the only annotation we can use during training, in order to guarantee high-quality negative samples (i.e., the non-salient frames) in the frame-level saliency learning, we propose a Frame Scrutinize Module (FSM) to generate frame-level pseudo labels for supervision. By doing so, the salient frames can be effectively distinguished from the non-salient frames. In addition to the frame-level supervision, we then propose a temporal attention module named Video Glimpse Module (VGM) to compensate for high-level information of video events by using video-level supervision. In order to introduce NS mechanism on video level, we first formulate a video representation as a linear combination of two components: the representation of the salient parts and the representation of the non-salient parts of a video. Following the aforementioned principle, we then assign the label of that salient representation as current video label, and the label of non-salient representation as the special non-salient category.</p><p>Overall, our contributions are three-folds:</p><p>-We introduce the Non-saliency Suppression mechanism for suppressing the responses of non-salient frames, which considerably improve the discrimination power of the temporally sampled video representations without increment of computation overhead. -We propose an discriminative and flexible multi-granularity frame sampling framework Non-saliency Suppression Network (NSNet), which leverages supervisions from both video level and frame level to measure frame saliency. We design two specific schemes for realizing Non-saliency Suppression mechanism on the two granularities.</p><p>-Extensive experiments are conducted with multiple backbone architectures on four well-known benchmarks, i.e., ActivityNet, FCVID, Mini-Kinetics and UCF101, which show that our NSNet achieves superior performance over existing state-of-the-art methods with limited computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video Recognition. Video Recognition has made significant progress in past decade for successful application of neural networks including 2D CNNs <ref type="bibr" target="#b41">[42]</ref>, 3D CNNs <ref type="bibr" target="#b2">[3]</ref> and Transformers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b52">53]</ref>. Although decent results are achieved by powerful spatiotemporal networks, it is still challenging for applying video recognition in resource-constraint scenarios for its superfluous computational complexity. TSM <ref type="bibr" target="#b22">[23]</ref>, TEA <ref type="bibr" target="#b21">[22]</ref>, MVFNet <ref type="bibr" target="#b49">[50]</ref>, etc., try to realize temporal modelling with pure 2D CNNs to improve efficiency by shifting operations, motion based channel selection and multi views fusion. While P3D <ref type="bibr" target="#b30">[31]</ref>, S3D <ref type="bibr" target="#b57">[58]</ref>, R(2+1)D <ref type="bibr" target="#b39">[40]</ref>, SlowFast <ref type="bibr" target="#b6">[7]</ref>, Ada3D <ref type="bibr" target="#b20">[21]</ref>, DSANet <ref type="bibr" target="#b53">[54]</ref> are proposed to improve the efficiency by decomposing 3D convolution or designing hybrid 2D-3D frameworks. Different from these approaches, we seek to achieve efficient video recognition by adaptively sampling salient frames and recognize selectively on a per-sample basis.</p><p>Adaptive Inference. The core idea of adaptive inference is to dynamically allocate computational resources (network layers, parameters, etc.) conditioned on the input to improve the trade-off between performance and cost <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">52]</ref>. For video analysis, adaptive inference are realized in several perspectives including temporal sampling, resolution, sub-networks and modality. FastForward <ref type="bibr" target="#b4">[5]</ref>, FrameGlimpse <ref type="bibr" target="#b59">[60]</ref>, AdaFrame <ref type="bibr" target="#b55">[56]</ref>, MARL <ref type="bibr" target="#b50">[51]</ref> and OCSampler <ref type="bibr" target="#b23">[24]</ref> model temporal sampling as a decision-making process, which is optimized by policy gradients or differentiable alternatives. ListenToLook <ref type="bibr" target="#b7">[8]</ref>, SMART <ref type="bibr" target="#b9">[10]</ref>, TSQNet <ref type="bibr" target="#b56">[57]</ref> design temporal frame samplers based on attention mechanism. Besides temporal sampling, AdaFocus series <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref> samples salient patches for each frame to reduce spatial redundancy. AR-Net <ref type="bibr" target="#b26">[27]</ref>, LiteEval <ref type="bibr" target="#b54">[55]</ref>, AdaMML <ref type="bibr" target="#b29">[30]</ref> strategically allocate higher resolution, more powerful sub-networks, or more expensive modality to more informative frames, respectively. The most closely related work to ours is an adaptive temporal sampling method, SCSampler <ref type="bibr" target="#b19">[20]</ref>, which proposes a frame-level classification task with video label and measure saliency based on classification confidence. However, there exist substantial differences between SCSampler and the proposed NSNet. SCSampler assigns each frame with the video label, while we argue that only the salient ones belong to video category, other ones should be labeled as a special category distinguishable from all semantic categories. Besides, we also consider to measure frame saliency with video level supervisions to enable context-aware saliency measurements, which is overlooked by SCSampler. Compared with SC-  Sampler, our NSNet achieves significantly superior performance with much less computational overhead.</p><formula xml:id="formula_0">) ? ? ?</formula><formula xml:id="formula_1">= = + ? " " $ # $ % $ ' $ # $ ' $ % $ $ $ ( Temp Attn ' ) * ' ' ( ' ' ( ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><formula xml:id="formula_2">Let V = {x i } T i=1</formula><p>denote a video of T frames. Firstly, our NSNet is designed to select K most salient frames from all T frames. Then, the K salient frames are fed into a recognizer model, the predictions of which are aggregated to yield the final video-level prediction. The workflow of our system is illustrated in <ref type="figure" target="#fig_3">Figure 2a</ref>. Note that we use an off-the-shell model as our recognizer, and therefore, the problem can be formulated as how to effectively sample the most salient frames from the input frames. In the following sections, we introduce the process of salient frames selection for our NSNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Overview of NSNet</head><p>In this section, we elaborate on the proposed Non-saliency Suppression Network (NSNet), which mainly consists of three components: a Feature Embedding module (FEM), a Frame Scrutinize module (FSM), and a Video Glimpse module (VGM). The feature embedding module generates feature embedding from the input video frames. The FSM (see <ref type="figure" target="#fig_3">Figure 2b</ref>) measures saliency of each frame by predicting the saliency confidence scores in frame-level classification, and the VGM (see <ref type="figure" target="#fig_3">Figure 2d</ref>) models saliency of frames from the temporal attention weights used to aggregate attention-based feature for video-level classification. To alleviate the lack of negative samples, we further apply NS mechanism in the FSM and VGM in different ways. For each of these two modules, a non-salient category is attached onto the frame-level and the video-level supervisions, respectively, resulting in a total of C + 1 categories for each supervision, where C is the total number of the original video categories. In the FSM, a Non-saliency Suppression frame-level pseudo label generation strategy is proposed to separate the negative samples from the truly salient frames for frame-level saliency learning. In the VGM, a Non-saliency Suppression loss is proposed to impose an extra constraint of non-salient representations of videos besides original classification objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Embedding Module (FEM)</head><p>Here we encode the input video frames</p><formula xml:id="formula_3">{x i } T i=1 into the robust feature sequence {x i } T i=1</formula><p>, which is then used by our FSM and VGM. Specifically, we first use the off-the-shelf lightweight feature extractor, e.g., MobileNet, EfficientNet, etc, to take the video frames as input and extract features for all frames. In order to allow message passing among features for all frames, we then apply a transformer encoder <ref type="bibr" target="#b40">[41]</ref> on top of these features and output the feature sequence</p><formula xml:id="formula_4">{x i } T i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Frame Scrutinize Module (FSM)</head><p>In our FSM, we first generate frame-level Non-saliency Supression (NS) pseudo labels, and then use them as supervision to train our FSM to perform framelevel saliency classification and produce saliency scores. Details of our FSM are provided as follows.</p><p>Non-saliency Suppression Pseudo Label Generation. Here we denote the label of a video of the c-th category as a C-dimension one-hot vector y v ? R C , where y v,c = 1 and y v,m = 0| m?[1,C],m? =c . To distinguish between the salient frames and non-salient frames in frame labels, we then introduce a guiding saliency score g i , which is obtained from the recognizer, i.e., the one we used for final recognition as described in Section 3.1 <ref type="figure" target="#fig_3">(Figure 2a</ref>). Although the classification response produced by the pre-trained model (i.e., recognizer) is widely used for pseudo labeling in weakly-supervised learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47]</ref>, we propose a prototype-based strategy for more robust frame level pseudo label generation. According to <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b60">61]</ref>, a sample could be more representative when it is closer to the centroid in feature space. As a result, we use distances of the feature for the i-th frame from the prototype features of all categories to obtain g i . Specifically, we first use the recognizer to extract features and confidence scores on groundtruth category for all frames for each video in training set. Then the prototype feature of each category is then calculated by averaging all video features in that category. Here each video feature is obtained by applying average pooling on the features of the top-K frames based on the predicted confidence scores (see our Appendix A.1 for more details) The guiding saliency score g i for the i-th frame is as follows:</p><formula xml:id="formula_5">g i = e ?(x g i ,pc) C j=1 e ?(x g i ,pj ) ,<label>(1)</label></formula><p>where ? is a distance function measuring the similarity of two feature vectors, e.g., Euclidean Distance,x g i is the feature for the i-th frame extracted by the recognizer, p j and p c are the prototype features of the j-th category and the ground truth category, respectively. Finally, we use g i to generate the NS pseudo label y ns</p><formula xml:id="formula_6">i = [ g i y v,1 , g i y v,2 , ... g i y v,C , 1 ? g i ] ? R C+1</formula><p>. Frame-level Saliency Classification. After generating the NS pseudo labels, we then use them to train our FSM to perform frame-level saliency classification over the feature sequence {x i } T i=1 . Mathematically, the frame-level classification objective is defined as follows:</p><formula xml:id="formula_7">L f = ? T i=1 C+1 j=1 y ns i,j log(? ns i,j ),<label>(2)</label></formula><p>where y ns i,j is the element of the j-th category in y ns i , and? ns i,j is the classification prediction. It is noteworthy that y ns i is a soft one-hot target, the cross entropy loss of which is similar to label smooth <ref type="bibr" target="#b37">[38]</ref>. During inference, the frames with very high response to any one of C categories are identified as salient frames. To this end, the maximum confidence across C semantic categories (except the C + 1-th category) of classification score after softmax normalization is used for saliency measurement. We then apply additional softmax normalization along the time axis to obtain final saliency score s f i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Video Glimpse Module (VGM)</head><p>In our VGM, we first generate attention weights ? i = TempAttn(x i ) for the features of all observed frames, where TempAttn(?) is implemented by a fullyconnected layer followed by a L1 normalization layer, which is used to rescale attention weights to [0, 1] range. The features of all observed frames are then aggregated with the attention weights to generate the video salient representation</p><formula xml:id="formula_8">x sal v = T i=1 ? ixi .</formula><p>To perform video-level classification, the salient representation of the videox sal v is fed to a fully-connected layer to compute the crossentropy loss with video label. In order to guide our VGM to separate negative samples (non-salient frames) from positive samples (salient frames) of current video category, we propose a Non-saliency Suppression (NS) loss to impose a constraint other than the regular classification objective. During inference, the attention weights are used as saliency scores</p><formula xml:id="formula_9">{s v i } T i=1 .</formula><p>The details of the NS loss are described next. Non-saliency Suppression Loss. It is obvious that all videos contains both salient and non-salient frames for a specific video category. Therefore, it is natural that a holistic video representationx v can be formulated as a linear combination of the salient representationx sal v and the non-salient representation</p><formula xml:id="formula_10">x ns v [29], i.e.,x v =x sal v + ?x ns v .</formula><p>The non-salient representationx ns v can be obtained as follows. We first compute the complementary part of attention weights</p><formula xml:id="formula_11">? i = 1 T (1 ? ? i )</formula><p>, and then aggregate the feature sequence with ? i and produce the non-salient representationx</p><formula xml:id="formula_12">ns v = T i=1 ? ixi .</formula><p>In this way, a video can be regarded as a positive sample of both its ground truth category and non-salient category to different proportions, at the same time. Bothx sal v andx ns v will be fed into the classification fully-connected layer to get different predictions? sal v and y ns v . Then we defines labels for both? sal v and? ns v : </p><formula xml:id="formula_13">y ns v = [0, 0, ..., 0, 1] ? R C+1 , y sal v = [y v,1 , y v,2 , ...y v,C , 0] ? R C+1 ,</formula><formula xml:id="formula_14">L v = L cls + ?L ns ,<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Learning Objectives</head><p>The overall objective function of our NSNet is formulated as follows:</p><formula xml:id="formula_15">L = L v + L f ,<label>(4)</label></formula><p>where L v and L f denote the loss function of the VGM and the FSM, respectively. This objective not only drives model to conduct discriminative saliency measuring according to video semantics and frame discrepancy, but also facilitates information exchange between the video context and parts in shared feature encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We evaluate our method on four large-scale video recognition benchmarks, i.e., ActivityNet, FCVID, Mini-Kinetics and UCF101. ActivityNet <ref type="bibr" target="#b1">[2]</ref> contains 19994 videos of 200 categories of most popular actions in daily life. FCVID <ref type="bibr" target="#b16">[17]</ref> contains 91,223 videos collected from YouTube and divided into 239 classes covering most common events, objects, and scenes in our daily lives. Mini-Kinetics <ref type="bibr" target="#b26">[27]</ref> is a subset of Kinetics <ref type="bibr" target="#b17">[18]</ref> presented by <ref type="bibr" target="#b26">[27]</ref>, including 200 categories of videos of Kinetics, with 121k videos for training and 10k videos for validation. UCF101 <ref type="bibr" target="#b33">[34]</ref> has 101 classes of actions and 13K videos with short duration (7.2sec). Mean Average Precision (mAP) is used as the main evaluation metric for Activi-tyNet and FCVID, while Top-1 accuracy is used for Mini-Kinetics and UCf-101 following previous works. We also report the computational cost (in FLOPs) to evaluate the efficiency of the proposed method. FLOPs of our method are composed of following parts: P total = P rec ? K + P f em + P vgm + P f sm , where P rec , P f em , P vgm , P f sm represent the FLOPs of the recognizer, FEM, VGM and FSM, respectively. An example of FLOPs computation of our model with the setting in <ref type="table">Table 2</ref> is: 4.109?5 (ResNet-50 with 5 frames) + (0.320?16+0.315) (MobileNetv2 with 16 frames+transformer) + 0.004 + 0.002 = 25.99(G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Training. Following previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b55">56]</ref>, we mainly use MobileNetv2 <ref type="bibr" target="#b31">[32]</ref> as the lightweight feature extractor in our FEM. Different high-capacity networks trained on target datasets are used as recognizers at the same time: ResNet family <ref type="bibr" target="#b11">[12]</ref> and Swin-Transformer family <ref type="bibr" target="#b25">[26]</ref>, etc. For the transformer encoder in our FEM, 2 encoder layers with 8 heads and learnable positional embedding are used. The distance function ? in our FSM used for guiding saliency score is Euclidean Distance. The non-saliency suppression loss weight ? is set to 0.2. See Appendix A.1. Inference. We fuse the results of FSM and VGM to obtain the final saliency measurements. Score sum, max, mul and index union, intersect, join are considered for fusion. See Appendix A.2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>Comparison with Simple Baselines. As shown in <ref type="table">Table 1</ref>, we compare our approach with multiple hand-crafted sampling methods on ActivityNet and UCF101 with ResNet-101 and ResNet-50 as recognizers (without TSN training strategy <ref type="bibr" target="#b41">[42]</ref>), respectively. The simple baselines include Uniform, Random, Dense, and Top-K sampling. For uniform and random, we uniformly and randomly sample 10 frames from all frames, respectively, while for Top-K, we sample top 10 frames with highest predicted confidence scores (i.e., the maximum confidence among all categories), from all frames. For our method, we first uniformly sample an observation number (100 for ActivityNet and 50 for UCF-101) of frames as the observation frames from the input videos, and then use our method to sample 5 frames from the observation frames. Dense sampling makes use of all frames for recognition. We observe that our NSNet outperforms all simple baselines by a large margin on both two datasets. In ActivityNet, our method relatively outperforms the competitive but heavy Top-K baseline by 2.4% in terms of mAP with 11.7? less GFLOPs, which verifies the effectiveness of our sampler. In UCF-101, the videos are much shorter than those in Ac-tivityNet (7sec v.s. 119sec on average), which constructs a much more difficult setting for sampler. However, the top-1 accuracy of our NSNet still relatively exceeds that of the most competitive Dense baseline by 2.1% with much less GFLOPs, which demonstrates NSNet can improve the video classification performance on trimmed videos. Comparison with SOTA on ActivityNet. We make a comprehensive comparison with recent state-of-the-art efficient video recognition methods on the ActivityNet dataset in <ref type="table">Table 2</ref>-3, and <ref type="figure">Figure 3</ref>. As shown in <ref type="table">Table 2</ref>, we compare our NSNet with other state-of-the-art efficient approaches using ResNet-50 as the recognizer. NSNet consistently outperforms all existing methods including sampler-based and sampler-free approaches. When compared with AR-Net <ref type="bibr" target="#b26">[27]</ref>, an adaptive resolution method, the mAP of our method improve by 3% with much less computational cost (26.0G v.s. 33.5G). Our NSNet also outperforms AdaMML <ref type="bibr" target="#b29">[30]</ref>, an adaptive modality approach, by 2.9% in terms of mAP while having 3.6? less FLOPs. In addition, sampler-free approaches often have relatively low FLOPs, because they do not need sampling process. However, although our NSNet has extra computational on the sampling process, it can achieve higher accuracy than FrameExit <ref type="bibr" target="#b8">[9]</ref>(76.8% v.s. 76.1%), a competing sampler-free framework, with comparable FLOPs, which demonstrates our sampler greatly improve the discrimination power of the video representation. In <ref type="table" target="#tab_1">Table 3</ref>, we show the results of the SOTAs using ResNet-152 and more advanced backbones on the ActivityNet dataset. We can see that our NSNet outperforms the sampler-free methods P3D <ref type="bibr" target="#b30">[31]</ref>, RRA <ref type="bibr" target="#b62">[63]</ref> by at least 1.7% in terms of mAP. When compared with competing sampler-based methods (MARL <ref type="bibr" target="#b50">[51]</ref>, Listen-ToLook <ref type="bibr" target="#b7">[8]</ref>, SMART <ref type="bibr" target="#b9">[10]</ref>), our method still shows superiority over them. Besides, when using a more advanced transformer-based network Swin-Transformer <ref type="bibr" target="#b25">[26]</ref> as the recognizer in our NSNet, the mAP can be further promoted to 94.3%, which is the highest performance on ActivityNet to our best knowledge. <ref type="figure">Figure 3</ref> illustrates the GFLOPs-mAP curve on the ActivityNet dataset. On this curve, the observation number T is set to 50, while the number of sampled frames K is tuned up as FLOPs budget increases. Following previous works  <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b50">51]</ref>, we use ResNet-101 without TSN-style training as the recognizer. Our NSNet presents significant accuracy improvement with much lower GFLOPs than other methods. Comparison with SOTA on FCVID and Mini-Kinetics. We further evaluate the performance of our method on two large-scale video recognition benchmarks, i.e., FCVID and Mini-Kinetics, with ResNet-50 as the recognizer in <ref type="table" target="#tab_2">Table 4</ref>. We have a similar observation that our NSNet can achieve superior mAP with the much lower computational cost, which demonstrate the efficacy of nonsaliency suppression (NS) mechanism in both untrimmed video and trimmed video scenarios. Practical Efficiency. We present the comparison results on inference speed between our NSNet and two SotA methods, FrameExit <ref type="bibr" target="#b8">[9]</ref> and AdaFocus <ref type="bibr" target="#b43">[44]</ref>, in <ref type="table" target="#tab_3">Table 5</ref>. Latency and throughput with batch size of 1 and 32 are reported 1 . It can be observed that our method achieves significantly superior latency (42.0 ms)  and than two methods (2.4? than FrameExit <ref type="bibr" target="#b8">[9]</ref> and 4.3? than AdaFocus <ref type="bibr" target="#b43">[44]</ref>), which demonstrate the superiority of our parallel temporal sampling framework over existing methods on practical efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>To comprehensively evaluate our NSNet, we provide extensive ablation studies on ActivityNet in <ref type="table" target="#tab_4">Table 6</ref>. Accordingly, the effectiveness of each component in our framework is analyzed as follows. We use ResNet-101 without TSN style training as the recognizer, as the same as in <ref type="table">Table 1</ref> and <ref type="figure">Figure 3</ref> in Section 4.3.</p><p>Effectiveness of non-saliency suppression. We explore the effectiveness of non-saliency suppression (NS) mechanism for two modules. For VGM, "baseline" denotes the variant without L ns . For FSM, "baseline" denotes the variant replacing our NS frame label with video label. As shown in <ref type="table" target="#tab_4">Table 6a</ref>, for the VGM, simply adding a non-salient class (from C classes to C + 1 classes) without according supervisions can not elevate performance. In contrast, by applying NS mechanism, the mAP significantly improves by 0.4%. In the FSM, we observe that "baseline" present very low performance, similar to Uniform baseline(68.4 v.s. 68.6), which is because it imposes many label noises when assigning the label of video to irrelevant or low-quality frames. We also observe that simply adding the non-salient class cannot address the issue (68.6 v.s. 68.4). NS mechanism elevates the performance significantly with effective and reasonable supervisions (74.7 v.s. 68.4). More ablations in supervision signals of FSM are in <ref type="table" target="#tab_4">Table 6d</ref>. Ablations of transformer encoder in feature embedding module. Table 6c presents the results of different choice in FEM to passing message among the features from the input frames, including Long short-term memory networks (LSTM) <ref type="bibr" target="#b12">[13]</ref>, 1-D convolutional networks (1D Conv), multi-layer perceptron (MLP) <ref type="bibr" target="#b47">[48]</ref>, Transformer Encoder (Transformer) <ref type="bibr" target="#b40">[41]</ref>. Among all these choices, the transformer encoder achieves the highest performance. Different learning objectives of the FSM. In <ref type="table" target="#tab_4">Table 6d</ref> we compare various objectives of FSM mentioned in Section 1 and Section 2, including frame classification with video labels ("baseline", as the same one as in <ref type="table" target="#tab_4">Table 6a</ref>), ranking <ref type="bibr" target="#b19">[20]</ref>, and regression <ref type="bibr" target="#b9">[10]</ref> with guiding saliency scores. With guiding saliency scores as supervisions, "regression" and "ranking" model saliency sampling as saliency score regression and ranking tasks respectively. They can achieve higher performance than "baseline" (72.0% &amp; 72.2%), whereas they overlook the exploitation of class-specific information, which limits their performance. We modify "baseline" by transforming hard one-hot video label to soft one-hot label using the guiding saliency score, which is denoted as "baseline+". This modification improves the result significantly (73.7% v.s. 68.4%) by taking into account both the discrimination between salient frames and non-salient frames and the use of category-specific information. With the same setting of guiding saliency score, our NS mechanism based objective outperforms 'baseline+' in a large margin (74.7% v.s. 73.7%), which verifies the proposed supervisions offer more robust saliency for supplying negative samples. Different numbers of sampled frames. As shown in <ref type="table" target="#tab_4">Table 6b</ref>, we report the performance of different numbers of sampled frames for NSNet. We can see that the fusion of two modules always improves the performance by 0.8% and 1.0% when sampling 10 and 5 frames, respectively. It demonstrates the effectiveness of our fusion strategies, especially when fewer frames are sampled. Besides, when fewer frames are sampled, the performance of FSM degrades more slowly than that of VGM (0.8% v.s. 1.2%), which is because FSM can distinguish between salient frames and non-salient frames in finer granularity with the help of framelevel supervisions. <ref type="figure" target="#fig_5">Figure 4</ref> shows frames sampled by different methods. Our NSNet can sample more discriminative salient frames than uniform baseline and the variant without non-saliency suppression. For example, in the 4 th column, the 3 rd row of this column shows that "ours w/o NS" is mainly attracted by frames with scenes of a cook, which is not discriminative for frequently appearing in other cooking events. In contrast, 4 th row shows NSNet can sample more indicative frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we present the Non-saliency Suppression Network (NSNet) to measure the saliency of frames by leveraging both video-level and frame-level supervisions. In Frame Scrutinize module, we propose a pseudo label generation strategy to enable negative sample aware frame-level saliency learning. Meanwhile, in Video Glimpse module, an attention module constrained by dual classification objectives is presented to compensate high-level information. Experiments show that our NSNet outperforms the state-of-the-arts on accuracy-efficiency tradeoff. In the future, we plan to explore non-saliency suppression on both spatial and temporal dimensions to save more redundancy.</p><p>In this appendix, we provide more implementation details and experimental results of our proposed NSNet. Accordingly, we organize the appendix as follows.</p><p>-In Section A, we present more implementation details for training and inference of our method. -In Section B, we provide more ablation studies to further analyze the capability of our proposed NSNet. -In Section C, we present predicted saliency score distributions to qualitatively analyze the capability of proposed NSNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Training</head><p>Pre-processing. Following previous works, the frames fed to recognizer are rescaled with a shorter side of 256 and center cropped to 224?224 for all datasets. The resolution of frames fed into Feature Embedding module is 224 ? 224 for ActivityNet, FCVID and UCF101, and 112 ? 112 for Mini-Kinetics. Note that we only use the RGB frames of these datasets for experiments. Following <ref type="bibr" target="#b50">[51]</ref>, before adaptive sampling by samplers, T frames are uniformly pre-sampled from frame sequence. For those videos whose lengths are shorter than T , we repeat multiple times and splice them to T frames.</p><p>Model training details and hyper-parameters. For transformer encoder, the hidden dimensions of query, key and value is set to the ratio between the number of input feature channels and the number of heads. The hidden dimension of FFN is set to be equal to the input feature channel number. Dropout [?] is used to reduce over-fitting. In Video Glimpse module, dropout layers are placed before classification fully-connected layer with ratio of 0.9 and after temporal attention layer with 0.2, respectively. In transformer encoder, the dropout rate after the positional encoding layer is set to 0.2. Temporal random shift is adopted as data augmentation strategy. The model is trained using SGD optimizer with momentum of 0.9 and batch size of 64 for 120 epochs. The learning rate is set to starting at 10 ?2 , decaying by the factor of 0.1 at the 50 th and 75 th epoch.</p><p>Prototype generation. For a video x v , we first apply the recognizer for each frame and obtain the predictions {? i ? R C } T i=1 of all frames. Then we collect the correctly predicted frames set from each video</p><formula xml:id="formula_16">X g = {x i | i?[1,T ],argmax j? i,j =c },</formula><p>where c is the ground truth category. We further select the top ? percent frames with highest confidence on the c-th category? i,c from X g and average pool the frame features of them as the guiding video featurex g v . Then, for the c-th category, the prototype feature p c can be computed by average pooling the all the guiding video features belonging to the c-th category. We use ? = 30 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Inference</head><p>We describe the combination strategies in detail here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Score Combination.</head><p>We consider 3 types of fusion operations, which includes addition, multiplication and maximization. For addition, we fuse the saliency scores of two branches in convex combination ?s f i + (1 ? ?)s v i , where ? is a combination ratio parameter. For multiplication and maximization, we fuse the saliency scores of two branches in element-wise muliplication s f i * s v i and element-wise maximization max(s f i , s v i ), respectively.</p><p>Index Combination. We consider three strategies, which involves intersection, union and join. We firstly get frame index lists</p><formula xml:id="formula_17">{? f i } T i=1 and {? v i } T i=1 by sorting {s f i } T i=1 and {s v i } T i=1</formula><p>in descending order, respectively. In intersection, given a budget of K salient frames at most, we firstly take top K frames from index lists,</p><formula xml:id="formula_18">{? f i } K i=1 and {? v i } K i=1</formula><p>respectively and get the intersection of them</p><formula xml:id="formula_19">I(K) = {? f i } K i=1 ? {? f i } K i=1</formula><p>. When there exist coincident frames, we expand I with one element from either</p><formula xml:id="formula_20">{? f i } T i=K+1 or {? v i } T i=K+1 by turns for i ? steps, until |I(K + i ? )| = K.</formula><p>For union, following <ref type="bibr" target="#b19">[20]</ref>, we try to obtain a set of salient frames whose length is represented by ?|? f | + (1 ? ?)|? v |. We firstly get the union of top saliency frames from two lists</p><formula xml:id="formula_21">U (K) = {? f i } ?K * ?? i=1 ? {? v i } ?K * (1??)? i=1</formula><p>. We expand U (K) with one element from ? f at a time for i ? steps until |U (K + i ? )| = K. For join, we concatenate {s f i } T i=1 and {s v i } T i=1 to a list with a length of 2T , from which K non-overlap top saliency score frames are selected as final salient frames set.</p><p>We use ? = 0.6 for score addition and index union in the ablation studies of fusion strategies. Union fusion are used in all other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Ablation Studies</head><p>B.1 Different guiding saliency score.</p><p>In <ref type="table">Table 1</ref>, we compare our prototype based guiding saliency score with an alternative choice, where we use the classification response of the ground truth category produced by the recognizer to generate the NS pseudo labels, namely response-based guiding saliency score. It is shown that the prototype based score achieves better performance than response based one, which demonstrates that the prototype distance in feature space can offer more robust saliency cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Different fusion strategies of two modules.</head><p>In <ref type="table">Table 2</ref>, we show the impacts of different fusion strategies which are described in Section A.2. We can observe that various fusion strategies consistently improve the performance of single modules. The 'index union' fusion gets slightly higher performance than others thus we choose it in all our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Different lightweight Feature Extractor in FEM.</head><p>In <ref type="table" target="#tab_1">Table 3</ref> we compare various backbones for lightweight feature extractor in FEM. As expected, the lightweight backbone with better performance is complementary to our method. Comparing with the ShuffleNetv2 [?] and MobileNetv2 <ref type="bibr" target="#b31">[32]</ref> counterparts, our NSNet gets additional improvement on EfficientNet-b0 [?] with extra computation overhead. For fair comparisons with previous works, we use the MobileNetv2 as the lightweight feature extractor by default. <ref type="table" target="#tab_1">Table 3</ref>. Study on different backbones for lightweight extractor in FEM. FLOPs/f means FLOPs for each frame processed by the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>mAP In <ref type="table" target="#tab_2">Table 4</ref> we investigate the impacts of various backbones of the recognizer, where we also report the training strategy of recognizer, i.e., with TSN or without TSN. The model without TSN is trained by sampling one frame from each video. It is shown that our method is complementary to more advanced recognizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Analysis</head><p>To take a closer look to how NS mechanism benefits saliency measurement, we present temporal saliency distributions produced by variants of our approach on <ref type="table" target="#tab_2">Table 4</ref>. Study on different backbones for the recognizer. "Train" refers to training strategy, viz., with TSN style training or without TSN style Training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Train We show the average value over all samples for a category for 3 variants, viz., guiding saliency score (Guid.), our approach without non-saliency suppression (Ours w/o NS) and our approach(Ours). Our approach can generate saliency measurements close to Guid. for all 3 categories. However, Ours w/o NS only produce a relatively flat line on difficult categories like rock climbing and making an omelette, which shows it cannot handle saliency measurement on difficult categories without NS mechansim. Predicted saliency distributions are smoothed by Exponential Moving Average with weight of 0.8 for a better sense of trend.</p><p>the validation set of ActivityNet in <ref type="figure" target="#fig_7">Figure 5</ref>, which is computed by averaging the temporal saliency distribution of all samples within a given class. We adopt guiding saliency score (Guid.) as an alternative of saliency "ground truth", for it exploits labels of validation set and represents a upper bound of any sampler, achieving mAP of 96.3 (v.s. 75.3 achieved by NSNet). We can see that the saliency distribution of NSNet is more close to that of Guid. and achieving much higher AP than that of NSNet w/o NS on all three categories. As the discrimination difficulty increases, AP decreases dramatically from left sub-figures to right ones in <ref type="figure" target="#fig_7">Figure 5</ref>. In the easiest category hopscotch, both NSNet and NSNet w/o NS show similar saliency trends to Guid. to varying degrees. However, in much more difficult categories with low AP, like rocking climbing and making a omelette, the saliency scores measured by NSNet w/o NS tend to generate temporal uniform distributions and NSNet still shows highly similar trends to Guid., which demonstrates that proposed NS based supervisions can enhance robustness of saliency measurements in many scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>A conceptual comparison between our proposed non-saliency suppression based sampler and existing approaches. (a) Feature space learned by sampler networks optimized by vanilla classification objectives with video labels. (b) Feature space learned by proposed NSNet. A video of Hopscotch is used for illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fusion</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>An overview of architecture of NSNet. (a) shows the whole architecture. (b) shows the Frame Scrutinize Module (FSM), which estimates the saliency of each frame by the prediction confidence in frame-level classification. (c) shows the proposed Non-saliency Suppression (NS) frame-level pseudo label generation strategy based on the distance between each frame and video category prototypes. (d) shows the Video Glimpse module (VGM), which measures the saliency of each frame using temporal attentions in video-level classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>where y v is the original video label. The cross-entropy loss between? sal v and y sal v is the original classification loss L cls , and the one between? ns v and y ns v is the NS loss L ns . Consequently, the objective function of this module is defined as follows, where ? is the weight of L ns .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of selected frames with different variants of our approach. 1 st row: Uniform, 2 nd row: Our approach without NS mechanism (Ours w/o NS), 3 rd row: Our approach (Ours). Intuitively salient frames are are outlined in aqua while non-salient ones are outlined in red. Please zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Measured saliency distribution by different variants of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison with several hand-crafted sampling strategies. ResNet-101 and ResNet-50 are adopted as the recognizers for ActivityNet and UCF-101, respectively. Comparisons with SOTA efficient video recognition methods with ResNet50 as the main recognizer on the ActivityNet dataset. The backbones used for sampler and recognizer are reported. MBv2 denotes MobileNetv2.</figDesc><table><row><cell></cell><cell cols="2">ActivityNet</cell><cell cols="2">UCF101</cell></row><row><cell cols="5">mAP(%) FLOPs Top-1(%) FLOPs</cell></row><row><cell>Uniform</cell><cell>68.6</cell><cell>195.8G</cell><cell>75.9</cell><cell>61.7G</cell></row><row><cell>Random</cell><cell>68.1</cell><cell>195.8G</cell><cell>75.7</cell><cell>61.7G</cell></row><row><cell>Dense</cell><cell>69.0</cell><cell>930.8G</cell><cell>76.1</cell><cell>753.4G</cell></row><row><cell>Top-K</cell><cell>72.5</cell><cell>930.8G</cell><cell>74.5</cell><cell>753.4G</cell></row><row><cell>Ours</cell><cell>74.9</cell><cell>73.2G</cell><cell>77.6</cell><cell>37.6G</cell></row><row><cell>Method</cell><cell></cell><cell>Backbone</cell><cell>mAP(%)</cell><cell>FLOPs</cell></row><row><cell>SCSampler[20]</cell><cell></cell><cell>MBv2+Res50</cell><cell>72.9</cell><cell>42.0G</cell></row><row><cell>AR-Net [27]</cell><cell></cell><cell>MBv2+ResNets</cell><cell>73.8</cell><cell>33.5G</cell></row><row><cell>AdaMML [30]</cell><cell></cell><cell>MBv2+Res50</cell><cell>73.9</cell><cell>94.0G</cell></row><row><cell>VideoIQ [37]</cell><cell></cell><cell>MBv2+Res50</cell><cell>74.8</cell><cell>28.1G</cell></row><row><cell>AdaFocus [44]</cell><cell></cell><cell>MBv2+Res50</cell><cell>75.0</cell><cell>26.6G</cell></row><row><cell cols="2">Dynamic-STE [19]</cell><cell>Res18+Res50</cell><cell>75.9</cell><cell>30.5G</cell></row><row><cell>FrameExit [9]</cell><cell></cell><cell>ResNet-50</cell><cell>76.1</cell><cell>26.1G</cell></row><row><cell>Ours</cell><cell></cell><cell>MBv2+Res50</cell><cell>76.8</cell><cell>26.0G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with SOTA video recognition methods with ResNet152 and more advanced networks as the recognizers on the ActivityNet dataset.</figDesc><table><row><cell>Method</cell><cell>Recognizer</cell><cell></cell><cell cols="6">Pretrain Top-1(%) mAP(%)</cell><cell></cell></row><row><cell cols="2">ResNet-152 w/ ImageNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P3D [31]</cell><cell cols="6">ResNet-152 ImageNet 75.1</cell><cell></cell><cell>78.9</cell><cell></cell></row><row><cell>RRA [63]</cell><cell cols="6">ResNet-152 ImageNet 78.8</cell><cell></cell><cell>83.4</cell><cell></cell></row><row><cell>MARL [51]</cell><cell cols="6">ResNet-152 ImageNet 79.8</cell><cell></cell><cell>83.8</cell><cell></cell></row><row><cell cols="7">ListenToLook [8] ResNet-152 ImageNet 80.3</cell><cell></cell><cell>84.2</cell><cell></cell></row><row><cell>Ours</cell><cell cols="6">ResNet-152 ImageNet 80.7</cell><cell></cell><cell>85.1</cell><cell></cell></row><row><cell cols="2">ResNet-152 w/ Kinetics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SMART [10]</cell><cell cols="2">ResNet-152</cell><cell cols="3">Kinetics</cell><cell>-</cell><cell></cell><cell>84.4</cell><cell></cell></row><row><cell>Ours</cell><cell cols="2">ResNet-152</cell><cell cols="3">Kinetics</cell><cell>84.5</cell><cell></cell><cell>88.7</cell><cell></cell></row><row><cell cols="5">More Advanced Networks w/ Kinetics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DSN [62]</cell><cell cols="5">R(2+1)D-34 Kinetics</cell><cell>82.6</cell><cell></cell><cell>87.8</cell><cell></cell></row><row><cell>Ada3D [21]</cell><cell cols="5">SlowOnly-50 Kinetics</cell><cell>-</cell><cell></cell><cell>84.0</cell><cell></cell></row><row><cell cols="6">ListenToLook [8] R(2+1)D-152 Kinetics</cell><cell>-</cell><cell></cell><cell>89.9</cell><cell></cell></row><row><cell cols="6">MARL [51] SEResNeXt-152 Kinetics</cell><cell>85.7</cell><cell></cell><cell>90.1</cell><cell></cell></row><row><cell>Ours</cell><cell>Swin-B</cell><cell></cell><cell cols="3">Kinetics</cell><cell>86.7</cell><cell></cell><cell>91.6</cell><cell></cell></row><row><cell>Ours</cell><cell>Swin-L</cell><cell></cell><cell cols="3">Kinetics</cell><cell>90.2</cell><cell></cell><cell>94.3</cell><cell></cell></row><row><cell cols="2">Fig. 3. Comparison with state-of-the-</cell><cell></cell><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">art sampling methods on ActivityNet dataset. Our proposed NSNet achieves better mAP with much fewer GLOPs (per video) than other methods. It is worth noting that we compare these methods with the same recognizer ResNet-101, under different computa-tion budgets. The results are quoted</cell><cell>Mean Average Precision(in %)</cell><cell>63 68 73</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Uniform FrameGlimpse AdaFrame5 MARL Ours</cell><cell cols="2">FastForward AdaFrame10 ListenToLook LiteEval</cell></row><row><cell>from the published works [8,27].</cell><cell></cell><cell></cell><cell>58</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GFLOPs</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison with previous methods on FCVID and Mini-Kinetics. Our NSNet consistently outperforms state-of-the-art in terms of accuracy and efficiency using ResNet-50 as the recognizer.</figDesc><table><row><cell>Methods</cell><cell cols="2">FCVID</cell><cell cols="2">Mini-Kinetics</cell></row><row><cell></cell><cell>mAP(%)</cell><cell>FLOPs</cell><cell>Top-1(%)</cell><cell>FLOPs</cell></row><row><cell>LiteEval [55]</cell><cell>80.0</cell><cell>94.3G</cell><cell>61.0</cell><cell>99.0G</cell></row><row><cell>AdaFrame [56]</cell><cell>80.2</cell><cell>75.1G</cell><cell>-</cell><cell>-</cell></row><row><cell>SCSampler [20]</cell><cell>81.0</cell><cell>42.0G</cell><cell>70.8</cell><cell>42.0G</cell></row><row><cell>AR-Net [27]</cell><cell>81.3</cell><cell>35.1G</cell><cell>71.7</cell><cell>32.0G</cell></row><row><cell>AdaFuse [28]</cell><cell>81.6</cell><cell>45.0G</cell><cell>72.3</cell><cell>23.0G</cell></row><row><cell>SMART [10]</cell><cell>82.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VideoIQ [37]</cell><cell>82.7</cell><cell>27.0G</cell><cell>72.3</cell><cell>20.4G</cell></row><row><cell>Dynamic-STE [19]</cell><cell>-</cell><cell>-</cell><cell>72.7</cell><cell>18.3G</cell></row><row><cell>FrameExit [9]</cell><cell>-</cell><cell>-</cell><cell>72.8</cell><cell>19.7G</cell></row><row><cell>AdaFocus [44]</cell><cell>83.4</cell><cell>26.6G</cell><cell>72.9</cell><cell>38.6G</cell></row><row><cell>Ours</cell><cell>83.9</cell><cell>26.0G</cell><cell>73.6</cell><cell>18.1G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparison of practical efficiency between SotA methods.</figDesc><table><row><cell>Method</cell><cell cols="2">mAP(%) GFLOPs</cell><cell>Latency (bs=1)</cell><cell>Throughput (bs=32)</cell></row><row><cell>AdaFocus [44]</cell><cell>75.0</cell><cell>26.6</cell><cell>181.8ms</cell><cell>73.8 vid/s</cell></row><row><cell>FrameExit [9]</cell><cell>76.1</cell><cell>26.1</cell><cell>102.0ms</cell><cell>-</cell></row><row><cell>Ours</cell><cell>76.8</cell><cell>26.1</cell><cell cols="2">42.0ms 132.5 vid/s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation studies on ActivityNet with mAP (%) as the evaluation metric. Unless otherwise specified, MobileNetv2 and ResNet-101 are used as the backbone for observation network and recognizer respectively.</figDesc><table><row><cell cols="2">(a) Evaluation of the effectiveness of</cell><cell cols="3">(b) Performance of different number of</cell></row><row><cell>NS mechanism.</cell><cell></cell><cell cols="2">sampled frames.</cell></row><row><cell>Method</cell><cell>VGM FSM</cell><cell cols="3">#F VGM FSM NSNet</cell></row><row><cell cols="2">baseline + non-salient class 73.4 68.6 73.4 68.4</cell><cell>5</cell><cell cols="2">72.6 73.9 74.9</cell></row><row><cell>+ NS</cell><cell>73.8 74.7</cell><cell cols="2">10 73.8 74.7</cell><cell>75.5</cell></row><row><cell cols="2">(c) Ablation of transformer encoder in</cell><cell cols="3">(d) Results of FS module with different</cell></row><row><cell cols="2">feature embedding module.</cell><cell cols="2">learning objectives.</cell></row><row><cell>Network</cell><cell>mAP(%)</cell><cell cols="3">FS Objective mAP(%)</cell></row><row><cell>1D Conv LSTM MLP</cell><cell>73.6 74.0 74.3</cell><cell cols="2">Baseline Regression Ranking Baseline+</cell><cell>68.4 72.0 72.2 73.7</cell></row><row><cell>Transformer</cell><cell>75.5</cell><cell></cell><cell>Ours</cell><cell>74.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Performance of different guiding saliency score in FS module. Comparison of various fusion strategies.</figDesc><table><row><cell cols="2">Guiding Saliency Score mAP(%)</cell></row><row><cell>Response-based</cell><cell>74.1</cell></row><row><cell>Prototype-based</cell><cell>74.7</cell></row><row><cell>Max Mul</cell><cell>Add</cell></row><row><cell cols="2">Score 75.1 75.2 75.3</cell></row><row><cell cols="2">Join Inter Union</cell></row><row><cell cols="2">Index 75.1 74.9 75.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The latency and throughput results of two SotA methods are obtained by running their official code<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44]</ref> on the same hardware (a NVIDIA 3090 GPU with a Intel Xeon E5-2650 v3 @ 2.30GHz CPU) as ours.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Action keypoint network for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.06304</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Watching a small portion could be as good as watching all: Towards efficient video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mamico: Macro-to-micro semantic correspondence for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Listen to look: Action recognition by previewing audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10457" to="10467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Frameexit: Conditional early exiting for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15608" to="15618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">SMART frame selection for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/16235" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1451" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ascnet: Self-supervised video representation learning with appearance-speed consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8096" to="8105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward efficient action recognition: Principal backpropagation for training two-stream networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1773" to="1782" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Step-wise hierarchical alignment network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/106</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2021/106" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</title>
		<editor>ijcai.org</editor>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence<address><addrLine>Virtual Event / Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-08-27" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="765" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting feature and class relationships in video categorization with regularized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2670560</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2670560" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="352" to="364" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient action recognition via dynamic knowledge propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13719" to="13728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">2d or not 2d? adaptive 3d convolution selection for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6155" to="6164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ocsampler: Compressing videos to one clip with single-step sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="13894" to="13903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ar-net: Adaptive frame resolution for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="86" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05775</idno>
		<title level="m">Adafuse: Adaptive temporal fusion network for efficient action recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5502" to="5511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adamml: Adaptive multi-modal learning for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05165</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving action localization by progressive cross-stream cooperation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stvgbert: A visual-linguistic transformer based framework for spatio-temporal video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="1533" to="1542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic network quantization for efficient video inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7375" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Symbiotic attention for egocentric action recognition with object-centric alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adaptive focus for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03245</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Glance and focus: a dynamic approach to reducing spatial redundancy in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2432" to="2444" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adafocus v2: End-to-end training of spatial dynamic networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="20062" to="20072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Applications of advances in nonlinear sensitivity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">System modeling and optimization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1982" />
			<biblScope unit="page" from="762" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weaklysupervised spatio-temporal anomaly detection in surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mvfnet: Multi-view fusion network for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2943" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning based frame sampling for effective untrimmed video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6222" to="6231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic inference: A new approach toward efficient video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="676" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<title level="m">Transferring textual knowledge for visual recognition. arXiv e-prints pp</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">2207</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dsanet: Dynamic segment aggregation network for video-level representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Liteeval: A coarse-to-fine framework for resource efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01601</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adaframe: Adaptive frame selection for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1278" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Temporal saliency query network for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporal action proposal generation with background constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3054" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Discriminability distillation in group representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dynamic sampling networks for efficient action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7970" to="7983" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fine-grained video categorization with redundancy reduction attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="136" to="152" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
