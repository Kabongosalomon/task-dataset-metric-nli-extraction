<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlun</forename><surname>Zheng</surname></persName>
							<email>tlzheng21@m.fudan.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhineng</forename><surname>Chen</surname></persName>
							<email>zhinchen@fudan.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shancheng</forename><surname>Fang</surname></persName>
							<email>fangsc@ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
							<email>htxie@ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Com-puter Science and Shanghai Collaborative Innovation Center of Intelligent Visual Computing</orgName>
								<orgName type="institution">Fudan Universtiy</orgName>
								<address>
									<addrLine>Shang-hai 200438</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">with School of Information Sci-ence and Technology, University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scene Text Recognition</term>
					<term>Attention Mechanism</term>
					<term>Positional Embedding</term>
					<term>Character Distance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Transformer-based encoder-decoder framework is becoming popular in scene text recognition, largely because it naturally integrates recognition clues from both visual and semantic domains. However, recent studies show that the two kinds of clues are not always well registered and therefore, feature and character might be misaligned in difficult text (e.g., with rare shapes). As a result, constraints such as character position are introduced to alleviate this problem. Despite certain success, a content-free positional embedding hardly associates stably with meaningful local image regions. In this paper, we propose a novel module called Multi-Domain Character Distance Perception (MDCDP) to establish a visual and semantic related positional encoding. MDCDP uses positional embedding to query both visual and semantic features following the attention mechanism. The two kinds of constrained features are then fused to produce a reinforced feature, generating a content-aware embedding that well perceives spacing variations and semantic affinities among characters, i.e., multi-domain character distance. We develop a novel network named CDistNet that stacks multiple MDCDPs to guide a gradually precise distance modeling. Thus, the feature-character alignment is well built even various recognition difficulties presented. We create two series of augmented datasets with increasing recognition difficulties and apply CDis-tNet to both them and six public benchmarks. The experiments demonstrate that CDistNet outperforms recent popular methods by large margins in challenging recognition scenarios. It also achieves state-of-the-art accuracy on standard benchmarks. In addition, the visualization shows that CDistNet achieves proper information utilization in both visual and semantic domains. Our code is given in https://github.com/simplify23/ CDistNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: Semantic Attention <ref type="bibr" target="#b36">Sheng et al., 2019;</ref><ref type="bibr" target="#b38">Shi et al., 2018;</ref><ref type="bibr" target="#b43">Wang et al., 2019a)</ref>: the semantic feature is employed as the query for attention modeling. It easily causes attention drift. (the first column in (b), line 3,4); Position Attention <ref type="bibr" target="#b46">Wang et al., 2021;</ref><ref type="bibr" target="#b50">Yu et al., 2020;</ref><ref type="bibr" target="#b51">Yue et al., 2020)</ref>: using a fixed positional embedding as the query, it may wrongly attend multiple characters in one time step. (the second column in (b), line 1,2,3); CDistNet: the positional embedding also interacts with semantic feature, resulting in proper attention localization (the third column in (b)). Dotted line and box are optional. The visual branch is omitted in (a) for simplicity.</p><p>in the visual space, by which a character-level recognition decision is derived. The pipeline can be summarized as the semantic attention scheme <ref type="bibr" target="#b0">(Baek et al., 2019;</ref><ref type="bibr" target="#b33">Qiao et al., 2020;</ref><ref type="bibr" target="#b36">Sheng et al., 2019)</ref>, as shown in the top part of <ref type="figure">Fig.1(a)</ref>. It has advantages such as providing a unified way of integrating recognition clues from both visual and semantic domains, which is then used to align the character to be recognized in the text image. However, recent studies <ref type="bibr" target="#b51">Yue et al., 2020)</ref> show the two kinds of clues depend on each other. It is explained as when one of the two clues is weak, the other could not stably find its counterpart. Therefore, feature and character are easily mismatched in difficult text such as with rare shapes or perspective text. Moreover, it is also observed in <ref type="bibr" target="#b51">(Yue et al., 2020)</ref> that the mismatch is more prevalent in long text. Since the semantic feature is gradually reinforced and dominates the recognition with the accumulation of decoded characters, features between neighbor time steps become similar, easily causing attention drift (see the first column of <ref type="figure">Fig.1(b)</ref>, line 3,4) <ref type="bibr" target="#b5">(Cheng et al., 2017;</ref><ref type="bibr" target="#b41">Wan et al., 2020;</ref><ref type="bibr" target="#b44">Wang et al., 2019b)</ref> and degraded accuracy.</p><p>In view of the problem above, several studies <ref type="bibr" target="#b41">Wan et al., 2020;</ref><ref type="bibr" target="#b50">Yu et al., 2020;</ref><ref type="bibr" target="#b51">Yue et al., 2020)</ref> decouple position and semantic features by employing a single positional branch for additional atten-tion catching (see the middle part of <ref type="figure">Fig.1(a)</ref>). It generates the so-called position attention scheme. These methods alleviate the mismatch somewhat and get improved accuracy. Nevertheless, their positional embedding is content-free, where a fixed embedding is employed to associate with visual features of different text images. It is more like a placeholder constraint rather than content-aware receptor, which can't represent the diverse character patterns appearing in a fixed position and generates sub-optimal representation. We observe that it may wrongly attend multiple characters in one time step, as shown in <ref type="figure">Fig.1(b)</ref> (the second column, line 1,2,3). Although the later fusion of it with the semantic branch can suppress the drawback to some extent <ref type="bibr" target="#b51">(Yue et al., 2020)</ref>, the position utility is underestimated by this placeholder-like usage.</p><p>More specifically, we argue that more robust featurecharacter alignments could be reached by modeling feature interactions among visual, semantic and position spaces, which is barely considered in existing studies. Besides being aware of the character spatial position, the position can also be utilized to delineate the character semantic affinity. Moreover, combining the clues is likely to generate a content-aware embedding that is aware of both spacing variations and semantic affinities among characters. It is vividly understood as describing the character distance in both visual and semantic domains, i.e., multi-domain character distance. Therefore, such a joint representation is in favor of perceiving accurate character positions especially in challenging recognition scenarios.</p><p>In this paper, we develop a novel module called Multi-Domain Character Distance Perception (MDCDP) to overcome the aforementioned issues. Similar to existing studies <ref type="bibr" target="#b40">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b51">Yue et al., 2020)</ref>, MD-CDP is initialized with a fixed positional embedding. But differently, the embedding is used to query both visual and semantic features following the cross-attention mechanism. Therefore, it does not only impose position enhancement on the visual domain, but also describes the character dependence in the semantic domain. By further merging features from both domains, it generates a new content-aware embedding that well perceives spatial and semantic distance among characters (see the bottom part of <ref type="figure">Fig.1(a)</ref>). The embedding is then used as the query vector of the next MDCDP, to obtain a more attention-focused feature-character alignment. Following this idea, we propose a novel architecture named CDistNet that stacks MDCDP several times to guide a gradually precise character distance modeling. As seen in the third column of <ref type="figure">Fig.1(b)</ref>, CDistNet accurately localizes the characters. It ensures that, at each time step, the correct feature is utilized for character decoding, therefore benefiting the recognition especially complex character spatial variations presented.</p><p>To evaluate CDistNet, we first create two series of datasets by augmenting the ICDAR2013 benchmark <ref type="bibr" target="#b15">(Karatzas et al., 2013)</ref> with horizontal and curved stretching of varying intensities. The generated datasets are termed HA-IC13 (horizontal) and CA-IC13 (curved) series, which have increasing recognition difficulties as the deformation intensity increases. Based on this, extensive experiments are conducted on both the augmented datasets and public benchmarks. In the former, CDis-tNet attains greater advantages as the deformation intensity rises when compared to recent popular methods, showing its superiority in handling difficult text. While on six popular regular and irregular text benchmarks, CDistNet achieves state-of-the-art recognition accuracy among the competitors. The results demonstrate the effectiveness of establishing a feature representation that is aware of the multi-domain character distance. We also visualize the visual attention map and the semantic affinity matrix during decoding. Generally, they show that not only a proper visual attention localization is obtained, but also a more complete character-involved semantic inference is observed. Both explain the accuracy gain.</p><p>Contribution of this paper is threefold. First, we propose MDCDP that employs position feature to query both visual and semantic features, the first attempt that establishes the cross-attention between position and semantic spaces. It enables a content-aware embedding that jointly takes into account visual, semantic and position feature. Second, we develop CDistNet, a novel scene text recognizer. It stacks multiple MD-CDPs to well perceive the multi-domain character distance and thus builds robust feature-character alignments, benefiting the recognition especially for difficult text. Third, we carry out extensive experiments and compare CDistNet with existing leading methods. It not only achieves state-of-the-art recognition accuracy, but also shows obvious advantages in challenging recognition scenarios. In addition, the visualization implies that CDistNet achieves proper information utilization in both visual and semantic domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Scene text recognition is a long-standing computer vision task intensively studied over decades. Comprehensive surveys can be found in <ref type="bibr" target="#b4">(Chen et al., 2021;</ref><ref type="bibr" target="#b24">Long et al., 2021;</ref><ref type="bibr" target="#b49">Ye and Doermann, 2014)</ref>. Recently, research efforts mainly lie in irregular text while sequencebased methods become popular <ref type="bibr" target="#b0">(Baek et al., 2019;</ref><ref type="bibr" target="#b2">Bhunia et al., 2021;</ref><ref type="bibr" target="#b6">Cheng et al., 2018;</ref><ref type="bibr" target="#b14">Jaderberg et al., 2016;</ref><ref type="bibr" target="#b27">Luo et al., 2021;</ref><ref type="bibr" target="#b28">Lyu et al., 2018;</ref><ref type="bibr" target="#b31">Nguyen et al., 2021;</ref><ref type="bibr" target="#b35">Rodriguez-Serrano et al., 2015)</ref>, as different recognition clues (e.g., visual, semantic) can be jointly modeled to get the prediction. Based on how these clues are utilized, we can broadly categorize them into semanticfree, semantic-attention, and position-attention methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic-free Methods</head><p>Sequence-based methods advance traditional character segmentation-based methods <ref type="bibr" target="#b1">(Bai et al., 2014;</ref><ref type="bibr" target="#b14">Jaderberg et al., 2016)</ref> in avoiding the difficult character segmentation problem, while allowing relationships between local image regions as well as semantic information to be mined. Earlier methods in this branch are semantic-free methods. They implement the recognition by utilizing image visual features mainly, while the semantic relationship among characters is not explicitly modeled. For example, <ref type="bibr" target="#b37">Shi et al. (Shi et al., 2017)</ref> proposed the CTC-based method, where the visual feature extracted by CNN was reshaped as a feature sequence and then modeled by RNN and CTC loss. Following this pipeline, several methods were developed with improved accuracy by developing a deep-text recurrent network <ref type="bibr" target="#b11">(He et al., 2016)</ref>, ensembling multiple RNNs <ref type="bibr" target="#b39">(Su and Lu, 2017)</ref>, devising a graph convolutional network guided CTC decoding <ref type="bibr" target="#b12">(Hu et al., 2020)</ref>, etc. Rather than decoding by RNN, segmentation-based methods <ref type="bibr" target="#b21">(Li et al., 2017;</ref><ref type="bibr" target="#b47">Xing et al., 2019)</ref> regarded the recognition as a standard or modified segmentation problem where each character is a target category. However, they generally required character level annotations which were not always readily available. While they are also sensitive to segmentation noisy. In sum, the performance of semantic-free methods was limited, partly due to not directly modeling other recognition clues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic-attention Methods</head><p>Semantic-attention methods <ref type="bibr" target="#b25">Luo et al., 2019;</ref><ref type="bibr" target="#b36">Sheng et al., 2019;</ref><ref type="bibr" target="#b38">Shi et al., 2018)</ref> utilize the semantic clue to reinforce the visual feature and generally using the attention-based encoder-decoder frame work. <ref type="bibr" target="#b18">Lee et al. (Lee and Osindero, 2016)</ref> first introduced the attention mechanism into scene text recognition. It employed both the 1D image feature sequence and the embedding of character sequence, by which the semantic clue was leveraged. Later, it was extended by designing a more natural 2D image feature , adding dedicated attention enhancement modules <ref type="bibr" target="#b5">(Cheng et al., 2017)</ref>, etc. By substituting RNN with Transformer <ref type="bibr" target="#b40">(Vaswani et al., 2017</ref>), a powerful model in capturing global dependence in vector sequence, many variants <ref type="bibr" target="#b36">Sheng et al., 2019;</ref><ref type="bibr" target="#b43">Wang et al., 2019a)</ref> were developed with improved accuracy. Note that the position encoding was commonly applied to Transformer-based models whose attention mechanism was non-local. For example, sinusoidal positional embedding was employed to record the character position in <ref type="bibr" target="#b36">(Sheng et al., 2019)</ref>. The embedding is then appended to the semantic feature vector as an auxiliary clue. Later, learnable embedding was developed to adaptively utilize the positional clue in <ref type="bibr" target="#b7">(Devlin et al., 2019;</ref><ref type="bibr" target="#b17">Lan et al., 2019)</ref>. Despite great processes made, it was observed that there was misalignment between feature and character (i.e., local image region) especially in long text, i.e., attention drift <ref type="bibr" target="#b5">(Cheng et al., 2017;</ref><ref type="bibr" target="#b44">Wang et al., 2019b;</ref><ref type="bibr" target="#b51">Yue et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Position-attention Methods</head><p>Position-attention methods emphasize developing a dedicated character position branch to ease the recognition, which has been taken into account by several recent methods. To suppress attention drift, TextScanner <ref type="bibr" target="#b41">(Wan et al., 2020)</ref> proposed a position order seg-mentation map to ensure that characters were read in the right order and separated properly. RobustScanner <ref type="bibr" target="#b51">(Yue et al., 2020)</ref> proposed a dedicated position enhancement branch along with a dynamic fusion mechanism, then the characters were decoded sequentially and achieved impressive results. Meanwhile, several parallel decoding efforts were conducted by introducing semantic loss of different forms <ref type="bibr" target="#b33">Qiao et al., 2020;</ref><ref type="bibr" target="#b46">Wang et al., 2021)</ref>, where a placeholder sequence was initialized and learned to describe the corresponding characters. Our work falls into the category of position-attention-based sequential decoding. However, different with <ref type="bibr" target="#b51">(Yue et al., 2020)</ref> that the positional embedding is content-free while only queries the visual feature, we inject the positional embedding to both visual and semantic features, thus characterizing character distance in both domains. They are then further fused to establish an attention-focused featurecharacter alignment. Moreover, an iterative reinforcement structure is developed for a more precise distance modeling. Our work thoroughly integrates the recognition clues among visual, semantic and position spaces. Therefore, it makes the recognition easier, especially for difficult text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The proposed CDistNet is illustrated in <ref type="figure" target="#fig_0">Fig.2</ref>. It is an end-to-end trainable network that falls into the Transformer-based encoder-decoder framework. Specifically, the encoder consists of three branches, respectively for encoding visual, position and semantic information. On the decoder side, the three kinds of information are fused by a dedicated designed MDCDP module, in which the positional branch is first leveraged to query both visual and semantic features. Then they are fused to generate a reinforced embedding, which records the distance between characters in both visual and semantic domains. It is used as the positional embedding of the next MDCDP to guide a more accurate distance modeling. The MDCDP is stacked several times in CDis-tNet to achieve a precise feature-character alignment. At last, characters are decoded sequentially based on the output of the last MDCDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>Visual Branch. The visual branch utilizes Thin-platesplines (TPS) as a preprocess to rectify the text image <ref type="bibr" target="#b0">(Baek et al., 2019;</ref><ref type="bibr" target="#b38">Shi et al., 2018)</ref>. Then, similar to <ref type="bibr" target="#b50">Yu et al., 2020)</ref>, ResNet-50 and Transformer unit are employed as the backbone, which captures both local and global feature dependencies. The same ResNet-50 as in <ref type="bibr" target="#b38">(Shi et al., 2018;</ref><ref type="bibr" target="#b45">Wang et al., 2020)</ref> is used. Besides, the Transformer unit is a threelayer self-attention encoder with 1024 hidden units per layer. The process can be described using the following formula.</p><formula xml:id="formula_0">F vis = ? (R(T(I))) ? R N?P?E<label>(1)</label></formula><p>where I ? R W ?H denotes the input text image, T is the TPS block. R denotes ResNet-50 and ? is the Transformer unit. N and E are batchsize and the channels of visual feature. P = W H 64 is the length of reshaped visual feature. Both the width and height are shrank to 1/8 of the original size. Semantic Branch. Similar to <ref type="bibr" target="#b36">(Sheng et al., 2019;</ref><ref type="bibr" target="#b51">Yue et al., 2020)</ref>, the semantic branch encodes the character labels during training or already decoded characters during inference. In training, the character labels are represented as a sequence of word embedding F sem ? R N ?T ?E , where T is the number of characters and the feature is decoded in parallel for training acceleration. While in inference, since the character number is unknown in advance, a start token F 0 sem ? R N ?1?E is defined as the semantic embedding at time step 0. It is used to decode the first character and generates F 1 sem ? R N ?2?E , where embedding of the just decoded character is appended. Then F 1 sem is applied to decode the next character. Following this scheme, F t sem ? R N ?(t+1)?E is obtained when decoding the tth character. The whole process is terminated when the end token is decoded. Therefore, the semantic feature is updated at every time step. Positional Branch. Similar to the semantic branch, the positional branch encodes feature in one-time during training while the feature is updated step-by-step during inference. To encode the character positions in training, we first generate a sequence of vectors each with a fixed constant in its position-indexed dimension while 0 otherwise. Then, the same sinusoidal positional embedding as in <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> is applied, followed by two MLP layers to get the embedding F pos ? R N ?T ?E . While in inference, the positional embedding is initialized with a placeholder F 1 sem ? R N ?1?E as described in training. It then grows with the accumulation of decoded characters, where F t pos ? R N ?t?E defines the embedding of the t-th time steps. Note that the positional embedding has no start or end token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MDCDP</head><p>We then explain how the MDCDP module is formulated on the decoder side. MDCDP consists of three parts, a self-attention enhancement (SAE) for position feature reinforcement, a cross-branch interaction (CBI) that utilizes position feature to query both the visual and semantic branches, obtaining position-enhanced representations, and a dynamic shared fusion (DSF) to get a visual, semantic and positional joint embedding. As a result, the three features are fused and a content-aware representation is obtained. An illustrative figure of MD- CDP is shown in <ref type="figure" target="#fig_1">Fig.3</ref>, where details of SAE, CBI and DSF blocks are depicted. From the figure we can get an intuitive understanding of the MDCDP, e.g., how Query (Q), Key (K) and Value (V) are applied. Self-Attention Enhancement (SAE). At time step t, with the positional embedding F t pos from the encoder, SAE is applied at first to reinforce the embedding using one multi-head attention block <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> but half its dimension to reduce the computational cost. In addition, an upper triangular mask M t pos is applied to the query vector to prevent from "seeing itself", or saying, leaking information across time steps, which happens in the second and subsequent MDCDPs. This operation is formulated as:</p><formula xml:id="formula_1">F t pos = Atten([F t pos , F t pos ], M t pos ) + FFN (2)</formula><p>where FFN represents the feed forward network. Atten is the multi-head attention. The first term in the square bracket is Q, while the second term represents both K and V. In this case is multi-head self-attention. Note that previous studies <ref type="bibr" target="#b50">Yu et al., 2020)</ref> did the enhancement on visual branch rather than positional branch. We argue that visual feature from the encoder side is powerful enough. Alternatively, SAE enables a more targeted positional embedding through gradient back-propagation. It leads to a more reasonable feature utilization especially for the second and subsequent MDCDPs. Cross-Branch Interaction (CBI). The enhanced positional embedding F t pos is then treated as the query vector and fed into the visual and semantic branches in parallel. When applied to the semantic branch, we call it CBI-S, and F t pos sem , the cross-attention between position and semantic feature is calculated. It simulates the semantic affinity between previously decoded characters and the character to be recognized. Similarly, M t pos is leveraged to prevent the semantic infor-mation leaking across time steps. Meanwhile, F t pos vis , the cross-attention between position and visual feature, is also generated on the CBI-V side. It is explained as using the previously decoded character positions to search for the character region to be recognized in the text image. Thus, both branches are strengthened after the interactions. The two cross-attention operations are defined as: </p><p>Note that previous studies <ref type="bibr" target="#b0">(Baek et al., 2019;</ref><ref type="bibr" target="#b20">Li et al., 2019)</ref> used semantic feature as the query to interact with visual feature, a means of establishing the visual-semantic correspondence that is in favor of finding the character to be recognized. RobustScanner <ref type="bibr" target="#b51">(Yue et al., 2020)</ref> extended it by adding an additional query from position feature to visual feature and enables a position-enhanced decoding. However, the query is contentfree and does not interact with the semantic branch. In contrast, we formulate the interactions as the positionbased enhancement to both visual and semantic domains. It reinforces not only visual but also semantic features and can be understood as delineating both spatial variations and semantic affinities among characters. Dynamic Shared Fusion (DSF). DSF aims to fusing the two position-enhanced features efficiently and effectively. It takes the two features as input. They are concatenated to form a hybrid feature whose channels are doubled. Then, the feature undergoes a 1 ? 1 convolution that encourages feature fusion while halves the channels, i.e., retaining the same channels as either input features. After that, a gating mechanism is designed to transform it to weight matrices?, which are applied to both visual and semantic features element-wisely, forming a dynamic fusion of features across visual and semantic domains. Formally,</p><formula xml:id="formula_3">S = ?([F t pos sem , F t pos vis ]W conv ) (5) F t out =? ? F t pos sem + (1 ??) ? F t pos vis (6)</formula><p>where ? is the sigmoid function, W conv ? R 2C?C denotes the corresponding convolution. ? is the elementwisely dot product. Note that the fusion is efficient and similar operations are also considered in <ref type="bibr" target="#b50">Yu et al., 2020)</ref>. One peculiarity of DSF is that W conv , the convolutional parameters, are shared across time steps and among MDCDP modules. As a consequence, DSF not only decouples the feature fusion with the previous crossattention modeling, but also eases the network learning. We will verify its effectiveness in ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CDistNet</head><p>We then explain the construction of an effective text recognizer based on the proposed MDCDP. Through DSF, we get a content-aware representation that integrates recognition-related visual, semantic and positional clues. To enable thorough feature interaction, a stacked structure with several MDCDPs appended oneby-one is developed, where the number of stacked MD-CDPs is empirically set to 3 (will be discussed in experiments). The structure constructs a gradually precise feature-character alignment modeling by allowing the relevant features interactions thoroughly among the three feature spaces. Formally, it regards F t,1 out , the output of the first MDCDP at time step t, as the positional embedding of the next MDCDP, then sequentially generating F t,2 out and F t,3 out . At last, a linear classifier with softmax activation is applied to F t,3 out to get the prediction, i.e., the t-th decoded character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>Following most scene text recognition methods, the text recognizer is trained on synthetic datasets and tested on widely used real datasets. To better evaluate CDistNet in recognizing difficult text, we further create two series of augmented datasets and also use them as the testbed. The datasets are elaborated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Augmented Datasets</head><p>We first construct two series of augmented datasets with different levels of recognition difficulties from the ICDAR2013 benchmark (IC13). Specifically, we employ horizontal and curved stretching of intensity from 1 (the smallest deformation) to 6 (the largest deformation) with an interval step of 1 to IC13 images. As a result, we obtain 12 counterparts of IC13 with different deformation intensities. We group the datasets according to the stretching type and termed them as HA-IC13 (horizontal) and CA-IC3 (curved) series, respectively. Each dataset simulates a certain degree of recognition challenge.</p><p>We elaborate the stretching details as follows. Given a text image I ? R W ?H as depicted in <ref type="figure" target="#fig_3">Fig.4(a)</ref>, we first assign 2(N +1) fiducial points equally distributed along upper and lower boundaries of the image, i.e., the green points in <ref type="figure" target="#fig_3">Fig.4(b)</ref> and (c). An W 4N ? H background region is also concatenated from the left side, as the first character may exceed the left boundary after the augmentation. Then, for each fiducial point p i = [l i , h i ] with h i ? {0, H}, we generate its horizontal and curved counterparts, i.e., the moved fiducial points defined as p HA i and p CA i . They are calculated by using Equ.7 and Equ.8, respectively.</p><formula xml:id="formula_4">p i HA = [l i + ? M , h i ] (7) p CA i = [l i + ? M , h i ? ? M ]<label>(8)</label></formula><p>where ? M = ? ? ? * s. s ? {1, 2, 3, 4, 5, 6} is the deformation intensity. ? ? 0, W 4N is a random value. ? is set to max( W 8N , ?), requiring ? ? ? such that ? M ? 0. That is to say, all fiducial points are move to the left side in the X-axis (the horizontal case), and simultaneously the upper side in the Y-axis (the curved case), as the purple points shown in <ref type="figure" target="#fig_3">Fig.4(b)</ref> and (c). In the following, all image pixels are transformed according to the moved fiducial points using Thin-plate-splines. By using the equations, we create elastic transformations to the text image while roughly maintain the geometric layout of each character.</p><p>In <ref type="figure" target="#fig_4">Fig.5</ref> we list two examples undergoing different horizontal and curved stretching intensities. As can be seen, recognition difficulties such as text deformation, unequally distributed characters of different levels are successfully created, while the text still remains recognizable by humans. Note that the augmentation is an extension of the work of Luo et al. . Compared to , the difference lies in that they focused on online augmentation and ignores constructing new datasets. While we generate two series of augmented datasets paying attention to horizontal and curved deformations with different intensities. The datasets have been made publicly available along with the code.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Public Datasets</head><p>Then we introduce the two synthetic datasets, and six standard benchmarks as follows. MJSynth (MJ) <ref type="bibr" target="#b13">(Jaderberg et al., 2014)</ref> and Syn-thText (ST) <ref type="bibr" target="#b10">(Gupta et al., 2016)</ref> are two synthetic datasets each with millions of text images. 8.91M text instances from MJ and 6.95M from ST are retained for model training. ICDAR2013 (IC13) is cropped from 288 scene real images. Following previous work <ref type="bibr" target="#b0">(Baek et al., 2019)</ref>, The version with 857 images is chosen for testing, which deletes non-alphanumeric characters and text shorter than 3 characters. Street View Text (SVT) <ref type="bibr" target="#b42">(Wang et al., 2011)</ref> contains 257 images for training and 647 images for testing, which are captured by Google Street View. Just testing images are chosen for our experiments.</p><p>IIIT5k-Words (IIIT5k) <ref type="bibr" target="#b30">(Mishra et al., 2012)</ref> consists of 3000 testing images, which are collected from Google Image Searches. These instances are almost horizontal. <ref type="figure" target="#fig_0">(Karatzas et al., 2015)</ref> is a irregular dataset with 1811 images for testing. Words in this dataset are mostly curved, perspective and shading, which are hard to recognize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICDAR2015 (IC15)</head><p>SVT-Perspective (SVTP) <ref type="bibr" target="#b32">(Phan et al., 2013)</ref> is also created from Google Street View. It contains 645 curved text images.</p><p>CUTE80 (CT80) <ref type="bibr" target="#b34">(Risnumawan et al., 2014)</ref> contains 288 irregular text images cropped from natural scene images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation details</head><p>We resize the text image to 32?128 and employ the data augmentation in , i.e., image quality deterioration, color jitter and geometry transformation. MJ and ST datasets are used for model training. While the two series of augmented datasets and the six public datasets are retained for model assessment at different scenarios. To train CDistNet, the initial learning rate is set to 4?10 ?4 . The first 10k iterations use Warm-up. The whole training iterations are determined by lr = d ?0.5 model ? min(n ?0.5 , n ? warm n ?1.5 )</p><p>where n and warm n denote the number of normal and Warm-up iterations. d model is set to 512. For ablation study and model verification, all models are trained for 6 epochs. When compared with existing methods, our model is trained for 8 epochs following Equ.9 and then another 2 epochs with a constant learning rate of 10 ?5 . The batch size is set to 700.</p><p>To reduce the computational cost, we shrink the transformer units employed, where dimension of the MLP layer is reduced from 2048 to 1024 in encoder, and from 2048 to 512 in decoder. The number of encoder and decoder layers are both set to 3. Beam search is applied to determine the decoded character sequence and its size is empirically set to 10. All models are trained by using a server with 6 NVIDIA 3080 GPUs on PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation study</head><p>To better understand CDistNet, we carry out controlled experiments on both SVT (regular text) and IC15 (irregular text) under different configurations as follows.</p><p>The effectiveness of SAE. SAE imposes feature refinement on the applied branch. Besides the positional branch, it is curious that the recognition would be improved by applying SAE to the visual and semantic features on the decoder side. With this doubt in mind, we apply SAE to different branches while Tab.1 gives the result. It is seen the differences are marginal when equipping SAE to either or both visual and semantic branches (line 2,3,4). Nevertheless, on the positional branch only the improvement is noticeable (line 14). It is better than applying SAE to none or all three branches (line 1,5). The result is in line with our expectations. The visual feature is extracted from a powerful hybrid backbone while the semantic feature is dynamically fortified during decoding. It is less meaningful to reinforce them further. In contrast, the position feature comes from a fixed embedding, while SAE allows dynamically to highlight its valuable information, especially for the subsequent MDCDP modules. Therefore improvements are observed. The effectiveness of CBI. There are multiple ways to establish the CBI. We enumerate six of them and give the results in <ref type="figure" target="#fig_3">Tab.1 (line 6-10,14)</ref>. CDistNet (line 14) achieves the best result among the competitors, better than the schemes that use semantic feature as the query vector, e.g., Transformer-like implementation <ref type="bibr" target="#b36">(Sheng et al., 2019</ref>) (line 7), RobustScanner-like implementation <ref type="bibr" target="#b51">(Yue et al., 2020</ref>) (line 9) and the scheme of using semantic feature to query position feature (S P, line 10). The results basically validate two hypotheses. First, imposing the positional clue on the visual branch is helpful. Second, it is effective to query semantic feature by using position feature, which perceives the semantic affinities between previous decoded characters and the character to be recognized. The effectiveness of DSF. We have tested four operations to fuse the position-enhanced visual and semantic features. As shown in Tab.1, DSF outperforms static-based fusions (Add and Dot) (line 11,12) as well as the scheme that not sharing weights among MDCDP modules (line 13), demonstrating the effectiveness of the proposed scheme. The number of MDCDP modules considered. Generally, stacking more MDCDPs trends to generate comprehensive feature representation, but increases the computational cost. Ablation study on this point is given by Tab.2 (the top half), where recognition accuracy and inference speed are both presented. The best accuracy is reported when three MDCDPs are equipped. It performs better than other cases where fewer (one and two) or more (four) modules are considered. Therefore the MDCDP modules in CDistNet are set to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Verification</head><p>Contribution of the positional utilization. There are several works encoding the position information. However, CDistNet differs with them not only in the position utilization but also other aspects. To make a fair comparison, we modified these methods where other differences are mostly eliminated. The results are listed in Tab.2 (the bottom half).</p><p>We explain how the methods are modified. CDistNet w/o Sem denotes the semantic branch is removed from CDistNet thus the position feature only queries the visual feature (line 6 in Tab.1). The absence of modeling semantic clue, despite faster, leads to a noticeable accu-   racy drop of over 3% on average. It implies the importance of semantic feature. In Transformer* (line 7 in Tab.1), we strengthen the raw implementation <ref type="bibr" target="#b36">(Sheng et al., 2019)</ref> using our visual encoding branch, from which 5% accuracy gain in IC15 is observed due to a superior visual feature. Besides the visual encoding branch, RobustScanner* uses our Transformer unit to replace its "CNN+LSTM" implementation. It improves the accuracy by 3.86% and 7.11% on SVT and IC15, respectively. Our modification leads to noticeable improvements to their raw implementations. With the modifications, accuracy gains from the position modeling can be quantitatively evaluated. CDis-tNet still attains nearly 1.7% accuracy gains on average in both datasets compared with RobustScanner*, largely attributed to that P S (i.e., using position fea-ture to query semantic feature) is a superior cross-attention scheme compared to S V (i.e., using semantic feature to query visual feature). Performance on augmented datasets. As described, HA-IC13 and CA-IC13 are simulated with different horizontal and curved deformation intensities, forming testbed with increasing difficulties, a nearly ideal scenario for assessing CDistNet in recognizing difficult text.</p><p>We validate CDistNet in both series of datasets and Tab.3 presents the results, where ABINet  and VisionLAN  are two leading models in standard benchmarks. All methods recognize well in raw IC13 due to its simplicity. With the rise of deformation intensity, CDistNet shows its superiority. It goes down slower. The accuracy gaps gradually become larger with the intensity increase, where mar- <ref type="table">Table 4</ref>: Accuracy comparison with existing methods on six standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Year Traning Data IC13 SVT IIITK IC15 SVTP CT80 CRNN <ref type="bibr" target="#b37">(Shi et al., 2017)</ref> 2017 <ref type="formula" target="#formula_5">90K</ref>  gins ranging from 5.2% to 16.7%, and from 2.3% to 10.4% are respectively observed compared with other methods in HA-IC13 with intensity 6 (H6) and CA-IC13 with intensity 6 (C6). Noticing that existing leading models perform near saturation in public benchmarks, where the accuracy gap is not obvious (elaborated later) and might not be enough to well distinguish these methods. The remarkable gap clearly verifies the great generalization ability of CDistNet especially in recognizing difficult text. The argumentation generates many unequally spaced and severely bent characters. However, CDistNet shows much better robustness to these challenges due to perceiving character distances in both visual and semantic domains.</p><p>We also observe that the sequential decoding methods behave differently from the parallel decoding ones. ABINet and VisionLAN, which employ the parallel decoding scheme for speed acceleration, experience even sharp decreases compared to the three sequential decoding methods. For example, CDistNet is better than Vi-sionLan and ABINet by 16.7% and 12.7% in H6, 10.4% and 8.6% in C6. Transformer* and RobustScanner* also have large margins compared to them. The significant accuracy gaps can be explained as not only their positional embedding is content-free, but also their parallel decoding schemes are established at the cost of sacrificing the attention quality in difficult text. Note that we are the first observing the inefficiency of popular parallel decoding schemes in handling difficult text. Tackling the drawback should be an interesting topic worthy of further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparisons with Existing Methods</head><p>We compare CDistNet with nineteen existing methods published from year 2017 to 2022, covering the majority of influential methods of these years. The results are given by Tab.4. It is shown that CDistNet achieves the best results on four datasets including IC13, IIITK, IC15 and CT80. As expected, the performance gains are more prominent in irregular text datasets compared to regular text datasets, demonstrating its superiority in modeling challenging characters. When comparing with previously state-of-the-art methods such as ABI-Net and S-GTR, CDistNet does not always show obvious accuracy differences (i.e., within 1% in five of the six datasets) although consistently ranks top-tier. It indicates that the performance of scene text recognition is approaching saturate on these benchmarks. When looking into the closely related sequential decoding methods <ref type="bibr" target="#b36">(Sheng et al., 2019;</ref><ref type="bibr" target="#b41">Wan et al., 2020;</ref><ref type="bibr" target="#b51">Yue et al., 2020)</ref>, CDistNet outperforms them by large margins on all six datasets. It again demonstrates the superiority of the proposed feature modeling. It incorporates visual, semantic and positional clues that model finegrained character distance. It, thus, better aligns the feature-character information even with various recognition difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Decoding Visualization</head><p>We visualize three illustrative examples in the form of semantic affinity matrices in <ref type="figure">Fig.6</ref>, where the semantic utilization of three different methods (i.e., Trans- <ref type="figure">Fig. 6</ref>: Visualization of the semantic affinity matrices of different methods. In each matrix a row represents the semantic affinities between previously decoded characters and the diagonal character to be decoded. Darker color indicates higher affinity value. The leftmost column is the text image and groundtruth. The second to fourth columns correspond to visualizations of Transformer*, RobustScanner* and CDistNet. Red color means incorrectly recognized characters. former*, RobustScanner* and CDistNet) during the decoding are shown. They are obtained by inspecting their respective (position-enhanced) semantic branches.</p><p>Specifically, in each matrix, Y-axis denotes the decoding time steps while X-axis represents the decoded characters. Darker color indicates higher affinity value, i.e., higher contribution in recognizing the diagonal character. Two observations are obtained from the matrices. First, the diagonal elements play more important roles (darker) in CDistNet than the other two methods. Since CDistNet jointly models recognition clues in visual, semantic and position spaces, the feature is more aware of the character to be recognized. It is thus more confident in semantic utilization. Second, for CDistNet the majority of previous decoded characters have darker color thus more deeply contributing to the decoding process. It produces more accurate prediction in general. On the contrary, in Transformer* the position and semantic features are not well decoupled. So the decoding mostly relies on the first column. While in RobustScanner*, although the position is decoupled, the decoding less considers characters far from the recognized one. It implies in part why the two methods sometimes make incorrect recognition. Conversely, again demonstrating that CDistNet achieves a more comprehensive semantic affinity utilization.</p><p>We then visualize the attention map based on the position-enhanced visual branch of the last MDCDP, where six examples are shown in <ref type="figure" target="#fig_5">Fig.7</ref>. The characters are properly localized on the attention maps in most cases. It implies that CDistNet retains powerful and universal recognition capability with the existence of various difficulties. The joint feature representation well encodes the spatial layout and semantic affinity among characters, thus ensuring that accurate featurecharacter alignments are established at most time steps. On the other hand, it is observed that there are also a few wrongly recognized cases. The failures can be summarized as three categories mainly, i.e., multiple text fonts (e.g., harry), severe blur (e.g., quilts) and vertical text. Most of them are even indistinguishable by humans and they are still common challenges for modern text recognizers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Targeting to improve the accuracy of scene text recognition especially in difficult text, we have presented the MDCDP module which utilizes position feature to query both visual and semantic features within the Transformer-based encoder-decoder framework. It learns a joint representation that delineates character distance in both visual and semantic domains. Accordingly, CDis-tNet has been developed for robust scene text recognition. We have carried out extensive experiments to verify its effectiveness. While ablation studies demonstrate the effect of each proposed component. The comparison results successfully validate our proposal. It not only reports state-of-the-art accuracy when compared with existing methods on six standard benchmarks, but also on two series of augmented datasets it shows greater advantages as the deformation intensity increases. In addition, the visualization experiments verify that superior attention localization and reasonable semantic utilization are reached in visual and semantic domains, respectively.</p><p>Accuracy and inference speed are always two key metrics for scene text recognition. It is observed that the accuracy on existing public benchmarks is near saturation, which might be insufficient to well distinguish the methods performed top-tier. Our study reveals these methods differ significantly on severely deformed HA-IC13 and CA-IC13 datasets. We hope the datasets can foster future research in designing robust scene text recognition models. Meanwhile, some recent studies reported very fast yet efficient solutions by leveraging the vision transformer architecture <ref type="bibr" target="#b19">Li et al., 2022)</ref>. Thus, we are also interested in developing methods to accelerate the recognition by following this pipeline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>An overview of the proposed CDistNet. The encoder consists of visual, semantic and positional branches. The three kinds of features are fed into the Multi-Domain Character Distance Perception (MDCDP) module that learns a content-aware representation. SAE, CBI and DSF are proposed blocks. CDistNet stacks three MDCDPs to get a gradually precise distance modeling. Output of the last MDCDP is leveraged to decode the characters sequentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Details of the MDCDP module. (a) Self-Attention Enhancement (SAE), (b) Cross-Branch Interaction (CBI) and (c) Dynamic Shared Fusion (DSF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>vis = Atten([F t pos , F vis ]) + FFN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Raw text image and the illustration of its augmentation. Green and purple points represent the fiducial points and their moved counterparts. (b) and (c) are the horizontal and curved stretching instances, respectively. Their deformation intensity both set to 6, the most severe one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Augmented text images in HA-IC13 (top) and CA-IC13 (bottom), which simulate horizontal and curved stretching of varying intensities, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Attention map visualization of the position-enhanced visual feature in the last MDCDP module. GT and Pred denote the ground-truth and predicted result, respectively. Red color means incorrectly recognized characters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation studies on SAE, CBI and DSF. Sem, Vis and Pos denote the semantic, visual and positional branches, respectively. S V denotes using semantic feature to query visual feature. The others are similarly defined. ( ) means switching the roles of the two features. WS denotes weight sharing among MDCDP modules.</figDesc><table><row><cell cols="2">Ablation Line Part</cell><cell>SAE Sem Vis Pos S V P V P S CBI</cell><cell>Fusion</cell><cell>SVT</cell><cell>IC15</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell>DSF</cell><cell>92.27</cell><cell>84.92</cell></row><row><cell></cell><cell>2</cell><cell></cell><cell>DSF</cell><cell>92.89</cell><cell>84.70</cell></row><row><cell>SAE</cell><cell>3</cell><cell></cell><cell>DSF</cell><cell>93.04</cell><cell>84.65</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell>DSF</cell><cell>93.04</cell><cell>84.76</cell></row><row><cell></cell><cell>5</cell><cell></cell><cell>DSF</cell><cell>93.66</cell><cell>84.70</cell></row><row><cell></cell><cell>6</cell><cell></cell><cell>DSF</cell><cell>90.26</cell><cell>82.11</cell></row><row><cell></cell><cell>7</cell><cell></cell><cell>DSF</cell><cell>91.34</cell><cell>84.04</cell></row><row><cell>CBI</cell><cell>8</cell><cell></cell><cell>DSF</cell><cell>92.27</cell><cell>85.37</cell></row><row><cell></cell><cell>9</cell><cell></cell><cell>DSF</cell><cell>92.89</cell><cell>84.93</cell></row><row><cell></cell><cell>10</cell><cell>( )</cell><cell>DSF</cell><cell>93.35</cell><cell>84.70</cell></row><row><cell></cell><cell>11</cell><cell></cell><cell>Add</cell><cell>91.96</cell><cell>85.48</cell></row><row><cell>DSF</cell><cell>12</cell><cell></cell><cell>Dot</cell><cell>91.81</cell><cell>84.21</cell></row><row><cell></cell><cell>13</cell><cell></cell><cell>DSF w/o WS</cell><cell>93.51</cell><cell>85.04</cell></row><row><cell></cell><cell>14</cell><cell></cell><cell>DSF</cell><cell cols="2">93.66 85.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison of different methods. * denotes our implementation with improved accuracy. Bracket values are accuracy improvements compared with results reported by their papers.</figDesc><table><row><cell>Method</cell><cell>#MDCDP</cell><cell>SVT</cell><cell>IC15</cell><cell>Speed (ms)</cell></row><row><cell>CDistNet</cell><cell>1</cell><cell>92.74</cell><cell>84.82</cell><cell>61.48</cell></row><row><cell>CDistNet</cell><cell>2</cell><cell>94.28</cell><cell>84.87</cell><cell>87.68</cell></row><row><cell>CDistNet</cell><cell>3</cell><cell>93.66</cell><cell>85.92</cell><cell>123.28</cell></row><row><cell>CDistNet</cell><cell>4</cell><cell>93.82</cell><cell>84.43</cell><cell>149.99</cell></row><row><cell>CDistNet w/o Sem</cell><cell>3</cell><cell>90.26</cell><cell>82.11</cell><cell>80.91</cell></row><row><cell>Transformer*(Sheng et al., 2019)</cell><cell>-</cell><cell>91.34 (-0.16)</cell><cell>84.04 (+5.00)</cell><cell>122.74</cell></row><row><cell>RobustScanner*(Yue et al., 2020)</cell><cell>-</cell><cell cols="2">91.96 (+3.86) 84.21 (+7.11)</cell><cell>122.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison of different methods on HA-IC13 (left) and CA-IC13 (right). H1 denotes HA-IC13 with stretching intensity 1. The others defined similarly.</figDesc><table><row><cell>Method</cell><cell>Raw H1</cell><cell>H2</cell><cell>H3</cell><cell>H4</cell><cell>H5</cell><cell>H6</cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>C5</cell><cell>C6</cell></row><row><cell cols="13">Transformer*(Sheng et al., 2019) 97.2 96.3 95.5 92.4 86.5 79.4 72.5 95.7 94.4 85.9 75.9 65.9 58.6</cell></row><row><cell cols="13">RobustScanner*(Yue et al., 2020) 96.9 96.2 95.3 93.2 88.9 81.1 71.5 95.2 94.9 85.3 76.6 68.4 60.8</cell></row><row><cell>VisionLAN(Wang et al., 2021)</cell><cell cols="12">96.3 93.6 92.9 90.0 82.3 72.2 61.0 94.9 92.8 84.0 75.0 64.3 52.7</cell></row><row><cell>ABINet(Fang et al., 2021)</cell><cell cols="12">97.0 95.9 95.2 92.0 85.8 73.8 65.0 96.6 95.9 87.9 76.3 65.5 54.5</cell></row><row><cell>CDistNet</cell><cell cols="12">97.1 96.6 96.2 94.3 90.0 83.4 77.7 96.3 95.6 88.5 79.6 70.4 63.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the National Natural Science Foundation of China under Grants 62172103 and 62102384.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="4714" to="4722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chinese image text recognition on grayscale pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1380" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Joint visual semantic reasoning: Multi-stage decoder for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="14920" to="14929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive embedding gate for attention-based scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Text recognition in the wild: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="5076" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Aon: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<idno>arXiv:220500159</idno>
		<title level="m">SVTR: scene text recognition with a single visual model</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Visual semantics allow for textual reasoning better in scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aaai He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
	<note>Reading scene text in deep convolutional sequences</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gtc: Guided training of ctc towards efficient and accurate scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11005" to="11012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv:14062227</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<title level="m">Icdar 2013 robust reading competition</title>
		<meeting><address><addrLine>De Las Heras LP</addrLine></address></meeting>
		<imprint>
			<publisher>ICDAR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Icdar 2015 competition on robust reading</title>
		<imprint>
			<publisher>ICDAR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for selfsupervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno>arXiv:190911942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno>arXiv:220603001</idno>
		<title level="m">Pp-ocrv3: More attempts for the improvement of ultra lightweight OCR system</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2359" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="532" to="548" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scene text recognition from twodimensional perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8714" to="8721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: The deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="184" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MORAN: A multi-object rectified attention network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learn to augment: Joint data augmentation and network optimization for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="13746" to="13755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Separating content from style using adversarial learning for recognizing text in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="960" to="976" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="67" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<idno>arXiv:190605708</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>2d attentional irregular scene text recognizer</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>BMVC</publisher>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dictionary-guided scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7383" to="7392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Seed: Semantics enhanced encoder-decoder framework for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="13525" to="13534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ESA</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Label embedding: A frugal baseline for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Rodriguez-Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="207" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Nrtr: A no-recurrence sequence-to-sequence model for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICDAR</publisher>
			<biblScope unit="page" from="781" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Accurate recognition of words in scenes without character segmentation using recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="397" to="405" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NIPS</publisher>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Textscanner: Reading characters in order for robust scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12120" to="12127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A simple and robust convolutional-attention network for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv:190401375</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene text recognition via gated cascade attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1018" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Decoupled attention network for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12216" to="12224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">From two to one: A new scene text recognizer with visual language modeling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="14174" to="14183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Convolutional character networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9126" to="9136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Primitive representation learning for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Towards accurate scene text recognition with semantic reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="12113" to="12122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Robustscanner: Dynamically enhancing positional clues for robust text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Esir: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2059" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
