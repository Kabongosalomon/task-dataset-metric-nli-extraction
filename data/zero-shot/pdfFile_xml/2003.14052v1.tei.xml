<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Sketch-aware Semantic Scene Completion via Semi-supervised Structure Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D Sketch-aware Semantic Scene Completion via Semi-supervised Structure Prior</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>SSCNet Ours Depth (NYUCAD) RGB</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ceil</head><p>Floor Wall Window Chair Bed Sofa <ref type="table">Table  TVs</ref> Furn. Objects <ref type="figure">Figure 1</ref>. Visualization of Semantic Scene Completion task. From left to right: (1) RGB input, (2) depth map, (3) ground truth of semantic scene completion, (4) result of SSCNet [27], (5) result of the proposed method. Our method generates a more reasonable result and obtains a better intra-class consistency and inter-class distinction compared with SSCNet [27], a classic method that models context on implicitly embedded depth feature that learnt from general 3D CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The goal of the Semantic Scene Completion (SSC) task is to simultaneously predict a completed 3D voxel representation of volumetric occupancy and semantic labels of objects in the scene from a single-view observation. Since the computational cost generally increases explosively along with the growth of voxel resolution, most current state-of-thearts have to tailor their framework into a low-resolution representation with the sacrifice of detail prediction. Thus, voxel resolution becomes one of the crucial difficulties that lead to the performance bottleneck.</p><p>In this paper, we propose to devise a new geometry-based strategy to embed depth information with low-resolution voxel representation, which could still be able to encode sufficient geometric information, e.g., room layout, object's sizes and shapes, to infer the invisible areas of the scene with well structure-preserving details. To this end, we first propose a novel 3D sketch-aware feature embedding to explicitly encode geometric information effectively and efficiently. With the 3D sketch in hand, we further devise a simple yet effective semantic scene completion framework that incorporates a light-weight 3D Sketch Hallucination mod-* This work was done during an internship at SenseTime Research.</p><p>? Gang Zeng is the corresponding author.</p><p>ule to guide the inference of occupancy and the semantic labels via a semi-supervised structure prior learning strategy. We demonstrate that our proposed geometric embedding works better than the depth feature learning from habitual SSC frameworks. Our final model surpasses stateof-the-arts consistently on three public benchmarks, which only requires 3D volumes of 60?36?60 resolution for both input and output. The code and the supplementary material will be available at https://charlesCXK.github.io.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Visualization of Semantic Scene Completion task. From left to right: (1) RGB input, (2) depth map, (3) ground truth of semantic scene completion, (4) result of SSCNet <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b4">(5)</ref> result of the proposed method. Our method generates a more reasonable result and obtains a better intra-class consistency and inter-class distinction compared with SSCNet <ref type="bibr" target="#b26">[27]</ref>, a classic method that models context on implicitly embedded depth feature that learnt from general 3D CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The goal of the Semantic Scene Completion (SSC) task is to simultaneously predict a completed 3D voxel representation of volumetric occupancy and semantic labels of objects in the scene from a single-view observation. Since the computational cost generally increases explosively along with the growth of voxel resolution, most current state-of-thearts have to tailor their framework into a low-resolution representation with the sacrifice of detail prediction. Thus, voxel resolution becomes one of the crucial difficulties that lead to the performance bottleneck.</p><p>In this paper, we propose to devise a new geometry-based strategy to embed depth information with low-resolution voxel representation, which could still be able to encode sufficient geometric information, e.g., room layout, object's sizes and shapes, to infer the invisible areas of the scene with well structure-preserving details. To this end, we first propose a novel 3D sketch-aware feature embedding to explicitly encode geometric information effectively and efficiently. With the 3D sketch in hand, we further devise a simple yet effective semantic scene completion framework that incorporates a light-weight 3D Sketch Hallucination mod-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic Scene Completion (SSC), which provides an alternative to understand the 3D world with both 3D geometry and semantics of the scene from a partial observation, is an emerging topic in computer vision for its wide applicability on many applications, e.g., augmented reality, surveillance and robotics. Due to the high memory and computational cost requirements on inherent voxel representation, most existing methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4]</ref> achieve semantic scene completion through sophisticated 3D context modeling on implicitly embedded depth feature that learnt from general 3D CNNs. These methods are either error-prone on classifying fine details of objects or have the difficulties in completing the scene when there exists a large portion of geometry missing, as shown in <ref type="figure">Figure 1</ref>.</p><p>Several recent studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref> present promising results on this topic by introducing high-resolution RGB images into the process. Though driven by various motivations, these methods could be thought as building crossmodality feature embedding with the assumption that the fine detail feature could be compensated from RGB counterpart and computation-efficient property could be guaranteed with 2D operators on RGB source. However, such an approach is highly relied on the effectiveness of crossmodality feature embedding module design and is vulnerable to complex scenes.</p><p>In contrast, from the human perception, it is a breeze to complete and recognize 3D scene even from the partial low-resolution observation, due to the prior knowledge on object's geometry properties, e.g., size and shape, of different categories. From this perspective, we hypothesize the feature embedding strategy that explicitly encodes the geometric information could facilitate the network learning the concept of object's structure, and therefore reconstructing and recognizing the scene precisely even from the low-resolution partial observation. To this end, the geometry properties need to be resolution-invariant or at least resolution-insensitive.</p><p>Based on this intuition, we present 3D sketch 1 -aware feature embedding, an explicit and compact depth feature embedding schema for the semantic scene completion task. It has been demonstrated in <ref type="bibr" target="#b22">[23]</ref> that the similar geometric cue in image space, i.e., 2D boundary, is resolutioninsensitive. We show that the 3D world also holds the same conclusion, as indicated in <ref type="figure">Figure 2</ref>. 60x36x60 80x48x80 120x72x120 240x144x240 <ref type="figure">Figure 2</ref>. Visualization of sketches extracted from semantic labels with different resolutions. From left to right, the sketch begins to lose some details as resolution decreases, while the structure description of the scene is well preserved. However, 3D sketch extracted from 2D depth image is still a 2D/2.5D observation from a single viewpoint. To fully utilize the strength of this new feature embedding, we further propose a 3D sketch-aware semantic scene completion network, which injects a 3D Sketch Hallucination Module to infer the full 3D sketch from the partial one at first, and then utilize the feature embedded from the hallucinated 3D sketch to guide the reconstruction and recognition. Specifically, since lifting the 2D/2.5D observation to full 3D sketch is intrinsically ambiguous, instead of directly 1 3D Sketch could be understood as a kind of 3D boundary. To distinguish it with the concept of edge/boundary in image space, we refer it as 3D Sketch.</p><p>regressing the ground-truth full 3D sketch, we seek a nature prior distribution to sample diverse reasonable 3D sketches. We achieve that by tailoring Conditional Variational Autoencoder (CVAE) <ref type="bibr" target="#b25">[26]</ref> into the 3D Sketch Hallucination Module design. We show that such a design could help to generate accurate and realistic results even when there is a large portion of geometry missing from the partial observation.</p><p>We summarize our contributions as follows:</p><p>? We devise a new geometric embedding from depth information, namely 3D sketch-aware feature embedding, to break the performance bottleneck of the SSC task caused by a low-resolution voxel representation.</p><p>? We introduce a simple yet effective semantic scene completion framework that incorporates a novel 3D Sketch Hallucination Module to guide the full 3D sketch inference from partial observation via semisupervised structure prior property of Conditional Variational Autoencoder (CVAE), and utilizes the feature embedded from the hallucinated 3D sketch to further guide the scene completion and semantic segmentation.</p><p>? Our model outperforms state-of-the-arts consistently on three public benchmarks, with only requiring 3D volumes of 60 ? 36 ? 60 resolution for both input and output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Shape Completion</head><p>Object shape completion has a long history in geometry processing. We summarize existing methods to two categories: knowledge-based and learning-based.</p><p>Knowledge-based methods complete partial input of an object by reasoning geometric cues or matching it with 3D models from an extensive shape database. Some works detect symmetries in meshes or point clouds and use them to fill in missing data, such as <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19</ref>]. An alternative is to match the partial input with CAD models from a large database <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13]</ref>. However, it is too expensive to retrieval, and it has poor generalization for new shapes that do not exist in the database.</p><p>Learning-based methods are more flexible and effective than knowledge-based ones. They usually infer the invisible area with a deep neural network, which has fast inference speed and better robustness. <ref type="bibr" target="#b1">[2]</ref> proposes a 3D-Encoder-Predictor Network, which first encodes the known and unknown space to get a relatively low-resolution prediction, and then correlates this intermediary result with 3D geometry from a shape database. <ref type="bibr" target="#b36">[37]</ref> proposes an end-to-end method that directly operates on raw point clouds without any structural assumption about the underlying shape. <ref type="bibr" target="#b28">[29]</ref> proposes a weakly-supervised approach that learns a shape prior on synthetic data and then conducts maximum likelihood fitting using deep neural networks.</p><p>These methods focus on reconstructing 3D shape from the partial input of a single object, which makes it hard for them to extend to partial scenes along with multiple objects estimated in semantic level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic Scene Completion</head><p>Semantic Scene Completion (SSC) is a fundamental task in 3D scene understanding, which produces a complete 3D voxel representation of volumetric occupancy and semantic labels. SSCNet <ref type="bibr" target="#b26">[27]</ref> is the first to combine these two tasks in an end-to-end way. ESSCNet <ref type="bibr" target="#b38">[39]</ref> introduces Spatial Group Convolution (SGC) that divides input volume into different groups and conduct 3D sparse convolution on them. VVNet <ref type="bibr" target="#b8">[9]</ref> combines 2D and 3D CNN with a differentiable projection layer to efficiently reduce computational cost and enable feature extraction from multi-channel inputs. ForkNet <ref type="bibr" target="#b32">[33]</ref> proposes a multi-branch architecture and draws on the idea of generative models to sample new pairs of training data, which alleviates the limited training samples problem on real scenes. CCPNet <ref type="bibr" target="#b40">[41]</ref> proposes a self-cascaded context aggregation method to reduce semantic gaps of multi-scale 3D contexts and incorporates local geometric details in a coarse-to-fine manner.</p><p>Some works also utilize RGB images as vital complementary to depth. TS3D <ref type="bibr" target="#b6">[7]</ref> designs a two-stream approach to leverage semantic and depth information, fused by a vanilla 3DCNN. SATNet <ref type="bibr" target="#b15">[16]</ref> disentangles semantic scene completion task by sequentially accomplishing 2D semantic segmentation and 3D semantic scene completion tasks. DDRNet <ref type="bibr" target="#b13">[14]</ref> proposes a light-weight Dimensional Decomposition Residual network and fused multi-scale RGB-D features seamlessly.</p><p>Above methods could be regraded as encoding depth information implicitly by either single-or cross-modality feature embedding. They map depth information into an inexplicable high-dimensional feature space and then use the feature to predict the result directly. Different from current methods, we propose an explicit geometric embedding strategy from depth information, which predicts 3D sketch first and utilize the feature embedded from it to guide the reconstruction and recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">2D Boundary Detection</head><p>2D Boundary detection is a fundamental challenge in computer vision. There are lots of methods proposed to detect boundaries. Sobel operator <ref type="bibr" target="#b24">[25]</ref> and Canny operator <ref type="bibr" target="#b0">[1]</ref> are two hand-craft based classics that detect boundaries with gradients of the image. Learning-based works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35]</ref> try to employ deep neural networks with supervision. Most of them directly concatenate multi-level features to extract the boundary. Since boundary includes a distinct geometric structure of objects, some other works try to inject boundary detection into other tasks to help boost the performance. <ref type="bibr" target="#b31">[32]</ref> combines boundary detection with salient object detection task to encourage better edge-preserving salient object segmentation. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref> introduce boundary detection into semantic segmentation task to obtain more precise semantic segmentation results. <ref type="bibr" target="#b33">[34]</ref> achieves robust facial landmark detection by utilizing facial boundary as an intermediate representation to remove the ambiguities. With similar spirits, we introduce a 3D sketch-aware feature embedding to break the performance bottleneck of the SSC task caused by a low-resolution voxel representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Structure Representation Learning</head><p>Deep generative models have demonstrated significant performance in structure representation learning. <ref type="bibr" target="#b25">[26]</ref> develops a deep conditional generative model to predict structured output using Gaussian latent variables, which can be trained efficiently in the framework of stochastic gradient variational Bayes. <ref type="bibr" target="#b41">[42]</ref> proposes an autoencoding formulation to discover landmarks as explicit structural representations in an unsupervised manner. <ref type="bibr" target="#b4">[5]</ref> proposes to synthesize images under the guidance of shape representations and conditions on the learned textural information. <ref type="bibr" target="#b21">[22]</ref> employs CVAE to stress the issue of the inherent ambiguity in 2D-to-3D lifting in the pose estimation task. Adopting the idea of structure representation learning, we embed the geometric structure of a 3D scene through a CVAE <ref type="bibr" target="#b25">[26]</ref> conditioned on the estimated sketch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The overall architecture of our network is illustrated in <ref type="figure">Figure 3</ref>. The proposed method consists of multiple stages and each stage adopts an encoder-decoder architecture. Taking a pair of RGB and depth images of a 3D scene as input, the network outputs a dense prediction and each voxel in the view frustum is assigned with a semantic label C i , where i ? [0, 1, ? ? ? , N ] and N is the number of semantic categories. C 0 stands for empty voxels.</p><p>More specifically, we stack two stages and let each stage handle different tasks. The first stage tackles the task of sketch extraction. It embeds the geometric cues contained in the scene and provides the structure prior information (which we call it sketch) for the next stage. Besides, we employ CVAE to guide the predicted sketch. The second stage tackles the task of semantic scene completion (SSC) based on the extracted sketch. Details are introduced below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generation of Ground-truth Sketch</head><p>We perform 3D Sobel operator on the semantic label to extract the sketch of the semantic scene. Suppose we have obtained gradients g i x , g i y , g i z at the i-th voxel V i along  <ref type="figure">Figure 3</ref>. Overview of our network. We first generate structure prior information from the TSDF input and use CVAE to refine the prediction. Then the prior information will be passed to the RGB-branch to predict occupancy and object labels for each voxel in the view frustum. The convolution parameters are shown as (kernel size, dilation). The DDR parameters are shown as (dilation, downsample rate). The Deconvolution parameters are shown as (kernel size, upsample rate).</p><formula xml:id="formula_0">Conv(3, 1) Conv(3, 1) Conv(3, 1) DDR(1, 2) DDR(1, 1) DDR(2, 1) DDR(3, 1) DDR(1, 2) DDR(1, 1) DDR(2, 1) DDR(3, 1) Deconv(3, 2) Deconv(3, 2) CVAE Decoder Z ~ N(0, I) Conv(3, 1) DDR(1, 2) DDR(1, 1) DDR(2, 1) DDR(3, 1) DDR(1, 2) DDR(1, 1) DDR(2, 1) DDR(3, 1) Deconv(3, 2) Deconv(3, 2) ResNet-50</formula><p>x, y, z axes, we first binarize these values to be 0 or 1 to eliminate the semantic gap. For example, the gap between class 1 and class 2 should be considered equal to the gap between class 1 and class 10 when generating the sketch. Finally, the extracted sketch can be described as a set:</p><formula xml:id="formula_1">S sketch = {V i : g i x + g i y + g i z &gt; 1}.</formula><p>To distinguish generated geometric representation with generally 2D edge/boundary, we refer it as 3D Sketch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sketch Prediction Stage</head><p>This stage takes a single-view depth map as input and encodes it as a 3D volume. We follow <ref type="bibr" target="#b26">[27]</ref> to rotate the scene to align with gravity and room orientation based on Manhattan assumption. We adopt Truncated Signed Distance Function (TSDF) to encode the 3D space, where every voxel stores the distance value d to its closest surface and the sign of the value indicates whether the voxel is in free space or occluded space. The encoder volume has a grid size of 0.02 m and a truncation value of 0.24 m, resulting in a 240 ? 144 ? 240 volume. For the saving of computational cost, <ref type="bibr" target="#b26">[27]</ref> downsamples the ground truth by a rate of 4, and we use the same setting. Following SAT-Net <ref type="bibr" target="#b15">[16]</ref>, we also downsample the input volume by a rate of 4 and use 60 ? 36 ? 60 resolution as input.</p><p>Previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref> demonstrate that contextual information is important for 2D semantic segmentation. Due to the sparseness and the high computational cost of 3D voxels, it is hard to obtain the context of the scene. To learn rich contextual information, we should make sure that our network has a large enough receptive field without significantly increasing the computational cost. To this end, <ref type="bibr" target="#b13">[14]</ref> proposed Dimensional Decomposition Residual (DDR) block which is computation-efficient compared with basic 3D residual block. We adopt DDR block as our basic unit and stack them layer by layer with different dilation rates to maintain big receptive fields. As shown in <ref type="figure">Figure 3</ref>, We first employ several convolutions to encode the TSDF volume into high dimensional features. Then we aggregate the contextual information of the input feature by several DDR blocks and downsample it by a rate of 4 to reduce computational cost. Finally, we employ two deconvolution layers to upsample the feature volume and obtain the dense predicted sketch, which we denote as? raw . Following <ref type="bibr" target="#b26">[27]</ref>, we add a skip connection between two layers for better gradient propagation, which is illustrated in <ref type="figure">Figure 3</ref>.</p><p>Due to the input of semantic scene completion task is not a complete scene, we assume that a more precise and complete sketch will bring more information increments to the subsequent stage. To some extent, it may make up for the inadequacy of incomplete input. Thus we design a 3D Sketch Hallucination Module to handle this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Sketch Hallucination Module</head><p>Lifting 2D/2.5D observation to full 3D sketch is intrinsically ambiguous, we thus seek a nature prior distribution to sample diverse reasonable 3D sketches instead of directly regressing the ground truth. Thus, we employ CVAE to further process the original predicted sketch by sampling an accurate and diverse sketch set S = {? k ref ined : k ? 1, 2, ..., K} conditioned on the estimated? raw .</p><p>The proposed 3D Sketch Hallucination Module (as shown in <ref type="figure" target="#fig_0">Figure 4</ref>) consists of a standard encoder-decoder structure. The encoder which we denote as E(G gt ,? raw ), performs some convolution operations on the input groundtruth sketch and a condition? raw to output the mean and diagonal covariance for the posterior q(?|G gt ,? raw ). Then the decoder which we denote as D(?,? raw ) will recon-struct the sketch by taking a latent? sampled from the posterior q(?|G gt ,? raw ) and the condition? raw as input. During training, we optimize the proposed module though minimizing the following objective function,</p><formula xml:id="formula_2">? #$ % &amp;'( % &amp;)*+,)- Encoder Decoder ?( #$, % &amp;'( ) (, % &amp;'( ) (? | #$, % &amp;'( )</formula><formula xml:id="formula_3">L CVAE = ? 1 KL(q(?|G gt ,? raw ) || p(z|? raw )) + ? 2 E z?q(?|Ggt,?raw) (G gt , D(?,? raw )),<label>(1)</label></formula><p>where is a cross-entropy loss and KL(x||y) is the Kullback-Leibler divergence loss. We use ? i as hyperparameter to weight these two loss items. E is the expectation which is taken over K samples. The p(z|? raw ) is the prior distribution. To ensure gradients can be backpropagated through the latent code, the KL divergence is required to be computed in a closed form. Thus, the latent space of CVAE is typically restricted to be a distribution over N (0, I). We follow this setting in our framework. Specifically, it draws a Gaussian prior assumption over the coarsestep geometry representation to fine-step geometry representation in our framework. Sketch is a simple yet compact geometry representation which suits the assumption. Since the encoder will not be used during inference, the current objective will introduce inconsistency between training and inference. To address this issue, we follow <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22]</ref> to set the encoder the same as prior network p(z) ? N (0, I), namely Gaussian Stochastic N eural N etwork (GSNN) and the reparameterization trick of CVAE can be used to train GSNN. We combine L GSNN and L CVAE with ? as weight term to obtain final objective for our refine network,</p><formula xml:id="formula_4">L GSNN = E z?N (0,I) (G gt , D(z,? raw )),<label>(2)</label></formula><formula xml:id="formula_5">L hybrid = L CVAE + ?L GSNN ,<label>(3)</label></formula><p>Durning inference, we randomly sample z from N (0, I) for K times and obtain K different D(z,? raw ), which are denoted as S = {? k ref ined : k ? 1, 2, ..., K}. We average them and obtain the refined sketch? ref ined .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Semantic Scene Completion Stage</head><p>In this stage, we will take a single RGB image and the pre-computed sketches from the former stage as input to densely predict the semantic scene labels. We divide this stage into three parts: 2D feature learning, 2D-3D projection and 3D feature learning. The input RGB image is firstly fed into a ResNet-50 <ref type="bibr" target="#b10">[11]</ref> to extract local and global textural features. For achieving stable training, we utilize the parameters pre-trained on ImageNet <ref type="bibr" target="#b2">[3]</ref> and freeze the weight of them. Due to the output tensor of ResNet-50 has too many channels, which will bring too much computational cost for 3D learning part, we adopt a convolution layer followed by a Batch Normalization <ref type="bibr" target="#b11">[12]</ref> and Rectified Linear Unit (ReLU) to reduce its dimensions.</p><p>Then the computed 2D semantic feature map will be projected into 3D space according to the depth map and the corresponding camera parameters. Given the depth image I depth , the intrinsic camera matrix K camera ? R 3?3 , and the extrinsic camera matrix E camera ? R 3?4 , each pixel p u,v in the 2D feature map can be projected to an individual 3D point p x,y,z . Because the resolution of the 3D volume is lower than the 2D feature map, multiple points may be divided into the same voxel in the process of voxelization. For those voxels, we only keep one feature vector in a certain voxel by max-pooling. After this step, the semantic feature vector for each pixel is assigned to its corresponding voxel via the mapping M. Since many areas are not visible, zero vectors are assigned to the occluded areas and empty foreground in the scene.</p><p>Given the projected 3D feature map F proj ? R C?H?W ?L , where C is the number of channels and H, W, L are size of the feature map. We now use the prior information? raw and? ref ined as guidance. We define two sketch mappings:</p><formula xml:id="formula_6">F raw :? raw ? F raw ? R C?H?W ?L and F ref ined :? ref ined ? F ref ined ? R C?H?W ?L</formula><p>to map these prior information to the same feature space with F proj . After these two mapping operations, both F raw and F ref ined have the same resolution and dimension with F proj . Thus we introduce the prior information by an element-wise addition operation on F proj , F raw and F ref ined . In pratice, these two mapping functions are implemented by 3 ? 3 convolution layers. In the following, the new feature map will be fed into a 3D CNN, whose architecture is the same with that of sketch-branch, and we obtain the final semantic scene completion predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><p>During training, the dataset is organized as a set {(X TSDF , X RGB , G gt , S gt )}, where G gt represents the ground-truth sketch and S gt represents the ground-truth semantic labels. We optimize the entire architecture by the following formulas:</p><formula xml:id="formula_7">L loss = L semantic + L hybrid + L sketch ,<label>(4)</label></formula><formula xml:id="formula_8">L semantic = (S gt , D s (E s (X RGB ))),<label>(5)</label></formula><formula xml:id="formula_9">L sketch = (G gt , D g (E g (X TSDF )),<label>(6)</label></formula><p>where D g , E g are the encoder and the decoder of the sketch stage, D s , E s are the encoder and the decoder of the semantic stage, L hybrid is defined in <ref type="figure">Eq. (3)</ref>, and denotes the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We evaluate the proposed method on three datasets: NYU Depth V2 <ref type="bibr" target="#b23">[24]</ref> (which is denoted as NYU in the following), NYUCAD <ref type="bibr" target="#b5">[6]</ref> and SUNCG <ref type="bibr" target="#b26">[27]</ref>. We will introduce these three datasets in detail in the supplementary material. We follow SSCNet <ref type="bibr" target="#b26">[27]</ref> and use precision, recall and voxel-level intersection over union (IoU) as evaluation metrics. Following <ref type="bibr" target="#b26">[27]</ref>, two tasks are considered: semantic scene completion (SSC) and scene completion (SC). For the task of SSC, we evaluate the IoU of each object class on both observed and occluded voxels in the view frustum. For the task of SC, we treat all voxels as binary predictions, i.e., empty or non-empty. We evaluate the binary IoU on occluded voxels in the view frustum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Training Details. We use PyTorch framwork to implement our experiments with 2 GeForce GTX 1080 Ti GPUs. We adopt mini-batch SGD with momentum to train our model with batch size 4, momentum 0.9 and weight decay 0.0005. We employ a poly learning rate policy where the initial learning rate is multiplied by (1 ? iter max iter ) 0.9 . For both NYU and NYUCAD, we train our network for 250 epochs with initial learning rate 0.1. For SUNCG, we train our network for 8 epochs with initial learning rate 0.01. The expection in Eq. (1) is estimated using K = 4 samples. ? 1 , ? 2 and ? in Eq. (1) and Eq. (3) are set to 2, 1 and 1.5 respectively. Oracle Ablation. To obtain the theoretical upper limit of the proposed method, we replace the output of the first stage with the ground-truth 3D sketch to supply the structure prior. Results are shown in <ref type="table" target="#tab_1">Table 1</ref>. Drop Rate means we randomly discard some voxels in the ground-truth 3D sketch by some ratio. We observe that with the whole 3D sketch as structure prior, our network could infer most of the invisible areas and obtain 94.2% SC IoU. As the drop rate increases to 80%, the performance has not dropped a lot and is still higher than the best performance of the proposed method, which verifies the validity of accurate structure prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with State-of-the-art Methods</head><p>We further compare the proposed method with state-ofthe-art methods. <ref type="table" target="#tab_4">Table 3</ref> shows the performances by stateof-the-art methods on NYU dataset. We observe that the proposed method outperforms all existing methods by a large margin, more specifically, we gain an increase of 7.8% SC IoU and 2.6% SSC mIoU compared to CCPNet <ref type="bibr" target="#b40">[41]</ref>. We argue that this improvement is caused by the novel twostage architecture which makes the full use of the structure prior. The provided structure prior can accurately infer invisible areas of the scene with well structure-preserving details.</p><p>We also conduct experiments on NYUCAD dataset to validate the generalization of the proposed method. <ref type="table" target="#tab_5">Table 4</ref> presents the quantitative results on NYUCAD dataset. Our proposed method maintains the performance advantage and outperforms CCPNet [41] by 1.8% SC IoU and 2.0% SSC mIoU. Note that although some works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b6">7]</ref> use larger input resolution than ours, the proposed method still outperforms them with a low-resolution input of 60 ? 36 ? 60.</p><p>Experiments on SUNCG dataset and the visualization of the SSC results compared with SSCNet <ref type="bibr" target="#b26">[27]</ref> on NYUCAD dataset are put in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To evaluate the effectiveness of the pivotal components of our method, we perform extensive ablation studies using the same hyperparameters. Details are illustrated below.  <ref type="table">Table 2</ref>. Ablation studies on different modules. We perform this ablation study on NYUCAD dataset.</p><p>Different Modules in the Framework. We first conduct ablation studies on different modules in the proposed method. Results are shown in <ref type="table">Table 2</ref>. From Row 1 and Row 2, we find that just adopting a dual-path structure could boost the performance, as more parameters are introduced.</p><p>In the third row, with the introduction of structure prior, our network could infer the invisible areas of the scene with well structure-preserving details, which brings great improvements. <ref type="figure">Finally</ref>     <ref type="table">Table 6</ref>. Ablation studies on different types of embeddings. We perform this ablation study on NYUCAD dataset. structure prior. We list three different representations of the prior here: shape, semantic labels and sketch. Shape is the binary description of the scene and we generate the ground-truth shape by binarizing the semantic labels. Semantic labels and sketch have been introduced in the above sections. From <ref type="table" target="#tab_6">Table 5</ref>, we observe that sketch is the best representation for modelling structure prior as it could infer the invisible regions with well structure-preserving details. Different Types of Embeddings. In this part, we conduct ablation studies on different types of embeddings. Results are shown in <ref type="table">Table 6</ref>. 'Implicit' represents taking the output of the last deconvolution layer in the first stage as the geometric embedding and feed it to the second stage as prior  <ref type="table" target="#tab_8">Table 7</ref>. Ablation studies on different modal input. We perform this ablation study on NYU dataset.</p><p>information. 'Explicit' represents we abstract a concrete structure based on the implicit embedding and use it a structure prior. We observe that even using implicit embedding, adding any reasonable supervision on it could boost the performance, such as semantics, shape and sketch. When we convert to explicit embedding, a better structure prior is obtained and the performance shows another boost. Note that the explicit embedding supervised by sketch outperforms its baseline using implicit embedding with no supervision by 3.1% SC IoU and 4.6% SSC mIoU, which demonstrates the effectiveness of the proposed sketch structure prior and the explicit embedding method. Different Modal Input. We adopt data from different modalities as input, more specifically, TSDF for the first stage and RGB for the second stage. We claim that TSDF embeds rich geometric information and is suitable for the sketch prediction task, while RGB is rich in semantic information and is suitable for semantic label prediction task. Results are shown in  <ref type="figure">Figure 5</ref>. Visualization of the sketch on NYUCAD dataset. With the proposed 3D Sketch Hallucination Module, which leverages CVAE to guide the inference of invisible areas, the sketch obtains a sharper boundary and is completer, resulting in better semantic predictions.  <ref type="table">Table 8</ref>. Ablation studies on input/output resolutions. We perform this ablation study on NYU and NYUCAD dataset both. Resolution(a, b) means the input resolution is (a ? 0.6a ? a) and the output resolution is (b ? 0.6b ? b).</p><p>mantic labels based on the same structure prior provided by TSDF, resulting in a gain of 3.9% SSC mIoU. From Row 1, Row 2 and Row 3, we observe that the introduction of other modalities would result in corresponding gains on the basis of single-mode data. Different Input/Output Resolutions. In this part, we conduct ablation studies to verify the impacts of different input/output resolutions on the performance. Results are shown in <ref type="table">Table 8</ref>. We observe that increasing input size would not make the performance worse. If we increase both the input and output resolutions, SC IoU increases substantially, while SSC mIoU only declines slightly. Hence we conclude that increasing resolution of either input or output is beneficial to semantic scene completion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results of 3D Sketch</head><p>We visualize the predicted 3D sketch with/without CVAE in <ref type="figure">Figure 5</ref>. We can observe that the sketch is more complete and precise with the proposed 3D Sketch Hallucination Module. Under the constraints of a more complete sketch, the semantic result shows great consistency in regions with the same semantic labels and has a sharper boundary. For example, in the first row, some regions in the bookcase are mislabeled as objects without CVAE, and those regions in the corresponding sketch are missing. In the second row, the sketch without CVAE fails to extract the outline of the object on the wall, leading to uncertainty of the semantic boundary. In the third row, the missing boundary in the sketch without CVAE brings confusing semantics. In the last row, the sketch of the photo frame is incomplete without CVAE, resulting in more areas to be mislabeled as wall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel 3D sketch-aware feature embedding scheme which explicitly embeds geometric information with structure-preserving details. Based on this, we further propose a semantic scene completion framework that incorporates a novel 3D Sketch Hallucination Module to guide full 3D sketch inference from partial observation via structure prior. Experiments show the effectiveness and efficiency of the proposed method, and state-of-the-art performances on three public benchmarks are achieved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of the proposed Sketch Hallucination Module. During training time, the original estimated sketch and the ground-truth sketch are fed into the encoder to generate mean and diagonal covariance for the posterior q. Then the decoder will reconstruct the ground-truth sketch with a latent sampled from q and the original estimated sketch as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>3D Sketch Hallucination Module</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Skip</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Connection</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Fine Sketch</cell></row><row><cell></cell><cell></cell><cell>Coarse Sketch</cell><cell></cell></row><row><cell>TSDF</cell><cell>3D Sketch-aware Feature Embedding</cell><cell>Sketch Mapping</cell><cell>Sketch Mapping</cell></row><row><cell>Projection</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Skip</cell><cell></cell><cell></cell></row><row><cell>RGB</cell><cell>Connection</cell><cell>Semantic Scene</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Oracle Ablation. (Oracle) Drop Rate means we randomly drop the ground-truth sketch in a certain proportion. We perform this ablation study on NYUCAD dataset.</figDesc><table><row><cell cols="3">Drop Rate(%) SC-IoU(%) SSC-mIoU(%)</cell></row><row><cell>0</cell><cell>94.2</cell><cell>65.0</cell></row><row><cell>20</cell><cell>93.7</cell><cell>63.6</cell></row><row><cell>40</cell><cell>93.2</cell><cell>62.3</cell></row><row><cell>60</cell><cell>92.0</cell><cell>59.9</cell></row><row><cell>80</cell><cell>89.9</cell><cell>57.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, with the proposed 3D Sketch Hallucination Module, we further boost the performance and achieve 84.2% SC IoU and 55.2% SSC mIoU, which are both new state-of-the-art performance on NYUCAD. IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. Lin et al. 71.3 43.1 93.6 40.5 24.3 30.0 57.1 49.3 29.2 14.3 42.5 28.6 41.1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">scene completion</cell><cell></cell><cell cols="3">semantic scene completion</cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">Resolution prec. recall [15] Trained on (240, 60) NYU 58.5 49.9 36.4 0.0</cell><cell>11.7 13.3 14.1</cell><cell>9.4</cell><cell>29.0 24.0</cell><cell>6.0</cell><cell>7.0</cell><cell>16.2</cell><cell>1.1</cell><cell>12.0</cell></row><row><cell cols="2">Geiger et al. [8]</cell><cell>(240, 60)</cell><cell>NYU</cell><cell>65.7</cell><cell cols="3">58.0 44.4 10.2 62.5 19.1 5.8</cell><cell>8.5</cell><cell>40.6 27.7</cell><cell>7.0</cell><cell>6.0</cell><cell>22.6</cell><cell>5.9</cell><cell>19.6</cell></row><row><cell cols="2">SSCNet [27]</cell><cell>(240, 60)</cell><cell>NYU</cell><cell>57.0</cell><cell cols="3">94.5 55.1 15.1 94.7 24.4 0.0</cell><cell cols="3">12.6 32.1 35.0 13.0</cell><cell>7.8</cell><cell>27.1 10.1 24.7</cell></row><row><cell cols="2">ESSCNet [39]</cell><cell>(240, 60)</cell><cell>NYU</cell><cell>71.9</cell><cell cols="3">71.9 56.2 17.5 75.4 25.8 6.7</cell><cell cols="3">15.3 53.8 42.4 11.2</cell><cell>0</cell><cell>33.4 11.8 26.7</cell></row><row><cell cols="2">DDRNet [14]*</cell><cell>(240, 60)</cell><cell>NYU</cell><cell>71.5</cell><cell cols="3">80.8 61.0 21.1 92.2 33.5 6.8</cell><cell cols="5">14.8 48.3 42.3 13.2 13.9 35.3 13.2 30.4</cell></row><row><cell cols="2">VVNetR-120 [9]</cell><cell>(120, 60)</cell><cell cols="2">NYU+SUNCG 69.8</cell><cell cols="8">83.1 61.1 19.3 94.8 28.0 12.2 19.6 57.0 50.5 17.6 11.9 35.6 15.3 32.9</cell></row><row><cell cols="2">TS3D [7]*</cell><cell>(240, 60)</cell><cell>NYU</cell><cell>-</cell><cell>-</cell><cell>60.0 9.7</cell><cell cols="6">93.4 25.5 21.0 17.4 55.9 49.2 17.0 27.5 39.4 19.3 34.1</cell></row><row><cell cols="2">SATNet-TNetFuse [16]*</cell><cell>(60, 60)</cell><cell cols="2">NYU+SUNCG 67.3</cell><cell cols="8">85.8 60.6 17.3 92.1 28.0 16.6 19.3 57.5 53.8 17.2 18.5 38.4 18.9 34.4</cell></row><row><cell cols="2">ForkNet [33]</cell><cell>(80, 80)</cell><cell>NYU</cell><cell>-</cell><cell>-</cell><cell cols="7">63.4 36.2 93.8 29.2 18.9 17.7 61.6 52.9 23.3 19.5 45.4 20.0 37.1</cell></row><row><cell cols="2">CCPNet [41]</cell><cell>(240, 240)</cell><cell>NYU</cell><cell>74.2</cell><cell cols="8">90.8 63.5 23.5 96.3 35.7 20.2 25.8 61.4 56.1 18.1 28.1 37.8 20.1 38.5</cell></row><row><cell>Ours*</cell><cell></cell><cell>(60, 60)</cell><cell>NYU</cell><cell>85.0</cell><cell>81.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Different Representations of Structure Prior. We also</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">perform ablation studies on different representations of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results on NYU dataset. Bold numbers represent the best scores. Resolution(a, b) means the input resolution is (a ? 0.6a ? a) and the output resolution is (b ? 0.6b ? b). '*' are RGB-D based methods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">scene completion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">semantic scene completion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Resolution</cell><cell>Trained on</cell><cell cols="15">prec. recall IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg.</cell></row><row><cell>Zheng et al. [43]</cell><cell>(240, 60)</cell><cell>NYUCAD</cell><cell>60.1</cell><cell cols="2">46.7 34.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Firman et al. [6]</cell><cell>(240, 60)</cell><cell>NYUCAD</cell><cell>66.5</cell><cell cols="2">69.7 50.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSCNet [27]</cell><cell>(240, 60)</cell><cell cols="2">NYUCAD+SUNCG 75.4</cell><cell cols="6">96.3 73.2 32.5 92.6 40.2 8.9</cell><cell cols="4">33.9 57.0 59.5 28.3</cell><cell>8.1</cell><cell cols="3">44.8 25.1 40.0</cell></row><row><cell>VVNetR-120 [9]</cell><cell>(120, 60)</cell><cell cols="2">NYUCAD+SUNCG 86.4</cell><cell cols="2">92.0 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DDRNet [14]*</cell><cell>(240, 60)</cell><cell>NYUCAD</cell><cell>88.7</cell><cell cols="10">88.5 79.4 54.1 91.5 56.4 14.9 37.0 55.7 51.0 28.8</cell><cell>9.2</cell><cell cols="3">44.1 27.8 42.8</cell></row><row><cell>TS3D [7]*</cell><cell>(240, 60)</cell><cell>NYUCAD</cell><cell>-</cell><cell>-</cell><cell cols="13">76.1 25.9 93.8 48.9 33.4 31.2 66.1 56.4 31.6 38.5 51.4 30.8 46.2</cell></row><row><cell>CCPNet [41]</cell><cell>(240, 240)</cell><cell>NYUCAD</cell><cell>91.3</cell><cell cols="14">92.6 82.4 56.2 94.6 58.7 35.1 44.8 68.6 65.3 37.6 35.5 53.1 35.2 53.2</cell></row><row><cell>Ours*</cell><cell>(60, 60)</cell><cell>NYUCAD</cell><cell>90.6</cell><cell cols="14">92.2 84.2 59.7 94.3 64.3 32.6 51.7 72.0 68.7 45.9 19.0 60.5 38.5 55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Results on NYUCAD dataset. Bold numbers represent the best scores. Resolution(a, b) means the input resolution is (a ? 0.6a ? a) and the output resolution is (b ? 0.6b ? b). '*' are RGB-D based methods.</figDesc><table><row><cell>Input</cell><cell cols="2">Shape Semantic Labels Sketch SC-IoU(%) SSC-mIoU(%)</cell></row><row><cell>TSDF+RGB</cell><cell>83.1</cell><cell>52.5</cell></row><row><cell>TSDF+RGB</cell><cell>82.6</cell><cell>53.2</cell></row><row><cell>TSDF+RGB</cell><cell>84.2</cell><cell>55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation studies on different representations of structure prior. We perform this ablation study on NYUCAD dataset.</figDesc><table><row><cell cols="4">Supervision Embedding SC-IoU(%) SSC-mIoU(%)</cell></row><row><cell>None</cell><cell>Implicit</cell><cell>81.1</cell><cell>50.6</cell></row><row><cell>Shape</cell><cell>Implicit Explicit</cell><cell>83.1 83.1</cell><cell>51.8 52.5</cell></row><row><cell>Semantic</cell><cell>Implicit Explicit</cell><cell>82.3 82.6</cell><cell>52.1 53.2</cell></row><row><cell>Sketch</cell><cell>Implicit Explicit</cell><cell>83.5 84.2</cell><cell>54.4 55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>From Row 1 and Row 4, we observe that TSDF generates better structure prior than RGB, resulting in a gain of 3.3% SC IoU. From Row 3 and Row 4, we observe that RGB generates more precise se-</figDesc><table><row><cell>RGB</cell><cell>Observed Surface</cell><cell cols="2">Sketch Ground Truth</cell><cell cols="2">Sketch w/o CVAE</cell><cell cols="2">Sketch with CVAE</cell><cell cols="3">SSC Ground Truth</cell><cell>SSC w/o CVAE</cell><cell>SSC with CVAE</cell></row><row><cell></cell><cell>Ceil</cell><cell>Floor</cell><cell>Wall</cell><cell>Window</cell><cell>Chair</cell><cell>Bed</cell><cell>Sofa</cell><cell>Table</cell><cell>TVs</cell><cell>Furn.</cell><cell>Objects</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5868" to="5877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aloisio</forename><surname>Dourado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teofilo</forename><surname>Emidio De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02893</idno>
		<title level="m">Semantic scene completion from rgb-d images</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A variational u-net for conditional appearance and shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8857" to="8866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Two stream 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint 3d object and layout inference from a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">View-volume network for semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJ-CAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bi-directional cascade network for perceptual edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Acquiring 3d indoor environments with variability and repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young Min Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Ming</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rgbd based dimensional decomposition residual network for 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">See and think: Disentangling semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shice</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beibei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3000" to="3009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A search-classify approach for cluttered indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Sharf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TOG</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">137</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discovering structural regularity in 3d geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Wallner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Pottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TOG</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An interactive approach to semantic modeling of indoor scenes with an rgbd camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">136</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><forename type="middle">Teja</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashast</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2325" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structure-preserving image super-resolution via contextualized multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2804" to="2815" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Feldman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A symmetry prior for convex variational 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Speciale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="313" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning 3d shape completion from laser scan data with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1955" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5229" to="5238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shape from symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wegbreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1824" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Forknet: Multi-branch volumetric semantic completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Joseph</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8608" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient semantic scene completion network with spatial group convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongen</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="733" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic scene completion with dense crf from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<editor>Syed Afaq Ali Shah, and Juan Song</editor>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="page" from="182" to="195" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cascaded context pyramid for full-resolution 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7801" to="7810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of object landmarks as structural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2694" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond point clouds: Scene understanding by reasoning geometry and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsushi</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3127" to="3134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
