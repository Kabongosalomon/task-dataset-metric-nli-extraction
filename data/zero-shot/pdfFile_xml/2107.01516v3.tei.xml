<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Introducing Self-Attention to Target Attentive Graph Neural Networks The Learning Machines surya.oju@pm.me 4 th Arshad Shaikh</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Java</surname></persName>
							<email>java.abhinav99@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byju&amp;apos;</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Technology</orgName>
								<address>
									<settlement>Tiruchirappalli</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Delhi Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">rd Surya Kant Sahu</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Introducing Self-Attention to Target Attentive Graph Neural Networks The Learning Machines surya.oju@pm.me 4 th Arshad Shaikh</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 st Sai Mitheran</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Session-based recommendation</term>
					<term>Graph neural networks</term>
					<term>Transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Session-based recommendation systems suggest relevant items to users by modeling user behavior and preferences using short-term anonymous sessions. Existing methods leverage Graph Neural Networks (GNNs) that propagate and aggregate information from neighboring nodes i.e., local message passing. Such graph-based architectures have representational limits, as a single sub-graph is susceptible to overfit the sequential dependencies instead of accounting for complex transitions between items in different sessions. We propose a new technique that leverages a Transformer in combination with a target attentive GNN. This allows richer representations to be learnt, which translates to empirical performance gains in comparison to a vanilla target attentive GNN. Our experimental results and ablation show that our proposed method is competitive with the existing methods on real-world benchmark datasets, improving on graph-based hypotheses. Code is available at https://github.com/The-Learning-Machines/SBR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Traditional recommendation systems use user-item interactions across multiple sessions to model user preferences. However, in session-based recommendation (SBR) systems, the users are anonymous; hence inter-session data cannot be used. The goal here is to predict the items with which the user is likely to interact, given previous item interactions within a single session. In general, the number of user-item interactions is limited, and as a result, modeling user intent is a challenging task. Nevertheless, session-based recommendation is gaining momentum due to increasing privacy concerns. Recent advancements in session-based recommendation systems have focused on modeling the user-item interaction as a directed graph and hence leverages graph-based architectures and related multi-level feature extraction techniques. However, these methods are disposed to representational limits <ref type="bibr" target="#b0">[1]</ref> as a sub-graph tends to overfit the sequential dependencies, while the essence of extracting the representations that connote complex transitions between multi-session items are lost.</p><p>In this work we present TAGNN++, incorporating Transformers as universal function approximators to enhance capturing complex transitions that address the limitations of GNNs in learning rich representations. We model item interactions using a GNN and both global and local user interaction with a Transformer. We show that our model is competitive with the existing state-of-the-art techniques on the Diginetica benchmark and on the Yoochoose benchmark. We also explore Adaptive Gradient Clipping <ref type="bibr" target="#b1">[2]</ref> for Transformer-based architectures specific to our task and present an ablation study to analyze the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Learning based SBRs</head><p>GNNs offer an intuitive approach to session-based recommendation since each session can be mapped into a graph's chain. Each node of the graph represents an item, and each edge represents an interaction. The natural compatibility between data modeled in such a manner and GNNs allow this method to perform well and was introduced in <ref type="bibr" target="#b2">[3]</ref>. Despite the convenient representation of sessions offered by GNNs, it lacks the ability to model long-range dependencies and intricate interactions as substantiated in Graph-Contextualised Self-Attention model <ref type="bibr" target="#b3">[4]</ref>. Qiu et al. <ref type="bibr" target="#b4">[5]</ref> proposed using a weighted graph attention layer for focusing on the essential parts of the item embeddings. Extending the idea of using attention with GNNs, Yu et al. <ref type="bibr" target="#b6">[6]</ref> proposed using both item embeddings with a GNN and target embedding with an attention mechanism to achieve significant performance gain. However, a GNN applied on a single sub-graph is susceptible to overfit on sequential dependencies instead of accounting for complex transitions between items in different sessions. Using a dual-channel GNN capable of complex item transition modeling addresses this issue <ref type="bibr" target="#b7">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Use of Adaptive Gradient Clipping (AGC)</head><p>Clipping the gradient is a commonly used approach to improving gradient descent <ref type="bibr" target="#b8">[8]</ref>, but manual selection of the clipping threshold increases the number of hyperparameters. However, the hyperparameter for gradient clipping needs to be tuned carefully, as it is susceptible to the loss function and architecture <ref type="bibr" target="#b9">[9]</ref>. The chosen threshold is vital as if it is set too high, then the gradient norm will always be smaller than that, and clipping is never applied. If too low, then the network's step size may be too small and cause convergence issues leading to unstable learning. We can clip gradients based on the unit-wise ratio of gradient norms to parameter norms <ref type="bibr" target="#b1">[2]</ref>. This helps ensure stable training across different batch sizes, allowing larger learning rates to ensure quick convergence, and is effective to tackle poorly conditioned loss landscapes. By incorporating this, we verify that performance is marginally increased for Transformer architectures specific to our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation and Preliminaries</head><p>The SBR problem contains three types of entities : Users, Items, and User-item interactions. Here, we formally introduce the SBR problem and associated notations.</p><formula xml:id="formula_0">Let S = [S 0 , S 1 , S 2 , ..., S N ?1 ] be the set of N sessions and V = [V 0 , V 1 , V 2 , ..., V M ?1 ] be the set of M unique items in the dataset. Let S k = [x 0 , x 1 , x 2 , ..</formula><p>., x t ] be the item sequence within a session S k , where x t ? V is the item clicked by a user at time step t = 0, 1, 2.... The goal is to estimate the parameter set ? such that p(x t+1 |x 0 ,</p><formula xml:id="formula_1">x 1 , x 2 , ..., x t ; ?) is the maximum-likelihood estimator of x t+1 , where k ? [0, N ? 1].</formula><p>The proposed TAGNN++ leverages two key components, namely -Graph Neural Networks and Self-Attention. We use the same background strategy for constructing the session graph from the given data as proposed in <ref type="bibr" target="#b6">[6]</ref> and <ref type="bibr" target="#b2">[3]</ref>. Each session is stored in memory as an adjacency matrix representing a directed graph. In this graph G, the i th node in the node set V represents an item such that n s,i ? V. Each edge connecting nodes i and j denotes subsequent item selections by the user in the given session s such that (n s,i , n s,j ) ? E <ref type="bibr" target="#b6">[6]</ref>. Lastly, each node has an incoming weight and an outgoing weight associated to it. We effectively capture item transitions and connections using a gated GNN <ref type="bibr" target="#b10">[10]</ref>, by learning both item and session embeddings as in <ref type="bibr" target="#b6">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed method -TAGNN++</head><p>We observe that a simple attention model is unable to capture both local and global context <ref type="bibr" target="#b11">[11]</ref>, hence indicating the need for a more robust way to represent sequences. Furthermore, GNNs build a representation of data by message passing or neighborhood aggregation, using only local information. Garg et al. show that even simple graph structures are indistinguishable by GNNs relying only on local information, making it hard for a GNN to compute several graph properties <ref type="bibr" target="#b0">[1]</ref>. Additionally, they perform a thorough study of generalization bounds for message passing, which accentuates the need for better function approximators. To that end, we propose a multi-headed Transformer-based design for target-aware predictions. The proposed design enables us to leverage both GNNs and powerful attention models; in essence, model item interaction in the form of a graph and a user's long and short-term interaction through the Transformer. We hypothesize that our design is a superior function approximator for the task of next-item prediction in session-based recommendation, providing empirical evidence and a carefully crafted ablative study. Recently, Transformers have been successfully used in Natural Language Processing and Computer Vision by parallelizing the self-attention <ref type="bibr" target="#b11">[11]</ref> mechanism over multiple heads achieving state of the art results. <ref type="bibr" target="#b12">[12]</ref> also gives a compact support based proof of why Transformers have high representation capacity and are universal approximators of permutation equivariant sequence to sequence functions.</p><p>At the l th layer the following update takes place in a single head:</p><formula xml:id="formula_2">h l+1 i = ?j?S ?( Q l h l i ? (K l h l j ) T ? d k )V l<label>(1)</label></formula><p>where h l j is the hidden state of the j th item in sequence S at layer l such that h l j ?j ? S. Q l , K l , V l are Queries, Keys and Values respectively, and ? denotes the softmax operation. Q l , K l , V l are learnable parameters that are updated in parallel as opposed to sequentially, and d k is a hyperparameter indicating the dimension of the linear layer in the Transformer. In our approach, we allow the Transformer to concatenate several representations or transformations of the interaction between the hidden states of the target and sequence. This is done using multiple such attention heads and the concatenation of the outputs before normalizing them across the layer, before being passed on to a Feed-forward module. At the feature level, it is possible that the values in the attention matrix cover a large domain, which is not desirable since the network finds it hard to learn the optimal parameter across different scales quickly. This is where Layer Normalization comes into play by normalizing across the feature space.</p><p>Finally, the token-wise Feed-forward Layer (FFL) transforms the normalized context vector to the output sequence. The composition of these FFLs implicitly implement a scalar quantization map such that each input is mapped to the output <ref type="bibr" target="#b12">[12]</ref>. We stress that the proposed augmentations to the TAGNN architecture make it more robust to different kinds of data streams. The various operations in the Transformer that include Self-attention, Layer Norm, and the Feed-forward Layer with skip connections play a vital role in enabling Transformers to be universal approximators of sequence-to-sequence functions. IV. SETUP AND EXPERIMENTAL RESULTS Evaluation Metrics: We use two metrics from previous studies, i.e., Hit Rate@N, and MRR@N, where N = 20. Hit Rate calculates the number of "hits" in an N-sized list of ranked items, where a "hit" refers to something that the user has clicked on, purchased, or "saved for later", based on context. Mean reciprocal rank (MRR) is used to judge a system where the order/positions of the retrieved items are important.</p><p>Datasets: For testing our hypothesis, we employ two widely used real-world datasets, Yoochoose 2 and Diginetica 1 . <ref type="table" target="#tab_0">Table  III</ref> provides information on the dataset contents. We use only the recent 1/64 fraction of the Yoochoose dataset, denoted as Yoochoose 1/64, which comprises various sessions that specify the clicks by a given user. It is a collection of records in a file containing a Session ID, Timestamp, Item ID, and Category. Diginetica contains anonymized search and browsing logs, product data, anonymized transactions, and an extensive collection of product images.</p><p>Preprocessing: For simplicity, we apply the same preprocessing as <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b2">[3]</ref>. We drop all unit length sessions and remove items that appear less than five times for both datasets as same as previous studies. For generating training and test sets, sessions of last days are used as the test set for Yoochoose 1/64, and sessions of last weeks as the test set for Diginetica. For an existing session, we generate a series of input session sequences and corresponding labels. We filter out items from the test set which do not appear in the training set.</p><p>Hyperparameters: We retain most hyperparameter settings from previous baselines to display the advantage of learning better representations using Transformers. We keep 10% of our datasets for validation and use a batch size of 50, for 15 epochs. We use the Adam <ref type="bibr" target="#b18">[18]</ref> optimizer with an initial learning rate of 10 ?4 , with momentum parameters ? 1 = 0.9, ? 2 = 0.999, and decay it by a factor of 0.1 every 3 epochs. Additionally, we set the L2 penalty to 10 ?6 . For Multi-Head Attention in the Transformer, we set the number of heads as 2, 8 and dropout of 0.1, with the embedding dimension as 100, 120 for Yoochoose 1/64 and Diginetica respectively.</p><p>Baselines: We compare our method with baseline GNN and Attention-based methods for session-based recommendation, as shown in <ref type="table" target="#tab_0">Table I</ref>. The RNN-based methods <ref type="bibr" target="#b13">[13]</ref> [14] <ref type="bibr" target="#b16">[16]</ref>, and further <ref type="bibr" target="#b15">[15]</ref> that takes repeat consumption into account were then outperformed by graph-based methods <ref type="bibr" target="#b2">[3]</ref>, those involving the notion of attention <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b6">[6]</ref>. Inferences: We can infer from the values of HR@20 and MRR@20 that our method is competitive with the previous state-of-the-art on the given benchmarks, whilst outperforming the graph-based TAGNN model <ref type="bibr" target="#b6">[6]</ref>. Hence, it is evident that Deep learning-based methods are more capable of learning a better user-item interaction representation by capturing complex data distributions and transitions in the dataset. Incorporating Transformers to overcome the representational limitations in a GNN improves the performance of our recommendation system. The improvement observed verifies that learning better representations as proposed is helpful to model complex patterns, as shown in <ref type="figure">Fig. 3</ref>. To verify the efficacy of our architecture, we also perform an ablation study as shown in <ref type="table" target="#tab_0">Table II</ref>. It can be observed that the performance is only marginally affected by the removal of secondary techniques such as AGC (Adaptive Gradient Clipping) and PE (Positional Embedding). Further, the removal of the Transformer decreases the overall performance, verifying that GNNs fail at learning accurate representations in some cases. <ref type="figure">Fig. 3</ref> indicates better performance on Yoochoose 1/64 than Diginetica, due to a greater average session length in the former -as more historical data would enhance the Transformer's attention mechanism to leverage global context. Both datasets consist of short, medium, and long sessions <ref type="bibr" target="#b19">[19]</ref>. When the user wishes to build a session-based recommender consisting of long sessions (Yoochoose 1/64) with better rank/order as the main objective (tweets, webpages, music), we show that our model outperforms existing methods by a considerable margin. For short/medium length sessions (Diginetica), where both order and the number of desired items in the top N of the ranked list are important (shopping items, movies/videos), our model is competitive with the existing methods. <ref type="figure">Fig. 3</ref>. Scatter plot comparison of models on the evaluation metrics V. CONCLUSION This paper proposes an alternative method for learning richer representations in session-based recommendation models, to overcome the limitations posed by message-passing GNNs. We perform intelligent feature extraction using Transformers with target attentive GNNs. We leverage Multi-head Attention to capture both local and global context. Our method is competitive with the previous state-of-the-art on the aforementioned benchmarks. We show that our method is suitable for rank-based retrieval in long sessions (Yoochoose), and also establish competitive performance for short/medium length sessions (Diginetica). Motivating research in this direction would enable making informative and practical choices in streaming, business operations, and E-Commerce.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Session-based Recommendation Systems</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Proposed Architecture : TAGNN++</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF SEVERAL BASELINE METHODS WITH THE PROPOSED METHOD.</figDesc><table><row><cell>Method</cell><cell cols="2">Yoochoose 1/64</cell><cell></cell><cell>Diginetica</cell></row><row><cell></cell><cell cols="4">HR@20 MRR@20 HR@20 MRR@20</cell></row><row><cell cols="2">GRU4REC [13] 60.64</cell><cell>22.89</cell><cell>29.45</cell><cell>8.33</cell></row><row><cell>NARM [14]</cell><cell>68.32</cell><cell>28.63</cell><cell>49.70</cell><cell>16.17</cell></row><row><cell>RepeatNet [15]</cell><cell>70.71</cell><cell>31.03</cell><cell>47.79</cell><cell>17.66</cell></row><row><cell>CSRM [16]</cell><cell>71.45</cell><cell>30.36</cell><cell>50.55</cell><cell>16.38</cell></row><row><cell>SR-GNN [3]</cell><cell>70.87</cell><cell>30.94</cell><cell>50.73</cell><cell>17.59</cell></row><row><cell>GC-SAN [4]</cell><cell>70.66</cell><cell>30.04</cell><cell>51.70</cell><cell>17.61</cell></row><row><cell>TAGNN [6]</cell><cell>71.02</cell><cell>31.12</cell><cell>51.31</cell><cell>18.03</cell></row><row><cell>LESSR [17]</cell><cell>70.64</cell><cell>30.97</cell><cell>51.71</cell><cell>18.15</cell></row><row><cell>TAGNN++</cell><cell>71.91</cell><cell>31.57</cell><cell>51.86</cell><cell>17.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">ABLATION STUDY</cell><cell></cell><cell></cell></row><row><cell>Architecture</cell><cell cols="2">Yoochoose 1/64</cell><cell></cell><cell>Diginetica</cell><cell></cell></row><row><cell></cell><cell cols="5">HR@20 MRR@20 HR@20 MRR@20</cell></row><row><cell>TAGNN++</cell><cell>71.91</cell><cell>31.57</cell><cell>51.86</cell><cell>17.93</cell><cell></cell></row><row><cell>-AGC</cell><cell>71.80</cell><cell>31.41</cell><cell>51.57</cell><cell>17.65</cell><cell></cell></row><row><cell>-GNN</cell><cell>71.75</cell><cell>31.62</cell><cell>51.48</cell><cell>17.59</cell><cell></cell></row><row><cell>-PE</cell><cell>71.69</cell><cell>31.48</cell><cell>51.64</cell><cell>17.66</cell><cell></cell></row><row><cell cols="2">-Transformer 71.03</cell><cell>30.69</cell><cell>51.42</cell><cell>17.84</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">DATASET STATS</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Clicks</cell><cell>Train Sessions</cell><cell>Test Sessions</cell><cell>Total Items</cell><cell>Avg. Length</cell></row><row><cell>Diginetica</cell><cell>982961</cell><cell>719470</cell><cell>68977</cell><cell cols="2">43074 5.13</cell></row><row><cell cols="3">Yoochoose 1/64 557248 375043</cell><cell>55898</cell><cell cols="2">16339 6.11</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="3419" to="3430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/3804" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence</title>
		<editor>P. V. Hentenryck and Z.-H. Zhou</editor>
		<meeting>the Twenty-Third AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph contextualized self-attention network for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/547</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/547" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3940" to="3946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the item order in session-based recommendation with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, ser. CIKM &apos;19</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, ser. CIKM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3357384.3358010</idno>
		<ptr target="https://doi.org/10.1145/3357384.3358010" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tagnn: Target attentive graph neural networks for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1921" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dgtn: Dual-channel graph transition network for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Data Mining Workshops (ICDMW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="236" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Why gradient clipping accelerates training: A theoretical justification for adaptivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jadbabaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Optimization and Control</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Autoclip: Adaptive gradient clipping for source separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 30th International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Are transformers universal approximators of sequence-to-sequence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10077</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sessionbased recommendations with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural attentive session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Repeatnet: A repeat aware neural recommendation machine for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4806" to="4813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A collaborative session-based recommendation approach with parallel memory modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331210</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331210" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, ser. SIGIR&apos;19</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, ser. SIGIR&apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Handling information loss of graph neural networks for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1172" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Da</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on session-based recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1902.04864</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
