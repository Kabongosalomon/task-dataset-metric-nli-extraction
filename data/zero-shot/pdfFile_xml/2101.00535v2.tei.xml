<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RV-GAN: Segmenting Retinal Vascular Structure in Fundus Photographs using a Novel Multi-scale Generative Adversarial Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharif</forename><forename type="middle">Amit</forename><surname>Kamran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khondker</forename><surname>Fariha Hossain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Tavakkoli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Lee Zuckerbrod</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Houston Eye Associates</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><forename type="middle">M</forename><surname>Sanders</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RV-GAN: Segmenting Retinal Vascular Structure in Fundus Photographs using a Novel Multi-scale Generative Adversarial Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Retinal Vessel Segmentation ? Generative Networks ? Medical Imag- ing ? Opthalmology ? Retinal Fundus</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High fidelity segmentation of both macro and microvascular structure of the retina plays a pivotal role in determining degenerative retinal diseases, yet it is a difficult problem. Due to successive resolution loss in the encoding phase combined with the inability to recover this lost information in the decoding phase, autoencoding based segmentation approaches are limited in their ability to extract retinal microvascular structure. We propose RV-GAN, a new multi-scale generative architecture for accurate retinal vessel segmentation to alleviate this. The proposed architecture uses two generators and two multi-scale autoencoding discriminators for better microvessel localization and segmentation. In order to avoid the loss of fidelity suffered by traditional GAN-based segmentation systems, we introduce a novel weighted feature matching loss. This new loss incorporates and prioritizes features from the discriminator's decoder over the encoder. Doing so combined with the fact that the discriminator's decoder attempts to determine real or fake images at the pixel level better preserves macro and microvascular structure. By combining reconstruction and weighted feature matching loss, the proposed architecture achieves an area under the curve (AUC) of 0.9887, 0.9914, and 0.9887 in pixel-wise segmentation of retinal vasculature from three publicly available datasets, namely DRIVE, CHASE-DB1, and STARE, respectively. Additionally, RV-GAN outperforms other architectures in two additional relevant metrics, mean intersection-over-union (Mean-IOU) and structural similarity measure (SSIM).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The fundoscopic exam is a procedure that provides necessary information to diagnose different retinal degenerative diseases such as Diabetic Retinopathy, Macular Edema, Cytomegalovirus Retinitis <ref type="bibr" target="#b26">[27]</ref>. A highly accurate system is required to segment retinal vessels and find abnormalities in the retinal subspace to diagnose these vascular diseases. Many image processing and machine learning-based approaches for retinal vessel segmentation have so far been proposed <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b12">13]</ref>. However, such methods fail to arXiv:2101.00535v2 [eess.IV] 14 May 2021 <ref type="figure">Fig. 1</ref>: RV-GAN segments vessel with better precision than other architectures. The 1st row is the whole image, while 2nd row is specific zoomed-in area of the image. The Red bounded box signifies the zoomed-in region. Here, the confidence score is, t &gt; 0.5. The row contains DRIVE, CHASE-DB1 and STARE data-set. Whereas the column contains fundus images, ground-truths and segmentation maps for RV-GAN, DFUNet, IterNet and UNet. precisely pixel-wise segment blood vessels due to insufficient illumination and periodic noises. Attributes like this present in the subspace can create false-positive segmentation <ref type="bibr" target="#b6">[7]</ref>. In recent times, UNet based deep learning architectures have become very popular for retinal vessel segmentation. UNet consists of an encoder to capture context information and a decoder for enabling precise localization <ref type="bibr" target="#b23">[24]</ref>. Many derivative works based on UNet have been proposed, such as Dense-UNet, Deformable UNet <ref type="bibr" target="#b9">[10]</ref>, IterNet <ref type="bibr" target="#b15">[16]</ref> etc. These models were able to achieve quite good results for macro vessel segmentation. However, these architectures fail when segmenting microvessels with higher certainty. One reason is successive resolution loss in the encoder, and failure to capture those features in the decoder results in inferior microvessel segmentation. Recent GAN-based architecture <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32]</ref> tries to address this by incorporating discriminative features from adversarial examples while training. However, the discriminator being an encoder <ref type="bibr" target="#b8">[9]</ref>, only trains on patches of images rather than pixels, affecting the true-positive-rate of the model. We need an architecture that can retain discriminative manifold features and segment microvessels on pixel-level with higher confidence. Con- fidence signifies the probability distribution function of the segmented pixel falling under vessel or background. By taking all of these into account, we propose Retinal-Vessel GAN, consisting of coarse and fine generators and multi-scale autoencoder-based discriminators for producing highly accurate segmentation of blood vessel with strong confidence scores. Additionally, we come up with a new weighted feature matching loss with inner and outer weights. And we combine it with reconstruction and hinge loss for adversarial training of our architecture. From <ref type="figure">Fig. 1</ref>, it is apparent that our architecture produces a segmentation map with a high confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-scale Generators</head><p>Pairing multi-scale coarse and fine generators produces high-quality domain-specific retinal image synthesis, as observed in recent generative networks, such as Fundus2Angio <ref type="bibr" target="#b11">[12]</ref>, V-GAN <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b28">[29]</ref>. Inspired by this, we also adopt this feature in our architecture by using two generators (G f ine and G coarse ), as visualized in <ref type="figure" target="#fig_0">Fig. 2</ref>. The generator G f ine synthesizes fine-grained vessel segmentation images by extracting local features such as micro branches, connections, blockages, etc. In contrast, the generator G coarse tries to learn and conserve global information, such as the structures of the maco branches, while producing less detailed microvessel segmentation. The detailed structure of these generators is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Residual Downsampling and Upsampling Blocks</head><p>Our generators and discriminators consist of both downsampling and upsampling blocks to get the desired feature maps and output. The downsampling block comprises of a convolution layer, a batch-norm layer and a Leaky-ReLU activation function consecutively and is illustrated in <ref type="figure" target="#fig_1">Fig. 3(a)</ref>. Contrarily, the decoder block consists of a transposed convolution layer, batch-norm, and Leaky-ReLU activation layer successively and can be visualized in <ref type="figure" target="#fig_1">Fig.3</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Distinct Identity Blocks for Generator &amp; Discriminator</head><p>For spatial and depth feature propagation, residual identity blocks have become goto building blocks for image style transfer, image inpainting, and image segmentation tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref>.Vanilla convolution layers are both computationally inefficient and fail to retain accurate spatial and depth information, as opposed to separable convolution <ref type="bibr" target="#b4">[5]</ref>. Separable convolution comprises of a depth-wise convolution and a point-wise convolution successively. As a result, it extracts and preserves depth and spatial features while forward propagating the network. Recent advancement in retinal image classification has shown that combining separable convolutional layers with dilation allows for more robust feature extraction <ref type="bibr" target="#b10">[11]</ref>. We design two unique residual identity blocks, for our generators and discriminators, as illustrated in <ref type="figure" target="#fig_1">Fig. 3(d</ref> <ref type="figure" target="#fig_1">Fig. 3</ref>(e).</p><formula xml:id="formula_0">) &amp;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Spatial Feature Aggregation</head><p>In this section, we discuss our proposed spatial feature aggregation (SFA) block, as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>(c). We use spatial feature aggregation block for combining spatial and depth features from the bottom layers of the network with the top layers of the network, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The rationale behind employing the SFA block is to extract and retain spatial and depth information, that is otherwise lost in deep networks. Consequently, these features can be combined with the learned features of the deeper layers of the network to get an accurate approximation, as observed in similar GAN architectures. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Auto-encoder as Discriminators</head><p>For better pixel-wise segmentation, we need an architecture that can extract both global and local features from the image. To mitigate this underlying problem, we need a deep and dense architecture with lots of computable parameters. It, in turn, might lead to overfitting or vanishing gradient while training the model. To address this issue, rather than having a single dense segmentation architecture, we adopt light-weight discriminators as autoencoders. Additionally, we use multi-scale discriminators for both our coarse and fine generators, as previously proposed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>. The arrangement consists of two discriminators with variable sized input and can help with the overall adversarial training.We define two discriminators as D f and D c as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Proposed Weighted Feature Matching Loss</head><p>Feature matching loss <ref type="bibr" target="#b29">[30]</ref> was incorporated by extracting features from discriminators to do semantic segmentation. The feature-matching loss is given in Eq. 1 As the authors used Patch-GAN as a discriminator, it only contains an encoding module. By contrast, our work involves finding pixel-wise segmentation of retinal vessel and background and thus requires an additional decoder. By successive downsampling and upsampling, we lose essential spatial information and features; that's why we need to give weightage to different components in the overall architecture. We propose a new weighted feature matching loss, as given in Eq. 2 that combines elements from both encoder and decoder and prioritizes particular features to overcome this. For our case, we experiment and see that giving more weightage to decoder feature map results in better vessel segmentation.</p><formula xml:id="formula_1">L f m (G, D enc ) = E x,y 1 N k i=1 D i enc (x, y) ? D i enc (x, G(x)) (1) L wf m (G, D n ) = E x,y 1 N k i=1 ? i enc D i enc (x, y)?D i enc (x, G(x)) +? i dec D i dec (x, y) ? D i dec (x, G(x))<label>(2)</label></formula><p>Eq. 2 is calculated by taking the features from each of the downsampling blocks and upsampling blocks of the discriminator's encoder and decoder. We insert the real and the synthesized segmentation maps consecutively. The N signifies the number of features.</p><p>Here, ? enc and ? dec is the inner weight multiplier for each of the extracted feature maps.</p><p>The weight values are between [0, 1], and the total sum of the weight is 1, and we use a higher weight value for the decoder feature maps than the encoder feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Weighted Objective and Adversarial Loss</head><p>For adversarial training, we use Hinge-Loss <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18]</ref> as given in Eq. 3 and Eq. 4. Conclusively, all the fundus images and their corresponding segmentation map pairs are normalized, to [?1, 1]. As a result, it broadens the difference between the pixel intensities of the real and synthesized segmentation maps. In Eq. 5, we multiply L adv (G) with ? adv as weight multiplier. Next, we add L adv (D) with the output of the multiplication.</p><formula xml:id="formula_2">L adv (D) = ?E x,y min(0, ?1 + D(x, y)) ? E x min(0, ?1 ? D(x, G(x))) (3) L adv (G) = ?E x,y (D(G(x), y)) (4) L adv (G, D) = L adv (D) + ? adv (L adv (G))<label>(5)</label></formula><p>In Eq. 4 and Eq. 5, we first train the discriminators on the real fundus, x and real segmentation map, y. After that, we train with the real fundus, x, and synthesized segmentation map, G(x). We begin by batch-wise training the discriminators D f , and D c for a couple of iterations on the training data. Following that, we train the G c while keeping the weights of the discriminators frozen. In a similar fashion, we train G f on a batch training image while keeping weights of all the discriminators frozen. The generators also incorporate the reconstruction loss (Mean Squared Error) as shown in Eq. 6. By utilizing the loss we ensure the synthesized images contain more realistic microvessel, arteries, and vascular structure.</p><formula xml:id="formula_3">L rec (G) = E x,y G(x) ? y 2<label>(6)</label></formula><p>By incorporating Eq. 2, 5 and 6, we can formulate our final objective function as given in Eq. 7.</p><formula xml:id="formula_4">min G f ,Gc max D f ,Dc (L adv (G f , G c , D f , D c ))+ ? rec L rec (G f , G c ) + ? wf m L wf m (G f , G c , D f , D c )<label>(7)</label></formula><p>Here, ? adv , ? rec , and ? wf m implies different weights, that is multiplied with their respective losses. The loss weighting decides which architecture to prioritize while training. For our system, more weights are given to the L adv (G), L rec , L wf m , and thus we select bigger ? values for those. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyper-parameter Initialization</head><p>For adversarial training, we used hinge loss <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18]</ref>. We picked ? enc = 0.4 (Eq. 1), ? dec = 0.6 (Eq. 2), ? adv = 10 (Eq. 5), ? rec = 10 (Eq. 6) and ? wf m = 10 (Eq. 7). We used Adam optimizer <ref type="bibr" target="#b13">[14]</ref>, with learning rate ? = 0.0002, ? 1 = 0.5 and ? 2 = 0.999. We train with mini-batches with batch size, b = 24 for 100 epochs in three stages using Tensorflow. It took between 24-48 hours to train our model on NVIDIA P100 GPU depending on data-set. Because DRIVE and STARE have lower number of patches compared to CHASE-DB1, it takes less amount to train. The inference time is 0.025 second per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Threshold vs. Confidence Score</head><p>Confidence score signifies the per-pixel probability density value of the segmentation map. U-Net-derived models incorporate binary cross-entropy loss with a threshold of 0.5 to predict if the pixel is a vessel or background. So a pixel, predicted with a 0.5001 probability, will be classified as a vessel. As a result, the model suffers from Type I error or a high false-positive rate (FPR). In contrast, we use the generators to produce realistic segmentation maps and utilize the weighted feature matching loss to combine inherent manifold information to predict real and fake pixels with higher certainty. Consequently, we can see in <ref type="figure" target="#fig_2">Fig. 4</ref> that our model's Receiver Operating (ROC) curves for three datasets are relatively better than other previous methods due to a high confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Quantitative Bench-marking</head><p>We compared our architecture with some best performing ones, including UNet <ref type="bibr" target="#b9">[10]</ref>, DenseBlock-UNet <ref type="bibr" target="#b16">[17]</ref>, Deform-UNet <ref type="bibr" target="#b9">[10]</ref> and IterNet <ref type="bibr" target="#b15">[16]</ref> as illustrated in <ref type="figure">Fig. 1</ref>. We trained and evaluated the first three architectures using their publicly available source code by ourselves on the three datasets. For IterNet, the pre-trained weight was provided, so we used that to get the inference result. Next, we do a comparative analysis with existing retinal vessel segmentation architectures, which includes both UNet and GAN based models. The prediction results for DRIVE, CHASE-DB1 and STARE are provided in <ref type="table">Table.</ref> 1. We report traditional metrics such as F1-score, Sensitivity, Specificity, Accuracy, and AUC-ROC. Additionally, we use two other metrics for predicting accurate segmentation and structural similarity of the retinal vessels, namely Mean-IOU (Jaccard Similarity Coefficient) and Structural Similarity Index <ref type="bibr" target="#b30">[31]</ref>. We chose Mean-IOU because its the gold standard for measuring segmentation results for many Semantic Segmentation Challenges such as Pascal-VOC2012 <ref type="bibr" target="#b5">[6]</ref>, MS-COCO <ref type="bibr" target="#b18">[19]</ref>. Contrarily, SSIM is a standard metric for evaluating GANs for image-to-image translation tasks. As illustrated in all the tables, our model outperforms both UNet derived architectures and recent GAN based models in terms of AUC-ROC, Mean-IOU, and SSIM, the three main metrics for this task. M-GAN achieves better Specificity and Accuracy in CHASE-DB1 and STARE. However, higher Specificity means better background pixel segmentation (True Negative), which is less essential than having better retinal vessel segmentation (True Positive). We want both, better Sensitivity and AUC-ROC, which equates to having a higher confidence score. In <ref type="figure" target="#fig_2">Fig. 4</ref> we can see that our True positive Rate is always better than other architectures for all three data-set. We couldn't report SSIM and Mean-IOU for some of the architectures as source codes and pre-trained, weights weren't provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a new multi-scale generative architecture called RV-GAN. By combining our novel featuring matching loss, the architecture synthesizes precise venular structure segmentation with high confidence scores for two relevant metrics. As a result, we can efficiently employ this architecture in various applications of ophthalmology. The model is best suited for analyzing retinal degenerative diseases and monitoring future prognosis. We hope to extend this work to other data modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>RV-GAN consisting of Coarse and Fine generators G f , G c and discriminators D f , D c . The generators incorporates reconstruction loss, Loss rec and Hinge loss Loss G . Whereas the discriminators uses weighted feature matching loss, Loss wf m and Hinge loss Loss D . All of these losses are multiplied by weight multiplier and then added in the final adversarial loss, Loss adv . The generators consists of Downsampling, Upsampling, SFA and its distinct residual blocks. On the other hand, the discriminators consists of Downsampling, Upsampling and counterpart residual blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Proposed Downsampling, Upsampling, Spatial Feature Aggregation block, Generator and Discriminator Residual blocks. Here, K=Kernel size, S=Stride, D=Dilation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>ROC Curves on (a) DRIVE (b) STARE (c) CHASE-DB1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on DRIVE<ref type="bibr" target="#b27">[28]</ref>, CHASE-DB1<ref type="bibr" target="#b19">[20]</ref>, &amp; STARE<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="8">Year F1 Score Sensitivity Specificity Accuracy AUC-ROC Mean-IOU SSIM</cell></row><row><cell></cell><cell>UNet [10]</cell><cell cols="3">2018 0.8174 0.7822</cell><cell>0.9808</cell><cell>0.9555</cell><cell>0.9752</cell><cell cols="2">0.9635 0.8868</cell></row><row><cell></cell><cell cols="4">Residual UNet [1] 2018 0.8149 0.7726</cell><cell>0.9820</cell><cell>0.9553</cell><cell>0.9779</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="4">Recurrent UNet [1] 2018 0.8155 0.7751</cell><cell>0.9816</cell><cell>0.9556</cell><cell>0.9782</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>R2UNet [1]</cell><cell cols="3">2018 0.8171 0.7792</cell><cell>0.9813</cell><cell>0.9556</cell><cell>0.9784</cell><cell>-</cell><cell>-</cell></row><row><cell>DRIVE</cell><cell>DFUNet [10]</cell><cell cols="3">2019 0.8190 0.7863</cell><cell>0.9805</cell><cell>0.9558</cell><cell>0.9778</cell><cell cols="2">0.9605 0.8789</cell></row><row><cell></cell><cell>IterNet [16]</cell><cell cols="3">2019 0.8205 0.7735</cell><cell>0.9838</cell><cell>0.9573</cell><cell>0.9816</cell><cell cols="2">0.9692 0.9008</cell></row><row><cell></cell><cell>SUD-GAN [32]</cell><cell>2020</cell><cell>-</cell><cell>0.8340</cell><cell>0.9820</cell><cell>0.9560</cell><cell>0.9786</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>M-GAN [21]</cell><cell cols="3">2020 0.8324 0.8346</cell><cell>0.9836</cell><cell>0.9706</cell><cell>0.9868</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="4">RV-GAN (Ours) 2020 0.8690 0.7927</cell><cell>0.9969</cell><cell>0.9790</cell><cell>0.9887</cell><cell cols="2">0.9762 0.9237</cell></row><row><cell></cell><cell>UNet [10]</cell><cell cols="3">2018 0.7993 0.7841</cell><cell>0.9823</cell><cell>0.9643</cell><cell>0.9812</cell><cell cols="2">0.9536 0.9029</cell></row><row><cell></cell><cell cols="4">DenseBlock-UNet [17] 2018 0.8006 0.8178</cell><cell>0.9775</cell><cell>0.9631</cell><cell>0.9826</cell><cell cols="2">0.9454 0.8867</cell></row><row><cell>CHASE-DB1</cell><cell>DFUNet [10] IterNet [16]</cell><cell cols="3">2019 0.8001 0.7859 2019 0.8073 0.7970</cell><cell>0.9822 0.9823</cell><cell>0.9644 0.9655</cell><cell>0.9834 0.9851</cell><cell cols="2">0.9609 0.9175 0.9584 0.9123</cell></row><row><cell></cell><cell>M-GAN [21]</cell><cell cols="3">2020 0.8110 0.8234</cell><cell>0.9938</cell><cell>0.9736</cell><cell>0.9859</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="4">RV-GAN (Ours) 2020 0.8957 0.8199</cell><cell>0.9806</cell><cell>0.9697</cell><cell>0.9914</cell><cell cols="2">0.9705 0.9266</cell></row><row><cell></cell><cell>UNet [10]</cell><cell cols="3">2018 0.7595 0.6681</cell><cell>0.9915</cell><cell>0.9639</cell><cell>0.9710</cell><cell cols="2">0.9744 0.9271</cell></row><row><cell></cell><cell cols="4">DenseBlock-UNet [17] 2018 0.7691 0.6807</cell><cell>0.9916</cell><cell>0.9651</cell><cell>0.9755</cell><cell cols="2">0.9604 0.9034</cell></row><row><cell></cell><cell>DFUNet [10]</cell><cell cols="3">2019 0.7629 0.6810</cell><cell>0.9903</cell><cell>0.9639</cell><cell>0.9758</cell><cell cols="2">0.9701 0.9169</cell></row><row><cell>STARE</cell><cell>IterNet [16]</cell><cell cols="3">2019 0.8146 0.7715</cell><cell>0.9886</cell><cell>0.9701</cell><cell>0.9881</cell><cell cols="2">0.9752 0.9219</cell></row><row><cell></cell><cell>SUD-GAN [32]</cell><cell>2020</cell><cell>-</cell><cell>0.8334</cell><cell>0.9897</cell><cell>0.9663</cell><cell>0.9734</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>M-GAN [21]</cell><cell cols="3">2020 0.8370 0.8234</cell><cell>0.9938</cell><cell>0.9876</cell><cell>0.9873</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="4">RV-GAN (Ours) 2020 0.8323 0.8356</cell><cell>0.9864</cell><cell>0.9754</cell><cell>0.9887</cell><cell cols="2">0.9754 0.9292</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06955</idno>
		<title level="m">Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-gan for object transfiguration in wild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="164" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blood vessel segmentation methodologies in retinal images-a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uyyanonvara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Barman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="407" to="433" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dunet: A deformable network for retinal vessel segmentation. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="149" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optic-net: A novel convolutional neural network for diagnosis of retinal diseases from optical tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Sabbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th IEEE International Conference On Machine Learning And Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="964" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fundus2angio: A conditional gan architecture for generating fluorescein angiography images from retinal fundus photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuckerbrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Sanders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="125" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving robustness using joint attention network for detecting retinal degeneration from optical coherence tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Zuckerbrod</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iternet: Retinal image segmentation utilizing structural redundancy in vessel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kawasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3656" to="3665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<title level="m">Geometric gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring retinal vessel tortuosity in 10-year-old children: validation of the computer-assisted image analysis of the retina (caiar) program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Monekosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Whincup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investigative ophthalmology &amp; visual science</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2004" to="2010" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">M-gan: Retinal blood vessel segmentation by balancing losses through stacked deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Retinal blood vessel segmentation using line operators and support vector classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perfetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1357" to="1365" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4570" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2-d gabor wavelet and supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on medical Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Jung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09318</idno>
		<title level="m">Retinal vessel segmentation in fundoscopic images with generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abr?moff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A novel deep learning conditional generative adversarial network for producing angiography images from retinal fundus photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Zuckerbrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sud-gan: Deep convolution generative adversarial network combined with short connection and dense block for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
