<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTI-TASK VOICE ACTIVATED FRAMEWORK USING SELF-SUPERVISED LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzeen</forename><surname>Hussain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm Technologies, Inc</orgName>
								<address>
									<settlement>San Diego California</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhua</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm Technologies, Inc</orgName>
								<address>
									<settlement>San Diego California</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Visser</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm Technologies, Inc</orgName>
								<address>
									<settlement>San Diego California</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MULTI-TASK VOICE ACTIVATED FRAMEWORK USING SELF-SUPERVISED LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-keyword spotting</term>
					<term>speaker verification</term>
					<term>wav2vec</term>
					<term>self-supervised</term>
					<term>multi-task learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning methods such as wav2vec 2.0 have shown promising results in learning speech representations from unlabelled and untranscribed speech data that are useful for speech recognition. Since these representations are learned without any task-specific supervision, they can also be useful for other voice activated tasks like speaker verification, keyword spotting, emotion classification etc. In our work, we propose a general purpose framework for adapting a pre-trained wav2vec 2.0 model for different voice activated tasks. We develop downstream network architectures that operate on the contextualized speech representations of wav2vec 2.0 to adapt the representations for solving a given task. Finally, we extend our framework to perform multi-task learning by jointly optimizing the network parameters on multiple voice activated tasks using a shared transformer backbone. Both of our single and multi-task frameworks achieve stateof-the-art results in speaker verification and keyword spotting benchmarks. Our best performing models achieve 1.98% and 3.15% EER on VoxCeleb1 test set when trained on Vox-Celeb2 and VoxCeleb1 respectively, and 98.23% accuracy on Google Speech Commands v1.0 keyword spotting dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Currently, the best performing solutions for voice activated tasks such as keyword spotting (KWS), speaker verification (SV), emotion classification, language identification etc. require training deep neural networks on large domain-specific and labeled speech datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. The most commonly used models typically operate on the mel-spectrogram representation of audio. Such models require task-specific engineering techniques like tuning the window and hop size of the short-time Fourier transform (STFT), determining the number of mel bins and various denoising transformations as input pre-processing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Additionally, since melspectrogram is a compressed audio representation, it may discard information that can potentially be useful for discriminating on voice activated tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* Work performed as an intern at Qualcomm Technologies</head><p>Recently proposed self-supervised learning methods such as wav2vec 2.0 <ref type="bibr" target="#b7">[8]</ref>, can learn useful speech representations directly from unlabeled/un-transcribed speech data. The authors of wav2vec 2.0 found a strong correlation between the learned representations and phonemes, and demonstrated that these learned representations can achieve state-of-the-art results on speech-to-text benchmarks with much less amount of labelled data as compared to prior work. However, these representations could also be useful for other voice activated tasks besides speech recognition, and can potentially address the limitations of spectrogram representations when performing voice activated tasks. Solving voice activated tasks such as keyword detection and speaker verification using learned representations from self-supervised learning can reduce the amount of labeled data required for training and also result in better performing models. Moreover, since these speech representations are derived directly from the waveform, it simplifies the detection and classification pipeline by providing an end-to-end solution.</p><p>In this work, we develop a framework for multi-task voice activated systems based on contextualized speech representations learned from self-supervised models. Our framework consists of two major components 1) Speech Representation Extractor (SRE): A self-supervised model (in our case wav2vec 2.0) that provides contextualized speech representations directly from a waveform. 2) Downstream Neural Network: To guide the training of SRE and shape the representations into solving multiple voice activated tasks such as KWS and SV.</p><p>In our framework, the SRE is first pre-trained in a selfsupervised manner on a large unlabelled speech corpus. Then the downstream networks and SRE are jointly optimized on single or multiple tasks using supervised learning. Our results demonstrate the representations learnt using self-supervised training can boost KWS and SV performance substantially. Our main contributions are as follows:</p><p>? We achieve state-of-the-art performance on KWS and SV tasks and outperform previous works using standard optimization objectives for both tasks.</p><p>? We propose a multi-task training strategy for our framework on disjoint datasets for different tasks. Our multitask training framework not only reduces the memory footprint of our models, but also achieves better performance than our single-task framework.</p><p>? We provide a comparative analysis of different neural network architectures when finetuning Wav2Vec 2.0 on downstream tasks. Our results suggest that a computationally inexpensive linear layer can achieve competitive results as compared to temporal models like Bi-LSTM and 1D-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head><p>Our framework for solving voice activated tasks consists of two major components -1) A Speech Representation Extractor (SRE) which is trained in a self-supervised manner on raw audio data. 2) Downstream networks that operate on the speech representations extracted from the SRE to map them to a target class or an embedding for solving the voice activated task. We first describe these two components and then discuss our multi-task training algorithm that trains multiple downstream heads jointly with a common SRE backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Speech Representation Extractor (SRE):</head><p>Our SRE is a wav2vec 2.0 model <ref type="bibr" target="#b7">[8]</ref>, comprised of convolutional and transformer layers that operate directly on the raw audio waveforms. <ref type="figure" target="#fig_0">Figure 1</ref> gives a high level overview of this network. The convolutional encoder serves as a feature extractor and operates on 20 milliseconds windows of the waveform and outputs the encoded representations Z . The transformer module operates on Z and produces contextualized audio representations C by attending over the future and past audio frames. The wav2vec model is trained in a selfsupervised manner by masking some of the encoded representations Z before feeding them to the transformer module. The transformer module is trained with the objective of predicting the quantized version Q of the encoded representations Z. The discrete representation Q is obtained from Z using a learnable quantization module. The training objective of wav2vec 2.0 uses contrastive loss where the network is encouraged to distinguish the true masked quantized representation from distractor samples of other time-steps. Additional loss terms are employed to ensure diversity in the codebooks of the quantization module and L2 regularization of network weights. For the SRE backbone of our framework, we use a pre-trained wav2vec 2.0 Base model that is trained on Lib-riSpeech dataset with around 1000 hours of speech. We refer the readers to <ref type="bibr" target="#b7">[8]</ref> for more details on wav2vec 2.0 pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Downstream Networks:</head><p>Downstream networks are needed to map the output of the SRE to task specific outputs. Fine-tuning of the wav2vec 2.0 model was originally explored for speech recognition in which the output of the wav2vec model at each time-step is mapped to the scores over the characters/phonemes in the vocabulary using a linear layer and optimized using CTC <ref type="bibr" target="#b8">[9]</ref> loss. However, for tasks like KWS, SV, etc. the entire utterance has to be mapped to a single class (for classification tasks) or an embedding vector (for tasks like speaker verification). To achieve this mapping, we need a learnable module that can operate on the outputs of the SRE at different time-steps. In our multi-task learning framework we have one downstream network for each task. We explore the following downstream networks to achieve this mapping: 1) Linear Layer: A Linear layer is our most light-weight downstream network in which we map the output of the SRE at the first-timestep to the class scores or an embedding vector using a linear layer. Since the SRE is a transformer-based neural network, temporal dependencies on the future audio frames can be learned by the trainable attention modules during fine-tuning.</p><p>2) Bidirectional LSTM (Bi-LSTM): In this setup, we have a bi-directional LSTM that operates over the vector outputs at each time-step of the SRE. Then the output of the first and last-timestep of the Bi-LSTM are combined and mapped to the task-specific output using a linear layer. We use a single-layer BiLSTM with 256 hidden units.</p><p>3) 1D-CNN: Our 1D-CNN comprises of strided convolutional blocks with filters of size 25. Each convolutional block consists of a convolutional layer with 128 filters followed by ReLU activation and a batch normalization layer. We use two CNN architectures for one second and two seconds audio input respectively. For one second audio, we use a single convolutional block with stride 4 to reduce the SRE output timesteps to 16. We flatten this final output before feeding it to a linear layer to map it to our task-specific output. For two seconds utterances, we have an additional convolutional block with stride 2 as the first block, followed by the same architecture as that used for one second audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-task Training:</head><p>In our multi-task learning framework, we train a shared SRE backbone with multiple downstream heads, with each downstream head corresponding to a different voice-activated task. Multi-task learning has two main advantages -1) A shared SRE backbone across multiple tasks reduces the memory footprint and makes our setup suitable for deployment in memory-constrained settings. 2) Features useful for discriminating on one voice activated task can also potentially be useful for alternate tasks. Performance gains due to multi-task learning have been demonstrated in the image domain <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Multi-task training setup is straightforward if the target classes/outputs for all the tasks are present for any given utterance in our dataset. However, most large-scale public datasets for voice activated tasks are disjoint. For example SV datasets like VoxCeleb <ref type="bibr" target="#b13">[14]</ref> do not contain keyword information and vice-versa. Therefore, we use our training mechanism described in Algorithm 1 for learning our multitask framework. We initialize the SRE model parameters from the pretrained wav2vec 2.0 checkpoint trained in a self-supervised manner and randomly initialize our downstream networks. We then initialize mini-batch dataloaders and loss criteria for each task. We use Cross-Entropy (CE) loss for KWS and angular softmax loss <ref type="bibr" target="#b14">[15]</ref> with cosine similarity on the embeddings for SV task. In our training loop, we go through the mini-batches for each task in a round-robin manner. We keep the SRE backbone frozen for the first one thousand mini-batch iterations and then unfreeze it to jointly update the SRE parameters with the downstream networks. We use Adam optimizer with a learning rate of 10 ?4 for the downstream network parameters and 10 ?5 for the SRE backbone. We use a lower learning rate for SRE to prevent overfitting during finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KWS:</head><p>For keyword spotting we choose the widely used Google Speech Command (GSC) Datasets V1 and V2 <ref type="bibr" target="#b15">[16]</ref>. There are total of 30 keywords and we use ten classes of "Yes", "No", "Up", "Down", "Left", "Right", "On", "Off", "Stop", and "Go" with two additional classes "Unknown Word (the remaining 20 words)" and "Silence (no speech detected)" following the settings of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>. This results in GSC1-12 and GSC2-12 with shared 12 commands from datasets V1 and V2. Both of these datasets contain 1 second utterances. We add the same data augmentation and noise mixing for the training samples as <ref type="bibr" target="#b16">[17]</ref>, use the same data-split as used in in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and report the Top-1 Test accuracy on the standard test sets. SV: For the speaker verification task we use VoxCeleb-1 and VoxCeleb-2 datasets, which are large-scale text-independent speaker recognition datasets containing 1,211 and 5,994 speakers respectively in training set <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14]</ref>. We use 2 second utterance slices for training our networks. Neither voice activity detection (VAD) nor data augmentation is used for SV training. Evaluations for both VoxCeleb-1 and VoxCeleb-2 are conducted on the full 37611 trial pairs of 40 unseen speakers from the original test set of VoxCeleb-1 <ref type="bibr" target="#b17">[18]</ref> available on their website 1 .The speaker identities in testing set are disjoint from the ones in training set. We evaluate the Equal Error Rate (EER) 2 metric on these trial pairs using 256 dimensional speaker embeddings. <ref type="table" target="#tab_1">Table 1</ref> lists the characteristics of individual datasets for both KWS and SV used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>Single-Task: In the single-task setup we train our models to solve individual voice activated tasks, that is, separate models for KWS and SV. For KWS, we trained our SRE and linear downstream network heads on both versions of Google Speech Commands dataset using a single-task framework. <ref type="table">Table 2</ref> lists the Top-1 Accuracy of experiments with the 12 shared commands in datasets V1 (GSC1-12) and V2 (GSC2-12). We compare the performance of our models against prior state-of-the-art results in <ref type="table">Table 2</ref>. While our multi-task framework achieves higher performance on KWS when compared to our single task framework, it is worth noting that even our single task framework achieves better performance than previous SOTA models.</p><p>For SV, we train two separate models on VoxCeleb-1 and Voxceleb-2 training sets and evaluate them on the unseen Vox-1 test set. We report the Equal Error Rate (EER) for this evaluation in <ref type="table">Table 2</ref>. Similar to KWS, our single task framework for SV also achieves better performance than previous SOTA. Even when training with only VoxCeleb-1 dataset we outperform previous baselines <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> that have trained with same dataset (3.35% EER compared to 3.61%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task:</head><p>We present the results of our multi-task framework on voice activated tasks KWS and SV in <ref type="table">Table 2</ref>. The results indicate that training tasks together can yield improvement over training them separately, as demonstrated by the higher Top-1 Accuracy for KWS and lower EER for SV achieved by multi-task setup compared to the single task setup in <ref type="table">Table 2</ref>. Our proposed multi-task framework enables shared SRE backbone to solve multiple tasks thereby reducing memory footprint when solving multiple tasks without hindering individual task accuracy. With our proposed framework, we achieve state-of-the-art performance on both KWS and SV voice activated tasks on all benchmark datasets explored in our work.</p><p>We perform two additional experiments: 1) Using an untrained randomly initialized SRE that is trained from scratch with the downstream network 2) using a pre-trained (no finetuning) Frozen SRE. The results for these experiments reported in <ref type="table">Table 2</ref> ascertain the importance of both pre-training and finetuning the SRE backbone.  <ref type="table">Table 2</ref>. Results of our multi-task learning experiments using different downstream heads and dataset combinations, and performance of prior state of the art methods on the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATION TO PRIOR WORKS</head><p>Concurrent works have shown that self-supervised learning models like wav2vec 2.0 can be fine-tuned to perform various voice activated tasks such as KWS <ref type="bibr" target="#b16">[17]</ref>, SV <ref type="bibr" target="#b18">[19]</ref>, language identification <ref type="bibr" target="#b18">[19]</ref>, emotion recognition <ref type="bibr" target="#b20">[21]</ref>. We demonstrate that our models outperform these previous approaches on two benchmark tasks: KWS and SV when using both single-task and multi-task frameworks. Earlier work <ref type="bibr" target="#b4">[5]</ref> performing multi-task learning for KWS and SV using melspectrograms, reports that multi-task learning can improve performance over single-task setup, and we find our results to corroborate this finding when using an end-to-end solution with contextualized speech representations from wav2vec. These results are in contrast to another attempt <ref type="bibr" target="#b18">[19]</ref> to jointly learn speaker verification and language identification, which claims that multi-task learning can negatively impact singletask performance. We conjecture that this difference in gains in <ref type="bibr" target="#b18">[19]</ref> from multi-task learning may be due to the chosen tasks that were solved jointly in their multi-task framework and the difference in their training algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this work, we propose a general purpose training framework for voice activated multi-tasks using self-supervised models. We outperform both prior methods using neural networks on mel-spectrogram representations and prior attempts at using waveform representation of speech. Our multi-task training strategy, not only reduces the memory footprint by solving multiple tasks using a shared backbone, but also improves the performance over single-task models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of our proposed multi-task training framework with wav2vec 2.0 based SRE backbone and various downstream networks to solve voice activated tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the datasets used in our experiments. We use the GSC datasets for KWS and VoxCeleb datasets of SV. #K refers to the number of speakers for SV and number of keywords for the GSC dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="2"># Utterances # Hours</cell><cell># K</cell></row><row><cell>GSC1 -(12) Train</cell><cell>22236</cell><cell>6.2</cell><cell>12</cell></row><row><cell>GSC2 -(12) Train</cell><cell>30769</cell><cell>8.6</cell><cell>12</cell></row><row><cell>GSC1 -(12) Test</cell><cell>3081</cell><cell>0.9</cell><cell>12</cell></row><row><cell>GSC2 -(12) Test</cell><cell>4890</cell><cell>1.4</cell><cell>12</cell></row><row><cell>VoxCeleb1 Train</cell><cell>143642</cell><cell cols="2">329.06 1211</cell></row><row><cell>VoxCeleb2 Train</cell><cell>1021161</cell><cell cols="2">2207 5994</cell></row><row><cell>VoxCeleb1 Test</cell><cell>4874</cell><cell>11.2</cell><cell>40</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.robots.ox.ac.uk/ vgg/data/voxceleb/vox1.html 2 Error at the threshold where Fase Positve Rate=False Negative Rate</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Utterance-level aggregation for speaker recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Margin matters: Towards more discriminative deep neural network embeddings for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Broadcasted residual learning for efficient keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeonggeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simyung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dooyong</forename><surname>Sung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04140</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exploring the encoding layer and loss function in end-toend speaker and language recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05160</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving multiscale aggregation using feature pyramid module for robust speaker verification of variable-duration utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmoon</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeunju</forename><surname>Seong Min Kye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunghun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoi-Rin</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Interspeech 2020. ISCA</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-channel spectrograms for speech processing applications using deep learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Arias-Vergara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Klumpp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Camilo</forename><surname>Vasquez-Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmar</forename><surname>N?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Rafael</forename><surname>Orozco-Arroyave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using convolutional neural network and long-short term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjana</forename><surname>Dangol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abeer</forename><surname>Alsadoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Indra</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Seher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hisham Alsadoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint face detection and facial expression recognition with mtcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 4th international conference on information science and control engineering (ICISCE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint face image restoration and frontalization for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multi-level face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wav2kws: Transfer learning from speech representations for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deokjin</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heung-Seon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchul</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voxceleb: Large-scale speaker verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101027</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring wav2vec 2.0 on speaker verification and language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/2012.06185</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attentive statistics pooling for deep speaker embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takafumi</forename><surname>Koshinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Emotion recognition from speech using wav2vec 2.0 embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Pepino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Riera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciana</forename><surname>Ferrer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03502</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
