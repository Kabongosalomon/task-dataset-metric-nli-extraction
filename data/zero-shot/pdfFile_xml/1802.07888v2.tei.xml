<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Techniques for the Weakly-Supervised Object Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Park</forename><surname>Choe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shim: Improved</forename><surname>Wsol</surname></persName>
						</author>
						<title level="a" type="main">Improved Techniques for the Weakly-Supervised Object Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an improved technique for weakly-supervised object localization. Conventional methods have a limitation that they focus only on most discriminative parts of the target objects. The recent study addressed this issue and resolved this limitation by augmenting the training data for less discriminative parts. To this end, we employ an effective data augmentation for improving the accuracy of the object localization. In addition, we introduce improved learning techniques by optimizing Convolutional Neural Networks (CNN) based on the state-of-the-art model. Based on extensive experiments, we evaluate the effectiveness of the proposed approach both qualitatively and quantitatively. Especially, we observe that our method improves the Top-1 localization accuracy by 21.4 -37.3% depending on configurations, compared to the current state-of-the-art technique of the weakly-supervised object localization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object localization aims to identify the location of an object in an image. The state-of-the-art object detection utilizes fully-supervised learning, which involves annotating locations, such as bounding boxes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. Unfortunately, rich annotations involves intensive manual tasks, and are often quite different from different human participants. Weakly supervised approaches to the object localization bypass the issue of annotating localization, with only image-level labels. Because weakly supervised approaches do not rely on the bounding box annotation, they can be a practical alternative.</p><p>Existing approaches can be categorized based on whether they derive discriminative features of the training dataset explicitly or implicitly. Explicit methods utilize handcrafted features to extract class-specific patterns for object localization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. Meanwhile, implicit methods first train deep convolutional neural networks (CNN) mostly for object classifications using image-level labels, then utilize its byproduct, the activation map, for the object localization. The final heatmap can be produced by leveraging the activation maps from the networks. Applying the simple post-processing on the extracted heatmap, it c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1802.07888v2 [cs.CV] 10 May 2018 <ref type="figure">Figure 1</ref>: Examples of augmentation methods of Hide-and-Seek (HnS) and GoogLeNet Resize (GR). HnS divides an image into grids, randomly removing some patches. GR randomly crops the input image into a rectangular patch, then resize it to the original input size.</p><p>is possible to localize a target object <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>. Both approaches, however, result in capturing most discriminative parts for object localization, discarding less discriminative parts. This causes the bounding box to lack coverage among entire parts of the object.</p><p>Recently, to resolve such shortcomings, several techniques have been proposed <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>. Singh and Lee <ref type="bibr" target="#b25">[26]</ref> suggested a new training technique, namely Hide-and-Seek (HnS), that involves a grid mask for introducing obscured training data. More specifically, they randomly hide the sub-regions of input image by eliminating each patch of the grid mask with a prefixed probability. Their method potentiates the possibility to hide the most discriminative parts of the object, allowing the CNN to seek the less discriminative part. Their approach can be interpreted as the data augmentation technique in a sense that only the training dataset is being modified <ref type="bibr" target="#b15">[16]</ref>, leading to an advantage that holds independence to a specific classification algorithm. Based on quantitative and qualitative evaluations, <ref type="bibr" target="#b25">[26]</ref> achieved the state-of-the-art performance among weakly-supervised object localization techniques.</p><p>In this paper, we suggest a data augmentation and improved training techniques that is effective on increasing the accuracy of the weakly-supervised object localization. We construct a baseline network with the state-of-the-art classification network <ref type="bibr" target="#b9">[10]</ref>. To produce a heatmap, the Class Activation Maps (CAM) algorithm <ref type="bibr" target="#b32">[33]</ref> is applied. Then, the baseline network outputs classification (i.e., object labels) and localization (i.e., bounding box) results. We investigate two aspects for improving the localization performance:</p><p>1. How to further improve the data augmentation proposed by Hide-and-Seek <ref type="bibr" target="#b25">[26]</ref>?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">How network capacity and batch size influence the localization performance?</head><p>In investigation of the data augmentation, we have found that a GoogLeNet Resize (GR) augmentation method <ref type="bibr" target="#b28">[29]</ref> can improve the localization performance even better than the state-of-the-art technique (HnS). As shown in <ref type="figure">Figure 1</ref>, both GR and HnS deal with how to increase the training dataset for encapsulating less discriminative parts of object. GR uses the partial regions of image, while HnS hides partial regions. Although both methods aim to include less discriminative part of the object in the bounding box, GR is inherently more aggressive in that it augments more challenging images with small, and valid regions. From the experimental evaluation, we show that GR outperforms HnS in weakly-supervised object localization.</p><p>For the improved training schemes, we investigate the effect of batch sizes and depths of a network. First, inspired by recent report <ref type="bibr" target="#b14">[15]</ref> that the size of the minibatch has critical influence on CNN optimization, we examine the influence of the batch size on the performance of weakly-supervised object localization. With empirical studies, we find that smaller batch size allows better performance. Secondarily, the network capacity can also influence the localization performance. We compare the network depths with 18-layer and 34-layer. With such comparison, we observe that deeper network performs better in the object localization.</p><p>For evaluation, we use Tiny ImageNet dataset, a reduced version of ImageNet <ref type="bibr" target="#b22">[23]</ref>. Notice that the Tiny version is more challenging for conducting a localization task than the original ImageNet due to the reduced resolution and the lack of training data. Finally, by integrating our improved training techniques, we have improved the localization performance (Top-1 localization accuracy <ref type="bibr" target="#b25">[26]</ref>) from 27.1% to 36.0%, which is approximately 30% improvement over the current state-of-the-art technique <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Algorithm Description</head><p>In this section, we first explain the baseline algorithms, and then show the effect of the data augmentation, minibatch size, and network depth on the performance of object localization.</p><p>Baseline algorithm. We implement the classification network using the pre-activation residual network <ref type="bibr" target="#b9">[10]</ref>. To produce the heatmaps, we use the Class Activation Maps (CAM) method <ref type="bibr" target="#b32">[33]</ref> for extracting the heatmap from the classification network. Note that the preactivation residual network that we choose is a state-of-the-art classifier, which is an improved version of ResNet <ref type="bibr" target="#b8">[9]</ref>. By conducting the large-scale experiments, we have found that the pre-activation ResNet based CAM is superior to original CAM, which is built based on GoogLeNet <ref type="bibr" target="#b28">[29]</ref> or VGG <ref type="bibr" target="#b23">[24]</ref> architecture. The CAM replaces the fully connected layer right after the last convolutional layer with the Global Average Pooling (GAP) layer, which the scheme is applicable to any type of CNN networks. With the GAP layer, the spatial information of the feature map is visualized. We then obtain the heatmap by aggregating the higher-layer activation maps with the weights between GAP layer and softmax layer. Following <ref type="bibr" target="#b32">[33]</ref>, the final output, a bounding box, is obtained by thresholding the heatmap.</p><p>Motivation. Hide-and-Seek (HnS) <ref type="bibr" target="#b25">[26]</ref> is one of the state-of-the-art techniques in weaklysupervised object localization. Note that previous methods only focuses on the most discriminative parts. Consequently, localization results from previous work tend to produce a smaller bounding box, ignoring less discriminative parts. To resolve this problem, HnS aims to learn less discriminative parts of the object by hiding random parts of the object. Specifically, HnS first divides an input image into small patches in a grid-style. HnS then randomly selects several patches in the grid to mask the patch. With such random masking, it is more likely to hide the most discriminative parts of the object; the network is more likely to learn less discriminative parts of the object. By doing this, <ref type="bibr" target="#b25">[26]</ref> declares that HnS can achieve the state-of-the-art performance.</p><p>The key idea of HnS is to hide the most discriminative parts of target object from CNN so that the network is also capable of learning the less discriminative parts of object. Motivated by this idea, we adopt the GoogLeNet Resize (GR) augmentation <ref type="bibr" target="#b28">[29]</ref>. GR augmentation randomly crops 8-100% of an input image with the aspect ratio between 0.75 and 1.33, and then resizes the cropped image to the original input image size. <ref type="figure">Figure 1</ref> visualizes the HnS and GR augmentation respectively. GR augmentation effectively produces the training data at various scales (i.e., various levels of zoom-in). In this way, we can exam the local structure as well as the global structure during training. As a result, patches drawn from GR augmentation enforce the classifier to learn the less discriminative local structure better than HnS, which only observes the global structure (i.e., a single scale) during training. GR augmentation is analogous with HnS in that it only provides partial information of an object to a CNN. However, we expect that the GR augmentation, which only utilizes partial parts of an object, is more robust to deformations and pose variations than HnS, which only hides partial parts of an object. Based on empirical study, we exam which one is superior, and whether two methods are complementary or mutually exclusive.</p><p>Batch size. Recently, Li et al. <ref type="bibr" target="#b14">[15]</ref> claimed that the smaller the batch size we use, the closer the CNN loss landscape approaches to the convex function; reaching the global optima more easily. However, the CNN classifier with the batch normalization (BN) <ref type="bibr" target="#b10">[11]</ref> requires the large batch size. Note that BN computes the local mean and variance within the minibatch. Hence, it is more likely to have the mismatch between the local and global statistics (i.e., mean and variance) when the batch size is small. Likewise, it is still controversial how batch sizes affect the actual performance of a CNN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12]</ref>. Heretofore, there is no consensus of what batch size is optimal for object localization. To reveal the effect of batch size in our application, we examine the performance of the object localization by varying the batch sizes. Network depth. Generally, the classification performance of deep neural networks outperforms that of a shallow neural network <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>. Meanwhile, if depth is too deep, the classification performance decreases due to gradient vanishing. This problem is well addressed by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>. We overcome such problem by utilizing an identity mapping <ref type="bibr" target="#b9">[10]</ref>, thereby making it possible to increase the depth of network without gradient vanishing. Note that all these discussions regarding the effect of network depth were made on the classification tasks. Hence, we empirically analyze how the performance of the object localization is influenced with the network depths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>In this section, we first describe the implementation detail, and then show how the data augmentation, batch size, and the network depth influence the accuracy of weakly-supervised object localization. In addition, we also show qualitative evaluation results as well.</p><p>Implementation details. We utilized Tiny ImageNet dataset for training the classification network. The Tiny ImageNet is simplified version of the ImageNet <ref type="bibr" target="#b22">[23]</ref>, with the image size reduced to 64 ? 64. There are total 200 categories, 500 training images, and 50 validation images per each category in the original Tiny ImageNet. For object recognition or localization tasks, handling the Tiny ImageNet is more challenging than the original ImageNet in two perspectives: the resolution and the volume of dataset. The resolution of the Tiny ImageNet is lower than that of ImageNet images, approximately one sixth. Previous study <ref type="bibr" target="#b17">[18]</ref> pointed out that low-resolution image is more difficult to recognize than high-resolution image. In addition, the number of data in ImageNet is two order of magnitude more than that of Tiny ImageNet. It is widely known that the larger dataset improves recognition performance. We use their 50 validation images as our test dataset. As we described in Section 2, we use pre-activation ResNet for classification network, with slight modification to the size of input layer. We train the network using Nesterov momentum optimizer <ref type="bibr" target="#b1">[2]</ref> for 1500 epochs, and set the momentum to 0.9. The initial learning rate is 0.1 and we reduce the learning rate by a factor of 10 every 250 epochs. The weight decay is 1e-4. We utilize Tensorflow <ref type="bibr" target="#b0">[1]</ref>, and Tensorpack <ref type="bibr" target="#b30">[31]</ref> for implementation of the codes. The HnS grid is implemented by following the mixed method, described further in their paper. The grid size [0 ? 0, 4 ? 4, 8 ? 8, 16 ? 16] is randomly applied to the input image at every iteration.</p><p>Evaluation metrics. For evaluation, we utilize the same metrics used in [26]:</p><p>1. Top-1 localization accuracy (Top-1 Loc): The ratio of samples, which intersection over union (IoU) between estimates and ground truth is more than 50% and, at the same time, classification result is correct.</p><p>2. Localization accuracy with known ground truth class (GT-known Loc): The ratio of samples, which IoU between estimates and ground truth is more than 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Top-1 classification accuracy (Top-1 Clas):</head><p>The ratio of correct answers among all test samples.</p><p>Quantitative evaluation. We compare the performance of object localization for HnS and the proposed algorithm utilizing GR augmentation. In addition, we sequentially apply both HnS and GR augmentation and observe the effect of the sequential order of HnS and GR to the performance. We set the batch size to 256 in all data augmentation experiments for a fair comparison. <ref type="table">Table 3</ref> shows the experimental results. We highlight superior results with bold letters. From these results, we observe that both HnS and GR performances are better than the baseline (CAM without data augmentation). Among HnS and GR, we conclude that GR clearly outperforms HnS in all three metrics. More specifically, the GT-known Loc of our results outperforms the HnS by 7% and the CAM by 8%. In addition, our results outperform the HnS by almost 25% with a Top-1 Loc metric. The Top-1 Clas performance of our algorithm is better than that of HnS and CAM. Lastly, we find that the performance decreases when HnS and GR are simultaneously applied, meaning that HnS and GR are mutually exclusive. From these observations, we finally conclude that solely applying GR is the better choice to improve performance.</p><p>Effect of batch size and network depth. Next, we examine how the batch size and network depth affect object localization performance. Note that our hyper-parameters are tuned for batch-256 and 18-layer settings, and fixed for all other experiments. <ref type="table" target="#tab_2">Table 2</ref> shows the experimental results. We again use bold letters for superior results. The performance is improved consistently by 10-15% in all three methods. Although the hyper-parameters (a) CAM <ref type="bibr" target="#b32">[33]</ref> (b) HnS <ref type="bibr" target="#b25">[26]</ref> (c) GR (Proposed) <ref type="figure">Figure 2</ref>: Qualitative evaluation results. GR experimental results clearly show better localization performance than the baseline and HnS.  Qualitative evaluation. Lastly, we visually compare our best model (i.e., GR augmentation with batch-32 and 34-layer) in quantitative experiments with the baseline and HnS. <ref type="figure">Figure 2</ref> shows the experimental results. The left image shows estimated (green color) and ground truth (blue color) bounding box. The heatmap is shown in middle. The right image shows an input image overlapping with the heatmap. Note that the bounding box is obtained by post-processing the heatmap as proposed by CAM <ref type="bibr" target="#b32">[33]</ref>.</p><p>The qualitative evaluation results clearly show that our results can capture the entire parts of object better than CAM and HnS. As discussed by HnS <ref type="bibr" target="#b25">[26]</ref>, CAM concentrates only on the most discriminative parts so to localize the partial region of target object. This issue is alleviated both HnS and the proposed approach; the combination of GR augmentation with deeper network and small batch size. Compared to HnS, our results can better cover the overall parts of target object, thus much closer to the ground truth bounding box. The same observation holds in heatmaps. For example, our results for heatmaps are more accurate than those of CAM and HnS, both in terms of the object coverage and position. Note that the green box is sometimes invisible in our results. It is because the estimated bounding box completely overlaps with the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we improve the performance of the weakly-supervised object localization in three aspects: the data augmentation, batch sizes, and the network depths. It is experimentally shown that the GoogLeNet Resize augmentation is better than the current state-of-theart data augmentation technique <ref type="bibr" target="#b25">[26]</ref> for weakly-supervised object localization. We also show that the performance increases with a small batch size. Finally, we show that deeper network produces better performance than shallower networks. We train pre-activation ResNet using Tiny ImageNet, and evaluate our methods both quantitatively and qualitatively. In the future, we aim to study new data augmentation methods to ensure better performance than GoogLeNet Resize augmentation, and analyze the performance saturation upon the network depths and the batch sizes for the weakly-supervised object localization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy comparison with HnS and various data augmentation techniques. The batch size is 256.</figDesc><table><row><cell>Depth</cell><cell>Metric</cell><cell cols="5">CAM [33] HnS [26] GR (Proposed) HnS after GR GR after HnS</cell></row><row><cell></cell><cell>GT-known Loc</cell><cell>53.46</cell><cell>54.30</cell><cell>57.82</cell><cell>57.49</cell><cell>57.43</cell></row><row><cell>ResNet34</cell><cell>Top-1 Loc</cell><cell>27.50</cell><cell>29.72</cell><cell>36.00</cell><cell>33.93</cell><cell>33.45</cell></row><row><cell></cell><cell>Top-1 Clas</cell><cell>44.54</cell><cell>46.67</cell><cell>54.13</cell><cell>51.28</cell><cell>50.45</cell></row><row><cell></cell><cell>GT-known Loc</cell><cell>53.42</cell><cell>52.89</cell><cell>57.00</cell><cell>56.99</cell><cell>56.76</cell></row><row><cell>ResNet18</cell><cell>Top-1 Loc</cell><cell>27.13</cell><cell>27.56</cell><cell>34.43</cell><cell>33.33</cell><cell>32.28</cell></row><row><cell></cell><cell>Top-1 Clas</cell><cell>43.90</cell><cell>44.12</cell><cell>52.23</cell><cell>50.14</cell><cell>48.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Top-1 localization accuracy of object localization upon various batch size and network depth.are not tuned for smaller batch size and deeper network, the experimental results clearly demonstrate that smaller batch size and deeper network produce higher accuracy in Top-1 localization (Top-1 Loc). From this experimental study, we can conclude that smaller batch size and deeper network increase the accuracy of weakly-supervised localization.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8624" to="8628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Entropy-SGD: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01838</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-fold mil training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2409" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="189" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sharp minima can generalize for deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04933</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scaleinvariant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>II-264-II-271</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04836</idno>
		<title level="m">On largebatch training for deep learning: Generalization gap and sharp minima</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-phase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3554" to="3563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<title level="m">Visualizing the loss landscape of neural nets. International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10171</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Is object localization for free? -Weaklysupervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, realtime object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hide-and-Seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1024</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of models for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/tensorpack/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06962</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
