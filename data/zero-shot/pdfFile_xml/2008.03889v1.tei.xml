<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Norm-in-Norm Loss with Faster Convergence and Better Performance for Image Quality Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingquan</forename><surname>Li</surname></persName>
							<email>dingquanli@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LMAM</orgName>
								<orgName type="department" key="dep2">School of Mathematical Sciences &amp; BICMR</orgName>
								<orgName type="institution" key="instit1">NELVT</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
							<email>ttjiang@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">NELVT</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
							<email>ming-jiang@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">NELVT, LMAM</orgName>
								<orgName type="department" key="dep2">School of Mathematical Sciences &amp; BICMR</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Norm-in-Norm Loss with Faster Convergence and Better Performance for Image Quality Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>IQA</term>
					<term>faster convergence</term>
					<term>loss function</term>
					<term>normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Currently, most image quality assessment (IQA) models are supervised by the MAE or MSE loss with empirically slow convergence. It is well-known that normalization can facilitate fast convergence. Therefore, we explore normalization in the design of loss functions for IQA. Specifically, we first normalize the predicted quality scores and the corresponding subjective quality scores. Then, the loss is defined based on the norm of the differences between these normalized values. The resulting "Norm-in-Norm" loss encourages the IQA model to make linear predictions with respect to subjective quality scores. After training, the least squares regression is applied to determine the linear mapping from the predicted quality to the subjective quality. It is shown that the new loss is closely connected with two common IQA performance criteria (PLCC and RMSE). Through theoretical analysis, it is proved that the embedded normalization makes the gradients of the loss function more stable and more predictable, which is conducive to the faster convergence of the IQA model. Furthermore, to experimentally verify the effectiveness of the proposed loss, it is applied to solve a challenging problem: quality assessment of in-the-wild images. Experiments on two relevant datasets (KonIQ-10k and CLIVE) show that, compared to MAE or MSE loss, the new loss enables the IQA model to converge about 10 times faster and the final model achieves better performance. The proposed model also achieves state-ofthe-art prediction performance on this challenging problem. For reproducible scientific research, our code is publicly available at https://github.com/lidq92/LinearityIQA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Image quality assessment (IQA) has received considerable attention <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref> and plays a key role in many vision applications, such as compression <ref type="bibr" target="#b24">[25]</ref> and super-resolution <ref type="bibr" target="#b33">[34]</ref>. It can be achieved by subjective study or objective models. Subjective study uses mean opinion score (MOS) to assess image quality. This is considered as the most reliable and accurate way, whereas it is expensive and time-consuming. So the objective models that can automatically predict image quality are in urgent need. In terms of the availability of the reference image, objective IQA models can be divided into three categories: full-reference IQA <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>, reducedreference IQA <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31]</ref>, and no-reference IQA <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Most classic learning-based IQA models are based on mapping the handcrafted features to image quality by support vector regression (SVR) <ref type="bibr" target="#b19">[20]</ref>. Recently, deep learning-based models, which jointly learn feature representation and quality prediction, show great promise in IQA <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref>. However, these models mostly treat IQA as a general regression problem. And they adopt standard regression loss functions for training, i.e., mean absolute error (MAE) and mean square error (MSE) between the predicted quality scores and the corresponding subjective quality scores. We notice a fact that the IQA models trained using MAE or MSE loss exhibit slow convergence. For example, on a dataset containing only about 10,000 images with a resolution of 664?498, training the model on an NVIDIA GeForce RTX 2080 Ti GPU (11GB) takes more than one day to reach convergence. Since the size of the training dataset becomes larger and larger in the deep learning era, faster convergence is preferable to reduce the training time.</p><p>In this work, we tackle the slow convergence problem in the context of IQA. In fact, slow convergence problem is common in machine learning and computer vision, which may be due to the non-smooth loss landscape <ref type="bibr" target="#b25">[26]</ref>. To provide faster convergence for the learning process, it is widely-used to do input data normalization or intermediate feature rescaling <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Normalizing the output predictions is rarely recommended. However, it is shown that normalizing the network output can lead to faster convergence of generative networks for image super-resolution <ref type="bibr" target="#b20">[21]</ref>. Inspired by this work, to achieve fast convergence of IQA model training, we explore normalization in the design of loss functions for IQA.</p><p>We propose a class of loss functions, denoted as "Norm-in-Norm", for training an IQA model with fast convergence. Specifically, the predicted quality scores is firstly subtracted by their mean, and then they are divided by their norm after centralization. Similar normalization is applied to the subjective quality scores. After the normalization, we define the new loss based on the norm of the differences between the normalized values. The new loss normalizes both labels and predictions while label normalization only normalizes labels, and the new loss encourages the IQA model to make linear predictions with respect to (w.r.t.) subjective quality scores. Hence, after training, the linear relationship can be determined by applying least squares regression (LSR) on the whole training set for mapping the predictions to the subjective quality scores. In the testing phase, this learned linear relationship is applied to the model prediction to get the final predicted quality score for a test image.</p><p>There are two interesting findings about the new loss. First, we derive that Pearson's linear correlation coefficient (PLCC)-induced loss <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref> is a special case of the proposed loss, where PLCC is a criterion for benchmarking IQA models. Second, after introducing a variant of the proposed loss, we show its connection to root mean square error (RMSE) -another IQA performance criterion. Further, we conduct theoretical analysis on the property of the new loss. And it is proved that due to the embedded normalization, the new loss has stronger Lipschitzness and ?-smoothness <ref type="bibr" target="#b21">[22]</ref>, which means the gradients of the loss function is more stable and more predictable. Thus, the gradient-based algorithm for learning the IQA model has a smoother loss landscape. And this is conducive to the faster convergence of the IQA model.</p><p>Generally, the proposed "Norm-in-Norm" loss can be applied to any regression problem, including IQA problems. In particular, we pick a challenging real-world IQA problem: quality assessment of in-the-wild images, to verify the effectiveness of the proposed loss. Quality assessment of in-the-wild images has two challenges, i.e., content dependency 1 and distortion complexity. We design <ref type="bibr" target="#b0">1</ref> Content dependency means human ratings of image quality depend on image content a no-reference IQA model based on aggregating and fusing multiple level deep features extracted from the image classification network. Deep features are considered for tackling the content dependency. And multiple level features are used for handling the distortion complexity. The model is trained with the proposed loss. The experiments are conducted on two benchmark datasets, i.e., CLIVE <ref type="bibr" target="#b5">[6]</ref> and KonIQ-10k <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows the convergence results on KonIQ-10k. By using the proposed loss, the model only needs to look at the training images once (i.e., in one epoch) to achieve a prediction performance indicated by the grey dash line. It is about 10 times faster than MAE loss and MSE loss. This verifies that the proposed loss facilitates faster convergence for training IQA models. We claim that it mainly benefits from the embedded normalization in our loss. Besides the faster convergence, we notice that the proposed loss also leads to a better prediction performance than MAE loss and MSE loss.</p><p>To sum up, our main contribution is that we propose a class of normalization-embedded loss functions in the context of IQA. The new loss is shown to have some connections to PLCC and RMSE. And our theoretical analysis proves that the embedded normalization in the proposed loss can facilitate faster convergence. For the quality assessment of in-the-wild images, it is experimentally verified that the new loss can provide both better prediction performance and faster convergence than MAE loss and MSE loss. What's more, the proposed model outperforms state-of-the-art models on this challenging IQA problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">"NORM-IN-NORM" LOSS</head><p>In this section, we explore the normalization in the design of loss functions, and propose a class of "Norm-in-Norm" loss functions for training IQA models with fast convergence. The idea is to apply normalization for the predicted quality scores and the subjective quality scores respectively using their own statistics when computing the loss.</p><p>Assume we have N images on the training batch. For the i-th image I i , we denote the predicted quality by an objective IQA model F (?; ? ) asQ i and its subjective quality score (i.e., MOS) as Q i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Loss Computation</head><p>Our loss computation can be generally divided into three steps: computation of the statistics, normalization based on the statistics, and loss as the norm of the differences between the normalized values. The left part of <ref type="figure" target="#fig_1">Figure 2</ref> shows an illustration of the forward path of the proposed loss. We detail each step in the following.</p><p>Computation of the Statistics. First, given the predicted quality scoresQ = (Q 1 , ? ? ? ,Q N ), we calculate their mean?. Similarly, given the subjective quality scores Q = (Q 1 , ? ? ? , Q N ), their mean a is calculated. The L q -norm of the centered values is then computed, respectively.?</p><formula xml:id="formula_0">= 1 N N i=1Q i ,b = N i=1 |Q i ??| q 1 q ,<label>(1)</label></formula><formula xml:id="formula_1">a = 1 N N i=1 Q i , b = N i=1 |Q i ? a| q 1 q ,<label>(2)</label></formula><p>where q ? 1 is a hyper-parameter.b and b are the norm of the centered predicted quality scores and the norm of the centered subjective quality scores, respectively.</p><p>Normalization Based on the Statistics. Second, we normalize the predicted quality scores and the subjective quality scores based on their own mean and centered norm, respectively. That is, we first subtract the mean from the predicted/subjective quality scores and then divide them by the norm.</p><formula xml:id="formula_2">S i =Q i ?? b , i = 1, ? ? ? , N ,<label>(3)</label></formula><formula xml:id="formula_3">S i = Q i ? a b , i = 1, ? ? ? , N ,<label>(4)</label></formula><p>where? = (? 1 , ? ? ? ,? N ) are the normalized predicted quality scores, and S = (S 1 , ? ? ? , S N ) are the normalized subjective quality scores. Loss As the Norm of the Differences. The final step computes the differences? ? S between the normalized predicted quality score? S and the normalized subjective quality scores S. Then the loss l is defined as the p-th power of the L p -norm of the differences (p ? 1), and it is normalized to [0, 1].</p><formula xml:id="formula_4">l(Q, Q) = 1 c N i=1 |? i ? S i | p ,<label>(5)</label></formula><p>where c is a normalization factor. c can be determined using Minkowski inequality and H?lder's inequality (see the Supplementary Materials A), and it follows the following equation.</p><formula xml:id="formula_5">c = 2 p N 1? p q if p &lt; q, 2 p if p ? q.<label>(6)</label></formula><p>Based on the forward propagation, we can easily conduct the backward propagation by the chain rule, which is described in the right part of <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Specifically, based on Eqn. (3), we have</p><formula xml:id="formula_6">?? i ?Q j = 1 b 1 i=j ? 1 N ?? i |? j | q S j +? i 1 N N k =1 |? k | q S k .<label>(7)</label></formula><p>where 1 i=j is an indicate function, and it equals 1 if i = j, otherwise, 0. DenoteR</p><formula xml:id="formula_7">j = |? j | q S j , ?R = 1 N N k =1 |? k | q S k = 1 N ?1,R?.<label>(8)</label></formula><p>Then ?l</p><formula xml:id="formula_8">?Q j = N i=1 ?l ?? i ?? i ?Q j = 1 b N i=1 ?l ?? i 1 i=j ? 1 N ?? iRj +? i ?R .<label>(9)</label></formula><p>So we get the derivative of l w.r.t.Q, i.e., ?l</p><formula xml:id="formula_9">?Q . ?l ?Q = 1 b ?l ?? ? 1 N 1 T 1, ?l ?? ?R T ?l ?? ,? + ?R 1 T ?l ?? ,? ,<label>(10)</label></formula><p>which includes four terms. The first term is related to ?l ?? . The second term is related to ?l ?? . The thrid term is related to ?l ?b . And the fourth term is related to both ?l ?? and ?l ?b .</p><formula xml:id="formula_10">Remark: The normalization in Eqn. (3) is invariant to linear pre- dictions. That is, for any k 1Q + k 2 (k 2 1 + k 2 2 0), we derive the sam? S. So, we have l(k 1Q + k 2 , Q) = l(Q, Q).<label>(11)</label></formula><p>The loss encourages the IQA model to make predictions that are linearly correlated with the subjective quality scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Special Case: PLCC-induced Loss</head><p>Pearson's Linear Correlation Coefficient (PLCC), ?, is a criterion for benchmarking IQA models, which is defined as follows.</p><formula xml:id="formula_11">?(Q, Q) = N i=1 (Q i ??)(Q i ? a) N i=1 (Q i ??) 2 N i=1 (Q i ? a) 2 .<label>(12)</label></formula><p>In the following, we will prove that PLCC-induced loss (1 ? ?)/2 is a special case of the "Norm-in-Norm" loss functions.</p><p>First, when q is set to 2 in the "Norm-in-Norm" loss,Q i ?? i and Q i ? S i defined in Eqn. <ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref> relate to the well-known z-score transformation. And? i and S i have the following properties.</p><formula xml:id="formula_12">N i=1? 2 i = N i=1 S 2 i = 1.<label>(13)</label></formula><p>With the notation of? i and S i , we can reformulate PLCC.</p><formula xml:id="formula_13">?(Q, Q) = N i=1 (Q i ??)(Q i ? a) N i=1 (Q i ??) 2 N i=1 (Q i ? a) 2 = N i=1Q i ?? b Q i ? a b = N i=1? i S i = 1 ? 1 2 N i=1 (? i ? S i ) 2 . [using Eqn. (13)]</formula><p>When p equals to 2, we have c = 4 using Eqn. (6), then we derive the following equation.</p><formula xml:id="formula_14">? = 1 ? cl 2 = 1 ? 2l ?? l = (1 ? ?)/2.<label>(14)</label></formula><p>That is, PLCC-induced loss is equivalent to the "Norm-in-Norm" loss l(Q, Q) when the p, q are all set to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A Variant and Its Connection to RMSE</head><p>In this subsection, we introduce a variant of the "Norm-in-Norm" loss and show its connection to root mean square error (RMSE).</p><p>In the end of Section 2.1, we remark that the "Norm-in-Norm" loss focuses on training an IQA model to make linear predictions w.r.t. subjective quality scores. Under this linearity assumption, we can only require the normalized predicted quality scores? and normalized subjective quality scores S to be linearly correlated.</p><p>That is |?(?, S)| = 1. With this expectation, we can get a variant of the "Norm-in-Norm" loss as follows.</p><formula xml:id="formula_15">l ? (Q, Q) = 1 c N i=1 |?(?, S)? i ? S i | p .<label>(15)</label></formula><p>Connection to RMSE. We apply the least squares regression (LSR) to find the linear mapping betweenQ i and Q i .</p><formula xml:id="formula_16">Q i = k 1Qi + k 2 , (i = 1, ? ? ? , N ),<label>(16)</label></formula><p>where k 1 and k 2 are two free parameters. It is equivalent to solving the following minimization problem. <ref type="formula" target="#formula_0">(17)</ref>, and can easily get the minimum loss as follows.</p><formula xml:id="formula_17">min k 1 ,k 2 N i=1 (k 1Qi + k 2 ? Q i ) 2 (17) =? k * 1 = N i=1 (Q i ??)(Q i ? a) N i=1 (Q i ??) 2 , k * 2 = a ? k * 1? . We substitute k * 1 , k * 2 into formula</formula><formula xml:id="formula_18">N i=1 (k * 1Q i + k * 2 ? Q i ) 2 = N i=1 b 2 |?(?, S)? i ? S i | 2 =4b 2 l ? ,</formula><p>where p = 2, q = 2 are considered in our loss variant l ? , and b is the centered norm of Q as defined in Eqn. <ref type="bibr" target="#b1">(2)</ref>. Thus, we derive the RMSE between the linearly mapped scores and the subjective quality scores as follows.</p><formula xml:id="formula_19">RMSE(k * 1Q + k * 2 , Q) = 4b 2 l ? (Q, Q)/N .<label>(18)</label></formula><p>That is, a special case of the loss variant l ? (Q, Q) with p = 2, q = 2 is connected with RMSE -another criterion for benchmarking IQA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL ANALYSIS</head><p>We introduce the concepts of Lipschitzness and ?-smoothness <ref type="bibr" target="#b21">[22]</ref>.</p><formula xml:id="formula_20">For a univariate function f , f is L-Lipschitz if | f (x 1 ) ? f (x 2 )| ? L|x 1 ? x 2 |, ?x 1 , x 2 . And f is ?-smooth if its gradient is ?-Lipschitz,</formula><p>i.e., ?-smoothness corresponds to the Lipschitzness of the gradient. The proposed loss l is a differentiate multivariate function w.r.t. the model predictionsQ. Its Lipschitzness is indicated by its gradient magnitude and its ?-smoothness in the gradient direction is indicated by the quadratic form of its Hessian matrix. Smaller gradient magnitude and quadratic form of its Hessian correspond to better Lipschitzness and ?-smoothness, respectively. In this section, we theoretically prove that, when q equals 2, the embedded normalization can improve the Lipschitzness and the ?smoothness of the proposed loss l. That is, the gradient magnitude and the quadratic form of its Hessian are reduced by the embedded normalization, which indicates the gradients of the proposed loss is more stable and more predictable. This ensures that the gradientbased algorithm has a smoother loss landscape, so the training of the IQA model gets more robust and the model converges faster.</p><p>First, we prove a theorem about Lipschitzness. When q = 2, based on Eqn.</p><formula xml:id="formula_21">(8),R =?, ?R = 1 N ?1,?? = 0. Denote g = ?l ?? , g n = ?l ?Q .</formula><p>Thus Eqn. (9) becomes</p><formula xml:id="formula_22">g n = 1 b g ? 1 N 1 T ?1, g? ?? T g,? .<label>(19)</label></formula><p>We show that the gradient magnitude of the new loss satisfies Eqn. <ref type="bibr" target="#b19">(20)</ref> in Theorem 3.1 (See proof in Supplemental Materials C). Theorem 3.1 (Lipschitzness). When q equals 2, the gradient magnitude of the proposed loss l has the following property.</p><formula xml:id="formula_23">?g n ? 2 = 1 b 2 ?g? 2 ? 1 N ?1, g? 2 ? g,? 2 ,<label>(20)</label></formula><p>where the right side contains three terms. The first term is directly related to the gradient of the loss w.r.t. the normalized predicted quality scores, i.e., The left side of Eqn. <ref type="bibr" target="#b19">(20)</ref>, ?g n ? 2 , indicates the Lipschitzness with the embedded normalization. Without the normalization, the Lipschitzness is indicated by ?g? 2 . From Eqn. <ref type="bibr" target="#b19">(20)</ref>, we derive that the Lipschitzness of the proposed loss is improved whenever the sum of the gradient g deviates from 0 or the gradient g correlates the normalized predicted quality scores?. In addition,b is larger than 1 in practice (see Supplemental Materials B), which also contributes to the improvement of the Lipschitzness. So from Theorem 3.1, we can infer that the embedded normalization improves the Lipschitzness.</p><p>Next, we prove a theorem about ?-smoothness. Denote H =</p><formula xml:id="formula_24">? 2 l ?? 2 , H n = ? 2 l ?Q 2 .</formula><p>We then prove that the quadratic form of the loss Hessian in the gradient direction satisfies Eqn. <ref type="bibr" target="#b21">(22)</ref> in Theorem 3.2 (The proof is provided in the Supplemental Materials D). Theorem 3.2 (?-smoothness). When q equals 2, the Hessian matrix of the proposed loss l has the following property.</p><formula xml:id="formula_25">g T n H n g n = 1 b 2 g T n Hg n ? ?g,?? ?g n ? 2 .<label>(21)</label></formula><p>Further, when p equals 2, we have g = 2 c (? ? S), H = 2 c I, where I is the identity matrix of order N . The above equation becomes</p><formula xml:id="formula_26">g T n H n g n = 1 b 4 g T Hg ? 2 c 1 ? ?S,?? 4 c 2 1 ? ?S,?? + ?g? 2 ? g,? 2 .<label>(22)</label></formula><p>The left side of Eqn. <ref type="bibr" target="#b21">(22)</ref>, g T n H n g n , indicates the ?-smoothness with the embedded normalization. Without the normalization, the ?-smoothness is indicated by g T Hg. From Eqn. <ref type="bibr" target="#b21">(22)</ref>, we can see that the ?-smoothness is improved when the normalized subjective quality scores S and normalized predicted quality scores? are not linearly correlated (?S,?? &lt; 1). And it is further improved if the gradient g and the normalized predicted quality scores? are also not linearly correlated (?g,?? &lt; ?g?). In addition,b &gt; 1 also contributes to the improvement of the ?-smoothness. So from Theorem 3.2,  we can infer that the embedded normalization improves the ?smoothness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IQA Model Linear Mapping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VERIFICATION ON QUALITY ASSESSMENT OF IN-THE-WILD IMAGES</head><p>Besides theoretical analysis, we also conduct an experimental verification. Quality assessment of in-the-wild images is important for many real-world applications, but few attention is paid to it.</p><p>In-the-wild images contain lots of unique contents and complex distortions. The greatest challenge for this problem is how to handle the content dependency and distortion complexity. In this section, we pick this challenging problem for verifying the effectiveness of our "Norm-in-Norm" loss in comparison with MAE loss and MSE loss. In addition, we intend to provide state-of-the-art prediction performance on this challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IQA Framework</head><p>Our IQA framework for in-the-wild images is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We introduce a model that extracts deep pre-trained features for tackling content dependency and fuses multi-level features for handling the distortion complexity. Specifically, it first extracts multi-level deep feature extraction from an image classification backbone (e.g., 32x8d ResNeXt-101 <ref type="bibr" target="#b28">[29]</ref>). Then feature aggregation is achieved by global average pooling (GAP). Next, features are encoded by an encoder with three fully-connected (FC) layers, where each FC layer is followed by a batch normalization (BN) <ref type="bibr" target="#b9">[10]</ref> layer and a ReLU activation function. After that, the IQA model concatenates the encoded features from different levels, and the concatenated features are finally mapped to the output by an FC layer. After training the IQA model, to determine the linear mapping from the predictions to the subjective quality scores, the least squares regression (LSR) method is applied on the whole training set. In the testing phase, based on the learned linear mapping, the prediction of the IQA model is linearly mapped to produce the final predicted quality score for a test image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We conduct experiments on two benchmark datasets: CLIVE <ref type="bibr" target="#b5">[6]</ref> and KonIQ-10k <ref type="bibr" target="#b8">[9]</ref>. We follow the same experimental setup as described in <ref type="bibr" target="#b8">[9]</ref>. KonIQ-10k contains 10073 images, and they are divided into three sets: a training set (7058 images), a validation set (1000 images), and a test set (2015 images). We train our model on the training set of KonIQ-10k, save the best performed model on the validation set of KonIQ-10k in terms of Spearman's Rank-Order Correlation Coefficient (SROCC). We report the SROCC, PLCC, and RMSE values on the test set of KonIQ-10k for prediction performance evaluation. CLIVE includes 1162 images, and it is used for cross-dataset evaluation.</p><p>Implementation Details. The input images is resized 664 ? 498. The backbone models for multi-level feature extraction are chosen from ResNet-18, ResNet-34, ResNet-50 <ref type="bibr" target="#b6">[7]</ref>, and ResNeXt-101 <ref type="bibr" target="#b28">[29]</ref> pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref>. And the features are extracted from the stage "conv4" and stage "conv5" of the backbone. To explicitly encode the features at each level to a task-aware feature space, we add auxiliary supervision to the encoded feature at each level. That is, the encoded feature is directly followed by a single FC layer to output the image quality score. Thus, beside the main stream loss, we get another two streams of losses. The final training loss is a weighted average of the three loss values, where the weight hyperparameters for the main stream loss and the other two streams of losses set to 1, 0.1, 0.1, respectively. We train the model with NVIDIA GeForce RTX 2080 Ti GPU using Adam optimizer for 30 epochs, where the learning rate drops to its 1/10 every 10 epochs. The initial learning rate is chosen from 1e-3, 1e-4, and 1e-5. The batch size varies from <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16</ref>. And the ratio between the learning rate of the backbone's parameters and of the other parameters, denoted as "fine-tuned rate", is selected from 0, 0.01, 0.1, and 1. By default, we use an initial learning rate 1e-4, batch size 8, and fine-tuned rate 0.1. The default values for hyper-parameters p and q in the "Norm-in-Norm" loss are 1 and 2, respectively. The proposed model is implemented with PyTorch <ref type="bibr" target="#b22">[23]</ref>. To support reproducible scientific research, we have released our code at https://github.com/ lidq92/LinearityIQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head><p>In this subsection, we show the experimental results and verify the proposed loss in different aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Model Convergence With MAE, MSE, or the Proposed Loss.</head><p>In this experiment, p, q in the proposed loss are set to 1, 2, and we adopt the backbone ResNeXt-101 for models trained with all losses. The training/validation/testing curves on KonIQ-10k are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Looking at the circle markers, to reach the prediction performance indicated by the grey dash line, MAE and MSE are empirically about ten times slower than the proposed loss. For MAE or MSE loss, to achieve a comparable prediction performance with the proposed loss, the models need to be trained with much more time. And the final state of the convergence also indicates that our proposed loss achieves better prediction performance (higher SROCC/PLCC and lower RMSE) than MAE loss and MSE loss. We experimentally conclude that the model trained with our proposed loss converges faster and better than that with MAE or MSE loss. Our method may look similar to adding a BN layer to the output of the current model. Thus, our method is also compared with "bnMSE", where a BN layer is added to the output of the model and the model is trained with MSE loss. <ref type="figure" target="#fig_4">Figure 4</ref> shows the validation curves on KonIQ-10k. When compared to MSE, "bnMSE" leads to faster convergence and better performance. However, it is worse than the proposed "Norm-in-Norm". This is because the learned linear relationship in "bnMSE" is based on the cumulation of the batch-sample statistics, which is not accurate at the beginning. Thus, it will slow down the convergence and somehow disturb the learning process. On the contrast, the proposed method separates the network learning and the learning of the linear relationship, where the network first focuses on making linear predictions and then LSR is applied on the whole training set to determine a more accurate linear relationship. Besides, it should be noted that, unlike "bnMSE", the proposed method does not change the architecture and it also normalizes subjective quality scores. Backbone p = 1 p = 1 p = 2 p = 2, q = 2, i.e. q = 1 q = 2 q = 1 PLCC-induced loss 2 Effects of p, q in the "Norm-in-Norm" Loss. In this experiment, we explore the effect of p, q in the proposed loss functions. We consider four choices: p = 1, q = 1, p = 1, q = 2, p = 2, q = 1, and p = 2, q = 2 (i.e., PLCC-induced loss). The PLCC values on KonIQ-10k test set for different p, q under different backbones are shown in <ref type="table" target="#tab_1">Table 1</ref>. We can see that p = 1 is generally better than p = 2. This can be explained by the fact that the loss with p = 2 is more sensitive to the outliers than the loss with p = 1. Although the loss with p = 1, q = 1 is slightly inferior to the loss with p = 1, q = 2, L 1 normalization (i.e., q = 1) may improve numerical stability in low-precision implementations as pointed out in <ref type="bibr" target="#b7">[8]</ref>. Besides, we now only focus on the task of quality assessment for in-the-wild images, and the results shows that p = 1, q = 2 is the best choice for KonIQ-10k. However, the best p, q may be task-dependent and dataset-dependent. It deserves a further study on how to adaptively determine these hyper-parameters in a probability framework, just like the study described in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Training Stability</head><p>With MAE, MSE, or the Proposed Loss Under Different Learning Hyper-parameters. In this experiment, we consider use ResNet-50 as the backbone model, and vary the default initial learning rate, batch size, and fine-tuned rate to see the training stability under these hyper-parameters. The validation curves on KonIQ-10k are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. We can see that training models with MAE or MSE loss are unstable when varying the learning rates, batch sizes. And training the model with MAE loss is unstable when fine-tuned rate is too large. Compared to MAE loss and MSE loss, the "Norm-in-Norm" loss is more stable under different choices. For all losses, the best results are achieved under an initial learning rate 1e-4 and a fine-tuned rate 0.1. However, our "Norm-in-Norm" loss can achieve a better validation performance under batch size 16 than under batch size 8, while the MAE or MSE loss does not. This is because our loss is a batch-correlated loss, and a larger batch size may lead to a more accurate estimation of the sample statistics. For fair comparison, in other experiments, we also use the default batch size, i.e., 8, for the proposed loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-18</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-34</head><p>ResNet   <ref type="figure" target="#fig_6">Figure 6</ref>. It can be seen that our proposed loss consistently achieves the best prediction performance under different backbone architectures. The scatter plots between the predicted quality scores by the models using backbone ResNeXt-101 and MOSs on KonIQ-10k test set are shown in <ref type="figure" target="#fig_7">Figure 7</ref>. The scatter points of the model with our loss are more centered in the diagonal line, which means a better prediction of image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison with SOTA</head><p>In this part, we compare our final model with the state-of-theart (SOTA) models, i.e., BRISQUE <ref type="bibr" target="#b19">[20]</ref>, CORNIA <ref type="bibr" target="#b31">[32]</ref>, HOSA <ref type="bibr" target="#b29">[30]</ref>, DeepBIQ <ref type="bibr" target="#b2">[3]</ref>, CNNIQA <ref type="bibr" target="#b10">[11]</ref>, DeepRN <ref type="bibr" target="#b26">[27]</ref>, and KonCept512 <ref type="bibr" target="#b8">[9]</ref>. The first three models map handcrafted features to image quality by SVR. The fourth model maps the fine-tuned deep features to image quality by SVR. The last three deep learning-based models adopt MAE, MSE, or their variant Huber loss for network training. And the results of these models are taken from Hosu et al. <ref type="bibr" target="#b8">[9]</ref>, while our results are obtained in the same setting. From <ref type="table" target="#tab_4">Table 2</ref>, we can see that our model outperforms classic and current deep learning-based models. We note that by combining the loss and its variant with  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Realizing that most IQA methods train models using MAE or MSE loss with empirically slow convergence, we address this problem by designing a class of loss functions with normalization. The proposed loss includes Pearson correlation-induced loss as a special case. And a special case of the loss variant is connected with RMSE between the linearly mapped predictions and the subjective ratings. We theoretically prove that the embedded normalization helps to improve the smoothness of the loss landscape. Besides, experimental verification of the proposed loss is conducted on the quality assessment of in-the-wild images. Results on two benchmark datasets (KonIQ-10k and CLIVE) show that the model converges faster and better with the proposed loss than that with MAE loss and MSE loss. The proposed loss is invariant to the scale of subjective ratings. Facilitated with the new loss, we can easily mix multiple datasets with different scales of subjective ratings for training a universal IQA model. In the future study, we intend to verify the effectiveness of this new loss in the universal image and video quality assessment problems. Besides, it is a future direction on how to optimally choose the hyper-parameters p and q in the class of proposed loss functions for a specific regression task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DERIVATION OF c IN EQN. (6)</head><p>Lemma A.1. When x = (x 1 , ? ? ? , x N ), 0 &lt; p 1 ? p 2 &lt; +?, we have the following norm inequality.</p><formula xml:id="formula_27">?x? p 2 ? ?x? p 1 ? N 1 p 1 ? 1 p 2 ?x? p 2<label>(23)</label></formula><p>Proof. We will separately prove the left part and the right part. 1. Proof of the left part.</p><formula xml:id="formula_28">Denote y i = |x i | p 2 ? 0. We have y i p 1 /p 2 = |x i | p 1 , 0 &lt; p 1 /p 2 ? 1, and 0 ? y i N j=1 y i ? 1. Then N i=1 y i N j=1 y i p 1 /p 2 ? N i=1 y i N j=1 y i = 1 (24) That is N i=1 y i p 1 /p 2 ? N i=1 y i p 1 /p 2 (25) N i=1 |x i | p 1 ? N i=1 |x i | p 2 p 1 /p 2 (26) N i=1 |x i | p 1 1/p 1 ? N i=1 |x i | p 2 1/p 2<label>(27)</label></formula><p>?x? p 2 ? ?x? p 1</p><p>2. Proof of the right part. Based on H?lder inequality, we directly get</p><formula xml:id="formula_30">N i=1 |x i | p 1 * 1 ? N i=1 (|x i | p 1 ) p 2 /p 1 p 1 /p 2 N i=1 1 1/(1?p 1 /p 2 ) 1?p 1 /p 2 =N 1?p 1 /p 2 N i=1 |x i | p 2 p 1 /p 2<label>(29)</label></formula><p>Applying p 1 -th root calculation to the above equation, we derive</p><formula xml:id="formula_31">?x? p 1 ? N 1 p 1 ? 1 p 2 ?x? p 2<label>(30)</label></formula><p>In summary, we proof the lemma. ? Derivation of c in Eqn. <ref type="formula" target="#formula_5">(6)</ref>: Based on p ? 1 and the Minkowski inequality, we have</p><formula xml:id="formula_32">?? ? S? p ? ??? p + ?S? p<label>(31)</label></formula><p>Together with the above lemma and ??? q = ?S? q = 1, we can derive</p><formula xml:id="formula_33">??? p + ?S? p ? (??? q + ?S? q ) * N 1 p ? 1 q 1 = 2N 1 p ? 1 q if p &lt; q, 2 if p ? q. (32) Thus c = max ?? ? q = ?S ? q =1 ?? ? S? p p = 2 p N 1? p q if p &lt; q, 2 p if p ? q.<label>(33)</label></formula><p>Bb CURVE IN OUR EXPERIMENT <ref type="figure" target="#fig_8">Figure 8</ref> shows theb curve with respect to iteration, and <ref type="figure" target="#fig_9">Figure 9</ref> shows the averageb curve with respect to epoch. We can see that b is larger than 1 in practice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PROOF OF THEOREM 3.1 (LIPSCHITZNESS)</head><p>Proof. Denote Z = g ? 1 N 1 T ?1, g?, and based on ?1,?? = 0, Eqn. <ref type="bibr" target="#b18">(19)</ref> becomes Thus</p><formula xml:id="formula_34">g n = 1 b Z ?? T ?Z,??<label>(34)</label></formula><formula xml:id="formula_35">?g n ? 2 = 1 b 2 Z ?? T ?Z,?? 2 = 1 b 2 ?Z? 2 ? 2?Z,?? 2 + ??,???Z,?? 2 = 1 b 2 ?Z? 2 ? 2?Z,?? 2 + ?Z,?? 2 (Because ??? 2 = 1) = 1 b 2 ?Z? 2 ? ?Z,?? 2 = 1 b 2 g ? 1 N 1 T ?1, g? 2 ? g ? 1 N 1 T ?1, g? ,? 2 = 1 b 2 g ? 1 N 1 T ?1, g? 2 ? g,? ? 1 N 1,? ?1, g? 2 = 1 b 2 g ? 1 N 1 T ?1, g? 2 ? g,? 2 = 1 b 2 ?g? 2 ? 1 N ?1, g? 2 ? g,? 2 (35) ? D PROOF OF THEOREM 3.2 (?-SMOOTHNESS)</formula><p>Proof. When q equals 2, Eqn. <ref type="formula" target="#formula_6">(7)</ref> becomes ??</p><formula xml:id="formula_36">?Q = 1 b I ? 1 N 11 T ??? T = 1 b K<label>(36)</label></formula><p>And based on ?1,?? = 0, ??,?? = 1, we derive the property of K as follow. </p><formula xml:id="formula_37">K? = 0,? T K = 0 T , K 2 = K = K T<label>(</label></formula><formula xml:id="formula_38">? 1 b ?Q = ? 1 b 3 (Q T ? 1 T? ) = ? 1 b 2? T (41) So H n = ? ?Q 1 b g ? 1 N 1 ?1, g? ???g,?? = 1 b ? ?? g ? 1 N 1 ?1, g? ???g,?? ?? ?Q + g ? 1 N 1 ?1, g? ???g,?? ? 1 b ?? = 1 b 2 H ? 1 N MH ? ?g,?? ??? T H ??g T I ? 1 N M ??? T ? 1 b 2 g ? 1 N 1 ?1, g? ???g,?? ? T = 1 b 2 I ? 1 N M ??? T H I ? 1 N M ??? T ? 1 b 2 ?g,?? +?g T I ? 1 N M ??? T ? 1 b 2 I ? 1 N M ??? T g? T<label>(42)</label></formula><p>Note that K = I ? 1 N 11 T ??? T , and we have</p><formula xml:id="formula_39">H n = 1 b 2 KHK ? ?g,??K ??g T K ? Kg? T (43) Then b 4 g T n H n g n =g T K KHK ? ?g,??K ??g T K ? Kg? T Kg =g T K 2 HK 2 g ? ?g,??g T K 3 g ? g T K?g T K 2 g ? g T K 2 g? T Kg =g T KHKg ? ?g,??g T K 2 g ? g T (K?)g T Kg ? g T Kg(? T K)g =(g T K)H(Kg) ? ?g,??(g T K)(Kg) =b 2 g T n Hg n ? ?g,?? ?g n ? 2<label>(44)</label></formula><p>That is</p><formula xml:id="formula_40">g T n H n g n = 1 b 2 g T n Hg n ? ?g,?? ?g n ? 2<label>(45)</label></formula><p>Further, when p equals 2, we derive g = 2 c (? ? S), H = 2 c I. Then </p><formula xml:id="formula_41">g T n Hg n = 1 b Kg T 2 c I 1 b Kg = 2 cb 2 g T K T Kg = 2 cb 2 g T K 2 g = 2 cb 2 g T Kg = 2 cb 2 g T I ? 1 N 11 T ??? T g = 1 b 2 g T H g ? 2 cN ?g, 1? 2 ? 2 c ?g,?? 2<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL RESULTS E.1 Different Optimizers</head><p>Besides Adam, we show additional results with the SGD/Adadelta optimizer. The experimental setting for <ref type="table" target="#tab_6">Table 3</ref> is similar to the experimental setting for <ref type="figure" target="#fig_5">Figure 5</ref>(a). We just replaced Adam with SGD or Adadelta. It can be seen that the model performances are very sensitive to the initial learning rates when using the SGD or Adadelta optimizer. Besides, the best initial learning rate is 1e-1 for the proposed loss, 1e-2 for the MAE loss, and 1e-4 for the MSE loss. From <ref type="table" target="#tab_6">Table 3</ref>, we can see that the proposed loss is better than the MAE and MSE losses when the SGD or Adadelta optimizer is used and the best initial learning rate is chosen. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Different Architectures</head><p>The experimental setting for <ref type="table" target="#tab_7">Table 4</ref> is similar to the experimental setting for Sec. 4.3.4. We just used the non-BN network architecture (AlexNet or VGG-16) instead of the ResNet-based network as the backbone. From <ref type="table" target="#tab_7">Table 4</ref>, we can see that the proposed loss is better than the MSE and MSE losses when AlexNet/VGG-16 is used as the backbone. Together with the experiments on ResNet-based backbones, it can be seen that Batch Normalization layers in networks will not affect the superiority new loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 t-test</head><p>In the paper, the results shown in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_4">Table 2</ref> were based on the experiments on one train-validation-test split provided by the KonIQ-10k dataset's owner. However, for performing the t-test to verify whether the performance gains in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_4">Table 2</ref> are statistically significant or not, we need several experiments on different train-validation-test splits. We conducted experiments on 10 random splits of CLIVE with the ResNet-18 backbone and performed the t-test for different combinations of p and q, as well as "l" and "l + 0.1l ? ". The results show that, in terms of PLCC, (a) "p = 1, q = 2" is significantly better than "p = 2, q = 1" (p-value 0.018) and "p = 2, q = 2" (p-value 0.028), while it is on par with "p = 1, q = 1" (p-value 0.385). (b) "l + 0.1l ? " is slightly (but not significantly) better than "l" (p-value 0.172).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The training/validation/testing curves on KonIQ-10k of the models trained with MAE, MSE, and the proposed "Normin-Norm" loss. SROCC, PLCC and RMSE are three criteria for benchmarking IQA models, where larger SROCC/PLCC and smaller RMSE indicate better prediction performance. The circle marker shows the first time it surpasses a prediction performance indicated by the grey dash line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the forward and backward paths of the proposed loss. m(?) denotes the mean function. F (?; ? ) is the IQA model, where ? represents the model parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?g? 2 .</head><label>2</label><figDesc>The second term (non-positive) is contributed by ?l ?? . The third term (non-positive) is contributed by ?l ?b . The contributions of ?l ?? and ?l ?b are independent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The framework for quality assessment of in-thewild images. Note that C 1 |C 2 denotes a fully-connected (FC) layer. BN and ReLU are omitted in the illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The validation curves on KonIQ-10k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>rate: 0.0 Fine-tuned rate: 0.01 Fine-tuned rate: 0.1 Fine-tuned rate: 1.0 (c) Different fine-tuned rates PLCC curves on KonIQ-10k validation set using models trained with MAE, MSE, or "Norm-in-Norm" loss. The incomplete curves indicate that the training process is stopped due to the encounter of NaNs or Infs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Performance under different backbone models 4.3.4 Performance Consistency for MAE, MSE, or the Proposed Loss Among Different Backbone Architectures. In this experiment, we consider ResNet-18, ResNet-34, ResNet-50, and ResNeXt-101 as the backbone architectures, and train the models with MAE, MSE, and the proposed loss. The PLCC on KonIQ-10k test set and the SROCC on CLIVE are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Scatter plots between the predicted quality and the MOS on KonIQ-10k test set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Theb curve with respect to iteration in our experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>The averageb curve with respect to epoch in our experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 ?g? 2 ? g,? 2 ( 49 )c 2 1 ? 2 . 2 ? 0 .</head><label>222491220</label><figDesc>on the above equation and Theorem 3.1, we have ?g n ? 2 = 1 b Use the above four equations to substitute the corresponding parts in Eqn. (45), we can derive the following equation. ?S,?? + ?g? 2 ? g,? Based on Cauchy inequality, ?S,?? ? ?S? 2 ??? 2 = 1, ?g,?? 2 ? ?g? 2 ??? 2 = ?g? 2 . So 1 ? ?S,?? 4 c 2 1 ? ?S,?? + ?g? 2 ? g,? This indicates that the embedded normalization reduces the quadratic form of the loss Hessian. At the meantime, since ?g, 1? = 0, ?l ?? does not contribute to the reduction and it is all provided by ?l ?b .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>PLCC on KonIQ-10k test set under different p, q</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with SOTA on KonIQ-10k test set and the whole CLIVE</figDesc><table><row><cell></cell><cell></cell><cell cols="2">KonIQ-10k</cell><cell cols="2">CLIVE</cell></row><row><cell>Model</cell><cell>Loss</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">SROCC PLCC SROCC PLCC</cell></row><row><cell>BRISQUE (TIP'12)</cell><cell>SVR loss</cell><cell>0.705</cell><cell>0.707</cell><cell>0.561</cell><cell>0.598</cell></row><row><cell>CORNIA (CVPR'12)</cell><cell>SVR loss</cell><cell>0.780</cell><cell>0.808</cell><cell>0.621</cell><cell>0.644</cell></row><row><cell>HOSA (TIP'16)</cell><cell>SVR loss</cell><cell>0.805</cell><cell>0.828</cell><cell>0.628</cell><cell>0.668</cell></row><row><cell>DeepBIQ (SIViP'18)</cell><cell>SVR loss</cell><cell>0.872</cell><cell>0.886</cell><cell>0.742</cell><cell>0.747</cell></row><row><cell cols="2">CNNIQA (CVPR'14) MAE loss</cell><cell>0.572</cell><cell>0.584</cell><cell>0.465</cell><cell>0.450</cell></row><row><cell>DeepRN (ICME'18)</cell><cell>Huber loss</cell><cell>0.867</cell><cell>0.880</cell><cell>0.720</cell><cell>0.750</cell></row><row><cell cols="2">KonCept512 (TIP'20) MSE loss</cell><cell>0.921</cell><cell>0.937</cell><cell>0.825</cell><cell>0.848</cell></row><row><cell>Proposed</cell><cell>l l + 0.1l ?</cell><cell>0.937 0.938</cell><cell>0.947 0.947</cell><cell>0.834 0.836</cell><cell>0.849 0.852</cell></row><row><cell cols="6">a weight of 1 and 0.1, our model can even achieve better results,</cell></row><row><cell cols="6">e.g., SROCC values are 0.938 and 0.836 on KonIQ-10k test set and</cell></row><row><cell>CLIVE, respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>37) Denote M = 11 T = M T , and we know H = H T . Thus</figDesc><table><row><cell></cell><cell cols="2">g n =</cell><cell>1 b</cell><cell cols="2">Kg</cell><cell>(38)</cell></row><row><cell cols="2">?1 ?1, g? ??</cell><cell cols="4">= 11 T ? 2 l ?? 2</cell><cell>= MH</cell><cell>(39)</cell></row><row><cell>???g,?? ??</cell><cell cols="2">=?g,??</cell><cell cols="2">?? ??</cell><cell>+?</cell><cell>??g,?? ??</cell></row><row><cell></cell><cell cols="5">=?g,?? +?? T H +?g T</cell><cell>(40)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>PLCC comparisons under SGD/Adadelta optimizer</figDesc><table><row><cell cols="2">Initial learning rate MAE loss</cell><cell>MSE loss</cell><cell>Proposed loss</cell></row><row><cell>1e-1</cell><cell cols="3">0.843/0.780 Failed/0.069 0.931/0.930</cell></row><row><cell>1e-2</cell><cell cols="2">0.909/0.861 0.781/0.690</cell><cell>0.916/0.911</cell></row><row><cell>1e-3</cell><cell cols="2">0.868/0.068 0.839/0.701</cell><cell>0.899/0.889</cell></row><row><cell>1e-4</cell><cell cols="2">0.620/0.007 0.890/0.739</cell><cell>0.868/0.808</cell></row><row><cell>1e-5</cell><cell cols="2">0.090/0.138 0.851/0.458</cell><cell>0.770/0.458</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>SROCC/PLCC comparisons under non-BN network architectures</figDesc><table><row><cell cols="2">Backbone MAE loss</cell><cell>MSE loss</cell><cell>Proposed loss</cell></row><row><cell>AlexNet</cell><cell cols="2">0.788/0.779 0.811/0.799</cell><cell>0.879/0.886</cell></row><row><cell>VGG-16</cell><cell cols="2">0.840/0.834 0.844/0.835</cell><cell>0.910/0.913</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the National Natural Science Foundation of China under contracts 61572042, 61527804 and 61520106004. We also acknowledge High-Performance Computing Platform of Peking University for providing computational resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SpEED-QA: Spatial efficient entropic differencing for image and video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Christos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praful</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1333" to="1337" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A general and adaptive robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4331" to="4339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the use of deep learning for blind image quality assessment. Signal,Image and Video Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Celona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Maniry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Massive online crowdsourced study of subjective and objective picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="372" to="387" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Norm matters: Efficient and accurate normalization schemes in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2160" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">KonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sziranyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4041" to="4056" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning of human visual sensitivity in image quality assessment framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1676" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hallucinated-IQA: No-reference image quality assessment via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanxiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="732" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end blind quality assessment of compressed videos using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfang</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RankIQA: Learning from rankings for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1040" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reduced-reference image quality assessment in free-energy principle and sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="379" to="391" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geometric transformation invariant image quality assessment using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfang</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6732" to="6736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Group MAD competition-a new methodology to compare objective image quality models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfang</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reduced-reference image quality assessment using reorganized DCT-based image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">King</forename><surname>Ngi Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="824" to="829" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noreference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization in the final layer of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Mullery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul F Whelan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07389</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introductory lectures on convex optimization: A basic course</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RAN4IQA: Restorative adversarial nets for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diqi</forename><surname>Hongyu Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7308" to="7314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learned video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carissa</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3454" to="3463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2483" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DeepRN: A content preserving deep architecture for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domonkos</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam?s</forename><surname>Szir?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Blind Image Quality Assessment Based on High Order Statistics Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaohong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="4444" to="4457" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fractal analysis for reduced reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2098" to="2109" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Peng Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">VSI: A visual saliency-induced index for perceptual image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="4270" to="4281" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RankSRGAN: Generative adversarial networks with ranker for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3096" to="3105" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
