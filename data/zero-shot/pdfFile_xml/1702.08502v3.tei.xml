<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Convolution for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TuSimple</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TuSimple</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
							<email>3dingliu2@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<postCode>4 UC</postCode>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TuSimple</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
							<email>xiaodi.hou@tusimple.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">TuSimple</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
						</author>
						<title level="a" type="main">Understanding Convolution for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to significant improvement over previous semantic segmentation systems.</p><p>Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are of both theoretical and practical value. First, we design dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive fields (RF) of the network to aggregate global information; 2) alleviates what we call the "gridding issue"caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a state-of-art result of 80.1% mIOU in the test set at the time of submission. We also have achieved state-of-theart overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Our source code can be found at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation aims to assign a categorical label to every pixel in an image, which plays an important role in image understanding and self-driving systems. The recent success of deep convolutional neural network (CNN) models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref> has enabled remarkable progress in pixel-wise semantic segmentation tasks due to rich hierarchical features and an end-to-end trainable framework <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3]</ref>. Most state-of-the-art semantic segmentation systems have three key components:1) a fullyconvolutional network (FCN), first introduced in <ref type="bibr" target="#b20">[21]</ref>, re-placing the last few fully connected layers by convolutional layers to make efficient end-to-end learning and inference that can take arbitrary input size; 2) Conditional Random Fields (CRFs), to capture both local and long-range dependencies within an image to refine the prediction map; 3) dilated convolution (or Atrous convolution), which is used to increase the resolution of intermediate feature maps in order to generate more accurate predictions while maintaining the same computational cost.</p><p>Since the introduction of FCN in <ref type="bibr" target="#b20">[21]</ref>, improvements on fully-supervised semantic segmentation systems are generally focused on two perspectives: First, applying deeper FCN models. Significant gains in mean Intersection-over-Union (mIoU) scores on PASCAL VOC2012 dataset <ref type="bibr" target="#b7">[8]</ref> were reported when the 16-layer VGG-16 model <ref type="bibr" target="#b25">[26]</ref> was replaced by a 101-layer ResNet-101 <ref type="bibr" target="#b12">[13]</ref> model <ref type="bibr" target="#b2">[3]</ref>; using 152 layer ResNet-152 model yields further improvements <ref type="bibr" target="#b27">[28]</ref>. This trend is consistent with the performance of these models on ILSVRC <ref type="bibr" target="#b22">[23]</ref> object classification tasks, as deeper networks generally can model more complex representations and learn more discriminative features that better distinguish among categories. Second, making CRFs more powerful. This includes applying fully connected pairwise CRFs <ref type="bibr" target="#b15">[16]</ref> as a post-processing step <ref type="bibr" target="#b2">[3]</ref>, integrating CRFs into the network by approximating its mean-field inference steps <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref> to enable end-to-end training, and incorporating additional information into CRFs such as edges <ref type="bibr" target="#b14">[15]</ref> and object detections <ref type="bibr" target="#b0">[1]</ref>.</p><p>We are pursuing further improvements on semantic segmentation from another perspective: the convolutional operations for both decoding (from intermediate feature map to output label map) and encoding (from input image to feature map) counterparts. In decoding, most state-of-the-art semantic segmentation systems simply use bilinear upsampling (before the CRF stage) to get the output label map <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>. Bilinear upsampling is not learnable and may lose fine details. Inspired by work in image super-resolution <ref type="bibr" target="#b24">[25]</ref>, we propose a method called dense upsampling convo-lution (DUC), which is extremely easy to implement and can achieve pixel-level accuracy: instead of trying to recover the full-resolution label map at once, we learn an array of upscaling filters to upscale the downsized feature maps into the final dense feature map of the desired size. DUC naturally fits the FCN framework by enabling end-toend training, and it increases the mIOU of pixel-level semantic segmentation on the Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref> significantly, especially on objects that are relatively small.</p><p>For the encoding part, dilated convolution recently became popular <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>, as it maintains the resolution and receptive field of the network by in inserting "holes"in the convolution kernels, thus eliminating the need for downsampling (by max-pooling or strided convolution). However, an inherent problem exists in the current dilated convolution framework, which we identify as "gridding": as zeros are padded between two pixels in a convolutional kernel, the receptive field of this kernel only covers an area with checkerboard patterns -only locations with non-zero values are sampled, losing some neighboring information. The problem gets worse when the rate of dilation increases, generally in higher layers when the receptive field is large: the convolutional kernel is too sparse to cover any local information, since the non-zero values are too far apart. Information that contributes to a fixed pixel always comes from its predefined gridding pattern, thus losing a huge portion of information. Here we propose a simple hybrid dilation convolution (HDC) framework as a first attempt to address this problem: instead of using the same rate of dilation for the same spatial resolution, we use a range of dilation rates and concatenate them serially the same way as "blocks"in ResNet-101 <ref type="bibr" target="#b12">[13]</ref>. We show that HDC helps the network to alleviate the gridding problem. Moreover, choosing proper rates can effectively increases the receptive field size and improves the accuracy for objects that are relatively big.</p><p>We design DUC and HDC to make convolution operations better serve the need of pixel-level semantic segmentation. The technical details are described in Section 3 below. Combined with post-processing by Conditional Random Fields (CRFs), we show that this approach achieves state-of-the art performance on the Cityscapes pixel-level semantic labeling task, KITTI road estimation benchmark, and PASCAL VOC2012 segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Decoding of Feature Representation: In the pixel-wise semantic segmentation task, the output label map has the same size as the input image. Because of the operation of max-pooling or strided convolution in CNNs, the size of feature maps of the last few layers of the network are inevitably downsampled. Multiple approaches have been proposed to decode accurate information from the downsampled feature map to label maps. Bilinear interpolation is commonly used <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>, as it is fast and memoryefficient. Another popular method is called deconvolution, in which the unpooling operation, using stored pooling switches from the pooling step, recovers the information necessary for feature visualization <ref type="bibr" target="#b29">[30]</ref>. In <ref type="bibr" target="#b20">[21]</ref>, a single deconvolutional layer is added in the decoding stage to produce the prediction result using stacked feature maps from intermediate layers. In <ref type="bibr" target="#b6">[7]</ref>, multiple deconvolutional layers are applied to generate chairs, tables, or cars from several attributes. Noh et al. <ref type="bibr" target="#b21">[22]</ref> employ deconvolutional layers as mirrored version of convolutional layers by using stored pooled location in unpooling step. <ref type="bibr" target="#b21">[22]</ref> show that coarseto-fine object structures, which are crucial to recover finedetailed information, can be reconstructed along the propagation of the deconvolutional layers. Fischer at al. <ref type="bibr" target="#b8">[9]</ref> use a similar mirrored structure, but combine information from multiple deconvolutional layers and perform upsampling to make the final prediction.</p><p>Dilated Convolution: Dilated Convolution (or Atrous convolution) was originally developed in algorithme? trous for wavelet decomposition <ref type="bibr" target="#b13">[14]</ref>. The main idea of dilated convolution is to insert "holes"(zeros) between pixels in convolutional kernels to increase image resolution, thus enabling dense feature extraction in deep CNNs. In the semantic segmentation framework, dilated convolution is also used to enlarge the field of convolutional kernels. Yu &amp; Koltun <ref type="bibr" target="#b28">[29]</ref> use serialized layers with increasing rates of dilation to enable context aggregation, while <ref type="bibr" target="#b2">[3]</ref> design an "atrous spatial pyramid pooling (ASPP)"scheme to capture multi-scale objects and context information by placing multiple dilated convolution layers in parallel. More recently, dilated convolution has been applied to a broader range of tasks, such as object detection <ref type="bibr" target="#b5">[6]</ref>, optical flow <ref type="bibr" target="#b23">[24]</ref>, and audio generation <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dense Upsampling Convolution (DUC)</head><p>Suppose an input image has height H, width W , and color channels C, and the goal of pixel-level semantic segmentation is to generate a label map with size H ?W where each pixel is labeled with a category label. After feeding the image into a deep FCN, a feature map with dimension h ? w ? c is obtained at the final layer before making predictions, where h = H/d, w = W/d, and d is the downsampling factor. Instead of performing bilinear upsampling, which is not learnable, or using deconvolution network (as in <ref type="bibr" target="#b21">[22]</ref>), in which zeros have to be padded in the unpooling step before the convolution operation, DUC applies convolutional operations directly on the feature maps to get the dense pixel-wise prediction map. <ref type="figure" target="#fig_0">Figure 1</ref> depicts the architecture of our network with a DUC layer.</p><p>The DUC operation is all about convolution, which is performed on the feature map from ResNet of dimension h ? w ? c to get the output feature map of dimension</p><formula xml:id="formula_0">h ? w ? (d 2 ? L),</formula><p>where L is the total number of classes in the semantic segmentation task. Thus each layer of the dense convolution is learning the prediction for each pixel.</p><p>The output feature map is then reshaped to H ? W ? L with a softmax layer, and an elementwise argmax operator is applied to get the final label map. In practice, the "reshape" operation may not be necessary, as the feature map can be collapsed directly to a vector to be fed into the softmax layer. The key idea of DUC is to divide the whole label map into equal d 2 subparts which have the same height and width as the incoming feature map. This is to say, we transform the whole label map into a smaller label map with multiple channels. This transformation allows us to apply the convolution operation directly between the input feature map and the output label maps without the need of inserting extra values in deconvolutional networks (the "unpooling"operation).</p><p>Since DUC is learnable, it is capable of capturing and recovering fine-detailed information that is generally missing in the bilinear interpolation operation. For example, if a network has a downsample rate of 1/16, and an object has a length or width less than 16 pixels (such as a pole or a person far away), then it is more than likely that bilinear upsampling will not be able to recover this object. Meanwhile, the corresponding training labels have to be downsampled to correspond with the output dimension, which will already cause information loss for fine details. The prediction of DUC, on the other hand, is performed at the original resolution, thus enabling pixel-level decoding. In addition, the DUC operation can be naturally integrated into the FCN framework, and makes the whole encoding and decoding process end-to-end trainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hybrid Dilated Convolution (HDC)</head><p>In 1-D, dilated convolution is defined as:</p><formula xml:id="formula_1">g[i] = L l=1 f [i + r ? l]h[l],<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">f [i] is the input signal, g[i] is the output signal , h[l]</formula><p>denotes the filter of length L, and r corresponds to the dilation rate we use to sample f [i]. In standard convolution, r = 1.</p><p>In a semantic segmentation system, 2-D dilated convolution is constructed by inserting "holes"(zeros) between each pixel in the convolutional kernel. For a convolution kernel with size k ? k, the size of resulting dilated filter is</p><formula xml:id="formula_3">k d ? k d , where k d = k + (k ? 1) ? (r ? 1)</formula><p>. Dilated convolution is used to maintain high resolution of feature maps in FCN through replacing the max-pooling operation or strided convolution layer while maintaining the receptive field (or "field of view"in <ref type="bibr" target="#b2">[3]</ref>) of the corresponding layer. For example, if a convolution layer in ResNet-101 has a stride s = 2, then the stride is reset to 1 to remove downsampling, and the dilation rate r is set to 2 for all convolution kernels of subsequent layers. This process is applied iteratively through all layers that have a downsampling operation, thus the feature map in the output layer can maintain the same resolution as the input layer. In practice, however, dilated convolution is generally applied on feature maps that are already downsampled to achieve a reasonable efficiency/accuracy trade-off <ref type="bibr" target="#b2">[3]</ref>.</p><p>However, one theoretical issue exists in the above dilated convolution framework, and we call it "gridding" <ref type="figure" target="#fig_1">(Figure 2</ref>): For a pixel p in a dilated convolutional layer l, the information that contributes to pixel p comes from a nearby k d ? k d region in layer l ? 1 centered at p. Since dilated convolution introduces zeros in the convolutional kernel, the actual pixels that participate in the computation from the k d ? k d region are just k ? k, with a gap of r ? 1 between them. If k = 3, r = 2, only 9 out of 25 pixels in the region are used for the computation <ref type="figure" target="#fig_1">(Figure 2 (a)</ref>). Since all layers have equal dilation rates r, then for pixel p in the top dilated convolution layer l top , the maximum possible number of locations that contribute to the calculation of the value of p is (w ? h )/r 2 where w , h are the width and height of the bottom dilated convolution layer, respectively. As a result, pixel p can only view information in a checkerboard fashion, and lose a large portion (at least 75% when r = 2) of information. When r becomes large in higher layers due to additional downsampling operations, the sample from the input can be very sparse, which may not be good for learning because 1) local information is completely missing; 2) the information can be irrelevant across large distances. Another outcome of the gridding effect is that pixels in nearby r ? r regions at layer l receive information from completely different set of "grids" which may impair the consistency of local information.</p><p>Here we propose a simple solution-hybrid dilated convolution (HDC), to address this theoretical issue. Suppose we have N convolutional layers with kernel size K ?K that have dilation rates of [r 1 , ..., r i , ..., r n ], the goal of HDC is to let the final size of the RF of a series of convolutional operations fully covers a square region without any holes or missing edges. We define the "maximum distance between two nonzero values" as</p><formula xml:id="formula_4">M i = max[M i+1 ? 2r i , M i+1 ? 2(M i+1 ? r i ), r i ],<label>(2)</label></formula><p>with M n = r n . The design goal is to let M 2 ? K. For example, for kernel size K = 3, an r = [1, 2, 5] pattern works as M 2 = 2; however, an r = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref> pattern does not work as M 2 = 5. Practically, instead of using the same dilation rate for all layers after the downsampling occurs, we use a different dilation rate for each layer. In our network, the assignment of dilation rate follows a sawtooth wave-like heuristic: a number of layers are grouped together to form the "rising edge"of the wave that has an increasing dilation rate, and the next group repeats the same pattern. For example, for all layers that have dilation rate r = 2, we form 3 succeeding layers as a group, and change their dilation rates to be 1, 2, and 3, respectively. By doing this, the top layer can access information from a broader range of pixels, in the same region as the original configuration <ref type="figure" target="#fig_1">(Figure 2 (b)</ref>). This process is repeated through all layers, thus making the receptive field unchanged at the top layer.</p><p>Another benefit of HDC is that it can use arbitrary dilation rates through the process, thus naturally enlarging the receptive fields of the network without adding extra modules <ref type="bibr" target="#b28">[29]</ref>, which is important for recognizing objects that are relatively big. One important thing to note, however, is that the dilation rate within a group should not have a common factor relationship (like 2,4,8, etc.), otherwise the gridding problem will still hold for the top layer. This is a key difference between our HDC approach and the atrous spatial pyramid pooling (ASPP) module in <ref type="bibr" target="#b2">[3]</ref>, or the context aggregation module in <ref type="bibr" target="#b28">[29]</ref>, where dilation factors that have common factor relationships are used. In addition, HDC is naturally integrated with the original layers of the network, without any need to add extra modules as in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We report our experiments and results on three challenging semantic segmentation datasets: Cityscapes <ref type="bibr" target="#b4">[5]</ref>, KITTI dataset <ref type="bibr" target="#b9">[10]</ref> for road estimation, and PASCAL VOC2012 <ref type="bibr" target="#b7">[8]</ref>. We use ResNet-101 or ResNet-152 networks that have been pretrained on the ImageNet dataset as a starting point for all of our models. The output layer contains the number of semantic categories to be classified depending on the dataset (including background, if applicable). We use the cross-entropy error at each pixel over the categories. This is then summed over all pixel locations of the output map, and we optimize this objective function using standard Stochastic Gradient Descent (SGD). We use MXNet <ref type="bibr" target="#b3">[4]</ref> to train and evaluate all of our models on NVIDIA TITAN X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cityscapes Dataset</head><p>The Cityscapes Dataset is a large dataset that focuses on semantic understanding of urban street scenes. The dataset contains 5000 images with fine annotations across 50 cities, different seasons, varying scene layout and background. The dataset is annotated with 30 categories, of which 19 categories are included for training and evaluation (others are ignored). The training, validation, and test set contains 2975, 500, and 1525 fine images, respectively. An additional 20000 images with coarse (polygonal) annotations are also provided, but are only used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Baseline Model</head><p>We use the DeepLab-V2 <ref type="bibr" target="#b2">[3]</ref> ResNet-101 framework to train our baseline model. Specifically, the network has a downsampling rate of 8, and dilated convolution with rate of 2 and 4 are applied to res4b and res5b blocks, respectively. An ASPP module with dilation rate of 6, 12, 18, and 24 is added on top of the network to extract multiscale context information. The prediction maps and training labels are downsampled by a factor of 8 compared to the size of original images, and bilinear upsampling is used to get the final prediction. Since the image size in the Cityscapes dataset is 1024 ? 2048, which is too big to fit in the GPU memory, we partition each image into twelve 800 ? 800 patches with partial overlapping, thus augmenting the training set to have 35700 images. This data augmentation strategy is to make sure all regions in an image can be visited. This is an improvement over random cropping, in which nearby regions may be visited repeatedly.</p><p>We train the network using mini-batch SGD with patch size 544?544 (randomly cropped from the 800?800 patch) and batch size 12, using multiple GPUs. The initial learning rate is set to 2.5 ? 10 ?4 , and a "poly"learning rate (as in <ref type="bibr" target="#b2">[3]</ref>) with power = 0.9 is applied. Weight decay is set to 5 ? 10 ?4 , and momentum is 0.9. The network is trained for 20 epochs and achieves mIoU of 72.3% on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Dense Upsampling Convolution (DUC)</head><p>We examine the effect of DUC on the baseline network. In DUC the only thing we change is the shape of the top convolutional layer. For example, if the dimension of the top convolutional layer is 68 ? 68 ? 19 in the baseline model (19 is the number of classes), then the dimension of the same layer for a network with DUC will be 68 ? 68 ? (r 2 <ref type="figure" target="#fig_0">? 19)</ref> where r is the total downsampling rate of the network (r = 8 in this case). The prediction map is then reshaped to size 544 ? 544 ? 19. DUC will introduce extra parameters compared to the baseline model, but only at the top convolutional layer. We train the ResNet-DUC network the same way as the baseline model for 20 epochs, and achieve a mean IOU of 74.3% on the validation set, a 2% increase compared to the baseline model. Visualization of the result of ResNet-DUC and comparison with the baseline model is shown in <ref type="figure" target="#fig_2">Figure 3</ref> From <ref type="figure" target="#fig_2">Figure 3</ref>, we can clearly see that DUC is very helpful for identifying small objects, such as poles, traffic lights, and traffic signs. Consistent with our intuition, pixel-level dense upsampling can recover detailed information that is generally missed by bilinear interpolation.</p><p>Ablation Studies We examine the effect of different settings of the network on the performance. Specifically, we examine: 1) the downsampling rate of the network, which controls the resolution of the intermediate feature map; 2) whether to apply the ASPP module, and the number of parallel paths in the module; 3) whether to perform 12-fold data augmentation; and 4) cell size, which determines the size of neighborhood region (cell ? cell) that one predicted pixel projects to. Pixel-level DUC should use cell = 1; however, since the ground-truth label generally cannot reach pixellevel precision, we also try cell = 2 in the experiments. From <ref type="table" target="#tab_0">Table 1</ref> we can see that making the downsampling rate smaller decreases the accuracy. Also it significantly raises the computational cost due to the increasing resolution of the feature maps. ASPP generally helps to improve the performance, and increasing ASPP channels from 4 to 6 (dilation rate 6 to 36 with interval 6) yields a 0.2% boost. Data augmentation helps to achieve another 1.5% improvement. Using cell = 2 yields slightly better performance when compared with cell = 1, and it helps to reduce computational cost by decreasing the channels of the last convolutional layer by a factor of 4. Bigger Patch Size Since setting cell = 2 reduces GPU memory cost for network training, we explore the effect of patch size on the performance. Our assumption is that, since the original images are all 1024 ? 2048, the network should be trained using patches as big as possible in order to aggregate both local detail and global context information that may help learning. As such, we make the patch size to be 880 ? 880, and set the batch size to be 1 on each of the 4 GPUs used in training. Since the patch size exceeds the maximum dimension (800 ? 800) in the previous 12-fold data augmentation framework, we adopt a new 7-fold data augmentation strategy: seven center locations with x = 512, y = {256, 512, ..., 1792} are set in the original image; for each center location, a 880 ? 880 patch is obtained by randomly setting its center within a 160 ? 160 rectangle area centered at each center. This strategy makes sure that we can sample all areas in the image, including edges. Training with a bigger patch size boosts the performance to 75.7%, a 1% improvement over the previous best </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network DS ASPP Augmentation Cell mIoU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>result.</head><p>Compared with Deconvolution We compare our DUC model with deconvolution, which also involves learning for upsampling. Particularly, we compare with 1) direct deconvolution from the prediction map (dowsampled by 8) to the original resolution; 2) deconvolution with an upsampling factor of 2 first, followed by an upsampling factor of 4. We design the deconv network to have approximately the same number of parameters as DUC. We use the ResNet-DUC bigger patch model to train the networks. The above two models achieve mIOU of 75.1% and 75.0%, respectively, lower than the ResNet-DUC model (75.7% mIoU).</p><p>Conditional Random Fields (CRFs) Fully-connected CRFs <ref type="bibr" target="#b15">[16]</ref> are widely used for improving semantic segmentation quality as a post-processing step of an FCN <ref type="bibr" target="#b2">[3]</ref>. We follow the formation of CRFs as shown in <ref type="bibr" target="#b2">[3]</ref>. We perform a grid search on parameters on the validation set, and use ? ? = 15, ? ? = 3, ? ? = 1 , w 1 = 3, and w 2 = 3 for all of our models. Applying CRFs to our best ResNet-DUC model yields an mIoU of 76.7%, a 1% improvement over the model does not use CRFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Hybrid Dilated Convolution (HDC)</head><p>We use the best 101 layer ResNet-DUC model as a starting point of applying HDC. Specifically, we experiment with several variants of the HDC module:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">No dilation: For all ResNet blocks containing dilation,</head><p>we make their dilation rate r = 1 (no dilation).</p><p>2. Dilation-conv: For all blocks contain dilation, we group every 2 blocks together and make r = 2 for the first block, and r = 1 for the second block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dilation-RF:</head><p>For the res4b module that contains 23 blocks with dilation rate r = 2, we group every 3 blocks together and change their dilation rates to be 1, 2, and 3, respectively. For the last two blocks, we keep r = 2. For the res5b module which contains 3 blocks with dilation rate r = 4, we change them to 3, 4, and 5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dilation-bigger:</head><p>For res4b module, we group every 4 blocks together and change their dilation rates to be 1, 2, 5, and 9, respectively. The rates for the last 3 blocks are 1, 2, and 5. For res5b module, we set the dilation rates to be 5, 9, and 17.</p><p>The result is summarized in <ref type="table">Table 2</ref>. We can see that increasing receptive field size generally yields higher accuracy. <ref type="figure" target="#fig_4">Figure 5</ref> illustrates the effectiveness of the ResNet-DUC-HDC model in eliminating the gridding effect. A visualization result is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. We can see our best ResNet-DUC-HDC model performs particularly well on objects that are relatively big.  <ref type="table">Table 2</ref>. Result of different variations of the HDC module. "RF increased"is the total size of receptive field increase along a single dimension compared to the layer before the dilation operation.</p><p>Deeper Networks We have also tried replacing our ResNet-101 based model with the ResNet-152 network, which is deeper and achieves better performance on the ILSVRC image classification task than ResNet-101 <ref type="bibr" target="#b12">[13]</ref>.  Due to the network difference, we first train the ResNet-152 network to learn the parameters in all batch normalization (BN) layers for 10 epochs, and continue fine-tuning the network by fixing these BN parameters for another 20 epochs. The results are summarized in <ref type="table">Table 3</ref>. We can see that using the deeper ResNet-152 model generally yields better performance than the ResNet-101 model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Test Set Results</head><p>Our results on the Cityscapes test set are summarized in <ref type="table">Table 4</ref>. There are separate entries for models trained using fine-labels only, and using a combination of fine and coarse labels. Our ResNet-DUC-HDC model achieves 77.6% mIoU using fine data only. Adding coarse data help us achieve 78.5% mIoU. In addition, inspired by the design of the VGG network <ref type="bibr" target="#b25">[26]</ref>, in that a single 5 ? 5 convolutional layer can be decomposed into two adjacent 3?3 convolutional layers to increase the expressiveness of the network while maintaining the receptive field size, we replaced the 7 ? 7 convolutional layer in the original ResNet-101 network by three 3 ? 3 convolutional layers. By retraining the updated network, we achieve a mIoU of 80.1% on the test set using a single model without CRF post-processing. Our result achieves the state-of-the-art performance on the Cityscapes dataset at the time of submission. Compared with the strong baseline of Chen et al. <ref type="bibr" target="#b2">[3]</ref>, we improve the mIoU by a significant margin (9.7%), which demonstrates the effectiveness of our approach.</p><p>Method mIoU fine FCN 8s <ref type="bibr" target="#b20">[21]</ref> 65.3% Dilation10 <ref type="bibr" target="#b28">[29]</ref> 67.1% DeepLabv2-CRF <ref type="bibr" target="#b2">[3]</ref> 70.4% Adelaide context <ref type="bibr" target="#b17">[18]</ref> 71.6% ResNet-DUC-HDC (ours) 77.6%</p><p>coarse LRR-4x <ref type="bibr" target="#b10">[11]</ref> 71.8% ResNet-DUC-HDC-Coarse 78.5% ResNet-DUC-HDC-Coarse (better network) 80.1% <ref type="table">Table 4</ref>. Performance on Cityscapes test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">KITTI Road Segmentation</head><p>Dataset The KITTI road segmentation task contains images of three various categories of road scenes, including 289 training images and 290 test images. The goal is to decide if each pixel in images is road or not. It is challenging to use neural network based methods due to the limited number of training images. In order to avoid overfitting, we crop patches of 320 ? 320 pixels with a stride of 100 pixels from the training images, and use the ResNet-101-DUC model pretrained from ImageNet during training. Other training settings are the same as Cityscapes experiment. We did not apply CRFs for post-processing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We achieve the state-of-the-art results at the time of submission without using any additional information of stereo, laser points and GPS. Specifically, our model attains the highest maximum F1-measure in the sub-categories of urban unmarked (UU ROAD), urban multiple marked (UMM ROAD) and the overall category URBAN ROAD of all sub-categories, the highest average precision across all three sub-categories and the overall category by the time of submission of this paper. Examples of visualization results are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. The detailed results are displayed in <ref type="table" target="#tab_3">Table 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">PASCAL VOC2012 dataset</head><p>Dataset The PASCAL VOC2012 segmentation benchmark contains 1464 training images, 1449 validation images, and 1456 test images. Using the extra annotations provided by <ref type="bibr" target="#b11">[12]</ref>, the training set is augmented to have 10582 images. The dataset has 20 foreground object categories and 1 background class with pixel-level annotation.</p><p>Results We first pretrain our 152 layer ResNet-DUC model using a combination of augmented VOC2012 training set and MS-COCO dataset <ref type="bibr" target="#b18">[19]</ref>, and then finetune the pretrained network using augmented VOC2012 trainval set. We use patch size 512?512 (zero-padded) throughout training. All other training strategies are the same as Cityscapes experiment. We achieve mIOU of 83.1% on the test set using a single model without any model ensemble or multiscale testing, which is the best-performing method at the time of submission 2 . The detailed results are displayed in <ref type="table">Table 6</ref>, and the visualizations are shown in <ref type="figure" target="#fig_6">Figure 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mIoU</head><p>DeepLabv2-CRF <ref type="bibr" target="#b2">[3]</ref> 79.7% CentraleSupelec Deep G-CRF <ref type="bibr" target="#b1">[2]</ref> 80.2%</p><p>ResNet-DUC (ours) 83.1% <ref type="table">Table 6</ref>. Performance on the Pascal VOC2012 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose simple yet effective convolutional operations for improving semantic segmentation systems. We designed a new dense upsampling convolution (DUC) operation to enable pixel-level prediction on feature maps, and hybrid dilated convolution (HDC) to solve the gridding problem, effectively enlarging the receptive fields of the network. Experimental results demonstrate the effectiveness of our framework on various semantic segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the architecture of ResNet-101 network with Hybrid Dilated Convolution (HDC) and Dense Upsampling Convolution (DUC) layer. HDC is applied within ResNet blocks, and DUC is applied on top of network and is used for decoding purpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the gridding problem. Left to right: the pixels (marked in blue) contributes to the calculation of the center pixel (marked in red) through three convolution layers with kernel size 3 ? 3. (a) all convolutional layers have a dilation rate r = 2. (b) subsequent convolutional layers have dilation rates of r = 1, 2, 3, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Effect of Dense Upsampling Convolution (DUC) on the Cityscapes validation set. From left to right: input image, ground truth (areas with black color are ignored in evaluation), baseline model, and our ResNet-DUC model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Effect of Hybrid Dilated Convolution (HDC) on the Cityscapes validation set. From left to right: input image, ground truth, result of the ResNet-DUC model, result of the ResNet-DUC-HDC model (Dilation-bigger).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Effectiveness of HDC in eliminating the gridding effect. First row: ground truth patch. Second row: prediction of the ResNet-DUC model. A strong gridding effect is observed. Third row: prediction of the ResNet-DUC-HDC (Dilation-RF) model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Examples of visualization on Kitti road segmentation test set. The road is marked in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Examples of visualization on the PASCAL VOC2012 segmentation validation set. Left to right: input image, ground truth, our result before CRF, and after CRF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation studies for applying ResNet-101 on the Cityscapes dataset. DS: Downsampling rate of the network. Cell: neighborhood region that one predicted pixel represents.</figDesc><table><row><cell>Baseline</cell><cell>8</cell><cell>4</cell><cell>yes</cell><cell>n/a</cell><cell>72.3</cell></row><row><cell>Baseline</cell><cell>4</cell><cell>4</cell><cell>yes</cell><cell>n/a</cell><cell>70.9</cell></row><row><cell>DU C</cell><cell>8</cell><cell>no</cell><cell>no</cell><cell>1</cell><cell>71.9</cell></row><row><cell>DU C</cell><cell>8</cell><cell>4</cell><cell>no</cell><cell>1</cell><cell>72.8</cell></row><row><cell>DU C</cell><cell>8</cell><cell>4</cell><cell>yes</cell><cell>1</cell><cell>74.3</cell></row><row><cell>DU C</cell><cell>4</cell><cell>4</cell><cell>yes</cell><cell>1</cell><cell>73.7</cell></row><row><cell>DU C</cell><cell>8</cell><cell>6</cell><cell>yes</cell><cell>1</cell><cell>74.5</cell></row><row><cell>DU C</cell><cell>8</cell><cell>6</cell><cell>yes</cell><cell>2</cell><cell>74.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Performance on different road scenes in KITTI test set. MaxF: Maximum F1-measure, AP: Average precision.</figDesc><table><row><cell>1 .</cell><cell></cell></row><row><cell></cell><cell>MaxF</cell><cell>AP</cell></row><row><cell>UM ROAD</cell><cell cols="2">95.64% 93.50%</cell></row><row><cell>UMM ROAD</cell><cell cols="2">97.62% 95.53%</cell></row><row><cell>UM ROAD</cell><cell cols="2">95.17% 92.73%</cell></row><row><cell cols="3">URBAN ROAD 96.41% 93.88%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For thorough comparison with other methods, please check http://www.cvlibs.net/datasets/kitti/eval road.php.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Result link: http://host.robots.ox.ac.uk:8080/anonymous/LQ2ACW.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We thank the members of TuSimple and Gary's Unbelievable Research Unit (GURU) for comments on this work. GWC was supported in part by Guangzhou Science and Technology Planning Project (201704030051) and NSF cooperative agreement SMA 1041755 to the Temporal Dynamics of Learning Center, an NSF Science of Learning Center.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08119</idno>
		<title level="m">Higher order potentials in end-to-end trainable conditional random fields</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08358</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06409</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1538" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haz?rba?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06852</idno>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new performance measure and evaluation benchmark for road detection algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fritsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kuehnl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Laplacian reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02264</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tchamitchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07386</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01013</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Optical flow with semantic segmentation and localized layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03911</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04339</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05442</idno>
		<title level="m">Semantic understanding of scenes through the ade20k dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
