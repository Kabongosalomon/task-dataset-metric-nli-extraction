<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
							<email>ybisk@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning has been widely used to train transformer-based vision-language models for video-text alignment and multi-modal representation learning. This paper presents a new algorithm called Token-Aware Cascade contrastive learning (TACo) that improves contrastive learning using two novel techniques. The first is the token-aware contrastive loss which is computed by taking into account the syntactic classes of words. This is motivated by the observation that for a video-text pair, the content words in the text, such as nouns and verbs, are more likely to be aligned with the visual contents in the video than the function words. Second, a cascade sampling method is applied to generate a small set of hard negative examples for efficient loss estimation for multi-modal fusion layers. To validate the effectiveness of TACo, in our experiments we finetune pretrained models for a set of downstream tasks including text-video retrieval (YouCook2, MSR-VTT and ActivityNet), video action step localization (CrossTask), video action segmentation (COIN). The results show that our models attain consistent improvements across different experimental settings over previous methods, setting new state-of-the-art on three public text-video retrieval benchmarks of YouCook2, MSR-VTT and ActivityNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Aligning or grounding language to videos is a challenging topic in the context of vision-language (VL) research as it requires the model to understand contents, dynamics, and causality presented in videos <ref type="bibr" target="#b2">[3]</ref>. Inspired by the success of BERT <ref type="bibr" target="#b9">[10]</ref> in natural language processing, there is a growing interest in applying transformer-based multi-modal models for video-text alignment and representation learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref>. These models are typically pretrained on large amounts of noisy video-text pairs using contrastive learning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref>, and then applied in a zeroshot manner or finetuned for various downstream tasks, such as text-video retrieval <ref type="bibr" target="#b51">[52]</ref>, video action step localization <ref type="bibr" target="#b60">[61]</ref>, video action segmentation <ref type="bibr" target="#b42">[43]</ref>, video question Language Encoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal Fusion Layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Loss</head><p>Video Encoder ? hard negatives</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Loss</head><p>(add, tomatoes, pan, stir) answering <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b26">27]</ref> and video captioning <ref type="bibr" target="#b57">[58]</ref>.</p><p>In this paper, we present a new variant of contrastive learning, Token-Aware Cascade contrastive learning (TACo) to improve the video-text alignment for both large-scale pretraining and downstream specific tasks. As the name indicates, TACo makes two modifications to the conventional contrastive learning used in video-language domain. The first is the token-aware contrastive loss which is computed by taking into account the syntactic classes of words. This is motivated by the observation that, given a video and its corresponding text, content words, such as nouns and verbs, are more likely than function words to be aligned with (or grounded to) visual contents in the video. Conventional contrastive learning typically compute the loss after aggregating over all the words in the text and frames in the video (loss L 1 or L 3 in <ref type="figure" target="#fig_0">Fig. 1</ref>). In contrast, the token-aware contrastive loss is computed using only a subset of words whose syntactic classes belong to a predefined set (e.g., nouns and verbs), which forces the grounding of individual words to the video (loss L2). For example, we pay particular attention to the words "add", "tomatos", "pan" and "stir" in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The second technique we introduce is a cascade sampling method to find a small set of hard negative examples for training the multi-modal fusion layers. Consider a batch of K video-text pairs. For each of the video-text pairs, the ideal case is that we use the remaining K ? 1 negative videos or texts to compute the contrastive loss after multi-modal fusion. However, the cost of computing the contrastive loss quickly becomes prohibitive when it is coupled with multi-modal fusion layers, considering its high complexity O(K 2 ? L 2 ) where L is total number of visual and textual tokens. A conventional way to address this is using random sampling to select a small subset of negative pairs. In this paper, instead of random sampling, we propose a cascade sampling method as shown in the top-right of <ref type="figure" target="#fig_0">Fig. 1</ref> to efficiently select a small set of hard negative examples on the fly during training. It leverages the videotext alignment scores computed in L 1 and L 2 before multimodal fusion layers, and helps to learn the multi-modal fusion layers more effectively without any extra overhead.</p><p>We perform a comprehensive empirical study to validate the effectiveness of TACo in both pretraining and dataset-specific scenarios. We apply TACo and different variants of contrastive losses to train or pretrain and finetune on various downstream tasks including text-video retrieval (YouCook2, MSR-VTT and ActivityNet) <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b11">12]</ref>, video action step localization (CrossTask) <ref type="bibr" target="#b60">[61]</ref> and action segmentation (COIN) <ref type="bibr" target="#b42">[43]</ref>. Our results show that TACo improves the text-video retrieval performance over current state-of-the-art across three benchmarks. Furthermore, the learned multi-modal representation and video representation can be effectively transferred to CrossTask and COIN, and achieve better or comparable performance to current state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Video-language pretraining. Realistic application scenarios around videos have prompted emergence of various video-language tasks, such as text-video retrieval <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b52">53]</ref>, video question answering <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>, video captioning <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b58">59]</ref>, etc. Inspired by the success of BERT for largescale pretraining in language domain <ref type="bibr" target="#b9">[10]</ref>, transformers have been employed in the video-language domain <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28]</ref> as well as image-language domain <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b28">29]</ref>. Combined with large scale datasets, e.g. Howto100M <ref type="bibr" target="#b34">[35]</ref> this approach has proven to be effective on various downstream tasks. Depending on the tasks of interest, some approaches train a multi-modal transformer using a combination of multiple losses including video-text alignment <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28]</ref>, masked token (words/frames/objects) prediction <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b32">33]</ref>, and frame order prediction <ref type="bibr" target="#b27">[28]</ref>, etc. Some other approaches exploited various contrastive learning techniques to directly optimize the feature space without multi-modal fusion <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14]</ref>. In most of previ-ous works, these two approaches were explored separately. Very recently, an updated version of <ref type="bibr" target="#b32">[33]</ref> used two independent alignment losses before and after multi-modal fusion in a single framework. In this paper, however, these two losses cooperate closely with each other during training in that the earlier stage helps to discover the hard negatives while the multi-modal layers with more capacity help to tackle those hard samples particularly.</p><p>Video-text alignment. Aligning videos to text requires the model to understand motion and temporal coherence. Some works have relied on attention mechanisms to extract key information from videos <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b54">55]</ref>, while others preserve visual information by composing pairwise joint representation using 3D tensors <ref type="bibr" target="#b52">[53]</ref> or use multi-level video encoders to separately encode the spatial and temporal cues <ref type="bibr" target="#b10">[11]</ref>. These models usually rely on a rank or margin loss to learn the correct alignment for video-text pairs. Another line of work learns fine-grained or hierarchical alignment between videos and texts <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b5">6]</ref>. In <ref type="bibr" target="#b48">[49]</ref>, the authors proposed a fine-grained alignment by extracting the nouns and verbs from action phrase in a sentence and projecting them into a shared space with videos. Alternatively, the authors in <ref type="bibr" target="#b5">[6]</ref> extract a hierarchical semantic graph and apply graph reasoning to achieve the alignment at different levels. Similar ideas have been also proposed in the imagetext alignment by decomposing the images and texts into sub-tokens <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b49">50]</ref>. Thus far, it has not been studied how these task-specific architectures can be integrated into largescale pretraining. In this paper, we are the first to propose a simple yet effective token-aware contrastive loss for finegrained alignment for pretraining and downstream tasks.</p><p>Negative sampling. Key to efficient contrastive training is a good source of negative examples. Most of current approaches use random sampling strategies for training videotext alignment <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b32">33]</ref>. However, in the domain of imagetext retrieval, a few works tried hard negative sampling to choose the hardest negatives for training. In <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>, the authors computed the alignment scores for all image-text pairs in a mini-batch and use the hardest negative sample to compute the marginal loss. However, this strategy can only be applied without multi-modal fusion. In those models which have multi-modal fusion layers for better representations <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref>, the authors instead compute the matching score offline and then use it to sample hard negatives for finetuning image-text retrieval model, which however is difficult for large-scale pretraining due to the high computational cost. In this paper, our cascade hard negative mining is particularly designed to address these issues as we efficiently select the hard negative samples online before multi-modal fusion and send them to the fusion layers for computing the loss. As we will show in our experiments, this technique can be seamlessly applied to both large-scale pretraining and downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>As depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>, our model has three components:</p><p>Video encoding module f ?v . It is implemented by a stack of self-attention layers parameterized by ? v . Here, we assume the input video features have been already extracted using some pre-trained models such as 2D CNN (e.g., ResNet <ref type="bibr" target="#b17">[18]</ref>) or 3D CNN (e.g., I3D <ref type="bibr" target="#b3">[4]</ref>, S3D <ref type="bibr" target="#b50">[51]</ref>). Given the input video embeddings, video encoder starts with a linear layer to project them to the same dimension d as following self-attention layers. We denote the output of our video encoder for a video clip by a sequence of m features, x = {x 1 , ..., x m } ? R m?d . The number of features m depends on the choice of sampling frame rate and the video feature extractor, which we will discuss in Sec. 4.</p><p>Language encoding module f ?t . We use pretrained tokenizer <ref type="bibr" target="#b47">[48]</ref> and BERT <ref type="bibr" target="#b9">[10]</ref> to tokenize the input texts and extract textual features, respectively. Given a raw sentence, we append a "[CLS]" and "[SEP]" to the beginning and end, respectively. At the top, we can obtain a sequence of n textual features y = {y 1 , ..., y n } ? R n?d . We ensure the output feature dimension of video encoder to be identical to that of language encoder. During training, we update the parameters ? t in our language encoder to adapt to the texts in specific domain, e.g., cooking instructions in YouCook2 <ref type="bibr" target="#b57">[58]</ref>. Multi-modal fusion module f ?m . It also consists of selfattention layers with learnable parameters ? m . It takes video features x ? R m?d and text features y ? R n?d from two separate modalities as inputs and output the (m + n) features z = {z 1 , ..., z (m+n) } ? R (m+n)?d . To help it to distinguish the video and language tokens, we use a token type embedding layer to learn two embeddings and add them to the visual and textual tokens, separately. Similar to original Transformer <ref type="bibr" target="#b46">[47]</ref>, we include a positional embedding layer to encode the absolute token positions in the input sequence.</p><p>The above three components comprise our video-text alignment model which is then trained with the proposed token-aware cascade contrastive loss. We start with a brief review of conventional contrastive learning and then introduce the proposed technique. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive learning: a revisit</head><formula xml:id="formula_0">v i , t i )} N i=1</formula><p>, our goal is to learn an optimal scoring function s such that paired video and text (v i , t i ) have higher scores than all the other unmatched pairs (v j , t k ), j = k. From the probabilistic perspective, aligning v i to t i is equivalent to maximizing the conditional probability p(v i |t i ) while minimizing the probability for all negative pairs p(v j |t i ), j = i. Accord-ing to <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37]</ref>, p(v j |t i ) can be approximated by:</p><formula xml:id="formula_1">p(vj|ti) ? exp s(v j ,t i ) N k=1 exp s(v k ,t i )<label>(1)</label></formula><p>where s(v, t) is the alignment score between v and t; the denominator is a sum over all possible videos, which is a partition function for normalization. Adding cross-entropy loss on p(v j |t i ), we can then derive the NCE loss <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_2">Lnce = N i=1 ? log p(vi|ti) ? N i=1 ? log exp s(v i ,t i ) exp s(v i ,t i ) + k =i exp s(v k ,t i )<label>(2)</label></formula><p>The denominator in Eq. 2 requires a sum over all videos in a dataset, which is intractable in practice. Therefore, we usually compute the NCE loss on a mini-batch of K(K N ) video-text pairs sampled from the whole dataset. Ideally, we want to learn the parameters ? = {? v , ? t , ? m } of the model to minimize the above NCE loss, such that</p><formula xml:id="formula_3">? = s(v i , t i ) ? s(v j , t i ) is maximized over all tuples (t i , v i , v j ), j = i.</formula><p>A number of previous works used the above formula for contrastive learning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b59">60]</ref>. Meanwhile, there are some variants of computing contrastive loss in video-langauge representation learning. For example, <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14]</ref> omits the denominator and incorporate a margin s.t. s(v i , t i ) &gt; s(v j , t i ) + ?, ?j = i in a mini-batch. <ref type="bibr" target="#b32">[33]</ref> optimizes binary cross-entropy (BCE) by assigning (v i , t i ) a positive label (1) and other pairs a negative label (0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">TACo: our approach</head><p>The way of using contrastive learning in previous works has two issues. First, the loss is computed at sentencelevel by taking '[CLS]' token <ref type="bibr" target="#b13">[14]</ref> or the maximum over all tokens <ref type="bibr" target="#b33">[34]</ref> in a sentence. Clearly, the content words (e.g., nouns, verbs) are more likely to align with the visual contents or concepts in the videos compared with function words (e.g., stop words). Second, the high computational cost in multi-modal fusion layers hinder the usage of large batch of negative samples, which however is essential to contrastive learning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7]</ref>. Motivated by these two issues, we introduce TACo, a simple yet effective method to improve the contrastive learning. We elaborate below how these contrastive losses are computed.</p><p>Given</p><formula xml:id="formula_4">the K video-text pairs {(v i , t i )} K i=1</formula><p>in a minibatch, we first use our video encoder f ?v and language encoder f ?t to obtain a batch of video features X = {x 1 , ..., x K } ? R K?m?d and text features Y = {y 1 , ..., y K } ? R K?n?d , respectively. Then, we average all tokens of a video clip v i to getx i ? R 1?d , and take the first '[CLS]' token for each text t i to get? i ? R 1?d . Based onx and?, we compute the sentence-level contrastive loss:</p><formula xml:id="formula_5">L1 = ? K i=1 log expx i ?? i /? 1 expx i ?? i /? 1 + j =i expx j ?? i /? 1 (3)</formula><p>where ? 1 is a scalar temperature parameter. In Eq. 3, the computation is simply a number of dot-products between video and text features. Giving such efficiency, we can use all the K ? 1 negative samples in a mini-batch to compute the loss. Through this, we optimize ? v and ? t so as to project the video and text samples into an aligned feature space. The '[CLS]' token and average of video tokens in Eq. 3 overlooks the differences across tokens and frames, and thus may not provide the pressure to push individual tokens (e.g., nouns and verbs) to ground on the specific video contents. To encourage correct alignment, in addition to the sentencelevel loss, we introduce a token-level contrastive loss:</p><formula xml:id="formula_6">L2 = ? K i=1 p?P i log ? ? ? exp s(x i ,y p i )/? 2 exp s(x i ,y p i )/? 2 + j =i exp s(x j ,y p i )/? 2 ? ? ? (4)</formula><p>where ? 2 is another scalar temperature parameter; P i is the indices of tokens of interest in i-th text, and y p i is the p-th token embedding in i-th text. s(?) measures the similarity between video features and specific token embedding y p i . It first computes the dot-product between y p i ? R 1?d and all m video tokens x ? R m?d , and then take the maximum over m scores to get the final alignment score. Through Eq. 4, the model uses individual tokens as anchors to align with video, which is complementary to the sentence-level loss in Eq. 3. Similar to Eq. 3, we can compute this tokenlevel contrastive loss efficiently, and thus use all the K ? 1 negative samples. As a whole, these two losses are used to optimize ? v and ? t in a token-aware manner. Token of interest. In Eq. 4, we need to decide which tokens should be included in P i . In this paper, we heuristically select nouns and verbs as the targets considering they are more "concrete" in the videos. In practice, nouns or verbs usually have different discriminativeness even if they are all the same type. For example, "man" is a noun but is less informative than "gymnast". To reflect this, we further assign different words with different weights by computing their inverse document frequency (idf) <ref type="bibr" target="#b21">[22]</ref>. A higher idf means it is more unique across the corpus, and hence will weigh more when computing the token-level contrastive loss. Another practical issue for computing the loss is that the tokens are usually sub-words due to the BERT tokenizer. Hence, for all tokens that belongs to the same word, we will assign the same weights accordingly.</p><p>After computing the token-aware contrastive loss, we feed the features from separate modalities to multi-modal fusion layers to enable more interactions between them two. Similar to previous work <ref type="bibr" target="#b59">[60]</ref>, we take the feature corresponding to the "[CLS]" in the (m + n) outputs. We regard this as the summary of two modalities and then compute the contrastive loss:</p><formula xml:id="formula_7">L3 = ? K i=1 log ? ? exp w?z cls i,i exp w?z cls i,i + j =i exp w?z cls j,i ? ? (5)</formula><p>where z cls j,i is the multi-modal fusion output for "[CLS]" token taking x j and y i as inputs; w ? R 1?d is the parameter in a linear layer 1 . Based on Eq. 5, we optimize all parameters in our model ? = {? v , ? t , ? m } in collaboration with Eq. 3 and Eq. 4.</p><p>In Eq. 5, a practical challenge is that we can hardly use all (K ? 1) negative samples in the mini-batch, due to the high computational and memory cost in the multi-modal fusion. The O(d(m + n) 2 ) complexity of self-attention layer makes it intractable to pass all K ? K pairs into the multimodal layers. Previous work solved this by performing random sampling to cut the number of negative samples to K . However, randomly choosing negative samples may result in sub-optimal learning since the pairs are scarce. We therefore introduce a cascade sampling strategy to find hard negatives instead of random ones. Cascade hard negative sampling. To reduce the computational cost in Eq. 5, we choose among all possible videotext pairs a small subset which are most difficult. However, computing the alignment scores for all pairs using Eq. 5 and then select the hard negatives is a "chicken-and-egg" problem. Instead, we propose to use the similarities between all video-text pairs computed in Eq. 3 and Eq. 4 as the guidance. Specifically, for each text-video pair (v j , t i ), we take their global similarityx j ?? i computed in Eq. 3 and tokenlevel similarity by aggregating p?Pi s(x j , y p i ) for all tokens of interest in t i . Then we sum the two similarities as the alignment score for the given pair. For each text, we choose the top K aligned negative videos and vice versa. The resulting 2K ? (K + 1) pairs are then fed into the multi-modal fusion layers. Through this strategy, we can effectively select the difficult negative samples on the fly at no extra cost. Since the multi-modal fusion layers has more capacity (parameters) to distinguish these hard negatives from positive ones, our sampling strategy naturally prompts the cooperation between the three contrastive losses.</p><p>Finally, we present a comprehensive comparison to differentiate our model with previous works with respect to the used contrastive learning method in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective</head><p>The training objective in our method is finding optimal ? = {? v , ? t , ? m } by minimizing the combination of the above three contrastive losses:</p><formula xml:id="formula_8">arg min ?v ,? t ,?m N i=1 (L1 + ?tL2 + L3)<label>(6)</label></formula><p>1 for clarity, we omit the bias term in the formula</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Token-aware Early stage Later stage Cascade Loss VideoBert <ref type="bibr" target="#b40">[41]</ref> BCE CBT <ref type="bibr" target="#b39">[40]</ref> NCE TJVE <ref type="bibr" target="#b34">[35]</ref> Margin MIL-NCE <ref type="bibr" target="#b33">[34]</ref> NCE ActBert <ref type="bibr" target="#b59">[60]</ref> BCE UniVL <ref type="bibr" target="#b32">[33]</ref> NCE MMT <ref type="bibr" target="#b13">[14]</ref> Margin TACo(Ours) NCE <ref type="table">Table 1</ref>: A comparison of video-language pretraining methods regarding contrastive learning strategies. "Early stage" and "Later stage" mean computing the loss before and after multi-modal fusion, respectively. "Cascade" means using cascade hard negative sampling.</p><p>where ? t is the weight of token-level loss (0.5 by default).</p><p>During inference, we make the prediction by summing the alignment scores from all the three scoring functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup 4.1. Datasets</head><p>In our experiments, we train and evaluate our model on the following established benchmarks: ? YouCook2 <ref type="bibr" target="#b57">[58]</ref> consists of 2k videos about routine cooking activities of 89 recipes. Each video contains multiple video clips annotated with text descriptions by human annotators. Following <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref>, we train our models on the training split, and report the text-video retrieval performance on around 3.5k validation clips.</p><p>? MSR-VTT <ref type="bibr" target="#b51">[52]</ref> contains 10k video clips associated with 200k sentences. There are two validation splits used in previous work. In <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14]</ref>, the training set has 9k clip-text pairs with the remaining 1k pairs for evaluation, which we denote by split1. In <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34]</ref>, 1k clip-text pairs are sampled from the 3k pairs in test set for evaluation, while the original 7k pairs are used for training. We denote this by split2. We report text-video retrieval results using both splits.</p><p>? ActivityNet <ref type="bibr" target="#b24">[25]</ref>. It consists of 20K YouTube videos, each of which is associated with multiple human-annotated captions. Following <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b13">14]</ref>, we concatenate all the captions for a video into a paragraph and evaluate the paragraphvideo retrieval on the "val1" split. ? Howto100M <ref type="bibr" target="#b34">[35]</ref>. We compare with previous work under the pretraining protocol on Howto100M <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b32">33]</ref>. It was collected from YouTube and contains over 1.2M narrated videos associated with automatically generated transcripts. Each video contains over 100 clips on average.</p><p>To further verify the transferrability or our learned multimodal representation from Howto100M, we also evaluate the action step localization and action segmentation on CrossTask <ref type="bibr" target="#b60">[61]</ref> and COIN <ref type="bibr" target="#b42">[43]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Settings</head><p>Previous work use a variety of different video and language representations which we find significantly affect the final performance. We summarize different choices below: ? Video representations. For 2D CNN, Resnet-152 <ref type="bibr" target="#b17">[18]</ref> is used to extract feature map and then globally pooled to 2048-d <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33]</ref>. For 3D features, commonly used models are I3D <ref type="bibr" target="#b4">[5]</ref>, R(2+1)D <ref type="bibr" target="#b45">[46]</ref> and S3D <ref type="bibr" target="#b50">[51]</ref>. In <ref type="bibr" target="#b59">[60]</ref>, the authors further extract objects from the video clips. In <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14]</ref>, the authors use collaborative experts to extract features from audio, scene, OCR, face, speech, etc.</p><p>? Language representations. There are primarily four variants: 1) GoogleNews pretrained word2vec (w2v) <ref type="bibr" target="#b35">[36]</ref> used in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34]</ref>; 2) LSTM or Bidirectional LSTM <ref type="bibr" target="#b18">[19]</ref>; 3) pretrained BERT <ref type="bibr" target="#b9">[10]</ref> used in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14]</ref> and 4) OpenAI-GPT <ref type="bibr" target="#b37">[38]</ref> used in <ref type="bibr" target="#b30">[31]</ref>.</p><p>In this paper, we use a pretrained BERT-base model for language representation as in <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b32">33]</ref>. For video features, following <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33]</ref>, we extract 2D CNN features using Resnet-152 (R-152) pretrained on ImageNet <ref type="bibr" target="#b8">[9]</ref>. For 3D CNN features, we use I3D (with Resnext-101 backbone) pretrained on Kinetics-400 <ref type="bibr" target="#b22">[23]</ref> and S3D <ref type="bibr" target="#b50">[51]</ref> pretrained on Howto100M <ref type="bibr" target="#b33">[34]</ref>. The off-the-shelf pretrained weights are provided by <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b33">[34]</ref>. For simplicity, we denote them by I3D-X101 and S3D-HM in the following.</p><p>Another discrepancy among different methods is the number of self-attention layers used in the model. In <ref type="bibr" target="#b59">[60]</ref>, the authors use 12 multi-modal self-attention layers while 6 video encoder layers and 2 multi-modal fusion layers are used in <ref type="bibr" target="#b32">[33]</ref>. Differently, 4 multi-modal self-attention layers are used in <ref type="bibr" target="#b13">[14]</ref>. In this paper, for all our ablation studies below, we use 1 and 2 self-attention layers for our video encoder and multi-modal fusion, respectively. To compare with previous work on specific dataset, we use 2 video encoding layers. While pretraining the model with largescale dataset Howto100M <ref type="bibr" target="#b34">[35]</ref>, we increase to 4 video encoding layers for comparable model capacity to previous works <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14]</ref>. Note that this largest model is still smaller than or on par with the aforementioned methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>For YouCook2 and MSR-VTT, the maximum number of video and text tokens are set to 48 and 30, respectively. For paragraph-video retrieval on ActivityNet, we set them both to 256. The 2D R-152 feature is extracted for one frame per second, and then globally pooled to 2048-d. For 3D CNN features, we follow <ref type="bibr" target="#b34">[35]</ref> to sample video frames at 24 fps and extract an I3D-X101 feature every 16 frames. This results in 1.5 2048-d feature per second. For Eq. 3 and 4, we set the temperatures ? 1 and ? 2 both equal to 1. Training on separate datasets. In this setting, we train models from scratch using the training set provided in YouCook2, MSR-VTT and ActivityNet separately. We train  Pretraining and finetuning. We pretrain our model on Howto100M <ref type="bibr" target="#b34">[35]</ref>. Since the original annotated video clips in Howto100M are usually short with a few seconds, we merge the adjacent clips so that the resulted text has at least 10 words. We use Adam <ref type="bibr" target="#b23">[24]</ref> as the optimizer with initial learning rate 1e ?4 . We train the model for 500k iterations with batch size 64, and also sample 8 hard negatives for each sample using our cascade sampling strategy. After pretraining, we finetune the pretrained models on different datasets using the same setting as above except for a lower initial learning rate 2e ?5 and less finetuning iterations 20k. Evaluation metrics. For text-video retrieval, we use Recalls at different points (Recall@n or Rn, with n as a specific number) and Median Rank (MR) as the metrics following previous works <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b32">33]</ref>. In all tables, we use ? or ? to indicate higher or lower is better, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We first evaluate text-video retrieval performance and then study whether the learned representations can be transferred to other tasks on CrossTask and COIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Text-video retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Comparing with baselines</head><p>We first show the comparisons with baselines to inspect the effects of different components in our model. Video representations. We train our model with different video representations as described above and compare it with the baseline model which has identical architecture but merely trained with L 3 as depicted in Eq. 5. The baseline    <ref type="table">Table 4</ref>: Text-video retrieval performance with different tokens of interest for computing token-level contrastive loss. "det" means determiner; "adp" means adposition. We use the same video features as in <ref type="table" target="#tab_1">Table 3</ref>.</p><p>contrastive learning method has been adopted in a number of previous works <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b32">33]</ref>. This comparison can verify the effectiveness of our proposed contrastive learning method considering two models have exactly the same number of parameters. In <ref type="table" target="#tab_0">Table 2</ref>, we can see our proposed method outperforms baseline across all feature types introduced in Sec. 4.2 on both YouCook2 and MSR-VTT. Note that our model uses exactly the same number of parameters to the baseline model. These consistent improvements demonstrate the effectiveness and generalization ability of our proposed method. As mentioned above, we also observe the text-video retrieval performance significantly depends on the feature types. We can find 3D features (I3D-X101 and S3D-HM) in general outperform 2D feature (R-152), which is expected since 2D feature does not capture the motions in the videos. Among all three feature types, S3D-HM outperforms the other two with large margin, which demonstrates the potential to learn good video representation by pretraining on large-scale noisy dataset (Howto100M <ref type="bibr" target="#b34">[35]</ref>). Because Howto100M mainly contains instructional videos, it is more close to YouCook2 than MSR-VTT, and hence we see more gain on YouCook2. These comparisons indicate video representations matter much to the final performance. Component Analysis. In our method, we combine L 1 , L 2 , and L 3 during training and inference. Here, we study how they perform separately and contribute to the final performance. In <ref type="table" target="#tab_0">Table 2</ref>, we use R-152+S3D-HM as the video feature and report the results with different loss combina-    <ref type="table">Table 7</ref>: Comparing text-video retrieval on ActivityNet.</p><p>tions. As we can see, solely using L 1 (row 1) or L 2 (row 2) for contrastive learning results in sub-optimal video-text alignment. Simply combining them together (row 3) improves the performance on two datasets. This implies that different levels of contrastive learning can be complementary to each other, which supports our earlier hypothesis that these two losses are synergistic with each other for a better video-text alignment. When incorporating the hard negative mining via our cascade sampling strategy (row 4), it further improves the performance. Finally, we can see adding token-level contrastive loss L 3 can further improve the performance across all settings (row 5). Tokens of Interest. We further study the effect of different tokens of interest on the model performance. By default, our model uses the noun and verb as the tokens of interest to compute the token-level contrast loss. Here, we vary them to other types such as adposition (adp) and determiner (det) for investigation. In <ref type="table">Table 4</ref>, we replace "noun+verb" <ref type="figure">Figure 2</ref>: Zero-shot performance on YouCook2 and MSR-VTT for different settings. score-1-5 correspond to the five settings in <ref type="table" target="#tab_1">Table 3</ref> from top to bottom.</p><p>with "det+adp", "noun" and "verb" and report the numbers on two text-video retrieval datasets. As we can see, using "det+adp" as the target tokens is worse than the baseline without any token-level contrastive loss. "noun" and "verb" can both improve the performance while "noun" is slightly better than "verb". Finally, combining noun and verb together achieves the best performance. These results align with our intuition to use nouns and verbs as the target token for fine-grained alignment between texts and videos considering they are usually grounded to video contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Comparing with state-of-the-art</head><p>We compare with previous works under three protocols: 1) training and evaluating on separate datasets; 2) pretraining on Howto100M and evaluating zero-shot performance and 3) finetuning pretrained model on separate datasets. Results on separate datasets. We separately show the comparisons on YouCook2, MSR-VTT and ActivityNet in <ref type="table" target="#tab_3">Table 5</ref>, 6 and 7. For a fair comparison with previous works, we use the same or similar features as listed in the tables. As we can see, our method outperforms all previous work across all datasets. These results validates its effectiveness to learn video-text alignment. Note that previous works either use a variety of loss functions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b27">28]</ref> or a collection of multiple features <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14]</ref>. In contrast, we achieve the best performance using a simpler contrastive learning pipeline with smaller model size. This supports our earlier claim on the efficiency. Comparing the numbers in <ref type="table" target="#tab_0">Table 2</ref>, <ref type="table" target="#tab_3">Table 5</ref> and   <ref type="table">Table 9</ref>: Action step localization on CrossTask (avg. recall) and action segmentation on COIN (acc.).</p><p>Zero-shot and finetuned performance. In <ref type="table" target="#tab_6">Table 8</ref>, we show the comparisons across different models pretrained on Howto100M. In the upper part of the table, we compare the zero-shot performance on YouCook2 and MSR-VTT. We do not evaluate on ActivityNet since it has different number of input video tokens compared with the pretrained model and thus is not directly compatible to the pretrained model. As we can see, TACo outperforms previous works significantly on YouCook2 and slightly on MSR-VTT. Since YouCook2 has closer domain gap to Howto100M than MSR-VTT, the improvement brought by large-scale pretraining is more significant. However, on MSR-VTT, our model still outperforms MIL-NCE <ref type="bibr" target="#b33">[34]</ref> which uses the same video features. In <ref type="figure">Fig. 2</ref>, we show the zero-shot performance on YouCook2 and MSR-VTT when pretraining our models with different contrastive losses as listed in <ref type="table" target="#tab_1">Table 3</ref>. Accordingly, it shows our proposed contrastive losses gradually improve the performance, and combining all techniques achieves the best performance. Based on the pretrained model, we further finetune it on specific datasets. In our experiments, we use two feature S3D-HM and R-152+S3D-HM, to compare with the methods with the same/similar settings. As we can see, our model using S3D-HM outperforms UniVL <ref type="bibr" target="#b32">[33]</ref> using the same feature but more video encoder layers <ref type="bibr" target="#b5">(6)</ref>. Different from zero-shot results, we observe more improvement on MSR-VTT than YouCook2 after finetuning. This implies that finetuning on specific datasets can compensate the domain gap to the pretraining datasets. To compare with the methods using features extracted from collaborative experts <ref type="bibr" target="#b13">[14]</ref>, we enrich our video representation by adding 2D R-152 feature, which achieves better performance on MSR-VTT, and better Recall@1 and Median Rank on Ac-tivityNet. Note that this combination hurts the performance on YouCook2, and we witnessed a similar trend for models without pretraining in <ref type="table" target="#tab_0">Table 2</ref>. Finally, comparing with the results without pretraining in <ref type="table" target="#tab_3">Table 5</ref>, 6 and 7, we clearly find large-scale pretraining and finetuning brings substantial improvements consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Other video-related tasks</head><p>Following <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b32">33]</ref>, we evaluate action step localization performance on CrossTask dataset <ref type="bibr" target="#b60">[61]</ref>. It covers 18 tasks and each video contains multiple video segments annotated with action steps and natural language descriptions. Similar to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b32">33]</ref>, we use our model to compute the similarity between each frame and the action step descriptions, which results in a score matrix. Using the official algorithm provided by <ref type="bibr" target="#b60">[61]</ref>, we can find the optimal framewise order of action steps for a video. By comparing it with the ground-truth annotations, we compute the recall for each task and then do the average. According to the results in <ref type="table">Table 9</ref>, our model achieves the best performance compared with previous works. This indicates that our model can learn good video-language representations.</p><p>We further evaluate our pretrained model on action segmentation task on COIN dataset, following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b59">60]</ref>. Unlike the above task, action segmentation does not rely on texts, and thus can be used to evaluate the learned video representation. As shown in <ref type="table">Table 9</ref>, our method significantly outperforms MIL-NCE and ActBert, and achieves comparable performance to UniVL. This indicates that our model is also a good video representation learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduced TACo, a simple yet effective contrastive learning method for learning video-text alignment. It is aimed at addressing two existing issues in current contrastive learning pipelines: missing finegrained alignment and inefficient sampling for multi-modal fusion. Without introducing any extra parameters, our method achieved promising results on three text-video retrieval benchmarks under various evaluation protocols. We further demonstrated the learned representations can be effectively transferred to other tasks such as action step localization and segmentation. Based on all these encouraging results, we believe TACo is a good alternative to conventional contrastive learning pipeline. We extract tokens of interest (T.O.I) using the pos-tagger provided by Spacy <ref type="bibr" target="#b19">[20]</ref>. In <ref type="table">Table 10</ref>, we show the statistics of tokens for three datasets. For each token that is tagged at VERB or NOUN, we compute the inverse document frequency (idf) by: idf (token) = log |D| 1 + |{d ? D : token ? d}| <ref type="bibr" target="#b6">(7)</ref> where D is the full set of corpus, which are the captions in the training set for a dataset; the denominator counts the number of captions which contain a specific token. Based on Eq. 7, we can compute the idf for each token of interest. The smaller the idf, the more frequent it appears in the corpus. We do not compute the tf term since usually a token only appears once in a single sentence. The full list of tokens and corresponding idfs can be found in <ref type="figure" target="#fig_7">Fig. 4</ref>. For a given sentence, we first assign the computed idfs to its nouns and verbs and then normalize the idfs, which are then used to weigh the token-level contrastive losses.  In this part, we investigate the contributions of three contrastive losses used in our model. After we train the videotext alignment model using all three losses, we report the performance using separate alignment scores in <ref type="table" target="#tab_9">Table 11</ref>. For reference, the top two rows are the performance for using early stage only and later stage only contrastive learning to train the model. The bottom four rows are the separate performance at different stages for our model. As we can see, combining three contrastive losses during training can boost the performance for both early and later stage (row 3 v.s. row 1, row 5 v.s. row 2). This indicates that the three losses are synergistic to each other for a better videotext alignment. On the other hand, the early stage alignment achieves better performance than other two (token-level and later stage), while the fused score is the best. We suspect that this is because early stage alignment is trained with all text-video pairs at sentence-level. In contrast, token-level contrast focuses on single tokens and the multi-modal fusion layers merely see a small part of hard text-video pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tokens of interest</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contribution of three contrastive losses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of cascade sampling</head><p>The proposed cascade sampling helps the later stage contrastive learning to focus on hard negative samples. As shown in our main submission, adding cascade sampling will improve the performance. We suspect this is because cascade sampling helps learn a better later stage alignment. To verify this, we compare the later stage alignment across three different settings: 1) merely applying later stage contrastive loss; 2) combine early state and later stage con-trastive losses and 3) using cascade sampling for later stage contrastive loss. We report the results on YouCook2 in <ref type="table" target="#tab_0">Table 12</ref>. Here, note that we only use the later stage alignment scores for evaluating the performance. As we can see, combining early stage and later stage together slightly improves the performance. This is probably because early stage contrastive loss helps to learn a better video and language encoder, from which the multi-modal module takes better representations for cross-modal fusion. After applying the cascade sampling for the later stage contrastive loss, the performance is further improved. Since our cascade sampling strategy can send more difficult samples to the later stage, the cross-modal fusion layers can learn more discriminative representations for video-text alignment. These results validate that the hard negative mining through cascade sampling indeed helps to improve the later-stage text-video alignment, and hence the final performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effect of video encoder layers</head><p>In our main paper, we noticed the number of video encoder layers affects the final performance. To have a more comprehensive study, we use R-152 and S3D-HM as the 2D and 3D features and train the video-text alignment model on YouCook2 with different video encoder layers. As shown in <ref type="table" target="#tab_1">Table 13</ref>, using more video encoder layers can significantly boost the text-video retrieval performance. Particularly, when no video encoder layers are used, the model can hardy capture the long-range temporal dynamics, and thus performs poorly. Once we add one video encoder layer, the performance improves significantly. With the increase of encoder layers, the performance is further improved, which is reasonable since more video encoder layers can encode more complicated video contents and dynamics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparing model size and FLOPs</head><p>Finally, we attempt to compare the model sizes and computational costs for different methods. Unfortunately, all previous methods did not report FLOPs and only MMT <ref type="bibr" target="#b13">[14]</ref> discussed #params. However, the results in <ref type="table" target="#tab_1">Table 13</ref> imply that bigger model can usually achieve better performance. Therefore, it is necessary to have a comparison of model size and computational cost between our model and those from other methods. For other methods which do not report the numbers, we estimate them based on the descriptions in the original paper. <ref type="table">Table 14</ref> summarizes the comparisons and also reports the #params and FLOPs (all underlined numbers are estimated based on the descriptions in original papers). As shown, our largest model has comparable size and FLOPs to others.  <ref type="table">Table 14</ref>: Comparison of model size and FLOPs. "mm" means multi-modal fusion, and "self" means self-attention layers while "cross" means cross-modal attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visualizations</head><p>We visualize the text-video retrieval results by varying the weights for the token-level alignment scores during testing. In <ref type="figure" target="#fig_6">Fig. 3</ref>, we show two text-video retrieval examples on YouCook (top) and MSR-VTT (bottom). From top to bottom, the five rows in each block correspond to the top five retrieved results from the whole test set. As we can see, when we gradually increase the weight for the tokenlevel alignment score, there are more related videos appearing in the top five candidates. For YouCook2, when we set the weight equal to 0.0, the third and fifth video are not well-aligned with the query since they are both not about "tomato". When we increase the weight to 0.1, we can observe the the fourth video moves to the third place. After we increase the weight to 0.5, we can see all top-5 videos are about cutting tomato. Similarly, for MSR-VTT, we can see the last three videos are not about "two people talking on a table". When we increase the weight to 0.1, the fifth video is replaced with a more matched video. Keeping increase the weight to 0.5, we can obtain the top 5 videos all about "two people talking with each other on a table". These visualizations demonstrate the efficacy of our proposed token-level contrastive learning. Weight=0.0 Weight=0.1 Weight=0.5 a man and a woman trying some sake there is a woman is talking with two guys leonardo dicaprio is portrayed as two different characters in this film a girl in a studio singing a cartoon man plays a card game with his friends a man and a woman trying some sake there is a woman is talking with two guys leonardo dicaprio is portrayed as two different characters in this film a girl in a studio singing two men talking about investors on a show two men talking about investors on a show a man and a woman trying some sake there is a woman is talking with two guys a man and woman arguing about fake arms used in a performance a man and another man speak to each other in a room  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed token-aware cascade contrastive learning pipeline. We compute three contrastive losses: 1) sentence-level loss L 1 over all negative examples; 2) tokenlevel loss L 2 on content words (noun, verb) over all negative examples; 3) sentence-level loss L 3 over hard negative examples sampled based on L 1 and L 2 online.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Given a set of N video-text pairs {(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>38.5 51.2 10.0 23.3 51.0 63.5 5.0 noun 15.4 39.3 51.8 10.0 24.0 51.8 65.1 5.0 verb 15.3 39.0 51.4 10.0 23.9 52.1 64.8 5.0 noun+verb 15.8 39.8 52.4 10.0 24.5 52.8 65.5 5.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>into quarters remove the seeds chop finely and add to the bowl chop a tomato into thin slices chop some red onions red pepper and green pepper into square pieces chop up the tomatoes cut the pepperoni in half cut the pepperoni in half chop some red onions red pepper and green pepper into square pieces chop up the tomatoes chop a tomato into thin slices cut a tomato into quarters remove the seeds chop finely and add to the bowl cut a tomato into quarters remove the seeds chop finely and add to the bowl chop a tomato into thin slices chop up the tomatoes slice tomatoes into thin slices cut tomatoes and place them in a bowl Query: two people are talking with each other on a table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Text-video retrieval results given a query on YouCook2 (top) and MSR-VTT (bottom). In each block, we show top 5 ranked videos from top to bottom. From left to right, we gradually increase the token-level alignment weight from 0.0 to 0.1 and then 0.5 (default in our main experiments). The change of the top 5 results demonstrate the benefit of token-level contrast when performing text-video retrieval. Below each video (depicted by three side-by-side frames), we show the associated descriptions provided in the original dataset. Better viewed by enlarging thefigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Token inverse document frequency (IDF) for noun and verb in YouCook2 and MSR-VTT. For clarity, we evenly sample the tokens and show their IDFs. From left to right, the noun/verb becomes more and more frequent gradually.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Text-video retrieval performance on YouCook2 and MSR-VTT with different feature types. S3D pretrained on HowTo100M outperforms others with large margin.</figDesc><table /><note>the model for 30k iterations with batch size 128. For each training sample, we use our cascade sampling strategy to sample 8 hard negatives. We use Adam [24] as the opti- mizer with initial learning rate 1e ?4 . A linear learning rate decay is applied after 5k warm-up iterations. The weight decay is set to 1e ?5 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Text-video retrieval performance with different</cell></row><row><cell>technique ensembles. It shows that using our proposed two</cell></row><row><cell>techniques produce best results. All experiments use R-</cell></row><row><cell>152+S3D-HM video features.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Comparing text-video retrieval on YouCook2.</figDesc><table><row><cell>Model</cell><cell>Lang.</cell><cell>Video</cell><cell>MSR-VTT</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R1? R5? R10? MR?</cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell>0.1 0.5 1.0 500.0</cell></row><row><cell>JSFusion [53]</cell><cell cols="2">BiLSTM R-152</cell><cell>10.2 31.2 43.2 13.0</cell></row><row><cell>JPoSE [49]</cell><cell>w2v</cell><cell>TSN+Flow</cell><cell>14.3 38.1 53.0 9.0</cell></row><row><cell>TVJE [35]</cell><cell>w2v</cell><cell>R-152+I-101</cell><cell>12.1 35.0 48.0 12.0</cell></row><row><cell cols="2">UniVL(v1)  *  [33] BERT</cell><cell>R-152+I-101</cell><cell>14.6 39.0 52.6 10.0</cell></row><row><cell>TACo (Ours)</cell><cell>BERT</cell><cell>R-152+I-101</cell><cell>19.2 44.7 57.2 7.0</cell></row><row><cell>CE [31]</cell><cell>GPT</cell><cell cols="2">Collaborative Experts 20.9 48.8 62.4 6.0</cell></row><row><cell>MMT [14]</cell><cell>BERT</cell><cell cols="2">Collaborative Experts 24.6 54.0 67.1 4.0</cell></row><row><cell>TACo (Ours)</cell><cell>BERT</cell><cell>R-152+S3D-HM</cell><cell>26.7 54.5 68.2 4.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Comparing text-video retrieval on MSR-VTT. The upper block and bottom block use split2 and split1, respectively. We report them separately for fair comparison.</figDesc><table><row><cell>Model</cell><cell cols="2">Lang. Video</cell><cell>ActivityNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R1? R5? R10? MR?</cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell>0.02 0.1 1.02 2458</cell></row><row><cell cols="3">DenseCap [25] LSTM C3D</cell><cell>14.0 32.0 65.0 34</cell></row><row><cell>FSE [56]</cell><cell cols="3">GRU C3D+TSN-Inception 18.2 44.8 89.1 7.0</cell></row><row><cell>CE [31]</cell><cell cols="3">GPT Collaborative Experts 18.2 47.7 91.4 6.0</cell></row><row><cell>MMT [14]</cell><cell cols="3">BERT Collaborative Experts 22.7 54.2 93.2 5.0</cell></row><row><cell cols="3">TACo (Ours) BERT R-152+S3D-HM</cell><cell>25.8 56.3 93.8 4.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell>Model</cell><cell>Video</cell><cell>YouCook2</cell><cell>MSR-VTT</cell><cell></cell><cell cols="2">ActivityNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">R1? R5? R10? MR? R1? R5? R10? MR? R1? R5? R50? MR?</cell></row><row><cell>Zero-shot</cell><cell cols="2">TJVE [35] ActBERT [60] O-101+ R(2+1)D R-152+I-101 MIL-NCE [34] S3D-HM TACo (Ours) S3D-HM</cell><cell cols="3">6.1 17.3 24.8 46.0 7.5 21.2 29.6 38.0 -9.6 26.7 38.0 19.0 8.6 23.4 33.1 36.0 -15.1 38.0 51.2 10.0 9.9 24.0 32.4 29.5 -19.9 43.2 55.7 8.0 9.8 25.0 33.4 29.0 -</cell><cell>----</cell><cell>----</cell><cell>----</cell></row><row><cell></cell><cell>TJVE [35]</cell><cell>R-152+I3D-X101</cell><cell cols="2">8.2 24.5 35.3 24.0 14.9 40.2 52.8 9.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Finetuned</cell><cell cols="7">UniVL(v3) [33] S3D-HM TACo (Ours) S3D-HM MMT [14] Collaborative Experts -28.9 57.6 70.0 4.0 21.2 49.6 63.1 6.0 29.6 59.7 72.7 4.0 24.8 52.1 64.0 5.0 28.3 56.8 92.6 4.0 -------26.6 57.1 69.6 4.0 28.7 61.4 94.5 3.3</cell></row><row><cell></cell><cell>TACo (Ours)</cell><cell>R-152+S3D-HM</cell><cell cols="5">27.3 56.5 68.8 4.0 28.4 57.8 71.2 4.0 30.4 61.2 93.4 3.0</cell></row></table><note>, we can find our model achieves better per- formance with the same video features when using deeper video encoder (2 layers v.s. 1 layer).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>A complete comparison of TACo under zero-shot and finetuning evaluation protocols. Note that the zero-shot and upper part of finetuned performance for MSR-VTT is on split2, while the bottom is on split1 for fair comparison.</figDesc><table><row><cell>Method</cell><cell cols="2">CrossTask COIN</cell></row><row><cell>Alayrac et al. [1]</cell><cell>13.3</cell><cell>-</cell></row><row><cell>Zhukov et al. [61]</cell><cell>22.4</cell><cell>-</cell></row><row><cell>Supervised [61]</cell><cell>31.6</cell><cell>-</cell></row><row><cell>NN-Viterbi [39]</cell><cell>-</cell><cell>21.2</cell></row><row><cell>CBT [40]</cell><cell>-</cell><cell>53.9</cell></row><row><cell>TVJE [35]</cell><cell>33.6</cell><cell>-</cell></row><row><cell>MIL-NCE [34]</cell><cell>40.5</cell><cell>61.0</cell></row><row><cell>ActBert [60]</cell><cell>41.4</cell><cell>57.0</cell></row><row><cell>UniVL(v3) [33]</cell><cell>42.0</cell><cell>70.0</cell></row><row><cell>TACo (Ours)</cell><cell>42.5</cell><cell>68.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Text-video retrieval performance using separate alignment scores on YouCook2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Text-video retrieval performance on YouCook2 only using later stage alignment score for different settings.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4575" to="4583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Image similarity using deep cnn and curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikar</forename><surname>Appalaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Chaoji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08761</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Experience grounds language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Nisnevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="5" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10638" to="10647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual encoding for zeroexample video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9346" to="9355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen Sp?rck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The Kinetics Human Action Video Dataset</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01696</idno>
		<title level="m">Tvqa: Localized, compositional video question answering</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hero: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00200</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Oscar: Objectsemantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ViL-BERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11 pages, 5 figures</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Univilm: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9876" to="9886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6426</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neuralnetwork-viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7386" to="7395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="7463" to="7472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="8" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coin: A large-scale dataset for comprehensive instructional video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dajun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through questionanswering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4631" to="4640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning language-visual embedding for movie understanding with natural-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fine-grained action retrieval through multiple partsof-speech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6609" to="6618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Video captioning and retrieval models with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02947</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3165" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unified vision-language pretraining for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2018</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Crosstask weakly supervised learning from instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramazan</forename><forename type="middle">Gokberk</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3537" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
