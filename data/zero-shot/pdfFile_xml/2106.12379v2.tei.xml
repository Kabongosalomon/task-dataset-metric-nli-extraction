<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Peste</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenia</forename><surname>Iofinova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">CNRS &amp; IRIF</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">IST Austria &amp; Neural Magic</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increasing computational requirements of deep neural networks (DNNs) have led to significant interest in obtaining DNN models that are sparse, yet accurate. Recent work has investigated the even harder case of sparse training, where the DNN weights are, for as much as possible, already sparse to reduce computational costs during training. Existing sparse training methods are often empirical and can have lower accuracy relative to the dense baseline. In this paper, we present a general approach called Alternating Compressed/DeCompressed (AC/DC) training of DNNs, demonstrate convergence for a variant of the algorithm, and show that AC/DC outperforms existing sparse training methods in accuracy at similar computational budgets; at high sparsity levels, AC/DC even outperforms existing methods that rely on accurate pre-trained dense models. An important property of AC/DC is that it allows co-training of dense and sparse models, yielding accurate sparse-dense model pairs at the end of the training process. This is useful in practice, where compressed variants may be desirable for deployment in resourceconstrained settings without re-doing the entire training flow, and also provides us with insights into the accuracy gap between dense and compressed models. The code is available at: https://github.com/IST-DASLab/ACDC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The tremendous progress made by deep neural networks in solving diverse tasks has driven significant research and industry interest in deploying efficient versions of these models. To this end, entire families of model compression methods have been developed, such as pruning <ref type="bibr" target="#b28">[29]</ref> and quantization <ref type="bibr" target="#b21">[22]</ref>, which are now accompanied by hardware and software support <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Neural network pruning, which is the focus of this paper, is the compression method with arguably the longest history <ref type="bibr" target="#b37">[38]</ref>. The basic goal of pruning is to obtain neural networks for which many connections are removed by being set to zero, while maintaining the network's accuracy. A myriad pruning methods have been proposed-please see <ref type="bibr" target="#b28">[29]</ref> for an in-depth survey-and it is currently understood that many popular networks can be compressed by more than an order of magnitude, in terms of their number of connections, without significant accuracy loss.</p><p>Many accurate pruning methods require a fully-accurate, dense variant of the model, from which weights are subsequently removed. A shortcoming of this approach is the fact that the memory and computational savings due to compression are only available for the inference, post-training phase, and not during training itself. This distinction becomes important especially for large-scale modern models, which can have millions or even billions of parameters, and for which fully-dense training can have high computational and even non-trivial environmental costs <ref type="bibr" target="#b52">[53]</ref>.</p><p>One approach to address this issue is sparse training, which essentially aims to remove connections from the neural network as early as possible during training, while still matching, or at least approximating, the accuracy of the fully-dense model. For example, the RigL technique [16] randomly * Correspondence to: Alexandra Peste &lt;alexandra.peste@ist.ac.at&gt; 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2106.12379v2 [cs.LG] 15 Dec 2021</p><p>removes a large fraction of connections early in training, and then proceeds to optimize over the sparse support, providing savings due to sparse back-propagation. Periodically, the method re-introduces some of the weights during the training process, based on a combination of heuristics, which requires taking full gradients. These works, as well as many recent sparse training approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b31">32]</ref>, which we cover in detail in the next section, have shown empirically that non-trivial computational savings, usually measured in theoretical FLOPs, can be obtained using sparse training, and that the optimization process can be fairly robust to sparsification of the support.</p><p>At the same time, this line of work still leaves intriguing open questions. The first is theoretical: to our knowledge, none of the methods optimizing over sparse support, and hence providing training speed-up, have been shown to have convergence guarantees. The second is practical, and concerns a deeper understanding of the relationship between the densely-trained model, and the sparsely-trained one. Specifically, (1) most existing sparse training methods still leave a non-negligible accuracy gap, relative to dense training, or even post-training sparsification; and (2) most existing work on sparsity requires significant changes to the training flow, and focuses on maximizing global accuracy metrics; thus, we lack understanding when it comes to co-training sparse and dense models, as well as with respect to correlations between sparse and dense models at the level of individual predictions.</p><p>Contributions. In this paper, we take a step towards addressing these questions. We investigate a general hybrid approach for sparse training of neural networks, which we call Alternating Compressed / DeCompressed (AC/DC) training. AC/DC performs co-training of sparse and dense models, and can return both an accurate sparse model, and a dense model, which can recover the dense baseline accuracy via fine-tuning. We show that a variant of AC/DC ensures convergence for general nonconvex but smooth objectives, under analytic assumptions. Extensive experimental results show that it provides state-of-the-art accuracy among sparse training techniques at comparable training budgets, and can even outperform post-training sparsification approaches when applied at high sparsities.</p><p>AC/DC builds on the classic iterative hard thresholding (IHT) family of methods for sparse recovery <ref type="bibr" target="#b5">[6]</ref>. As the name suggests, AC/DC works by alternating the standard dense training phases with sparse phases where optimization is performed exclusively over a fixed sparse support, and a subset of the weights and their gradients are fixed at zero, leading to computational savings. (This is in contrast to error feedback algorithms, e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref> which require computing fully-dense gradients, even though the weights themselves may be sparse.) The process uses the same hyper-parameters, including the number of epochs, as regular training, and the frequency and length of the phases can be safely set to standard values, e.g. 5-10 epochs. We ensure that training ends on a sparse phase, and return the resulting sparse model, as well as the last dense model obtained at the end of a dense phase. This dense model may be additionally fine-tuned for a short period, leading to a more accurate dense-finetuned model, which we usually find to match the accuracy of the dense baseline.</p><p>We emphasize that algorithms alternating sparse and dense training phases for deep neural networks have been previously investigated <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b24">25]</ref>, but with the different goal on using sparsity as a regularizer to obtain more accurate dense models. Relative to these works, our goals are two-fold: we aim to produce highly-accurate, highly-sparse models, but also to maximize the fraction of training time for which optimization is performed over a sparse support, leading to computational savings. Further, we are the first to provide convergence guarantees for variants of this approach.</p><p>We perform an extensive empirical investigation, showing that AC/DC provides consistently good results on a wide range of models and tasks (ResNet [28]  and MobileNets [30] on the ImageNet [49] / CIFAR [36] datasets, and Transformers [56, 10] on WikiText [42])</p><p>, under standard values of the training hyper-parameters. Specifically, when executed on the same number of training epochs, our method outperforms all previous sparse training methods in terms of the accuracy of the resulting sparse model, often by significant margins. This comes at the cost of slightly higher theoretical computational cost relative to prior sparse training methods, although AC/DC usually reduces training FLOPs to 45-65% of the dense baseline. AC/DC is also close to the accuracy of state-of-the-art posttraining pruning methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b51">52]</ref> at medium sparsities (80% and 90%); surprisingly, it outperforms them in terms of accuracy, at higher sparsities. In addition, AC/DC is flexible with respect to the structure of the "sparse projection" applied at each compressed step: we illustrate this by obtaining semi-structured pruned models using the 2:4 sparsity pattern efficiently supported by new NVIDIA hardware <ref type="bibr" target="#b42">[43]</ref>. Further, we show that the resulting sparse models can provide significant real-world speedups for DNN inference on CPUs <ref type="bibr" target="#b11">[12]</ref>. An interesting feature of AC/DC is that it allows for accurate dense/sparse co-training of models. Specifically, at medium sparsity levels (80% and 90%), the method allows the co-trained dense</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>model to recover the dense baseline accuracy via a short fine-tuning period. In addition, dense/sparse co-training provides us with a lens into the training dynamics, in particular relative to the sample-level accuracy of the two models, but also in terms of the dynamics of the sparsity masks. Specifically, we observe that co-trained sparse/dense pairs have higher sample-level agreement than sparse/dense pairs obtained via post-training pruning, and that weight masks still change later in training.</p><p>Additionally, we probe the accuracy differences between sparse and dense models, by examining their "memorization" capacity <ref type="bibr" target="#b59">[60]</ref>. For this, we perform dense/sparse co-training in a setting where a small number of valid training samples have corrupted labels, and examine how these samples are classified during dense and sparse phases, respectively. We observe that the sparse model is less able to "memorize" the corrupted labels, and instead often classifies the corrupted samples to their true (correct) class. By contrast, during dense phases model can easily "memorize" the corrupted labels. (Please see <ref type="figure" target="#fig_1">Figure 2b</ref> for an illustration.) This suggests that one reason for the higher accuracy of dense models is their ability to "memorize" hard-to-classify samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has recently been tremendous research interest into pruning techniques for DNNs; we direct the reader to the recent surveys of <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b28">[29]</ref> for a more comprehensive overview. Roughly, most DNN pruning methods can be split as (1) post-training pruning methods, which start from an accurate dense baseline, and remove weights, followed by fine-tuning; and (2) sparse training methods, which perform weight removal during the training process itself. (Other categories such as data-free pruning methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b53">54]</ref> exist, but they are beyond our scope.) We focus on sparse training, although we will also compare against state-of-the-art post-training methods.</p><p>Arguably, the most popular metric for weight removal is weight magnitude <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b61">62]</ref>. Betterperforming approaches exist, such as second-order metrics <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b51">52]</ref>, or Bayesian approaches <ref type="bibr" target="#b45">[46]</ref>, but they tend to have higher computational and implementation cost.</p><p>The general goal of sparse training methods is to perform both the forward (inference) pass and the backpropagation pass over a sparse support, leading to computational gains during the training process as well. One of the first approaches to maintain sparsity throughout training was Deep Rewiring <ref type="bibr" target="#b3">[4]</ref>, where SGD steps applied to positive weights are augmented with random walks in parameter space, followed by inactivating negative weights. To maintain sparsity throughout training, randomly chosen inactive connections are re-introduced in the "growth" phase. Sparse Evolutionary Training (SET) <ref type="bibr" target="#b43">[44]</ref> introduces a non-uniform sparsity distribution across layers, which scales with the number of input and output channels, and trains sparse networks by pruning weights with smallest magnitude and re-introducing some weights randomly. RigL <ref type="bibr" target="#b15">[16]</ref> prunes weights at random after a warm-up period, and then periodically performs weight re-introduction using a combination of connectivity-and gradient-based statistics, which require periodically evaluating full gradients. RigL can lead to state-of-the-art accuracy results even compared to post-training methods; however, to achieve high accuracy it requires significant additional data passes (e.g. 5x) relative to the dense baseline. Top-KAST <ref type="bibr" target="#b31">[32]</ref> alleviated the drawback of periodically having to evaluate dense gradients by updating the sparsity masks using gradients of reduced sparsity relative to the weight sparsity. The latter two methods set the state-of-the-art for sparse training: when executing for the same number of epochs as the dense baseline, they provide computational reductions the order of 2x, while the accuracy of the resulting sparse models is lower than that of leading post-training methods, executed at the same sparsity levels. To our knowledge, none of these methods have convergence guarantees.</p><p>Another approach towards faster training is training sparse networks from scratch. The masks are updated by continuously pruning and re-introducing weights. For example, <ref type="bibr" target="#b39">[40]</ref> uses magnitude pruning after applying SGD on the dense network, whereas <ref type="bibr" target="#b12">[13]</ref> update the masks by re-introducing weights with the highest gradient momentum. STR <ref type="bibr" target="#b36">[37]</ref> learns a separate pruning threshold for each layer and allows sparsity both during forward and backward passes; however, the desired sparsity can not be explicitly imposed, and the network has low sparsity for a large portion of training. These methods can lead to only limited computational gains, since they either require dense gradients, or the sparsity level cannot be imposed. By comparison, our method provides models of similar or better accuracy at the same sparsity, with computational reductions. We also obtain dense models that match the baseline accuracy, with a fraction of the baseline FLOPs.</p><p>The idea of alternating sparse and dense training phases has been examined before in the context of neural networks, but with the goal of using temporary sparsification as a regularizer. Specifically, Dense-Sparse-Dense (DSD) <ref type="bibr" target="#b24">[25]</ref> proposes to first train a dense model to full accuracy; this model is then sparsified via magnitude; next, optimization is performed over the sparse support, followed by an additional optimization phase over the full dense support. Thus, this process is used as a regularization mechanism for the dense model, which results in relatively small, but consistent accuracy improvements relative to the original dense model. In <ref type="bibr" target="#b32">[33]</ref>, the authors propose a similar approach to DSD, but alternate sparse phases during the regular training process. The resulting process is similar to AC/DC, but, importantly, the goal of their procedure is to return a more accurate dense model. (Please see their Algorithm 1.) For this, the authors use relatively low sparsity levels, and gradually increase sparsity during optimization; they observe accuracy improvements for the resulting dense models, at the cost of increasing the total number of epochs of training. By contrast, our focus is on obtaining accurate sparse models, while reducing computational cost, and executing the dense training recipe. We execute at higher sparsity levels, and on larger-scale datasets and models. In addition, we also show that the method works for other sparsity patterns, e.g. the 2:4 semi-structured pattern <ref type="bibr" target="#b42">[43]</ref>.</p><p>More broadly, the Lottery Ticket Hypothesis (LTH) <ref type="bibr" target="#b18">[19]</ref> states that sparse networks can be trained in isolation from scratch to the same performance as a post-training pruning baseline, by starting from the "right" weight and sparsity mask initializations, optimizing only over this sparse support. However, initializations usually require the availability of the fully-trained dense model, falling under post-training methods. There is still active research on replicating these intriguing findings to largescale models and datasets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>. Previous work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">62]</ref> have studied progressive sparsification during regular training, which may also achieve training time speed-up, after a sufficient sparsity level has been achieved. However, AC/DC generally achieves a better trade-off between validation accuracy and training time speed-up, compared to these methods.</p><p>Parallel work by <ref type="bibr" target="#b44">[45]</ref> investigates a related approach, but focusing on low-rank decompositions for Transformer models. Both their analytical approach and their application domain are different to the ones of the current work.</p><p>3 Alternating Compressed / DeCompressed (AC/DC) Training</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background and Assumptions</head><p>Obtaining sparse solutions to optimization problems is a problem of interest in several areas <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref>, where the goal is to minimize a function f : R N ? R under sparsity constraints:</p><p>min</p><formula xml:id="formula_0">??R N f (?) s.t. ? 0 ? k .<label>(1)</label></formula><p>For the case of 2 regression, f (?) = b ? A? 2 2 , a solution has been provided by Blumensath and Davies <ref type="bibr" target="#b5">[6]</ref>, known as the Iterative Hard Thresholding (IHT) algorithm, and subsequent work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b57">58]</ref> provided theoretical guarantees for the linear operators used in compressed sensing. The idea consists of alternating gradient descent (GD) steps and applications of a thresholding operator to ensure the 0 constraint is satisfied. More precisely, T k is defined as the "top-k" operator, which keeps the largest k entries of a vector ? in absolute value, and replaces the rest with 0. The IHT update at step t + 1 has the following form:</p><formula xml:id="formula_1">? t+1 = T k (? t ? ??f (? t )).<label>(2)</label></formula><p>Most convergence results for IHT assume deterministic gradient descent steps. For DNNs, stochastic methods are preferred, so we describe and analyze a stochastic version of IHT.</p><p>Stochastic IHT. We consider functions f : R N ? R, for which we can compute stochastic gradients g ? , which are unbiased estimators of the true gradient ?f (?). Define the stochastic IHT update as:</p><formula xml:id="formula_2">? t+1 = T k (? t ? ?g ?t ).<label>(3)</label></formula><p>This formulation covers the practical case where the stochastic gradient g ? corresponds to a minibatch stochastic gradient. Indeed, as in practice f takes the form</p><formula xml:id="formula_3">f (?) = 1 m m i=1 f (?; x i ), where S = {x 1 , . .</formula><p>. , x m } are data samples, the stochastic gradients obtained via backpropagation take the form 1 |B| i?B ?f (?; x i ), where B is a sampled mini-batch. We aim to prove strong convergence bounds for stochastic IHT, under common assumptions that arise in the context of training DNNs.</p><p>Analytical Assumptions. Formally, our analysis uses the following assumptions on f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warmup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternating sparse and dense phases Fine-tune</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy Sparsity</head><p>Training Epoch <ref type="figure">Figure 1</ref>: The AC/DC training process. After a short warmup we alternatively prune to maximum sparsity and restore the pruned weights. The plot shows the sparsity and validation accuracy throughout the process for a sample run on ResNet50/ImageNet at 90% sparsity.</p><p>1) Unbiased gradients with variance ?:</p><formula xml:id="formula_4">E[g ? |?] = ?f (?), and E[ g ? ? ?f (?) 2 ] ? ? 2 .</formula><p>2) Existence of a k * -sparse minimizer ? * :</p><formula xml:id="formula_5">?? * ? arg min ? f (?), s.t. ? * 0 ? k * . 3)</formula><p>For ? &gt; 0, the ?-smoothness condition when restricted to t coordinates ((t, ?)-smoothness):</p><formula xml:id="formula_6">f (? + ?) ? f (?) + ?f (?) ? + ? 2 ? 2 , for all ?, ? s.t. ? 0 ? t .<label>(4)</label></formula><p>4) For ? &gt; 0 and number of indices r, the r-concentrated Polyak-?ojasiewicz ((r, ?)-CPL) condition:</p><formula xml:id="formula_7">T r (?f (?)) ? ? 2 (f (?) ? f (? * )) , for all ?.<label>(5)</label></formula><p>The first assumption is standard in stochastic optimization, while the existence of very sparse minimizers is a known property in over-parametrized DNNs <ref type="bibr" target="#b18">[19]</ref>, and is the very premise of our study.</p><p>Smoothness is also a standard assumption, e.g. <ref type="bibr" target="#b39">[40]</ref>-we only require it along sparse directions, which is a strictly weaker assumption. The more interesting requirement for our convergence proof is the (r, ?)-CPL condition in Equation <ref type="formula" target="#formula_7">(5)</ref>, which we now discuss in detail.</p><p>The standard Polyak-?ojasiewicz (PL) condition <ref type="bibr" target="#b33">[34]</ref> is common in non-convex optimization, and versions of it are essential in the analysis of DNN training <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b1">2]</ref>. Its standard form states that small gradient norm, i.e. approximate stationarity, implies closeness to optimum in function value. We require a slightly stronger version, in terms of the norm of the gradient contributed by its largest coordinates in absolute value. This restriction appears necessary for the success of IHT methods, as the sparsity enforced by the truncation step automatically reduces the progress ensured by a gradient step to an amount proportional to the norm of the top-k gradient entries. This strengthening of the PL condition is supported both theoretically, by the mean-field view, which argues that gradients are sub-gaussian <ref type="bibr" target="#b49">[50]</ref>, and by empirical validations of this behaviour <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>We are now ready to state our main analytical result.</p><p>Theorem 1. Let f : R N ? R be a function with a k * -sparse minimizer ? * . Let ? &gt; ? &gt; 0 be parameters, let k = C ? k * ? (?/?) 2 for some appropriately chosen constant C, and suppose that f is (2k + 3k * , ?)-smooth and (k * , ?)-CPL. For initial parameters ? 0 and precision &gt; 0, given access to stochastic gradients with variance ?, stochastic</p><formula xml:id="formula_8">IHT (3) converges in O ? ? ? ln f (?0)?f (? * ) iterations to a point ? with ? 0 ? k, such that E [f (?) ? f (? * )] ? + 16? 2 ? .</formula><p>Assuming a fixed objective function f and tolerance , we can obtain lower loss and faster running time by either 1) increasing the support k demanded from our approximate minimizer ? relative to the optimal k * , or by reducing the gradient variance. We provide a complete proof of this result in the Appendix. Our analysis approach also works in the absence of the CPL condition (Theorem 3), in which case we prove that a version of the algorithm can find sparse nearly-stationary points. As a bonus, we also simplify existing analyses for IHT and extend them to the stochastic case (Theorem 2). Another interpretation of our results is in showing that, under our assumptions, error feedback <ref type="bibr" target="#b39">[40]</ref> is not necessary for recovering good sparse minimizers; this has practical implications, as it allows us to perform fully-sparse back-propagation in sparse optimization phases. Next, we discuss our practical implementation, and its connection to these theoretical results.  <ref type="bibr" target="#b47">[48]</ref> or Adam <ref type="bibr" target="#b34">[35]</ref>, and it preserves all standard training hyper-parameters. It will only periodically modify the support for optimization. Please see Algorithm 1 for pseudocode.</p><p>We partition the set of training epochs into compressed epochs C, and decompressed epochs D. We begin with a dense warm-up period of ? w consecutive epochs, during which regular dense (decompressed) training is performed. We then start alternating compressed optimization phases of length ? c epochs each, with decompressed (regular) optimization phases of length ? d epochs each. The process completes on a compressed fine-tuning phase, returning an accurate sparse model. Alternatively, if our goal is to return a dense model matching the baseline accuracy, we take the best dense checkpoint obtained during alternation, and fine-tune it over the entire support. In practice, we noticed that allowing a longer final decompressed phase of length ? D &gt; ? d improves the performance of the dense model, by allowing it to better recover the baseline accuracy after fine-tuning. Please see <ref type="figure">Figure 1</ref> for an illustration of the schedule.</p><p>In our experiments, we focus on the case where the compression operation is unstructured or semistructured pruning. In this case, at the beginning of each sparse optimization phase, we apply the top-k operator across all of the network weights to obtain a mask M over the weights ?. The top-k operator is applied globally across all of the network weights, and will represent the sparse support over which optimization will be performed for the rest of the current sparse phase. At the end of the sparse phase, the mask M is reset to all-1s, so that the subsequent dense phase will optimize over the full dense support. Furthermore, once all weights are re-introduced, it is beneficial to reset to 0 the gradient momentum term of the optimizer; this is particularly useful for the weights that were previously pruned, which would otherwise have stale versions of gradients.</p><p>Discussion. Moving from IHT to a robust implementation in the context of DNNs required some adjustments. First, each decompressed phase can be directly mapped to a deterministic/stochastic IHT step, where, instead of a single gradient step in between consecutive truncations of the support, we perform several stochastic steps. These additional steps improve the accuracy of the method in practice, and we can bound their influence in theory as well, although they do not necessarily provide better bounds. This leaves open the interpretation of the compressed phases: for this, notice that the core of the proof for Theorem 1 is in showing that a single IHT step significantly decreases the expected value of the objective; using a similar argument, we can prove that additional optimization steps over the sparse support can only improve convergence. Additionally, we show convergence for a variant of IHT closely following AC/DC (please see Corollary 1 in the Supplementary Material), but the bounds do not improve over Theorem 1. However, this additional result confirms that the good experimental results obtained with AC/DC are theoretically motivated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Validation</head><p>Goals and Setup. We tested AC/DC on image classification tasks (CIFAR-100 <ref type="bibr" target="#b35">[36]</ref> and Ima-geNet <ref type="bibr" target="#b48">[49]</ref>) and on language modelling tasks <ref type="bibr" target="#b41">[42]</ref> using the Transformer-XL model <ref type="bibr" target="#b9">[10]</ref>. The goal is to examine the validation accuracy of the resulting sparse and dense models, versus the induced sparsity, as well as the number of FLOPs used for training and inference, relative to other sparse training methods. Additionally, we compare to state-of-the-art post-training pruning methods <ref type="bibr" target="#b51">[52]</ref>. We also examine prediction differences between the sparse and dense models. We use PyTorch <ref type="bibr" target="#b46">[47]</ref> for our implementation, Weights &amp; Biases <ref type="bibr" target="#b4">[5]</ref> for experimental tracking, and NVIDIA GPUs for training. All reported image classification experiments were performed in triplicate by varying the random seed; we report mean and standard deviation. Due to computational limitations, the language modelling experiments were conducted in a single run.</p><p>ImageNet Experiments. On the ImageNet dataset <ref type="bibr" target="#b48">[49]</ref>, we test AC/DC on ResNet50 <ref type="bibr" target="#b27">[28]</ref> and MobileNetV1 <ref type="bibr" target="#b29">[30]</ref>. In all reported results, the models were trained for a fixed number of 100 epochs, using SGD with momentum. We use a cosine learning rate scheduler and training hyper-parameters following <ref type="bibr" target="#b36">[37]</ref>, but without label smoothing. The models were trained and evaluated using mixed precision (FP16). On a small subset of experiments, we noticed differences in accuracy of up to 0.2-0.3% between AC/DC trained with full or mixed precision. However, the differences in evaluating the models with FP32 or FP16 are negligible (less than 0.05%). Our dense ResNet50 baseline has 76.84% validation accuracy. Unless otherwise specified, weights are pruned globally, based on their magnitude and in a single step. Similar to previous work, we did not prune biases, nor the Batch Normalization parameters. The sparsity level is computed with respect to all the parameters, except the biases and Batch Normalization parameters and this is consistent with previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>For all results, the AC/DC training schedule starts with a "warm-up" phase of dense training for 10 epochs, after which we alternate between compression and de-compression every 5 epochs, until the last dense and sparse phase. It is beneficial to allow these last two "fine-tuning" phases to run longer: the last decompression phase runs for 10 epochs, whereas the final 15 epochs are the compression fine-tuning phase. We reset SGD momentum at the beginning of every decompression phase. In total, we have an equal number of epochs of dense and sparse training; see <ref type="figure">Figure (</ref>2a) for an illustration. We use exactly the same setup for both ResNet50 and MobileNetV1 models, which resulted in high-quality sparse models. To recover a dense model with baseline accuracy using AC/DC, we finetune the best dense checkpoint obtained during training; practically, this replaces the last sparse fine-tuning phase with a phase where the dense model is fine-tuned instead.    whereas the results in the tables do not restrict the sparsity pattern. For fairness, we executed AC/DC using the same layer-wise sparsity distribution as Top-KAST, for both uniform and global magnitude pruning. For 90% global pruning, results for AC/DC improved; the best sparse model reached 75.64% validation accuracy (0.6% increase over <ref type="table" target="#tab_1">Table 1</ref>), while the best dense model had 76.85% after fine-tuning. For uniform sparsity, our results were very similar: 75.04% validation accuracy for the sparse model and 76.43% -for the fine-tuned dense model. We also note that Top-KAST has better results at 98% when increasing the number of training epochs 2 times, and considerably fewer training FLOPs (e.g. 15% of the dense FLOPs). For fairness, we compared against all methods on a fixed number of 100 training epochs and we additionally trained AC/DC at high sparsity without pruning the first and last layers. Our results improved to 74.16% accuracy for 95% sparsity, and 71.27% for 98% sparsity, both surpassing Top-KAST with prolonged training. We provide a more detailed comparison in the Supplementary Material, which also contains results on CIFAR-100.</p><p>An advantage of AC/DC is that it provides both sparse and dense models at cost below that of a single dense training run. For medium sparsity, the accuracy of the dense-finetuned model is very close to the dense baseline. Concretely, at 90% sparsity, with 58% of the total (theoretical) baseline training FLOPs, we obtain a sparse model which is close to state of the art; in addition, by fine-tuning the best dense model, we obtain a dense model with 76.56% (average) validation accuracy. The whole process takes at most 73% of the baseline training FLOPs. In general, for 80% and 90% target sparsity, the dense models derived from AC/DC are able to recover the baseline accuracy, after finetuning, defined by replacing the final compression phase with regular dense training. The complete results are presented in the Supplementary Material, in <ref type="table" target="#tab_8">Table 6</ref>.</p><p>The sparsity distribution over layers does not change dramatically during training; yet, the dynamic of the masks has an important impact on the performance of AC/DC. Specifically, we observed that masks update over time, although the change between consecutive sparse masks decreases. Furthermore, a small percentage of the weights remain fixed at 0 even during dense training, which is explained by filters that are pruned away during the compressed phases. Please see the Supplementary Material for additional results and analysis.</p><p>We additionally compare AC/DC with Top-KAST and RigL, in terms of the validation accuracy achieved depending on the number of training FLOPs. We report results at uniform sparsity, which ensures that the inference FLOPs will be the same for all methods considered. For AC/DC and Top-KAST, the first and last layers are kept dense, whereas for RigL, only the first layer is kept dense; however, this has a negligible impact on the number of FLOPs. Additionally, we experiment with extending the number of training iterations for AC/DC at 90% and 95% sparsity two times, similarly to Top-KAST and RigL which also provide experiments for extended training. The comparison between AC/DC, Top-KAST and RigL presented in <ref type="figure" target="#fig_3">Figure 3</ref> shows that AC/DC is similar or surpasses Top-KAST 2x at 90% and 95% sparsity, and RigL 5x at 95% sparsity both in terms of training FLOPs and validation accuracy. Moreover, we highlight that extending the number of training iterations two times results in AC/DC models with uniform sparsity that surpass all existing methods at both 90% and 95% sparsity; namely, we obtain 76.1% and 74.3% validation accuracy with 90% and 95% uniform sparsity, respectively.</p><p>Compared to purely sparse training methods, such as Top-KAST or RigL, AC/DC requires dense training phases. The length of the dense phases can be decreased, with a small impact on the accuracy of the sparse model. Specifically, we use dense phases of two instead of five epochs in length, and we MobileNet Results. We perform the same experiment, using exactly the same setup, on the Mo-bileNetV1 architecture <ref type="bibr" target="#b29">[30]</ref>, which is compact and thus harder to compress. On a training budget of 100 epochs, our method finds sparse models with higher Top-1 validation accuracy than existing sparse-and post-training methods, on both 75% and 90% sparsity levels <ref type="table" target="#tab_4">(Table 3)</ref>. Importantly, AC/DC uses exactly the same hyper-parameters used for training the dense baseline <ref type="bibr" target="#b36">[37]</ref>. Similar to ResNet50, at 75% sparsity, the dense-finetuned model recovers the baseline performance, while for 90% it is less than 1% below the baseline. The only method which obtains higher accuracy for the same sparsity is the version of RigL <ref type="bibr" target="#b15">[16]</ref> which executes for 5x more training epochs than the dense baseline. However, this version also uses more computation than the dense model. We limit ourselves to a fixed number of 100 epochs, the same used to train the dense baseline, which would allow for savings in training time. Moreover, RigL does not prune the first layer and the depth-wise convolutions, whereas for the results reported we do not impose any sparsity restrictions. Overall, we found that keeping these layers dense improved our results on 90% sparsity by almost 0.5%. Then, our results are quite close to RigL 2? , with half the training epochs, and less training FLOPs. We provide a more detailed comparison in the Supplementary Material.  Semi-structured Sparsity. We also experiment with the recent 2:4 sparsity pattern (2 weights out of each block of 4 are zero) proposed by NVIDIA, which ensures inference speedups on the Ampere architecture. Recently, <ref type="bibr" target="#b42">[43]</ref> showed that accuracy can be preserved under this pattern, by re-doing the entire training flow. Also, <ref type="bibr" target="#b60">[61]</ref> proposed more general N:M structures, together with a method for training such sparse models from scratch. We applied AC/DC to the 2:4 pattern, performing training from scratch and obtained sparse models with 76.64% ? 0.05 validation accuracy, i.e. slightly below the baseline. Furthermore, the dense-finetuned model fully recovers the baseline performance (76.85% accuracy). We additionally experiment with using AC/DC with global pruning at 50%; in this case we obtain sparse models that slightly improve the baseline accuracy to 77.05%. This confirms our intuition that AC/DC can act as a regularizer, similarly to <ref type="bibr" target="#b24">[25]</ref>.</p><p>Language Modeling. Next, we apply AC/DC to compressing NLP models. We use Transformer-XL <ref type="bibr" target="#b9">[10]</ref>, on the WikiText-103 dataset <ref type="bibr" target="#b41">[42]</ref>, with the standard model configuration with 18 layers and 285M parameters, trained using the Lamb optimizer <ref type="bibr" target="#b56">[57]</ref> and standard hyper-parameters, which we describe in the Supplementary Material. The same Transformer-XL model trained on WikiText-103 was used in Top-KAST <ref type="bibr" target="#b31">[32]</ref>, which allows a direct comparison. Similar to Top-KAST, we did not prune the embedding layers, as this greatly affects the quality, without reducing computational cost. (For completeness, we do provide results when embeddings are pruned to 50% sparsity.) Our sparse training configuration consists in starting with a dense warm-up phase of 5 epochs, followed by alternating between compression and decompression phases every 3 epochs; we follow with a longer decompression phase between epochs 33-39, and end with a compression phase between epochs 40-48. The results are shown in <ref type="table" target="#tab_5">Table 4</ref>. Relative to Top-KAST, our approach provides significantly improved test perplexity at 90% sparsity, as well as better results at 80% sparsity with sparse back-propagation. The results confirm that AC/DC is scalable and extensible. We note that our hyper-parameter tuning for this experiment was minimal.</p><p>Output Analysis. Finally, we probe the accuracy difference between the sparse and dense-finetuned models. We first examineed sample-level agreement between sparse and dense-finetuned pairs produced by AC/DC, relative to model pairs produced by gradual magnitude pruning (GMP). Cotrained model pairs consistently agree on more samples relative to GMP: for example, on the 80%-pruned ResNet50 model, the AC/DC model pair agrees on the Top-1 classification of 90% of validation samples, whereas the GMP models agree on 86% of the samples. The differences are better seen in terms of validation error (10% versus 14%), which indicate that the dense baseline and GMP model disagree on 40% more samples compared to the AC/DC models. A similar trend holds for the cross-entropy between model outputs. This is a potentially useful side-effect of the method; for example, in constrained environments where sparse models are needed, it is important to estimate their similarity to the dense ones.</p><p>Second, we analyze differences in "memorization" capacity <ref type="bibr" target="#b59">[60]</ref> between dense and sparse models. For this, we apply AC/DC to ResNet20 trained on a variant of CIFAR-10 where a subset of 1000 samples have randomly corrupted class labels, and examine the accuracy on these samples during training. We consider 90% and 95% sparsity AC/DC runs. <ref type="figure" target="#fig_1">Figure 2b</ref> shows the results, when the accuracy for each sample is measured with respect to the true, un-corrupted label. During early training and during sparse phases, the network tends to classify corrupted samples to their true class, "ignoring" label corruption. However, as training progresses, due to dense training phases and lower learning rate, networks tend to "memorize" these samples, assigning them to their corrupted class. This phenomenon is even more prevalent at 95% sparsity, where the network is less capable of memorization. We discuss this finding in more detail in the Supplementary Material.</p><p>Practical Speedups. One remaining question regards the potential of sparsity to provide real-world speedups. While this is an active research area, e.g. <ref type="bibr" target="#b14">[15]</ref>, we partially address this concern in the Supplementary Material, by showing inference speedups for our models on a CPU inference platform supporting unstructured sparsity <ref type="bibr" target="#b11">[12]</ref>: for example, our 90% sparse ResNet50 model provides 1.75x speedup for real-time inference (batch-size 1) on a resource-constrained processor with 4 cores, and 2.75x speedup on 16 cores at batch size 64, versus the dense model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion, Limitations, and Future Work</head><p>We introduced AC/DC-a method for co-training sparse and dense models, with theoretical guarantees. Experimental results show that AC/DC improves upon the accuracy of previous sparse training methods, and obtains state-of-the-art results at high sparsities. Importantly, we recover near-baseline performance for dense models and do not require extensive hyper-parameter tuning. We also show that AC/DC has potential for real-world speed-ups in inference and training, with the appropriate software and hardware support. The method has the advantage of returning both an accurate standard model, and a compressed one. Our model output analysis confirms the intuition that sparse training phases act as a regularizer, preventing the (dense) model from memorizing corrupted samples. At the same time, they prevent the memorization of hard samples, which can affect accuracy.</p><p>The main limitations of AC/DC are its reliance on dense training phases, which limits the achievable training speedup, and the need for tuning the length and frequency of sparse/dense phases. We believe the latter issue can be addressed with more experimentation (we show some preliminary results in Section 4 and Appendix B.1); however, both the theoretical results and the output analysis suggest that dense phases may be necessary for good accuracy. We plan to further investigate this in future work, together with applying AC/DC to other compression methods, such as quantization, as well as leveraging sparse training on hardware that could efficiently support it, such as Graphcore IPUs <ref type="bibr" target="#b22">[23]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Convergence Proofs</head><p>In this section we provide the convergence analysis for our algorithms. We prove Theorem 1 and show as a corollary that under reasonable assumptions our implementation of AC/DC converges to a sparse minimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Overview</head><p>We use the notation and assumptions defined in Section 3.1. As all of our analyses revolve around bounding the progress made in a single iteration, to simplify notation we will generally use ? to denote the current iterate, and ? to denote the iterate obtained after the IHT update:</p><formula xml:id="formula_9">? = T k (? ? ?g ? ) .</formula><p>Additionally, we let S, S , S * ? [N ] to denote the support of ?, ? and ? * respectively, where ? * is the promised k * -sparse minimizer. Given an arbitrary vector x, we use supp(x) to denote its support, i.e. the set of coordinates where x is nonzero. We may also refer to the minimizing value of f as f * = f (? * ).</p><p>Before providing more intuition for the analysis, we give the main theorem statements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Stochastic IHT for Functions with Concentrated PL Condition</head><p>We restate the Theorem from Section 3.1.</p><p>Theorem 1. Let f : R N ? R be a function with a k * -sparse minimizer ? * . Let ? &gt; ? &gt; 0 be parameters, let k = C ? k * ? (?/?) 2 for some appropriately chosen constant C, and suppose that f is (2k + 3k * , ?)-smooth and (k * , ?)-CPL. For initial parameters ?0 and precision &gt; 0, given access to stochastic gradients with variance</p><formula xml:id="formula_10">?, stochastic IHT (3) converges in O ? ? ? ln f (? 0 )?f (? * ) iterations to a point ? with ? 0 ? k, such that E [f (?) ? f (? * )] ? + 16? 2 ? .</formula><p>Additionally, we give a corollary that justifies our implementation of AC/DC. As opposed to the theoretical stochastic IHT algorithm, AC/DC performs a sequence of several dense SGD steps before applying a single pruning step. We show that even with this change we can provide theoretical convergence bounds, although these bounds can be weaker than the baseline IHT method under our assumptions.</p><formula xml:id="formula_11">Corollary 1 (Convergence of AC/DC). Let f : R N ? R be a function that decomposes as f (?) = 1 m m i=1 fi(?)</formula><p>, and has a k * -sparse minimizer ? * . Let ? &gt; ? &gt; 0 be parameters, let k = C ? k * ? (?/?) 2 for some appropriately chosen constant C, suppose that each fi is (N, ?)-smooth, and L-Lipschitz, and that f is (k * , ?)-CPL. </p><formula xml:id="formula_12">? = 1 |S i | j?S i ?fj(?).</formula><p>Suppose we replace the IHT iteration with a dense/sparse phase consisting of 1) ?c phases during each of which we perform a full pass over the data by performing the iteration ? = ? ? ?g</p><formula xml:id="formula_13">(i) ? for all i ? [B]</formula><p>, with an appropriate step size ?; 2) a pruning step implemented via an application the truncation operator T k ;</p><p>3) an optional sparse training phase which fully optimizes f over the sparse support.</p><p>For initial parameters ?0 and precision &gt; 0, this algorithm converges</p><formula xml:id="formula_14">in O ? ? ? ln f (? 0 )?f (? * ) dense/sparse phases to a point ? with ? 0 ? k, such that f (?) ? f (? * ) ? + O L 2 ? .</formula><p>To provide more intuition for these results, let us understand how various parameters affect convergence. As we will argue in more detail in Section A.1.4, the main idea behind these algorithms is based on the vanilla IHT algorithm, which consists of alternating full gradient steps with pruning/truncation steps. Pruning to the largest k coordinates in absolute value essentially represents projecting the current weights onto the non-convex set of k-sparse vectors, so a natural approach is to simply try to adapt the analysis of projected gradient methods to this setup. The major caveat of this idea is that projecting onto a non-convex set can potentially undo some of the progress made so far, unlike the standard case of projections over convex sets. However, we can argue that if we set the target sparsity k sufficiently large compared to the promised sparsity k * of the optimum, the pruning step can only increase the function value by a fraction of how much it was decreased by the full gradient step.</p><p>Notably, in order to guarantee this property, we require the target sparsity k to be of order ?(k * ? (?/?) 2 ), where ?/? represents the "restricted condition number" of f . Hence "well conditioned" functions allow for better sparsity. The number of iterations is same as in vanilla smooth and strongly convex optimization, which only depends on the function f , and not on the target sparsity.</p><p>The next step is specializing this approach to the non-convex case, under the restricted smoothness and CPL conditions, when only stochastic gradients are available. To handle non-convexity, we show that in fact a weaker property than strong convexity is required, and we can achieve similar results only under these restricted assumptions. More importantly, we can also handle stochastic gradients, which occur naturally in deep learning. In fact, the stochastic variance ends up contributing an extra additive error of O(? 2 /?) to our final error bound (see Theorem 1). This can be reduced by decreasing variance, i.e. taking larger batches. Additionally, this term carries a dependence in the CPL parameter ? -the larger ?, the smaller the error. We leave as an open problem whether this dependence on variance can be eliminated for stochastic IHT methods.</p><p>Building on this, in Corollary 1 we provide theoretical guarantees for the algorithm we implemented. Notably, while stochastic IHT takes a single stochastic gradient step, followed by pruning, in practice we replace this with a dense training phase, followed by optimization in the sparse support. We show that this does not significantly change the state of affairs. The dense training phase can be thought of as a replacement for a single stochastic gradient step. The difficulty in AC/DC comes from the fact that the total movement of the weights during the dense phase does not constitute an unbiased estimator for the true gradient, as it was previously the case. However, by damping down the learning rate we can see that under reasonable assumptions this total movement does not differ too much from the step we would have made using a single full gradient. The sparse training phase can only help, since our entire analysis is based on proving that the function value decreases in each step. Hence as long as sparse training reduces the function value, it can only improve convergence.</p><p>We provide the main proofs for these statements in Section A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Stochastic IHT for Functions with Restricted Smoothness and Strong Convexity</head><p>We also provide a streamlined analysis for stochastic IHT under standard assumptions. Compared to <ref type="bibr" target="#b30">[31]</ref> which achieves similar guarantees (in particular both suffer from a blow-up in sparsity that is quadratic in the restricted condition number of f ) we significantly simplify the analysis and provide guarantees when only stochastic gradients are available. Notably, the quadratic dependence in condition number can be improved to linear, at the expense of a significantly more sophisticated algorithm which requires a longer running time <ref type="bibr" target="#b2">[3]</ref>. Theorem 2. Let f : R N ? R be a function with a k * -sparse minimizer ? * . Let ? &gt; ? &gt; 0 be parameters, let k = C ? k * ? (?/?) 2 for some appropriately chosen constant C, and suppose that f is (2k + k * , ?)-smooth and (k + k * , ?)-strongly convex in the sense that</p><formula xml:id="formula_15">f (? + ?) ? f (?) + ?f (?) ? + ? 2 ? 2 , for all ?, ? s. t. ? 0 ? k + k * .</formula><p>For initial parameters ?0 and precision &gt; 0, given access to stochastic gradients with variance ?, IHT (3) converges in O ? ? ? ln</p><formula xml:id="formula_16">? 0 ?? * /?+? 2 /(??) iterations to a point ? with ? 0 ? k, such that E ? ? ? * 2 ? ? + 2? 2 ?? and E [f (?) ? f (? * )] ? + 2? 2 ? .</formula><p>We prove this statement in Section A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Finding a Sparse Nearly-Stationary Point</head><p>We can further relax the assumptions, and show that IHT can recover sparse iterates that are nearly-stationary (i.e. have small gradient norm). Finding nearly-stationary points is a standard objective in non-convex optimization. However, enforcing the sparsity guarantee is not. Here we show that IHT can further provide stationarity guarantees even with minimal assumptions.</p><p>Intuitively, this results suggests that error feedback may not be necessary for converging to stationary points under parameter sparsity.</p><p>In this case, we alternate IHT steps with optimization steps over the sparse support, which reduce the norm of the gradient resteicted to the support to some target error . Theorem 3. Let f : R N ? R. Let ?, &gt; 0 be parameters, let k be the target sparsity, and suppose that f is (2k, ?)-smooth. Furthermore, suppose that after each IHT step with a step size ? = ? ?1 and target sparsity k, followed by optimizing over the support to gradient norm , all the obtained iterates satisfy ? ? ? R? .</p><p>For initial parameters ?0 and precision &gt; 0, given access to stochastic gradients with variance ?, in</p><formula xml:id="formula_17">O ? (f (?0) ? f (?T )) ? min 1 2 , 1 ? 2 kR 2 ? , 1 ? 2 ? 2</formula><p>iterations we can obtain an k-sparse iterate ? such that</p><formula xml:id="formula_18">?f (?) ? = O ?R? ? k + ?? + .</formula><p>This theorem provides provable guarantees even in absence of properties that bound distance to the optimum via function value (such as restricted strong convexity or the CPL condition. Instead, it uses the reasonable assumption that the ? norm of all the sparse iterates witnessed during the optimization process is bounded by a parameter R?. In fact, in practice we often notice that weights stay in a small range, so this assumption is well motivated.</p><p>Since it can not possibly offer guarantees on global optimality, this theorem instead provides a sparse nearstationary point, in the sense that the ? norm of the gradient at the returned point is small. In fact this depends on several parameters, including the target sparsity k, smoothness ? and the promise on R?. It is worth mentioning that the ? norm of the gradient depends on the norm of the output sparsity, which is independent on the sparsity of any promise, unlike in the previously analyzed cases.</p><p>Several other works attempted to compute sparse near-stationary points <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b44">45]</ref>. As opposed to these we do not require feedback, nor do we sparsify the gradients. Instead, we simply prune the weights after updating them with a dense gradient, following which we optimize in the sparse support until we reach a point whose gradient has only small coordinates within that support.</p><p>Since all iterates are in a small ? box, we can show that the progress guaranteed by the dense gradient step alone is "mildly" affected by pruning. Hence unless we are already at a near-stationary point in, which case the algorithm can terminate, we decrease the value of f significantly, while only suffering a bit of penalty which is bounded by the parameters of the instance.</p><p>We provide full proofs in Section A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4 Proof Approach</head><p>Let us briefly explain the intuition behind our theoretical analyses. We can view IHT as a version of projected gradient descent, where iterates are projected onto the non-convex domain of k-sparse vectors.</p><p>In general, when the domain is convex, projections do not hurt convergence. This is because under specific assumptions, gradient descent makes progress by provably decreasing the 2 distance between the iterate and the optimal solution. As projecting the iterate back onto the convex domain can only improve the distance to the optimum, convergence is unaffected even in those constrained settings.</p><p>In our setting, as the set of k-sparse vectors is non-convex, the distance to the optimal solution can increase. However, we can show a trade-off between how much this distance increases and the ratio between the target and optimal sparsity k/k * .</p><p>Intuition. Intuitively, consider a point ? obtained by taking a gradient step</p><formula xml:id="formula_19">? = ? ? ??f (?) .<label>(6)</label></formula><p>While this step provably decreases the distance to the optimum ? ? ? * ? ? ? * , after applying the projection by moving to the projection T k ( ?), the distance T k ( ?) ? ? * may increase again. The key is that this increase can be controlled. For example, the new distance can be bounded via triangle inequality by</p><formula xml:id="formula_20">T k ( ?) ? ? * ? ? ? T k ( ?) + ? ? ? * ? 2 ? ? ? * .</formula><p>The last inequality follows from the fact that by definition T k ( ?) is the closest k-sparse vector to ? in 2 norm, and thus the additional distance payed to move to this projected point is bounded by ? ? ? * . Thus, if the gradient step (6) made sufficient progress, for example ? ? ? * ? 1 3 ? ? ? * we can conclude that the additional truncating step does not fully undo progress, as</p><formula xml:id="formula_21">T k ( ?) ? ? * ? 2 ? ? ? * ? 2 3 ? ? ? * ,</formula><p>so the iterate still converges to the optimum.</p><p>In reality, we can not always guarantee that a single gradient step reduces the distance by a large constant fraction -as a matter of fact this is determined by how well the function f is conditioned. However we can reduce the lost progress that is caused by the truncation step simply by increasing the number of nonzeros of the target solution, i.e. increasing the ratio k/k * . This is captured by the following crucial lemma which also appears in <ref type="bibr" target="#b30">[31]</ref>. A short proof can be found in Section A.5.</p><p>Lemma 1. Let ? * be a k * -sparse vector, and let ? be an n-sparse vector. Then</p><formula xml:id="formula_22">T k (?) ? ? 2 n ? k ? ? * ? ? 2 n ? k * .</formula><p>Its usefulness is made obvious in the proof of Theorem 2 which explicitly tracks as a measure of progress the distance between the current iterate and the sparse optimum. This is shown in detail in Section A.3.</p><p>The proofs of Theorems 1 and 3 are slightly more complicated, as they track progress in terms of function value rather than distance to the sparse optimum. However, similar arguments based on Lemma 1 still go through. An added benefit is that these analyses show that alternating IHT steps with gradient steps over the sparse support can only help convergence, as these additional steps further decrease error in function value. This fact is important to theoretically justify the performance of the AC/DC algorithm. In the following sections we provide proofs for our theorems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Stochastic IHT for Non-Convex Functions with Concentrated PL Condition</head><p>In this section we prove Theorem 1 by showing that the ideas developed before apply to non-convex settings. We analyze IHT for a class of functions that satisfy a special version of the Polyak-?ojasiewicz (PL) condition <ref type="bibr" target="#b33">[34]</ref> which is standard in non-convex optimization, and certain versions of it were essential in several works analyzing the convergence of training methods for deep neural networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b1">2]</ref>. Usually this condition says that small gradient norm i.e. approximate stationarity implies closeness to optimum in function value. Here we use the stronger (r, ?)-CPL condition (see <ref type="bibr">Equation 5</ref>), which considers the norm of the gradient contributed by its largest coordinates in absolute value.</p><p>We prove strong convergence bounds for functions that satisfy the CPL condition. Compared to the classical Polyak-?ojasiewicz condition, this adds the additional assumption that most of the mass of the gradient is concentrated on a small subset of coordinates. This phenomenon has been witnessed in several instances, and is implicitly used in <ref type="bibr" target="#b39">[40]</ref>.</p><p>Before proceeding with the proof we again provide a few useful lemmas.</p><p>Lemma 2. If f is ( , ?)-smooth, then for any -sparse ?, one has that</p><formula xml:id="formula_23">f (? + ?) ? f (?) + ? 2 1 ? ?f (?) + ? supp(?) 2 ? 1 2? ?f (?) supp(?) 2 .</formula><p>Proof. Applying smoothness we bound</p><formula xml:id="formula_24">f (? + ?) ? f (?) + ?f (?) , ? + ? 2 ? 2 = f (?) + 1 2? ?f (?) 2 + ?f (?) , ? + ? 2 ? 2 ? 1 2? ?f (?) 2 = f (?) + 1 2 1 ? ? ?f (?) + ?? 2 ? 1 2? ?f (?) 2 = f (?) + ? 2 1 ? ?f (?) + ? 2 ? 1 2? ?f (?) 2 .</formula><p>Next we notice that the contributions of the two terms ? 2 1 ? ?f (?) + ? 2 and 1 2? ?f (?) 2 exactly match on the coordinates not touched by ?. Hence everything outside the support of ? cancels out, which yields the desired conclusion.</p><p>We require another lemma which will be very useful in the analysis. Then</p><formula xml:id="formula_25">(? + g) S\S 2 ? g S?S 2 ? (? + g) Z\S 2 ? gS * 2 ,</formula><p>for some set Z such that |Z \ S | ? 2 |S * |.</p><p>Proof. We prove this as follows. We write</p><formula xml:id="formula_26">(? + g) S\S 2 ? g S?S 2 = (? + g) (S * ?S)\S 2 ? (? + g) S * \(S?S ) 2 ? g S?S 2 = (? + g) (S * ?S)\S 2 ? g S * \(S?S ) 2 ? g S?S 2 = (? + g) (S * ?S)\S 2 ? g S * ?S?S 2 = (? + g) (S * ?S)\S 2 ? g S \(S * ?S) 2 ? gS * ?S 2 . Since g S \(S * ?S) 2 = (? + g) S \(S * ?S) 2 ? (? + g) R 2 ,</formula><p>where R is a subset R ? (S * ? S) \ S with |R| = |S \ (S * ? S)|. Such a set definitely exists as</p><formula xml:id="formula_27">(S * ? S) \ S ? S \ S = S \ S ? S \ (S * ? S) = |R| .</formula><p>Hence we obtain that</p><formula xml:id="formula_28">(? + g) S\S 2 ? g S?S 2 ? (? + g) ((S * ?S)\S )\R 2 ? gS * ?S 2 .</formula><p>Note that</p><formula xml:id="formula_29">(S * ? S) \ S \ R = (S * ? S) \ S ? |R| = (S * ? S) \ S ? S \ (S * ? S) ? |S * | + S \ S ? S \ S ? |S * | = 2 |S * | .</formula><p>This concludes the proof.</p><p>Using this we derive the following useful corollary.</p><p>Corollary 2. Let ?, g ? R N such that supp (?) = S, and let S , S * be arbitrary subsets, with |S | = |S| &gt; |S * |. Furthermore suppose that T k (? + g) = (? + g) S .</p><p>Then one has that</p><formula xml:id="formula_30">(? + g) S\S 2 ? g S?S 2 ? 2 |S * | + |supp (? * )| |S | ? |supp (? * )| ? (? + g) T ? ? * 2 ? gS * 2 ,</formula><p>for some T , such that |T | ? 2 |S * | + |supp (? * )| + |S | and supp (? * ) ? T .</p><p>Proof. Using Lemma 3 we can write</p><formula xml:id="formula_31">(? + g) S\S 2 ? g S?S 2 ? (? + g) Z\S 2 ? gS * 2 ? (? + g) (Z?supp(? * ))\S 2 ? gS * 2 = (? + g) Z?supp(? * )?S ? (? + g) S 2 ? gS * 2 .</formula><p>where |Z ? supp (? * ) ? S | ? 2 |S * | + |supp (? * )| + |S |. Applying Lemma 1 we furthermore obtain that</p><formula xml:id="formula_32">(? + g) Z?S * ?S ? (? + g) S 2 ? |Z ? supp (? * ) ? S | ? |S | |Z ? supp (? * ) ? S | ? |supp (? * )| ? (? + g) Z?supp(? * )?S ? ? * 2 ? 2 |S * | + |supp (? * )| |S | ? |supp (? * )| ? (? + g) Z?supp(? * )?S ? ? * 2 .</formula><p>We can now proceed with the main proof.</p><p>Proof of Theorem 1. For simplicity we first provide the proof for the deterministic version, which roughly follows the ideas described in <ref type="bibr" target="#b30">[31]</ref>. Afterwards, we extend it to the stochastic setting. To simplify notation, throughout this proof we will use ? = 1 ? . Using Lemma 2 we can write that for the update</p><formula xml:id="formula_33">? = T k (? ? ??f (?)) ? ? , we have f ? ? f (?) + 1 2? 1 ? ?f (?) + T k (? ? ??f (?)) ? ? supp(?) 2 ? 1 2? ??f (?) supp(?) 2 = f (?) + 1 2? (T k (? ? ??f (?)) ? (? ? ??f (?))) supp(?) 2 ? 1 2? ??f (?) supp(?) 2 .</formula><p>At this point we use the fact that by definition supp (?) = S ? S. Furthermore we see that</p><formula xml:id="formula_34">(T k (? ? ??f (?)) ? (? ? ??f (?))) S ?S 2 = (? ? ??f (?)) S\S 2 since T k (? ? ??f (?)</formula><p>) exactly matches ? ? ??f (?) for the coordinates in S , and is 0 for all the others. Thus we have that</p><formula xml:id="formula_35">f ? ? f (?) + 1 2? (? ? ??f (?)) S\S 2 ? ??f (?) S?S 2 .</formula><p>In order to apply the CPL property, we need to relate this to the contribution to the gradient norm given by the heavy signal T k * (?f (?)) 2 . Let S * be the support of T k * (?f (?)). We apply Corollary 2 to further bound</p><formula xml:id="formula_36">f ? ? f (?) + 1 2? 3k * k ? k * (? ? ??f (?)) T ? ? * 2 ? ??f (?) S * 2 ,</formula><p>where |T | ? 3k * + k. Now we apply the CPL property as follows. From Lemma 6 we upper bound</p><formula xml:id="formula_37">(? ? ??f (?)) T ? ? * 2 ? (? ? ??f (?)) T ?S ? ? * 2 ? 8 ? f ? ? ??f (?) T ?S ? f (? * ) ? 8 ? (f (?) ? f (? * )) ,</formula><p>In the first inequality we used the fact that supp (? * ) ? T . In the second one we applied Lemma 6. In the third one we applied (3k * + 2k, ?)-smoothness (since |T ? S| ? 3k * + 2k) together with the fact that ? = 1/?, and so f ? ? ??f (?) T ?S ? f (?).</p><p>Similarly we apply the CPL inequality to conclude that</p><formula xml:id="formula_38">f ? ? f (?) + 1 2? 3k * k ? k * ? 8 ? (f (?) ? f (? * )) ? ? 2 ? ? 2 (f (?) ? f (? * )) .</formula><p>Thus equivalently:</p><formula xml:id="formula_39">f ? ? f (? * ) ? (f (?) ? f (? * )) 1 + 12k * k ? k * ? 1 ?? ? ?? 4 = (f (?) ? f (? * )) 1 + 12k * k ? k * ? ? ? 1 4? ,</formula><p>where ? = ?/? = 1/(??). Since k ? k * ? 96? 2 + 1 we equivalently have that k * k?k * ? 1 96k 2 and thus</p><formula xml:id="formula_40">12k * k?k * ? ? ? 1 8? . Hence f ? ? f (? * ) ? (f (?) ? f (? * )) ? 1 ? 1 8? . This shows that in O ? ln f (? 0 )?f (? * ) we reach a point ? such that f (?) ? f (? * ) ? , which concludes the proof.</formula><p>We extend this proof to the stochastic version in Section A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now let us show how Corollary 1 follows from the same analysis.</head><p>Proof of Corollary 1. The proof extends from that to Theorem 1. The difference we need to handle is the error introduced by performing ?c passes through the data instead of a single stochastic gradient step. Suppose that before starting the dense training phase, the current iterate is ?. The key is to bound the error introduced by performing these ?c passes instead of simply changing the iterate by ???f (?), prior to applying the pruning step. To do so we first upper bound the change in the iterate after ?c passes, which lead to a new iterate ?. Since we perform ?c ? B iterations instead of a single one, we damp down our step size by setting ? = ?/(?cB), and measure the movement we made compared to the one we would have made with a single deterministic gradient step.</p><p>Using the Lipschitz property of the functions in the decomposition, we see that each step changes our iterate by at most ? L in 2 norm. Hence over ?c passes through the data, each of which involves B mini-batches, the total change in the iterate is at most:</p><formula xml:id="formula_41">? ? ? ? ?c ? B ? ? L = ?L .</formula><p>Using the smoothness property of each fi this guarantees that for all the seen iterates ?, the gradients of fi's never deviate significantly from their values at the original point ?:</p><formula xml:id="formula_42">?fi( ?) ? ?fi(?) ? ??L ,</formula><p>Thus if we interpret the scaled total movement in iterate 1 ? (? ? ?) as a gradient mapping, it satisfies</p><formula xml:id="formula_43">1 ? (? ? ?) ? ?f (?) ? ??L .</formula><p>Applying the analysis for the stochastic version of Theorem 1 we can treat the error in the gradient mapping exactly as the stochastic noise. For the specific choice of step size used there ? = 1/?(?), we thus get that</p><formula xml:id="formula_44">1 ? (? ? ?) ? ?f (?) 2 = O(L 2 ) ,</formula><p>which enables us to conclude the analysis. Note that the steps performed during the sparse training phases do not affect convergence as they can only improve error in function value, which is the main quantity that our analysis tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Stochastic IHT for Functions with Restricted Smoothness and Strong Convexity</head><p>Here we prove Theorem 2. Before proceeding with the proof, we provide a few useful statements related to the fact that f is well conditioned along sparse directions. Lemma 4. If f is (2k + k * , ?)-smooth and (k + k * , ?)-strongly convex then</p><formula xml:id="formula_45">f (? * ) ? f (?) + ?f (?) S?S ?S * , ? * ? ? + ? 2 ? ? ? * 2 , f ? ? 1 ? ?f (?) S?S ?S * ? f (?) ? 1 2? ?f (?) S?S ?S * 2 .</formula><p>Proof. The former follows directly from the definition. For the latter we have</p><formula xml:id="formula_46">f ? ? 1 ? ?f (?) S?S ?S * ? f (?) ? 1 ? ?f (?), ?f (?) S?S ?S * + ? 2 ?f (?) S?S ?S * 2 = f (?) ? 1 2? ?f (?) S?S ?S * 2 .</formula><p>Finally we can prove the main statement:</p><p>Proof. We will track progress by measuring the distance ? ? ? * 2 . To do so we write</p><formula xml:id="formula_47">? ? ? * 2 = T k (? ? ?g ? ) ? ? * 2 = (? ? ?g ? ? ? * ) + (T k (? ? ?g ? ) ? (? ? ?g ? )) 2 = (? ? ?g ? ? ? * ) S ?S * + T k (? ? ?g ? ) S ?S * ? (? ? ?g ? ) S ?S * 2 .</formula><p>The last identity follows from the fact that the term inside the norm is only supported at S ? S * . Thus, after applying the triangle inequality, we obtain that:</p><formula xml:id="formula_48">? ? ? * 2 ? (? ? ?g ? ) S ?S * ? ? * + T k (? ? ?g ? ) S ?S * ? (? ? ?g ? ) S ?S * 2 .</formula><p>The second term can now be bounded using Lemma 1. Since the sparsity of the projected point is k + k * we have that</p><formula xml:id="formula_49">T k (? ? ?g ? ) S ?S * ? (? ? ?g ? ) S ?S * 2 ? k + k * ? k k + k * ? k * (? ? ?g ? ) S ?S * ? ? * 2 = k * k ? (? ? ?g ? ) S ?S * ? ? * 2 .</formula><p>Therefore:</p><formula xml:id="formula_50">? ? ? * 2 ? 1 + s * s 2 ? (? ? ?g ? ) S ?S * ? ? * 2 ? 1 + s * s 2 ? (? ? ?g ? ) S?S ?S * ? ? * 2 .</formula><p>The second inequality follows from the fact that as we increase the support of ???g ? to also include S\(S ? S * ), the norm inside can only increase. Finally by expanding, we write:</p><formula xml:id="formula_51">(? ? ?g ? ) S?S ?S * ? ? * 2 = ? ? ? (g ? ) S?S ?S * ? ? * 2 = ? ? ? * 2 ? 2? (g ? ) S?S ?S * , ? ? ? * + ? 2 (g ? ) S?S ?S * 2 ? ? ? ? * 2 ? 2? ?f (?) S?S ?S * , ? ? ? * + 2? 2 ?f (?) S?S ?S * 2 + 2? (?f (?) ? g ? ) S?S ?S * , ? ? ? * + 2? 2 (?f (?) ? g ? ) S?S ?S * 2 ? ,</formula><p>where we use ? to denote the error term introduced by using stochastic gradients. Next we bound the fist part of the term above. To do so we use lemma to write</p><formula xml:id="formula_52">? ? ? * 2 ? 2? ?f (?) S?S ?S * , ? ? ? * + 2? 2 ?f (?) S?S ?S * 2 ? ? ? ? * 2 ? 2? f (?) ? f (? * ) ? ? 2 ? ? ? * 2 + 2? 2 ?f (?) S?S ?S * 2 = ? ? ? * 2 (1 ? ??) ? 2? f (?) ? f (? * ) ? ? ?f (?) S?S ?S * 2 = ? ? ? * 2 (1 ? ??) ? 2? f (?) ? f (? * ) ? 1 2? ?f (?) S?S ?S * 2 ? 2? ? 1 2? ? ? ?f (?) S?S ?S * 2 ? ? ? ? * 2 (1 ? ??) ? 2? ? 1 2? ? ? ?f (?) S?S ?S * 2 .</formula><p>For the first inequality we used the restricted strong convexity property, while for the second we used the restricted smoothness property. Now for the error term we have that:</p><formula xml:id="formula_53">E [?|?] ? 2? 2 ? 2 , which enables us to conclude that E ? ? ? * 2 ? ? 1 + k * k 2 ? ? ? ? * 2 (1 ? ??) ? 2? ? 1 2? ? ? ?f (?) S?S ?S * 2 + 2? 2 ? 2 .</formula><p>Setting ? = 1 2? this gives us that</p><formula xml:id="formula_54">E ? ? ? * 2 ? ? 1 + s * s 2 ? ? ? ? * 2 1 ? 1 2 ? ? ? + 1 2 ? ? 2 .</formula><p>Thus for as long as ? ? ? * 2 ? 2 ? 2 ?? we have that</p><formula xml:id="formula_55">E ? ? ? * 2 ? ? 1 + k * k 2 ? 1 ? 1 4 ? ? ? ? ? ? ? * 2 .</formula><p>We see that the expected squared distance contracts by setting the ratio k/k * sufficiently large. Indeed if k ? 81 (?/?) 2 ? k * we have that:</p><formula xml:id="formula_56">E ? ? ? * 2 ? ? 1 + 1 9 ? ? ? 2 ? 1 ? 1 4 ? ? ? ? ? ? ? * 2 ? 1 ? 1 36 ? ? ? ? ? ? ? * 2 .</formula><p>Taking expectation over the entire history of iterates we can thus conclude that after</p><formula xml:id="formula_57">T = O ? ? ? ln ? 0 ?? * 2 /?+? 2 /?? iterations we obtain E ?T ? ? * 2 ? ? + 2? 2 ?? .</formula><p>Applying the restricted smoothness property, this also gives us that:</p><formula xml:id="formula_58">E [f (?T ) ? f (? * )] ? + 2? 2 ? .</formula><p>which is what we wanted.</p><p>Here we prove Theorem 3. For simplicity, we first prove the deterministic version of the theorem, where ? = 0.</p><p>We show how to extend this proof to the stochastic version in Section A.5.</p><p>Proof. The proof is similar to that of Theorem 1, but in addition requires that the algorithm alternates standard IHT steps with optimizing inside the support of the iterate. More precisely given a current iterate supported at S, we additionally run an inner loop which optimizes only over the coordinate in S, seeking a near stationary point ? such that ?f (?) S ? . Thus in our analysis we can assume that before performing a gradient, followed by a pruning step, our current iterate supported at S satisfies ?f (?) S ? . Hence following the previous analyses and setting ? = 1/?, we have:</p><formula xml:id="formula_59">f ? ? f (?) + 1 2? (? ? ??f (?)) S\S 2 ? ??f (?) S?S 2 ? f (?) + 1 2? 2 ? S\S 2 + 2? 2 2 ? ??f (?) 2 ? .</formula><p>For the second inequality we first applied triangle inequality together with the near stationarity condition for ? to bound</p><formula xml:id="formula_60">(? ? ??f (?)) S\S ? ? S\S + ??f (?) S\S ? ? S\S + ? , then applied (a + b) 2 ? 2a 2 + 2b 2 .</formula><p>In addition we used the fact that</p><formula xml:id="formula_61">?f (?) S?S ? = ?f (?) ? .</formula><p>This follows from a simple case analysis. If one of the coordinates of the gradient with the largest absolute value lies in S ? S , we are done. Otherwise, we have two possibilities. Either S is different from S, so S ? S contains one coordinate outside of S. Since these coordinates are obtained by hard thresholding and ? is supported only at S, the absolute value of ?f (?) at the largest coordinate in S \ S must be at least as large as the largest in S * \ S, which yields our claim. Otherwise we have that S = S , which means that the pruning step did not change the support, and thus</p><formula xml:id="formula_62">??f (?) S ? ? min i?S |? ? ??f (?)| i ? R? + ? which guarantees that ?f (?) ? = max ?f (?) S ? , ?f (?) S ? ? max , + R? ? = + ?R? ,</formula><p>and so we are done.</p><p>In the former case we thus see that</p><formula xml:id="formula_63">? 2 ?f (?) 2 ? ? f (?) ? f ? + ? S\S 2 ? + ? 2 ? f (?) ? f ? + kR 2 ? ? + ? 2 .</formula><p>Telescoping over T iterations we see that</p><formula xml:id="formula_64">? 2 T ?1 t=0 ?f (?t) 2 ? ? f (?0) ? f (?T ) + T ? kR 2 ? ? + ? 2</formula><p>and so returning a random point ? among those witnessed during the algorithm we have</p><formula xml:id="formula_65">E ?f (?) 2 ? ? 2 (f (?0) ? f (?T )) ?T + 2 kR 2 ? ? 2 + 2 By AM-QM, E ?f (?) ? ? E ?f (?) 2 ?</formula><p>, which enables us to conclude that after sufficiently many iterations we are guaranteed to find a point such that</p><formula xml:id="formula_66">?f (?) ? = O R? ? k ? + = O ?R? ? k + .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Deferred Proofs</head><p>Proof of Lemma 1. Our proofs crucially rely on the following lemma. Intuitively it shows that projecting a vector ? onto the non-convex set of sparse vectors does not increase the distance to the optimum by too much. While this is indeed always true for projections onto convex sets, in this case we can provably show that the possible increase in distance is small.</p><p>where the last inequality follows from setting ? = 1 2? , which makes ?? 2 + ? ? ? 1</p><formula xml:id="formula_67">? 2 ? 1 2? = 0.</formula><p>Finally, repeating the argument used for the deterministic proof in Section A.2, we further bound</p><formula xml:id="formula_68">E f ? ? f * |? ? f (?) ? f * + ? 3k * k ? k * ? 8 ? (f (?) ? f (? * )) ? ? 2 ? ? 2 (f (?) ? f (? * )) + 2?? 2 ? 2 = (f (?) ? f * ) 1 + 24k * k ? k * ? ? ? ? ?? 2 ? 2 + 2?? 2 ? 2 = (f (?) ? f * ) 1 + 24k * k ? k * ? ? ? ? ? 8? + ? 2 2? . Setting k = k * ? 384 ? ? 2 + 1 we have 24k * k?k * ? ? ? ? 24 ? 1 384?(?/?) 2 ? ? ? = 1 16 ? ? ? , so E f ? ? f * |? ? (f (?) ? f * ) 1 ? ? 16? + ? 2 2? .</formula><p>Thus for as long as</p><formula xml:id="formula_69">? 2 2? ? (f (?) ? f * ) ? ? 32? ?? f (?) ? f * ? 16 ? ? 2 ? one has that E f ? ? f * |? ? (f (?) ? f * ) 1 ? ? 32? .</formula><p>Taking expectation over the entire history, this shows that after T = O ? ? ln f (? 0 )?f * iterations we obtain an iterate ?T such that Using the PL condition we have</p><formula xml:id="formula_70">?g (?) 2 = 1 4 (f (?) ? f * ) ? ?f (?) 2 ? 1 4 (f (?) ? f * ) ? ? 2 ? (f (?) ? f * ) = ? 8 .</formula><p>Now starting at some ?0, we consider the dynamic? = ??g (?). We see that this always decreases function value until it reaches some ?T for which ?g (?T ) = 0 and hence by the PL inequality, ?T is a minimizer i.e. f (?T ) = f * . Now we can write</p><formula xml:id="formula_71">g (?T ) = g (?0) + T 0 ?g (?t) ,?t dt = g (?0) + T 0 ?g (?t) , ??g (?t) dt = g (?0) ? T 0 ?g (?t) 2 dt . Thus g (?0) ? g (?T ) = T 0 ?g (?t) 2 dt ? ? 8 ? T 0 ?g (?t) dt = ? 8 ? T 0 ? t dt ,</formula><p>where we used our lower bound on the norm of ?g (?). Finally, we use the fact that the last integral lower bounds the total movement of ? as it moves from ?0 to ?T . Thus</p><formula xml:id="formula_72">T 0 ? t dt ? ?0 ? ?T , so g (?0) ? g (?T ) ? ? 8 ?0 ? ?T ,</formula><p>which enables us to conclude that</p><formula xml:id="formula_73">f (?0) ? f * ? ? 8 ?0 ? ?T 2 ,</formula><p>where ?T is some global minimizer of f . This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 CIFAR-100 Experiments</head><p>We tested the IHT pruning algorithm on the CIFAR-100 dataset <ref type="bibr" target="#b35">[36]</ref>. We used the WideResNet-28-10 architecture <ref type="bibr" target="#b58">[59]</ref>, which to our knowledge gives state-of-the-art performance on this dataset. Models were trained for a fixed 200 epochs, using Stochastic Gradient Descent (SGD) with momentum, and a stepwise decreasing learning rate scheduler. For our main experiment, after a warm-up period of ten epochs, we alternated sparse and dense phases of 20 epochs until epoch 170, at which point we allowed the sparse model to train to convergence for 30 more epochs. In addition, we found a small benefit to resetting momentum to 0 at each transition from sparse to dense, and we have done so throughout the trials. These experiments were replicated starting from three different seeds, and we report average results and their standard deviations, and they are shown in table 5 in rows labeled "AC/DC-20".</p><p>We further explored the possibility of using even smaller dense phases to further reduce training FLOPs. These trials, presented in 5 show the results of reducing the sparse phase from 20 to 14 or even 7 epochs while keeping the length of dense phases the same, but increasing the number of total epochs to roughly match overall FLOPs (thus increasing the number of sparse and dense phases); overall we train the AC/DC-14 runs for 225 epochs and AC/DC-7 runs for 240 epochs (vs 200 for AC/DC-20). Our experiments show that it is possible to obtain competitive accuracy results with shorter dense phases, and, at higher sparsities, a further FLOP reductionhowever, reducing the dense phase too much may lead to some accuracy degradation at higher sparsities. These results suggest that a further reduction in FLOPs is possible by adjusting the length of the dense phase and the overall epochs. We emphasize that (i) these gains are only theoretical until hardware is available that can take advantage of sparsity in training and (ii) these results, although promising, are highly preliminary; additionally, each trial was only run once due to timing constraints.</p><p>We compare our results with Gradual Magnitude Pruning <ref type="bibr" target="#b61">[62]</ref>. To our knowledge, we are the first to release CIFAR-100 pruning results for this network architecture, and GMP was chosen as a baseline due to its generally strong performance against a range of other approaches <ref type="bibr" target="#b20">[21]</ref>. We obtain the GMP baseline by training on WideResNet architecture at full density for 50 epochs, then gradually increase the sparsity over the next 100 before allowing the model to converge for the final 50, matching the 200 training epochs of AC/DC. We further validated this baseline by training dense WideResNet-28-10 models for 200 epochs and then gradually pruning over 50 and finetuning over 50 more, for a total of 300 epochs, which gave similar performance at the cost of greater FLOPs and training time.</p><p>The results are shown in <ref type="table" target="#tab_7">Table 5</ref>. We see that AC/DC pruning significantly outperforms Gradual Magnitude Pruning at all sparsity levels tested, and further that AC/DC models pruned at 50% and 75% even outperform the dense baseline, while the models pruned at 90% at least match it.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ResNet50 on ImageNet</head><p>Performance of dense models. The AC/DC method has an advantage over other pruning methods of obtaining both sparse and dense models. The performance of the dense baseline can be recovered after fine-tuning the resulting AC/DC dense model, for a small number of epochs. Namely, we start from the best dense baseline, which is usually obtained after 85 epochs, and replace the final compression phase of 15 epochs with regular dense training; we use the same learning rate scheduler and keep all other training hyper-parameters the same. For 80% sparsity we recover the dense baseline accuracy completely, while for 90% we are slightly below the baseline by 0.3%. We note that for 90% sparsity, when the first and last layers are dense, our fine-tuned dense model recovers the baseline accuracy fully. The results for the dense models, together with the baseline accuracy, are presented in <ref type="table" target="#tab_8">Table 6</ref>, where ( ) denotes that the first and last layers of the network are dense.   Moreover, an interesting property of AC/DC is that the resulting dense networks have a small percentage of zero-valued weights, as shown in <ref type="table" target="#tab_10">Table 8</ref>. This is most likely caused by "dead" neurons or convolutional filters resulted after each compression phase; the corresponding weights do not get re-activated during the dense stages, as they can no longer receive gradients. This can be easily seen particularly for high sparsity (95% and 98%) where a non-trivial percentage of the weights remain inactive.</p><p>Dynamics of masks and FLOPs during training. The mask dynamics, measured by the relative change between two consecutive compression masks, have an important influence on the AC/DC training process. Namely, more changes between consecutive compression masks typically imply more exploration of the weights' space, and faster recovery from sub-optimal pruning decision, which in turn results in more accurate sparse models. As can be seen in <ref type="figure">Figure 5b</ref>, the relative mask difference between consecutive compression phases decreases during training, but it is critical to be maintained at a non-trivial level. For completeness, we also included the evolution of the validation accuracy during AC/DC training, for all sparsity levels (please see <ref type="figure">Figure 5a</ref>); at 98% sparsity in particular, it is easiest to see that dense phases enable the exploration of better pruning masks, which ensure that the sparse model improves continuously during training.</p><p>Despite the dynamics of the compression masks, we noticed that the sparsity distribution does not change significantly. This can be observed from the number of inference FLOPs per sample, at the end of each compression phase, in <ref type="figure">Figure 6a</ref>. Interestingly, as training progresses, AC/DC also induces structured sparsity, as more neurons and convolutional filters get pruned. This was previously discussed in more detail (see <ref type="table" target="#tab_10">Table 8</ref>), but can also be deduced from the decreasing inference FLOPs at the end of each dense phase, as shown in <ref type="figure">Figure 6b</ref>.  AC/DC with uniform pruning. As discussed, for example, in <ref type="bibr" target="#b51">[52]</ref>, global magnitude pruning usually performs better than its uniform counterpart. Interestingly, with global magnitude pruning later layers (which also tend to be the largest) are pruned the most. Moreover, we did not encounter convergence issues caused by entire layers being pruned, as hypothesized in some previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref>. However, one concern related to global magnitude pruning is a potential FLOP inefficiency of the resulting models; in theory, this would be a consequence of the earlier layers being pruned the least. For this reason, we performed additional experiments with AC/DC at uniform sparsity, with the first and last layers dense (as commonly used in the literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref>). Our results show that there are no significant differences compared to AC/DC with global magnitude pruning. However, keeping the first and last layers dense significantly improves the results with global magnitude pruning. These observations emphasize that AC/DC is an easy-to-use method which works reliably well with different pruning criteria. For complete results, please see <ref type="table" target="#tab_11">Table 9</ref>.</p><p>Direct comparison with Top-KAST. As previously highlighted, Top-KAST is the closest to us, in terms of validation accuracy, out of existing sparse training methods. However, for the results reported, the authors kept the first convolutional and final fully-connected layers dense. To obtain a fair comparison, we used AC/DC on the same sparse distribution, and for 90% sparsity over the pruned layers (82.57% overall network sparsity), our results improved significantly. Namely, the best sparse model reached 75.64% validation accuracy (0.6% increase from the results in <ref type="table" target="#tab_1">Table 1</ref>), while the accuracy of the best dense model was 76.85% after fine-tuning. For more details, we also provide in <ref type="table" target="#tab_1">Table 10</ref> the results for Top-KAST when all layers are pruned, as they were provided to us by the authors. Notice that AC/DC surpasses even Top-KAST with dense back-propagation.</p><p>It is important to note, however, that because of its flexibility in choosing the gradients density, Top-KAST can theoretically obtain significantly better training speed-ups than AC/DC, the latter being constrained by its dense training phases. This allows Top-KAST to improve the accuracy of the models by increasing the number of training epochs, while still enabling (theoretical) training speed-up. We present in <ref type="table" target="#tab_1">Table 11</ref> another comparison between AC/DC and Top-KAST, when the training time for the latter is increased 2 or 5 times; for all results (which were provided to us by the authors), the first and last layers for Top-KAST are dense. When comparing with AC/DC with all layers pruned, Top-KAST obtains better results at 98% and 95% sparsity, with increased training epochs. However, when using the same sparse distribution as Top-KAST (not pruning the first and last layers), the results for AC/DC at 95% and 98% sparsity are significantly better than Top-KAST with increased steps. For all the results reported on AC/DC the number of training steps was fixed at 100 epochs.</p><p>We note that the results obtained with AC/DC can be improved as well with increased number of training epochs.</p><p>As an example, when using the same sparsity schedule extended over 150 epochs, the best sparse model obtained with AC/DC on 90% sparsity reached 75.99% accuracy, using fewer training FLOPs compared to the original dense baseline trained on 100 epochs (namely 87%). Furthermore, when we fine-tune the dense model by replacing the final 15 epochs compression phase with dense training, we obtain a dense model with 76.95% accuracy, higher than the original dense baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 MobileNet on ImageNet</head><p>Performance of dense models. Similar to ResNet50, we observed that dense models obtained with AC/DC are able to recover the baseline accuracy after additional fine-tuning. We performed fine-tuning identically to the ResNet50 experiments and observe that AC/DC models obtained with a 75% target sparsity recovered the baseline accuracy, while for 90% the gap is just below 1%. We present results for the (fine-tuned) dense models in <ref type="table" target="#tab_9">Table 7</ref>, where ( ) indicates that the first layer and depth-wise convolutions were never pruned.</p><p>Masks dynamics. Similar to ResNet50, the change between consecutive AC/DC compression masks plays an important role in obtaining accurate sparse models on MobileNet. As shown in <ref type="figure" target="#fig_6">Figure 7b</ref>, the compression masks stabilize as training progresses. For completeness, we also illustrate the evolution of the validation accuracy during AC/DC training on MobileNet, at 75% and 90% sparsity, in <ref type="figure" target="#fig_6">Figure 7a</ref>.    Comparison with RigL. We note that the results obtained by RigL <ref type="bibr" target="#b15">[16]</ref> improve significantly when increasing the number of training steps 2 or 5 times. Moreover, for all results reported with RigL on MobileNet the first convolutional layer and all depth-wise convolutions are dense, whereas we do not impose such restrictions on our sparse model. Our results can further be improved by using the same sparsity distribution; namely, for 90% sparsity over the pruned parameters (88.57% overall sparsity), the best sparse model obtained with AC/DC achieved 66.56% accuracy (0.5% improvement), while the best dense improved from 67.64% to 70.97% after fine-tuning. In <ref type="table" target="#tab_1">Table 12</ref> we present results for AC/DC and RigL at 75% and 90% sparsity, when the latter is trained over the same number of epochs, or with 2x or 5x the number of passes through the training data. We conclude that AC/DC has very similar validation accuracy to RigL2?. For 75% sparsity, AC/DC achieves similar performance with significantly fewer training and inference FLOPs than RigL. At 90% sparsity, AC/DC and RigL2? are close in terms of both validation accuracy and training FLOPs; however, the validation accuracy of AC/DC can be improved by almost 0.5% when the first and depth-wise convolutional layers are kept dense. We note that RigL5? has significantly higher validation accuracy, and for 75% sparsity it even matches the baseline; however, this variant of RigL also uses 2.6? and 1.5? the dense baseline training FLOPs for 75% and 90% sparsities, respectively, which makes it impractical due to its high computational training cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Inference Speedups</head><p>We now examine the potential for real-world speedup of models produced through our framework. For this, we use the CPU-based inference framework of <ref type="bibr" target="#b11">[12]</ref>, which supports efficient inference over unstructured sparse models, and is free to use for non-commercial purposes. Specifically, we export our Pytorch-trained models to the ONNX intermediate format, preserving weight sparsity, and then execute inference on a subset of samples, at various batch sizes, measuring time per batch. We execute on an Intel i9-7980XE CPU with 16 cores and 2.60GHz core frequency. We simulate two scenarios: the first is real-time inference, i.e. samples are processed one at a time, in a resource-constrained environment, using only 4 cores. The second is batch inference, for which we pick batch size 64, in a cloud environment, for which we use all 16 cores. We measure average time per batch for the sparse models against dense baselines, for which we use both the Deepsparse engine, and the ONNX runtime (ONNXRT). We present the average over 10 runs. The variance is extremely low, so we omit it for readability. We now briefly discuss the results. First, notice that the dense baselines offer similar performance for real-time inference, but that the Deepsparse engine has a slight edge at batch 64. We will therefore compare against its timings below. The results show a speedup of 1.6x for the 90% global-pruned ResNet50 model, and 1.8x for the uniformly pruned one: the uniformly-pruned model is slightly faster, which correlates with its lower FLOP count due to the uniform pruning pattern. This pattern is preserved in MobileNetV1 experiments, although the speedups are relatively lower, since the architecture is more compact. We note that the speedups are more significant for batched inference, where the engine has more potential for parallelization, and our setup uses more cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Sparse-Dense Output Comparison</head><p>To the best of our knowledge, our results are the first ones to show that both a dense and a sparse model can be trained jointly. Although other sparse training techniques such as RigL or Top-KAST can train sparse models faster, none of them offer the additional benefit of an accurate dense model.</p><p>One method that does generate sparse-dense model couples is Sparse Variational Dropout (SparseVD) <ref type="bibr" target="#b45">[46]</ref>; there, after training a dense model with variational inference, a large proportion of the weights can be pruned in a single step, without affecting the accuracy of the dense model. (We note however that Sparse Variational Dropout doubles the FLOP cost of training, due to the variational parameters.) However, our investigation of the SparseVD models trained by <ref type="bibr" target="#b20">[21]</ref> shows that the sparse and dense models agree in over 98% of their predictions as measured on the ImageNet validation set, and are of no better quality than the sparse model -if anything, they are slightly worse. Please see <ref type="table" target="#tab_1">Table 14</ref> for complete results.</p><p>In comparison, AC/DC with finetuning produces dense models of validation accuracy that is comparable to that of a dense model trained without any compression, and that therefore do differ from their sparse co-trained counterparts. To understand the relative sizes of these differences, we used GMP pruning as the baseline. In particular, we compared the similarity of a fully trained dense model with GMP trained over 100 epochs. We note AC/DC and GMP show comparable accuracy for both their sparse and dense models in this scenario; however, the total training epochs are substantially lower for producing these models with AC/DC.</p><p>We use two metrics to investigate the difference in sparse-dense model pairs: the proportion of validation examples on which the top prediction agreed between the two models, and the average cross-entropy of the predictions across all validation examples. In both metrics, <ref type="table" target="#tab_1">Table 14</ref> shows that model similarity is higher for 80% sparsity than 90%, and higher for AC/DC than GMP training: in particular, sparse/dense cross-entropy is about 20% lower for AC/DC, and the number of top-prediction disagreements is about 25% lower for AC/DC at 90% sparsity, and 37% lower for AC/DC at 80% sparsity. <ref type="bibr" target="#b9">10</ref>   <ref type="figure">Figure 8</ref>: Accuracy during training with AC/DC at 90% and 95% target sparsity, for 1000 randomly labelled CIFAR10 images. No data augmentation was applied to the training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Memorization Experiments on CIFAR10</head><p>In what follows, we study the similarities between the sparse and dense models learned with AC/DC, on the particular setup of memorizing random labels. Specifically, we select 1000 i.i.d. training samples from the CIFAR10 dataset and randomly change their labels. We train a ResNet20 model using AC/DC, at various target sparsity levels, ranging from 50% to 95%. We use SGD with momentum, weight decay, and initial learning rate 0.1 which is decayed by a factor of 10 every 60 epochs, starting with epoch 65.</p><p>Using data augmentation dramatically affects the memorization of randomly-labelled training samples, and thus we differentiate between the two possible cases. Namely, the regular baseline can easily memorize (in the sense of reaching perfect accuracy) the randomly-labelled samples, when no data augmentation is used; in comparison, with data augmentation memorization is more difficult, and the accuracy on randomly-labelled samples for the baseline is just above 60%. In addition to the accuracy on the perturbed samples with respect to their new random labels, we also track the accuracy with respect to the "true" or correct labels. This differentiation offers a better understanding regarding where memorization fails and a glimpse into the robustness properties of neural networks in general, and of AC/DC, in particular.</p><p>No data augmentation. As previously mentioned, in this case the baseline model can perfectly memorize the perturbed data, with respect to their random labels. Interestingly, prior to the initial learning rate decay, most (? 70%) perturbed samples are still correctly classified with respect to their "true" labels, and memorization happens very quickly after the learning rate is decreased. In the case of AC/DC with low target sparsity (50% and 75%), memorization has a very similar behavior to the dense baseline. However, for higher sparsity levels (90% and 95%) we can see a clear difference between the sparse and dense models. Namely, during each compression phase most perturbed samples are correctly classified with respect to their true labels, whereas in decompression phases their random labels are memorized. This phenomenon is illustrated in <ref type="figure">Figure 8</ref>.</p><p>Data augmentation. In this case, memorization of the perturbed samples is more difficult, and it happens later on during training, usually after the second learning rate decrease for the baseline model. Interestingly, in the case of AC/DC we can see <ref type="figure">(Figure 9</ref>) a clear inverse relationship between the amount of memorization and the target sparsity. Although low sparsity enables more memorization, most perturbed samples are still correctly classified with respect to their true labels. For higher sparsity levels (90% and 95%), most perturbed samples are correctly classified with respect to their true labels (almost 90%) and very few are memorized. Furthermore, the dense model resulted from AC/DC training is more robust than the original baseline, as it still learns the correct labels of the perturbed samples, despite being presented with random ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Computational Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Hardware Details</head><p>Experiments were run on NVIDIA RTX 2080 GPUs for image classification tasks, and NVIDIA RTX 3090 GPUs for language modelling. Each ImageNet run took approximately 2 days for ResNet50 and one day for MobileNet, while each Transformer-XL experiment took approximately 2 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 FLOPs Computation</head><p>When computing FLOPs, we take into account the number of zero-valued weights for linear and convolutional layers. To compute the FLOPs required for a backward pass over a sample, we use the same convention as RigL <ref type="bibr" target="#b15">[16]</ref>; namely, if F denotes the inference FLOPs per sample, the number of backward FLOPs is estimated as B = 2 ? F , as we need F FLOPs to backpropagate the error, and additional F to compute the gradients w.r.t. the  <ref type="figure">Figure 9</ref>: Accuracy during training with AC/DC at 50%, 75%, 90% and 95% target sparsity, for 1000 randomly labelled CIFAR10 images. Here, all samples were trained using data augmentation.</p><p>weights. For ImageNet experiments, we ignore the FLOPs required for Batch Normalization, pooling, ReLU or Cross Entropy, similarly to other methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b36">37]</ref>; however, these layers have a negligible impact on the total FLOPs number (at most 0.01? the dense number).</p><p>For compression and decompression phases C and D, we consider FC and FD the compression and decompression inference FLOPs per sample, respectively. We use F to denote the inference FLOPs per sample for the baseline network. During each compression phase, the training FLOPs per sample can be estimated as 3 ? FC . For decompression phases, we noticed that a small fraction of weights remain zero, and therefore FD &lt; F . When doing a backward pass we have additional FD from back-propagating the error, and F extra FLOPs for the gradients with respect to all parameters. Therefore, we estimate the training FLOPs per sample during a decompression phase as 2 ? FD + F . We measure the number of FLOPs on a random input sample, at the end of each training epoch and use this value to estimate the total training FLOPs for that particular epoch. To obtain the final number of FLOPs, we compute the inference FLOPs on a random input sample, estimate the backward FLOPs, compute the estimated training FLOPs over all training epochs as described above, and scale by the number of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Choice of Hyper-parameters</head><p>Length of compression/decompression phases. AC/DC alternates between compression and decompression phases to co-train sparse and dense models. It is important to note, however, that the length of these phases, together with the warm-up and fine-tuning phases, could have a significant impact on the quality of the resulting models. Before settling on the sparsity pattern we used for all our ImageNet experiments (see <ref type="figure">Figure 5a</ref> and <ref type="figure" target="#fig_6">Figure 7a</ref>), we experimented with different lengths for the sparse/dense phases, but found that ultimately the pattern used in the paper had the best trade-off between training FLOPs and validation accuracy.</p><p>Notably, we experimented on ResNet50, 90% sparsity, with increasing the training epochs to 130 and with different lengths for the compression/decompression phases. For example, we found that alternating between sparse/dense training every 10 epochs yielded slightly better results after 130 epochs: 75.34% for the sparse model, 76.87% for the fine-tuned dense model; however, this also had higher training FLOPs requirements (0.7? for the sparse model and 0.9? including the fine-tuned dense). We additionally experimented with longer dense phases (10 epochs), compared to sparse phases (5 epochs); this also resulted in more accurate models: 75.45% accuracy for the sparse model and 76.78% -for the fine-tuned dense model. However, the training FLOPs were substantially higher: 0.85? for the sparse model and 1.15? for the fine-tuned dense.</p><p>Due to computational limitations, and to ensure a fair comparison with the dense baseline and other pruning methods, we decided on using a fixed number of 100 training epochs (the same used for the dense baseline).</p><p>In this setup, we experimented mainly with the lengths for the compression/decompression phases used in <ref type="figure">Figure 5a</ref> and <ref type="figure" target="#fig_6">Figure 7a</ref>, but noticed that having a longer final decompression phase had a positive impact on the fine-tuned dense model. For instance, when following a sparsity schedule as in <ref type="figure">Figure 1</ref>, the sparse model at 90% sparsity had a very similar performance to the reported results (75.18% accuracy, from one seed), while the fine-tuned dense model was significantly below the dense baseline (76.05% validation accuracy). We believe having a short warm-up period and a longer fine-tuning phase are both beneficial for the sparse model; in our experiments, we only used warm-up phases of 10 epochs, but believe that shorter phases are worth exploring as well. Furthermore, the mask difference between consecutive compression phases is an important guide for choosing the sparsity schedule: as it was previously discussed, having a non-trivial difference between the masks typically results in better sparse models. Illustrations of the pruning masks during training on ImageNet are presented in <ref type="figure">Figure 5b</ref> and <ref type="figure" target="#fig_6">Figure 7b</ref>.</p><p>When choosing the sparsity schedule for the language models experiments on Transformers-XL, we followed the same principles as for ImageNet. In fact, the sparsity schedule is very similar to the one used for ImageNet, scaled by the number of training epochs ( 48 epochs or 100,000 steps for Transformers).</p><p>In the case of CIFAR100, we used for AC/DC the same number of 200 training epochs as for the dense baseline. We experimented with sparse/dense phases of lengths 10 or 20, and found that generally switching every 20 epochs between sparse and dense training yielded the best results.</p><p>Training Hyper-parameters for ImageNet. We used the same hyper-parameters for all our ImageNet experiments, on both ResNet50 and MobileNetV1. Namely, we trained using SGD with momentum and batch size 256. We used a cosine learning rate scheduler, after an initial warm-up phase of 5 epochs, when the learning rate was linearly increased to 0.256. The momentum value was 0.875 and weight decay was 0.00003051757813. These hyper-parameters have the standard values used in the implementation of STR <ref type="bibr" target="#b36">[37]</ref>. Furthermore, to improve efficiency, we train and evaluate the models using mixed precision (FP16). For models trained with mixed precision, the difference in accuracy between evaluating them with FP32 versus FP16 is negligible (&lt;0.05%). However, we noticed larger differences (around 0.2-0.3%) in accuracy when training AC/DC with FP16 versus FP32.</p><p>Training Hyper-parameters for Transformer-XL. For our Transformer-XL experiments, we integrated into our code-base the implementation provided by NVIDIA 2 , which also follows closely the original implementation in <ref type="bibr" target="#b9">[10]</ref>. We used the same hyper-parameters for training the large Transformer-XL model with 18 layers on WikiText-103, including the Lamb optimizer <ref type="bibr" target="#b56">[57]</ref> with cosine learning rate scheduler.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Percentage of samples with corrupted training labels classified to their true class (ResNet20/CIFAR10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Accuracy vs. sparsity during training, for the ResNet50/ImageNet experiment (left) and accuracy on the corrupted samples for ResNet20/CIFAR10, w.r.t. the true class (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Let</head><label></label><figDesc>?c and B be integers, and let {S1, . . . , SB} be a partition of [m] into B subsets of cardinality O(m/B) each. Given ?, let g (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 3 .</head><label>3</label><figDesc>Let ?, g ? R N such that supp (?) = S, and let S , S * be some arbitrary subsets, with |S | = |S| &gt; |S * |. Furthermore suppose that T k (? + g) = (? + g) S .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Validation accuracy and sparsity during training, together with differences in consecutive masks for WideResNet on CIFAR-100 using AC/DC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Validation accuracy and sparsity during training, together with differences in consecutive masks for ResNet50 on ImageNet using AC/DC. (a) Test FLOPs after each sparse phase (b) Test FLOPs after each dense phase Dynamics of sparse and dense inference FLOPs for ImageNet on ResNet50, as a percentage of the dense baseline FLOPs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Validation accuracy and sparsity during training, together with differences in consecutive masks for ImageNet with MobileNetV1 using AC/DC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Accuracy on the mis-labelled data (w.r.t. the true labels)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Alternating Compressed/Decompressed (AC/DC) TrainingRequire: Weights ? ? R N , data S, sparsity k, compression phases C, decompression phases D 1: Train the weights ? for ?w epochs Warm-up phase 2: while epoch ? max epochs do</figDesc><table><row><cell>3:</cell><cell>if entered a compression phase then</cell><cell></cell></row><row><cell>4: 5:</cell><cell>? ? T k (?, k) m ? 1[?i = 0]</cell><cell>apply compression (top-k) operator on weights create masks</cell></row><row><cell>6:</cell><cell>end if</cell><cell></cell></row><row><cell>7: 8:</cell><cell>if entered a decompression phase then m ? 1N</cell><cell>reset all masks</cell></row><row><cell>9:</cell><cell>end if</cell><cell></cell></row><row><cell>10:</cell><cell>? ? ? m</cell><cell>apply the masks (ensure sparsity for compression phases)</cell></row><row><cell cols="2">11:? ? {?i|mi = 0, 1 ? i ? N }</cell><cell>get the support for the gradients</cell></row><row><cell>12:</cell><cell>for x mini-batch in S do</cell><cell></cell></row><row><cell>13:</cell><cell>? ? ? ? ???f (?; x)</cell><cell>optimize the active weights</cell></row><row><cell>14:</cell><cell>end for</cell><cell></cell></row><row><cell>15:</cell><cell>epoch ? epoch +1</cell><cell></cell></row><row><cell cols="2">16: end while</cell><cell></cell></row><row><cell cols="2">17: return ?</cell><cell></cell></row><row><cell cols="3">3.2 AC/DC: Applying IHT to Deep Neural Networks</cell></row></table><note>AC/DC starts from a standard DNN training flow, using standard optimizers such as SGD with momentum</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>ResNet50/ImageNet, medium sparsity results.</figDesc><table><row><cell>Method</cell><cell>Sparsity (%)</cell><cell>Top-1 Acc. (%)</cell><cell>GFLOPs Inference</cell><cell>EFLOPs Train</cell></row><row><cell>Dense</cell><cell>0</cell><cell>76.84</cell><cell>8.2</cell><cell>3.14</cell></row><row><cell>AC/DC</cell><cell>80</cell><cell>76.3 ? 0.1</cell><cell>0.29?</cell><cell>0.65?</cell></row><row><cell>RigL1?</cell><cell>80</cell><cell>74.6 ? 0.06</cell><cell>0.23?</cell><cell>0.23?</cell></row><row><cell>RigL1?(ERK)</cell><cell>80</cell><cell>75.1 ? 0.05</cell><cell>0.42?</cell><cell>0.42?</cell></row><row><cell>Top-KAST</cell><cell>80 fwd, 50 bwd</cell><cell>75.03</cell><cell>0.23?</cell><cell>0.32?</cell></row><row><cell>STR</cell><cell>79.55</cell><cell>76.19</cell><cell>0.19?</cell><cell>-</cell></row><row><cell>WoodFisher</cell><cell>80</cell><cell>76.76</cell><cell>0.25?</cell><cell>-</cell></row><row><cell>AC/DC</cell><cell>90</cell><cell>75.03 ? 0.1</cell><cell>0.18?</cell><cell>0.58?</cell></row><row><cell>RigL1?</cell><cell>90</cell><cell>72.0 ? 0.05</cell><cell>0.13?</cell><cell>0.13?</cell></row><row><cell>RigL1? (ERK)</cell><cell>90</cell><cell>73.0 ? 0.04</cell><cell>0.24?</cell><cell>0.25?</cell></row><row><cell>Top-KAST</cell><cell>90 fwd, 80 bwd</cell><cell>74.76</cell><cell>0.13?</cell><cell>0.16?</cell></row><row><cell>STR</cell><cell>90.23</cell><cell>74.31</cell><cell>0.08?</cell><cell>-</cell></row><row><cell>WoodFisher</cell><cell>90</cell><cell>75.21</cell><cell>0.15?</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>ResNet50/ImageNet, high sparsity results.</figDesc><table><row><cell>Method</cell><cell>Sparsity (%)</cell><cell>Top-1 Acc. (%)</cell><cell>GFLOPs Inference</cell><cell>EFLOPs Train</cell></row><row><cell>Dense</cell><cell>0</cell><cell>76.84</cell><cell>8.2</cell><cell>3.14</cell></row><row><cell>AC/DC</cell><cell>95</cell><cell>73.14 ? 0.2</cell><cell>0.11?</cell><cell>0.53?</cell></row><row><cell>RigL1?</cell><cell>95</cell><cell>67.5 ? 0.1</cell><cell>0.08?</cell><cell>0.08?</cell></row><row><cell>RigL1? (ERK)</cell><cell>95</cell><cell>69.7 ? 0.17</cell><cell>0.12?</cell><cell>0.13?</cell></row><row><cell>Top-KAST</cell><cell>95 fwd, 50 bwd</cell><cell>71.96</cell><cell>0.08?</cell><cell>0.22?</cell></row><row><cell>STR</cell><cell>94.8</cell><cell>70.97</cell><cell>0.04?</cell><cell>-</cell></row><row><cell>WoodFisher</cell><cell>95</cell><cell>72.12</cell><cell>0.09?</cell><cell>-</cell></row><row><cell>AC/DC</cell><cell>98</cell><cell>68.44 ? 0.09</cell><cell>0.06?</cell><cell>0.46?</cell></row><row><cell>Top-KAST</cell><cell>98 fwd, 90 bwd</cell><cell>67.06</cell><cell>0.05?</cell><cell>0.08?</cell></row><row><cell>STR</cell><cell>97.78</cell><cell>62.84</cell><cell>0.02?</cell><cell>-</cell></row><row><cell>WoodFisher</cell><cell>98</cell><cell>65.55</cell><cell>0.05?</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Training FLOPs vs validation accuracy for AC/DC, RigL and Top-KAST, with uniform sparsity, at 90% and 95% sparsity levels. (ResNet50/ImageNet). no longer extend the final decompressed phase prior to the finetuning phase. For 90% global sparsity, this resulted in 74.6% validation accuracy for the sparse model, using 44% of the baseline FLOPs.</figDesc><table><row><cell>Validation Accuracy (%)</cell><cell>72 73 74 75 76</cell><cell cols="2">Train FLOPs (fraction of dense) ResNet50 90% Uniform Sparsity</cell><cell>Validation Accuracy (%)</cell><cell>Train FLOPs (fraction of dense) 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 68 70 72 74 ResNet50 95% Uniform Sparsity</cell></row><row><cell></cell><cell></cell><cell>Top-KAST 1x</cell><cell>Top-KAST 2x</cell><cell cols="2">Top-KAST 5x</cell><cell>AC/DC 1x and 2x</cell><cell>RigL 1x and 5x</cell></row><row><cell>Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Similarly, for uniform sparsity, we obtain 74.7% accuracy on the 90% sparse model, with 40% of the baseline FLOPs; this value can be further improved to 75.8% validation accuracy when extending two times the number of training iterations. Furthermore, at 95% uniform sparsity, we reach 72.8% accuracy with 35% of the baseline training FLOPs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>MobileNetV1/ImageNet sparsity results</figDesc><table><row><cell>Method</cell><cell>Sparsity (%)</cell><cell>Top-1 Acc. (%)</cell><cell>GFLOPs Inference</cell><cell>EFLOPs Train</cell></row><row><cell>Dense</cell><cell>0</cell><cell>71.78</cell><cell>1.1</cell><cell>0.44</cell></row><row><cell>AC/DC</cell><cell>75</cell><cell>70.3 ? 0.07</cell><cell>0.34?</cell><cell>0.64?</cell></row><row><cell>RigL1? (ERK)</cell><cell>75</cell><cell>68.39</cell><cell>0.52?</cell><cell>0.53?</cell></row><row><cell>STR</cell><cell>75.28</cell><cell>68.35</cell><cell>0.18?</cell><cell>-</cell></row><row><cell>WoodFisher</cell><cell>75.28</cell><cell>70.09</cell><cell>0.28?</cell><cell>-</cell></row><row><cell>AC/DC</cell><cell>90</cell><cell>66.08 ? 0.09</cell><cell>0.18?</cell><cell>0.56?</cell></row><row><cell>RigL1? (ERK)</cell><cell>90</cell><cell>63.58</cell><cell>0.27?</cell><cell>0.29?</cell></row><row><cell>STR</cell><cell>89.01</cell><cell>62.1</cell><cell>0.07?</cell><cell>-</cell></row><row><cell>WoodFisher</cell><cell>89</cell><cell>63.87</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Transformer-XL/WikiText sparsity results</figDesc><table><row><cell>Method</cell><cell>Sparsity (%)</cell><cell>Perplexity Sparse</cell><cell>Perplexity Dense</cell><cell>Perplexity Finetuned Dense</cell></row><row><cell>Dense</cell><cell>0</cell><cell>-</cell><cell>18.95</cell><cell>-</cell></row><row><cell>AC/DC</cell><cell>80</cell><cell>20.65</cell><cell>20.24</cell><cell>19.54</cell></row><row><cell>AC/DC</cell><cell>80, 50 embed.</cell><cell>20.83</cell><cell>20.25</cell><cell>19.68</cell></row><row><cell>Top-KAST</cell><cell>80, 0 bwd</cell><cell>19.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Top-KAST</cell><cell>80, 60 bwd</cell><cell>21.3</cell><cell>-</cell><cell>-</cell></row><row><cell>AC/DC</cell><cell>90</cell><cell>22.32</cell><cell>21.0</cell><cell>20.28</cell></row><row><cell>AC/DC</cell><cell>90, 50 embed.</cell><cell>22.84</cell><cell>21.34</cell><cell>20.41</cell></row><row><cell>Top-KAST</cell><cell>90, 80 bwd</cell><cell>25.1</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Background and Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 AC/DC: Applying IHT to Deep Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . 6 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.1.1 Stochastic IHT for Functions with Concentrated PL Condition . . . . . . . . . . . . . 15 A.1.2 Stochastic IHT for Functions with Restricted Smoothness and Strong Convexity . . . 16 A.1.3 Finding a Sparse Nearly-Stationary Point . . . . . . . . . . . . . . . . . . . . . . . . 16 A.1.4 Proof Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.2 Stochastic IHT for Non-Convex Functions with Concentrated PL Condition . . . . . . . . . . 18 A.3 Stochastic IHT for Functions with Restricted Smoothness and Strong Convexity . . . . . . . . 21 A.4 Finding a Sparse Nearly-Stationary Point . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A.5 Deferred Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 CIFAR-100 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B.2 ResNet50 on ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.3 MobileNet on ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 B.4 Inference Speedups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 B.5 Sparse-Dense Output Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 B.6 Memorization Experiments on CIFAR10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 Hardware Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 C.2 FLOPs Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 C.3 Choice of Hyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35</figDesc><table><row><cell>Contents</cell><cell></cell></row><row><cell>1 Introduction</cell><cell>1</cell></row><row><cell>2 Related Work</cell><cell>3</cell></row><row><cell>3 Alternating Compressed / DeCompressed (AC/DC) Training</cell><cell>4</cell></row><row><cell>3.1 4 Experimental Validation</cell><cell>7</cell></row><row><cell>5 Conclusion, Limitations, and Future Work</cell><cell>10</cell></row><row><cell>Appendices</cell><cell>14</cell></row><row><cell>A Convergence Proofs</cell><cell>14</cell></row><row><cell>A.1 B Additional Experiments</cell><cell>27</cell></row><row><cell>B.1 C Computational Details</cell><cell>34</cell></row><row><cell>C.1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>CIFAR-100 Sparsity results on WideResNet</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell>Top-1 Acc. (%)</cell><cell cols="3">Sparsity</cell><cell>GFLOPs Inference</cell><cell></cell><cell>EFLOPs Train</cell></row><row><cell></cell><cell></cell><cell>Dense</cell><cell>79.0 ? 0.25</cell><cell></cell><cell cols="2">0%</cell><cell>11.9</cell><cell></cell><cell>0.36</cell></row><row><cell></cell><cell></cell><cell cols="2">AC/DC-20 79.6 ? 0.17</cell><cell></cell><cell cols="2">49.98%</cell><cell>0.50?</cell><cell></cell><cell>0.72?</cell></row><row><cell></cell><cell></cell><cell>AC/DC-14</cell><cell>79.99</cell><cell></cell><cell cols="2">49.98%</cell><cell>0.50?</cell><cell></cell><cell>0.75?</cell></row><row><cell></cell><cell></cell><cell>AC/DC-7</cell><cell>79.92</cell><cell></cell><cell cols="2">49.98%</cell><cell>0.49?</cell><cell></cell><cell>0.73?</cell></row><row><cell></cell><cell></cell><cell>GMP</cell><cell>79.2 ? 0.17</cell><cell></cell><cell cols="2">49.98%</cell><cell>0.46?</cell><cell></cell><cell>1.64?</cell></row><row><cell></cell><cell></cell><cell cols="5">AC/DC-20 80.0 ? 0.17 74.96%</cell><cell>0.29?</cell><cell></cell><cell>0.6?</cell></row><row><cell></cell><cell></cell><cell>AC/DC-14</cell><cell>79.5</cell><cell></cell><cell cols="2">74.96%</cell><cell>0.29?</cell><cell></cell><cell>0.59?</cell></row><row><cell></cell><cell></cell><cell>AC/DC-7</cell><cell>79.7</cell><cell></cell><cell cols="2">74.96%</cell><cell>0.29?</cell><cell></cell><cell>0.54?</cell></row><row><cell></cell><cell></cell><cell>GMP</cell><cell>78.9 ? 0.14</cell><cell></cell><cell cols="2">74.96%</cell><cell>0.26?</cell><cell></cell><cell>1.52?</cell></row><row><cell></cell><cell></cell><cell cols="5">AC/DC-20 79.1 ? 0.07 89.96%</cell><cell>0.14?</cell><cell></cell><cell>0.51?</cell></row><row><cell></cell><cell></cell><cell>AC/DC-14</cell><cell>79.0</cell><cell></cell><cell cols="2">89.96%</cell><cell>0.14?</cell><cell></cell><cell>0.47?</cell></row><row><cell></cell><cell></cell><cell>AC/DC-7</cell><cell>78.4</cell><cell></cell><cell cols="2">89.96%</cell><cell>0.14?</cell><cell></cell><cell>0.39?</cell></row><row><cell></cell><cell></cell><cell>GMP</cell><cell>77.7 ? 0.23</cell><cell></cell><cell cols="2">89.96%</cell><cell>0.08?</cell><cell></cell><cell>1.44?</cell></row><row><cell></cell><cell></cell><cell cols="2">AC/DC-20 78.2 ? 0.12</cell><cell></cell><cell cols="2">94.95%</cell><cell>0.08?</cell><cell></cell><cell>0.47?</cell></row><row><cell></cell><cell></cell><cell>AC/DC-14</cell><cell>78.5</cell><cell></cell><cell cols="2">94.95%</cell><cell>0.08?</cell><cell></cell><cell>0.41?</cell></row><row><cell></cell><cell></cell><cell>AC/DC-7</cell><cell>77.8</cell><cell></cell><cell cols="2">94.95%</cell><cell>0.08?</cell><cell></cell><cell>0.33?</cell></row><row><cell></cell><cell></cell><cell>GMP</cell><cell>76.6 ? 0.07</cell><cell></cell><cell cols="2">94.95%</cell><cell>0.07?</cell><cell></cell><cell>1.41?</cell></row><row><cell>Validation Acc. (%)</cell><cell>20 40 60 80</cell><cell cols="3">0 20 40 60 80 100 120 140 160 180 200 Epoch 50 % Sparsity 75 % Sparsity 90 % Sparsity 95 % Sparsity Compressed Decompressed</cell><cell>Mask difference (%)</cell><cell>0 10 20 30 40</cell><cell>50</cell><cell>90</cell><cell>Epoch</cell><cell>130</cell><cell>170 95 % 90 % 75 % 50 %</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) Sparsity pattern and test accuracy</cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) Relative change in consecutive masks</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>AC/DC Dense ResNet50</figDesc><table><row><cell>Target</cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell>Sparsity</cell><cell>Dense (%)</cell><cell>Finetuned (%)</cell></row><row><cell>0%</cell><cell>76.84</cell><cell>-</cell></row><row><cell>80%</cell><cell cols="2">73.82 ? 0.02 76.83 ? 0.07</cell></row><row><cell>90%</cell><cell>73.25 ? 0.16</cell><cell>76.56 ? 0.1</cell></row><row><cell>90%</cell><cell>73.66</cell><cell>76.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>AC/DC Dense MobileNetV1</figDesc><table><row><cell>Sparsity</cell><cell>Accuracy Dense (%)</cell><cell>Accuracy Finetuned (%)</cell></row><row><cell>0%</cell><cell>71.78</cell><cell>-</cell></row><row><cell>75%</cell><cell>68.55 ? 0.2</cell><cell>71.63 ? 0.1</cell></row><row><cell>90%</cell><cell cols="2">67.47 ? 0.13 70.86 ? 0.08</cell></row><row><cell>90%</cell><cell>67.65</cell><cell>70.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Accuracy, sparsity, inference FLOPs and percentage of inactive weights for the resulting AC/DC dense models on ResNet50 (before fine-tuning, one seed).</figDesc><table><row><cell>Target</cell><cell>Top-1</cell><cell>Inference</cell><cell>Inactive</cell></row><row><cell>Sparsity</cell><cell>Accuracy (%)</cell><cell>FLOPs</cell><cell>Weights (%)</cell></row><row><cell>80</cell><cell>73.8</cell><cell>0.98?</cell><cell>3.2</cell></row><row><cell>90</cell><cell>73.1</cell><cell>0.93?</cell><cell>10.5</cell></row><row><cell>95</cell><cell>72.9</cell><cell>0.85?</cell><cell>22.0</cell></row><row><cell>98</cell><cell>70.8</cell><cell>0.67?</cell><cell>49.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>AC/DC with uniform vs global magnitude pruning on ResNet50 (one seed), where ( ) denotes that the first and last layers are dense.</figDesc><table><row><cell>Sparsity</cell><cell>Target</cell><cell>Global</cell><cell>Top-1</cell><cell>FLOPs</cell></row><row><cell>Distribution</cell><cell>Sparsity(%)</cell><cell>Sparsity(%)</cell><cell>Accuracy (%)</cell><cell>Inference</cell></row><row><cell>global</cell><cell>90</cell><cell>89.8</cell><cell>75.14</cell><cell>0.18?</cell></row><row><cell>global</cell><cell>90</cell><cell>82.6</cell><cell>75.64</cell><cell>0.21?</cell></row><row><cell>uniform</cell><cell>90</cell><cell>82.6</cell><cell>75.04</cell><cell>0.13?</cell></row><row><cell>global</cell><cell>95</cell><cell>94.8</cell><cell>73.15</cell><cell>0.11?</cell></row><row><cell>global</cell><cell>95</cell><cell>87.2</cell><cell>74.16</cell><cell>0.13?</cell></row><row><cell>uniform</cell><cell>95</cell><cell>87.2</cell><cell>73.28</cell><cell>0.08?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Comparison with Top-KAST when pruning all layers (ResNet50)</figDesc><table><row><cell>Method</cell><cell>Sparsity (%)</cell><cell>Backward Sparsity (%)</cell><cell>Sparse Top-1 Accuracy (%)</cell></row><row><cell>AC/DC</cell><cell>80</cell><cell>80 / 0</cell><cell>76.3 ? 0.1</cell></row><row><cell>Top-KAST</cell><cell>80</cell><cell>0</cell><cell>75.64</cell></row><row><cell>Top-KAST</cell><cell>80</cell><cell>50</cell><cell>74.78</cell></row><row><cell>Top-KAST</cell><cell>80</cell><cell>80</cell><cell>72.19</cell></row><row><cell>AC/DC</cell><cell>90</cell><cell>90 / 0</cell><cell>75.03 ? 0.1</cell></row><row><cell>Top-KAST</cell><cell>90</cell><cell>0</cell><cell>74.42</cell></row><row><cell>Top-KAST</cell><cell>90</cell><cell>50</cell><cell>74.09</cell></row><row><cell>Top-KAST</cell><cell>90</cell><cell>80</cell><cell>73.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Comparison with Top-KAST with increased training steps (ResNet50). ( ) indicates that the first and last layers are dense for AC/DC, while this is the case for all Top-KAST results.</figDesc><table><row><cell>Method</cell><cell>Sparsity (%)</cell><cell>Backward Sparsity (%)</cell><cell>Sparse Top-1 Accuracy (%)</cell><cell>Train FLOPs (%)</cell><cell>Inference FLOPs (%)</cell></row><row><cell>AC/DC</cell><cell>80</cell><cell>80 / 0</cell><cell>76.3 ? 0.1</cell><cell>0.65?</cell><cell>0.29?</cell></row><row><cell>Top-KAST 1?</cell><cell>80</cell><cell>0</cell><cell>75.59</cell><cell>0.48?</cell><cell>0.23?</cell></row><row><cell>Top-KAST 1?</cell><cell>80</cell><cell>60</cell><cell>74.59</cell><cell>0.29?</cell><cell>0.23?</cell></row><row><cell>Top-KAST 2?</cell><cell>80</cell><cell>0</cell><cell>76.11</cell><cell>0.97?</cell><cell>0.23?</cell></row><row><cell>Top-KAST 2?</cell><cell>80</cell><cell>60</cell><cell>75.29</cell><cell>0.58?</cell><cell>0.23?</cell></row><row><cell>AC/DC</cell><cell>90</cell><cell>90 / 0</cell><cell>75.03 ? 0.1</cell><cell>0.58?</cell><cell>0.18?</cell></row><row><cell>AC/DC</cell><cell>90</cell><cell>90 / 0</cell><cell>75.64</cell><cell>0.6?</cell><cell>0.21?</cell></row><row><cell>AC/DC unif.</cell><cell>90</cell><cell>90/0</cell><cell>75.04</cell><cell>0.55?</cell><cell>0.13?</cell></row><row><cell>Top-KAST 1?</cell><cell>90</cell><cell>0</cell><cell>74.65</cell><cell>0.42?</cell><cell>0.13?</cell></row><row><cell>Top-KAST 1?</cell><cell>90</cell><cell>80</cell><cell>73.03</cell><cell>0.16?</cell><cell>0.13?</cell></row><row><cell>Top-KAST 2?</cell><cell>90</cell><cell>0</cell><cell>75.35</cell><cell>0.84?</cell><cell>0.13?</cell></row><row><cell>Top-KAST 2?</cell><cell>90</cell><cell>80</cell><cell>74.16</cell><cell>0.32?</cell><cell>0.13?</cell></row><row><cell>AC/DC</cell><cell>95</cell><cell>95 / 0</cell><cell>73.14 ? 0.2</cell><cell>0.53?</cell><cell>0.11?</cell></row><row><cell>AC/DC</cell><cell>95</cell><cell>95 / 0</cell><cell>74.16</cell><cell>0.54?</cell><cell>0.13?</cell></row><row><cell>AC/DC (unif)</cell><cell>95</cell><cell>95 / 0</cell><cell>73.28</cell><cell>0.5?</cell><cell>0.08?</cell></row><row><cell>Top-KAST 1?</cell><cell>95</cell><cell>0</cell><cell>71.83</cell><cell>0.39?</cell><cell>0.08?</cell></row><row><cell>Top-KAST 1?</cell><cell>95</cell><cell>90</cell><cell>70.42</cell><cell>0.1?</cell><cell>0.08?</cell></row><row><cell>Top-KAST 2?</cell><cell>95</cell><cell>0</cell><cell>73.29</cell><cell>0.77?</cell><cell>0.08?</cell></row><row><cell>Top-KAST 2?</cell><cell>95</cell><cell>90</cell><cell>72.42</cell><cell>0.19?</cell><cell>0.08?</cell></row><row><cell>Top-KAST 5?</cell><cell>95</cell><cell>0</cell><cell>74.27</cell><cell>1.94?</cell><cell>0.08?</cell></row><row><cell>Top-KAST 5?</cell><cell>95</cell><cell>90</cell><cell>73.17</cell><cell>0.48?</cell><cell>0.08?</cell></row><row><cell>AC/DC</cell><cell>98</cell><cell>98 / 0</cell><cell>68.44 ? 0.09</cell><cell>0.46?</cell><cell>0.06?</cell></row><row><cell>AC/DC</cell><cell>98</cell><cell>98 / 0</cell><cell>71.27</cell><cell>0.47?</cell><cell>0.08?</cell></row><row><cell>Top-KAST 1?</cell><cell>98</cell><cell>90</cell><cell>67.06</cell><cell>0.08?</cell><cell>0.05?</cell></row><row><cell>Top-KAST 1?</cell><cell>98</cell><cell>95</cell><cell>66.46</cell><cell>0.06?</cell><cell>0.05?</cell></row><row><cell>Top-KAST 2?</cell><cell>98</cell><cell>90</cell><cell>68.99</cell><cell>0.15?</cell><cell>0.05?</cell></row><row><cell>Top-KAST 2?</cell><cell>98</cell><cell>85</cell><cell>68.87</cell><cell>0.12?</cell><cell>0.05?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Comparison between AC/DC and RigL on MobileNet, where ( ) denotes that the first and depth-wise convolutions were kept dense.</figDesc><table><row><cell>Method</cell><cell>Sparsity (%)</cell><cell>Top-1 Accuracy (%)</cell><cell>Inference FLOPs</cell><cell>Train FLOPs</cell></row><row><cell>AC/DC</cell><cell>75</cell><cell>70.3</cell><cell>0.34?</cell><cell>0.64?</cell></row><row><cell>AC/DC</cell><cell>75</cell><cell>70.41</cell><cell>0.36?</cell><cell>0.66?</cell></row><row><cell>RigL (ERK)</cell><cell>75</cell><cell>68.39</cell><cell>0.52?</cell><cell>0.52?</cell></row><row><cell>RigL 2? (ERK)</cell><cell>75</cell><cell>70.49</cell><cell>0.52?</cell><cell>1.05?</cell></row><row><cell>RigL 5? (ERK)</cell><cell>75</cell><cell>71.9</cell><cell>0.52?</cell><cell>2.63?</cell></row><row><cell>AC/DC</cell><cell>90</cell><cell>66.08</cell><cell>0.18?</cell><cell>0.56?</cell></row><row><cell>AC/DC</cell><cell>90</cell><cell>66.56</cell><cell>0.21?</cell><cell>0.58?</cell></row><row><cell>RigL (ERK)</cell><cell>90</cell><cell>63.58</cell><cell>0.27?</cell><cell>0.29?</cell></row><row><cell>RigL 2? (ERK)</cell><cell>90</cell><cell>65.92</cell><cell>0.27?</cell><cell>0.59?</cell></row><row><cell>RigL 5? (ERK)</cell><cell>90</cell><cell>68.1</cell><cell>0.27?</cell><cell>1.47?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Time per batch (milliseconds) using a sparse inference engine<ref type="bibr" target="#b11">[12]</ref>.</figDesc><table><row><cell cols="2">Model/Setup</cell><cell cols="4">Real-Time Inference, 4 cores Batch 64 Inference, 16 cores</cell></row><row><cell cols="3">ResNet50 ONNXRT v1.6</cell><cell>14.773</cell><cell></cell><cell>329.734</cell></row><row><cell cols="2">ResNet50 Dense</cell><cell></cell><cell>15.081</cell><cell></cell><cell>285.958</cell></row><row><cell cols="3">ResNet50 90% Pruned</cell><cell>9.46</cell><cell></cell><cell>124.193</cell></row><row><cell cols="3">ResNet50 90% Unif. Pruned</cell><cell>8.495</cell><cell></cell><cell>116.897</cell></row><row><cell cols="3">MobileNetV1 ONNXRT v1.6</cell><cell>2.552</cell><cell></cell><cell>80.748</cell></row><row><cell cols="2">MobileNetV1 Dense</cell><cell></cell><cell>2.513</cell><cell></cell><cell>55.845</cell></row><row><cell cols="3">MobileNetV1 Pruned 75%</cell><cell>1.96</cell><cell></cell><cell>40.976</cell></row><row><cell cols="3">MobileNetV1 Pruned 90%</cell><cell>1.468</cell><cell></cell><cell>34.909</cell></row><row><cell cols="6">Table 14: Sample agreement between ResNet50 sparse and dense models</cell></row><row><cell>Method</cell><cell>Sparsity</cell><cell>Sparse Top-1 Accuracy (%)</cell><cell>Dense Top-1 Accuracy (%)</cell><cell>Sparse-Dense Agreement (%)</cell><cell>Sparse-Dense Cross-entropy</cell></row><row><cell>AC/DC</cell><cell>80%</cell><cell>76.3 ? 0.1</cell><cell>76.8 ? 0.07</cell><cell>89.8 ? 0.3</cell><cell>0.85 ? 0.005</cell></row><row><cell>SparseVD</cell><cell>80%</cell><cell>75.3</cell><cell>75.2</cell><cell>98.6</cell><cell>-</cell></row><row><cell>GMP</cell><cell>80%</cell><cell>76.4</cell><cell>76.9</cell><cell>86.0</cell><cell>1.03</cell></row><row><cell>AC/DC</cell><cell>90%</cell><cell>75.0 ? 0.1</cell><cell>76.6 ? 0.09</cell><cell>86.8 ? 1.5</cell><cell>1.02 ? 0.004</cell></row><row><cell>SparseVD</cell><cell>90%</cell><cell>73.8</cell><cell>73.6</cell><cell>98.3</cell><cell>-</cell></row><row><cell>GMP</cell><cell>90%</cell><cell>74.7</cell><cell>76.9</cell><cell>83.5</cell><cell>1.29</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">f (?) ? f * ?f (?) .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/ LanguageModeling/Transformer-XL</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 805223 ScaleML), and a CNRS PEPS grant. This research was supported by the Scientific Service Units (SSU) of IST Austria through resources provided by Scientific Computing (SciComp). We would also like to thank Christoph Lampert for his feedback on an earlier version of this work, as well as for providing hardware for the Transformer-XL experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The convergence of sparsified gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarit</forename><surname>Khirirat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Konstantinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Renggli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via overparameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparse convex optimization via adaptively regularized hard thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyriakos</forename><surname>Axiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Sviridenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kappel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Legenstein</surname></persName>
		</author>
		<title level="m">Deep rewiring: Training very sparse deep networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Experiment Tracking with Weights and Biases, 2020. Software available from wandb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Biewald</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iterative thresholding for sparse approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fourier analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="629" to="654" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Near-optimal signal recovery from random projections: Universal encoding strategies?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5406" to="5425" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TVM: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TensorFlow Lite Micro: Embedded machine learning on TinyML systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advait</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nat</forename><surname>Jeffries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Kreeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Nappier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghna</forename><surname>Natraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomi</forename><surname>Regev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08678</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">NeuralMagic DeepSparse Inference Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepsparse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sparse networks from scratch: Faster training without losing performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04840</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to prune deep neural networks via layer-wise optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinno Jialin</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07565</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast sparse convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rigging the lottery: Making all tickets winners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Samuel</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hard thresholding pursuit: an algorithm for compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Foucart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2543" to="2563" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparse recovery algorithms: sufficient conditions in terms of restricted isometry constants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Foucart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Approximation Theory XIII</title>
		<meeting><address><addrLine>San Antonio</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="65" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linear mode connectivity and the lottery ticket hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09574</idno>
		<title level="m">The state of sparsity in deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A survey of quantization methods for efficient neural network inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13630</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graphcore Poplar SDK 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphcore</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple and effective method for removal of hidden units and weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masafumi</forename><surname>Hagiwara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<title level="m">Dense-sparse-dense training for deep neural networks. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimal brain surgeon and general network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory J</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on neural networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikoli</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Peste</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00554</idno>
		<title level="m">Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On iterative hard thresholding methods for highdimensional M-estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<title level="m">Top-KAST: Top-K always sparse training. Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05423</idno>
		<title level="m">Training skinny deep neural networks with iterative hard thresholding methods</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Linear convergence of gradient and proximal-gradient methods under the Polyak-?ojasiewicz condition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="795" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Soft threshold weight reparameterization for learnable sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Vivek Ramanujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">SNIP: Single-shot network pruning based on connection sensitivity. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic model pruning with feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Dmitriev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00307</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">Albericio</forename><surname>Latorre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darko</forename><surname>Stosic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dusan</forename><surname>Stosic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08378</idno>
		<title level="m">Accelerating sparse deep neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Decebal Constantin Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirkeivan</forename><surname>Mohtashami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08895</idno>
		<title level="m">Simultaneous training of partially masked neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Variational dropout sparsifies deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ning Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Landscape connectivity and dropout stability of SGD solutions for over-parameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shevchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Mondelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Understanding top-k sparsification in distributed deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Chun</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>See</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08772</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Woodfisher: Efficient second-order approximation for neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pal</forename><surname>Sidak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alistarh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for modern deep learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pruning neural networks without any data by iteratively conserving synaptic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidenori</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient inference with TensorRT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Vanholder</surname></persName>
		</author>
		<ptr target="https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=23425-efficient+inference+with+tensorrt" />
	</analytic>
	<monogr>
		<title level="j">NVIDIA GTC On-Demand. Slides avail</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gradient hard thresholding pursuit for sparsity-constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC). British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning N: M fine-grained structured sparse neural networks from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01878</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
