<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Normalized Gaussian Wasserstein Distance for Tiny Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
							<email>xuchangeis@whu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
							<email>yangwen@whu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Electronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Electronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Electronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Electronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Normalized Gaussian Wasserstein Distance for Tiny Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Codes are available at: https://github.com/jwwangchn/NWD.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting tiny objects is a very challenging problem since a tiny object only contains a few pixels in size. We demonstrate that state-of-the-art detectors do not produce satisfactory results on tiny objects due to the lack of appearance information. Our key observation is that Intersection over Union (IoU) based metrics such as IoU itself and its extensions are very sensitive to the location deviation of the tiny objects, and drastically deteriorate the detection performance when used in anchor-based detectors. To alleviate this, we propose a new evaluation metric using Wasserstein distance for tiny object detection. Specifically, we first model the bounding boxes as 2D Gaussian distributions and then propose a new metric dubbed Normalized Wasserstein Distance (NWD) to compute the similarity between them by their corresponding Gaussian distributions. The proposed NWD metric can be easily embedded into the assignment, non-maximum suppression, and loss function of any anchor-based detector to replace the commonly used IoU metric. We evaluate our metric on a new dataset for tiny object detection (AI-TOD) in which the average object size is much smaller than existing object detection datasets. Extensive experiments show that, when equipped with NWD metric, our approach yields performance that is 6.7 AP points higher than a standard fine-tuning baseline, and 6.0 AP points higher than state-of-the-art competitors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tiny objects are ubiquitous in many real world applications including driving assistance, large-scale surveillance, and maritime rescue. Even though object detection has achieved significant progress due to the development of deep neural networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>, most of them are devoted to detecting objects with normal size. While tiny objects (less than 16 ? 16 pixels in the AI-TOD dataset <ref type="bibr" target="#b28">[29]</ref>) often exhibit with extremely limited appearance information, which increases difficulty in learning discriminative features, leading to enormous failure cases when detecting tiny objects <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Recent advances for tiny object detection (TOD) mainly focus on improving the feature discrimination <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref>. Some efforts have been devoted to normalizing the scale of input images to enhance the resolution of small objects and corresponding features <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. While the Generative  <ref type="figure">Figure 1</ref>: The sensitivity analysis of IoU on tiny and normal scale objects. Note that each grid denotes a pixel, box A denotes the ground truth bounding box, box B, C denote the predicted bounding box with 1 pixel and 4 pixels diagonal deviation respectively. Adversarial Network (GAN) is proposed to directly generate super-resolved representations for small objects <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref>. Besides, the Feature Pyramid Network (FPN) is proposed to learn multi-scale features to achieve scale-invariant detectors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b19">20]</ref>. Indeed, existing approaches have improved TOD performance to some extent, but the precision boost is commonly achieved with additional cost.</p><p>In addition to learning discriminative features, the quality of the training sample selection plays an important role for anchor-based tiny object detectors <ref type="bibr" target="#b35">[36]</ref> where the assignment of positive and negative (pos/neg) labels is essential. However, for tiny object, the properties of few pixels will increase the difficulty of training sample selection. As shown in <ref type="figure">Fig. 1</ref>, we can observe that the sensitivity of IoU to objects with different scales is of great variance. Specifically, for the tiny object with 6 ? 6 pixels, a minor location deviation will lead to notable IoU drop (from 0.53 to 0.06), resulting in inaccurate label assignment. However, for the normal object with 36 ? 36 pixels, the IoU changes slightly (from 0.90 to 0.65) with the same location deviation. In addition, <ref type="figure" target="#fig_1">Fig. 2</ref> shows four IoU-Deviation curves with different object scales, the curve declines faster as the object size becomes smaller. It is worth noting that, the sensitivity of IoU results from the particularity that the location of bounding box can only change discretely.</p><p>This phenomenon implies that IoU metric is no longer invariant to object scale with discretized location deviations and finally leads to the following two flaws in label assignment. Specifically, IoU thresholds (? p , ? n ) are used to assign pos/neg training samples in anchor-based detectors, and (0.7, 0.3) are used in Region Proposal Network (RPN) <ref type="bibr" target="#b6">[7]</ref>. Fisrtly, the sensitivity of IoU on tiny object makes a minor location deviation flip the anchor label, leading to pos/neg sample features' similarity and the network's difficulty in convergence. Secondly, we find that the average number of positive samples assigned to each ground-truth (gt) in AI-TOD dataset <ref type="bibr" target="#b28">[29]</ref> is less than one using IoU metric since the IoU between some gt and any anchor is lower than minimum positive threshold. Therefore, there will be insufficient supervision information for training tiny object detectors. Although dynamic assignment strategies such as ATSS <ref type="bibr" target="#b35">[36]</ref> can adaptively attain IoU thresholds for assigning pos/neg labels according to the statistical characteristics of objects, the sensitivity of IoU makes it difficult to find a good threshold and provide high-quality pos/neg samples for tiny object detectors.</p><p>Observing that IoU is not a good metric for tiny objects, in this paper, we propose a new metric to measure the similarity of bounding boxes by Wasserstein distance to replace standard IoU. Specifically, we firstly model the bounding boxes as 2-D Gaussian distributions, and then use our proposed Normalized Wasserstein Distance (NWD) to measure the similarity of derived Gaussian distributions. The major advantage of Wasserstein distance is that it can measure the distribution similarity even if there is no overlap or the overlap is negligible. In addition, the NWD is insensitive to objects with different scales and thus more suitable for measuring the similarity between tiny objects.</p><p>NWD can be applied to both single-stage and multi-stage anchor-based detectors. Besides, NWD can not only replace IoU in label assignment, but also replace IoU in Non-maximum Suppression (NMS) and regression loss function. Extensive experiments on a new TOD dataset AI-TOD <ref type="bibr" target="#b28">[29]</ref> demonstrate that our proposed NWD can consistently improve the detection performance for all the detectors experimented. The contributions of this paper are summarized as follows.</p><p>? We analyze the sensitivity of IoU to location deviations of tiny objects, and propose NWD as a better metric for measuring the similarity between two bounding boxes.</p><p>? We design a powerful tiny object detector by applying NWD to label assignment, NMS and loss function in anchor-based detectors.</p><p>? Our proposed NWD can significantly improve TOD performance of the popular anchor-based detectors, and it achieves performance improvement from 11.1% to 17.6% on Faster R-CNN on AI-TOD dataset.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tiny Object Detection</head><p>Most of the previous small/tiny object detection methods can be roughly divided into three categories: multi-scale feature learning, designing better training strategy and GAN-based detection <ref type="bibr" target="#b27">[28]</ref>.</p><p>Multi-scale Feature Learning: A simple and classic way is to resize input images into different scales and to train different detectors, each of which can achieve best performance in a certain range of scales. To reduce the computation cost, some works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">37]</ref> try to construct feature-level pyramid of different scales. For instance, SSD <ref type="bibr" target="#b17">[18]</ref> detects objects from feature maps of different resolutions. Feature Pyramid Network (FPN) <ref type="bibr" target="#b13">[14]</ref> constructs a top-down structure with lateral connections to combine feature information of different scales for improving object detection performance. After that, lots of methods are proposed to further improve FPN performance, including PANet <ref type="bibr" target="#b16">[17]</ref>, BiFPN <ref type="bibr" target="#b25">[26]</ref>, Recursive-FPN <ref type="bibr" target="#b19">[20]</ref>. Besides, TridentNet <ref type="bibr" target="#b12">[13]</ref> constructs a parallel multi-branch architecture with different receptive fields to generate scale-specific feature maps.</p><p>Designing Better Training Strategy: Inspired by the observation that it is difficult to detect tiny objects and large objects simultaneously, Singh et al. propose SNIP <ref type="bibr" target="#b23">[24]</ref> and SNIPER <ref type="bibr" target="#b24">[25]</ref> to selectively train objects within a certain scale range. Besides, Kim et al. <ref type="bibr" target="#b9">[10]</ref> introduce Scale-Aware Network (SAN) and map the features extracted from different spaces onto a scale-invariant subspace, making detectors more robust to scale variation.</p><p>GAN-based Detectors: Perceptual GAN <ref type="bibr" target="#b11">[12]</ref> is the first to attempt to apply GAN to small object detection, it improves small object detection through narrowing representation difference of small objects from the large ones. Besides, Bai et al. <ref type="bibr" target="#b0">[1]</ref> propose a MT-GAN to train the image-level super-resolution model for enhancing the features of small RoIs. Furthermore, the work in <ref type="bibr" target="#b18">[19]</ref> proposes a feature-level super-resolution approach to improve small object detection performance for proposal based detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation Metric in Object Detection</head><p>IoU is the mostly widely used metric for measuring the similarity between bounding boxes. However, IoU can only work when the bounding boxes have overlap. To handle this problem, generalized IoU (GIoU) <ref type="bibr" target="#b21">[22]</ref> is proposed by adding a penalty term of the smallest box converting bounding boxes. Nevertheless, GIoU will degrade to IoU when one bounding box contains another. Thus, DIoU <ref type="bibr" target="#b37">[38]</ref> and CIoU <ref type="bibr" target="#b37">[38]</ref> are proposed to overcome the limitations of IoU and GIoU by taking three geometric properties into account, i.e., overlap area, central point distance and aspect ratio. GIoU, CIoU and DIoU are mainly applied in NMS and loss function to replace IoU for improving general object detection performance, but the application in label assignment is rarely discussed. In co-current work, Yang et al. <ref type="bibr" target="#b31">[32]</ref> also propose a Gaussian Wasserstein Distance (GWD) loss for oriented object detection by measuring the positional relationship of oriented bounding boxes. However, the motivation of GWD is to solve the boundary discontinuity and square-like problem in oriented object detection. Our motivation is to alleviate the sensitivity of IoU for location deviations of tiny objects and our proposed method can replace IoU in all parts of anchor-based object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Label Assignment Strategies</head><p>It is a challenging task to assign high-quality anchors to gt boxes of tiny objects. A simple way is to lower the IoU threshold when selecting positive samples. Although it can make tiny objects match more anchors, the overall quality of training samples will deteriorate. Besides, many recent works try to make the label assignment process more adaptive, aiming to improve the detection performance <ref type="bibr" target="#b5">[6]</ref>. For instance, Zhang et al. <ref type="bibr" target="#b35">[36]</ref> propose an Adaptive Training Sample Selection (ATSS) to automatically compute the pos/neg threshold for each gt by statistic value of IoU from a set of anchors. Kang et al. <ref type="bibr" target="#b8">[9]</ref> introduce Probabilistic Anchor Assignment (PAA) by assuming that the distribution of joint loss for pos/neg samples follows the Gaussian distribution. In addition, Optimal Transport Assignment (OTA) <ref type="bibr" target="#b5">[6]</ref> formulates the label assignment process as an Optimal Transport problem from a global perspective. However, these methods all use IoU metric to measure the similarity between two bounding boxes, and mainly focus on the threshold setting in the label assignment which are not suitable for TOD. In contrast, our method mainly focuses on designing a better evaluation metric which can be used to replace IoU metric in tiny object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Inspired by the fact that IoU is actually the Jaccard similarity coefficient for computing similarity of two limited sample sets, we design a better metric for tiny objects based on Wasserstein Distance since it can consistently reflect the distance between distributions even if they have no overlap. Therefore, the new metric has better properties than IoU in measuring similarity between tiny objects. The details are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gaussian Distribution Modeling for Bounding Box</head><p>For tiny objects, there tend to be some background pixels in their bounding boxes since most real objects are not strict rectangles. In these bounding boxes, foreground pixels and background pixels are concentrated on the center and boundary of the bounding boxes, respectively <ref type="bibr" target="#b29">[30]</ref>. To better describe the weights of different pixels in bounding boxes, the bounding box can be modeled into two dimension (2D) Gaussian distribution, where the center pixel of bounding box has the highest weight and importance of the pixel decreases from the center to the boundary. Specifically, for horizontal bounding box R = (cx, cy, w, h), where (cx, cy), w and h denote the center coordinates, width and height, respectively. The equation of its inscribed ellipse can be represented as</p><formula xml:id="formula_0">(x ? ? x ) 2 ? 2 x + (y ? ? y ) 2 ? 2 y = 1,<label>(1)</label></formula><p>where (? x , ? y ) is the center coordinates of the ellipse, ? x , ? y are the lengths of semi-axises along x and y axises. Accordingly, ? x = cx, ? y = cy, ? x = w 2 , ? y = h 2 . The probability density function of a 2D Gaussian distribution is given by:</p><formula xml:id="formula_1">f (x|?, ?) = exp(? 1 2 (x ? ?) ? ?1 (x ? ?)) 2?|?| 1 2 ,<label>(2)</label></formula><p>where x, ? and ? denote the coordinate (x, y), the mean vector and the co-variance matrix of Gaussian distribution. When</p><formula xml:id="formula_2">(x ? ?) ? ?1 (x ? ?) = 1,<label>(3)</label></formula><p>the ellipse in Eq. 1 will be a density contour of the 2D Gaussian distribution. Therefore, the horizontal bounding box R = (cx, cy, w, h) can be modeled into a 2D Gaussian distribution N (?, ?) with</p><formula xml:id="formula_3">? = c x c y , ? = w 2 4 0 0 h 2 4 .<label>(4)</label></formula><p>Furthermore, the similarity between bounding box A and B can be converted to the distribution distance between two Gaussian distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Normalized Gaussian Wasserstein Distance</head><p>We use the Wasserstein distance which comes from Optimal Transport theory to compute distribution distance. For two 2D Gaussian distributions ? 1 = N (m 1 , ? 1 ) and ? 2 = N (m 2 , ? 2 ), the 2 nd order Wasserstein distance between ? 1 and ? 2 is defined as: and it can be simplified as:</p><formula xml:id="formula_4">W 2 2 (? 1 , ? 2 ) = m 1 ? m 2 2 2 + Tr ? 1 + ? 2 ? 2 ? 1/2 2 ? 1 ? 1/2 2 1/2 ,<label>(5)</label></formula><formula xml:id="formula_5">W 2 2 (? 1 , ? 2 ) = m 1 ? m 2 2 2 + ? 1/2 1 ? ? 1/2 2 2 F ,<label>(6)</label></formula><p>where ? F is the Frobenius norm.</p><p>Furthermore, for Gaussian distributions N a and N b which are modeled from bounding boxes A = (cx a , cy a , w a , h a ) and B = (cx b , cy b , w b , h b ), Eq. 6 can be further simplified as:</p><formula xml:id="formula_6">W 2 2 (N a , N b ) = cx a , cy a , w a 2 , h a 2 T , cx b , cy b , w b 2 . h b 2 T 2 2 .<label>(7)</label></formula><p>However, W 2 2 (N a , N b ) is a distance metric, and cannot be directly used as similarity metric (i.e., a value between 0 and 1 as IoU). Therefore, we use its exponential form normalization and obtain the new metric dubbed Normalized Wasserstein Distance (NWD):</p><formula xml:id="formula_7">N W D (N a , N b ) = exp ? W 2 2 (N a , N b ) C ,<label>(8)</label></formula><p>where C is a constant closely related to the dataset. In the following experiments, we empirically set C to the average absolute size of AI-TOD and achieve the best performance. Moreover, we observe that C is robust in a certain range, details will be shown in supplementary materials.</p><p>Compared with IoU, NWD has the following advantages for detecting tiny objects: (1) scale invariance, (2) smoothness to location deviation, (3) the capability of measuring the similarity between non-overlapping or mutually inclusive bounding boxes. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, without losing generality, we discuss the change of metric value in the following two scenarios. In the first row of <ref type="figure" target="#fig_1">Fig. 2</ref>, we keep box A and B the same scale and move away B along the diagonal of A. It can be seen that the four curves of NWD completely coincide, which indicates that NWD is insensitive to the scale variance of boxes. Moreover, we can observe that IoU is too sensitive to minor location deviation, but the NWD change resulting from location deviation is more smooth. The smoothness to location deviation indicates a possibility of a better distinction between pos/neg samples than IoU under the same threshold. In the second row of <ref type="figure" target="#fig_1">Fig. 2</ref>, we set the side length of B to half of the side length of A and move away B along the diagonal of A. Compared with IoU, the curve of NWD is much more smooth and it can consistently reflect the similarity between A and B even if |A ? B| = A or B and |A ? B| = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NWD-based Detectors</head><p>The proposed NWD can be easily integrated into any anchor-based detector to replace IoU. Without loss of generality, the representative anchor-based Faster R-CNN is adopted to describe the usage of NWD. Specifically, all the modifications are conducted in the three parts which originally employ IoU, including pos/neg label assignment, NMS and regression loss function. The details are as follows.</p><p>NWD-based Label Assignment. Faster R-CNN <ref type="bibr" target="#b20">[21]</ref> consists of two networks: RPN for generating region proposals and R-CNN <ref type="bibr" target="#b6">[7]</ref> for detecting objects based on these proposals. The RPN and R-CNN both include label assignment process. For the RPN, anchors of different scales and ratios are firstly generated, and then binary labels are assigned to the anchors for training the classification and regression head. For the R-CNN, the label assignment process is similar with the RPN, and the difference is that the input of R-CNN is the output of RPN. In order to overcome the aforementioned shortcomings of IoU in tiny object detection, we design NWD-based label assignment strategy, which utilizes NWD to assign labels. Specifically, for training RPN, the positive label will be assigned to two kinds of anchors: (1) the anchor with the highest NWD value with a gt box and the NWD value is larger than ? n or (2) the anchor that has the NWD value higher than the positive threshold ? p with any gt . Accordingly, the negative label will be assigned to the anchor if its NWD value is lower than the negative threshold ? n with all gt boxes. In addition, the anchors that are neither assigned positive labels nor negative labels do not participate in the training process. Note that, in order to apply NWD to anchor-based detectors directly, ? p and ? n as the original detectors are used in the experiments.</p><p>NWD-based NMS. NMS is an integral part of the object detection pipeline to suppress the redundant prediction bounding boxes, in which the IoU metric is applied. First, it sorts all prediction boxes based on their scores. The prediction box M with the highest score is selected and all other prediction boxes with a significant overlap (using a pre-defined threshold N t ) with M are suppressed. This process is recursively applied on the remaining boxes. However, the sensitivity of IoU to tiny object will make the IoU values lower than N t for lots of prediction boxes, which further leads to false positive predictions. To handle this problem, we suggest that NWD is a better criterion for NMS in tiny object detection since NWD overcomes the scale sensitivity problem. Moreover, the NWD-based NMS is flexible to be integrated into any tiny object detector with only a few codes.</p><p>NWD-based Regression Loss. IoU-Loss <ref type="bibr" target="#b33">[34]</ref> is introduced to eliminate the performance gap between training and testing <ref type="bibr" target="#b21">[22]</ref>. However, IoU-Loss cannot provide gradient for optimizing network in the following two cases: <ref type="bibr" target="#b0">(1)</ref> there is no overlap between the predicted bounding box P and the ground-truth box G (i.e., |P ? G| = 0) or (2) box P contains box G completely or vice versa (i.e., |P ? G| = P or G). In addition, these two cases are very common for tiny objects. Specifically, on one hand, the deviation of a few pixels in P will cause no overlap between P and G, on the other hand, the tiny object is easy to be false predicted, leading to |P ? G| = P or G. Therefore, IoU-Loss is not suitable for tiny object detector. Although CIoU and DIoU can handle above two situations, they are sensitive to the location deviation of the tiny objects since they are both based on IoU. To handle above problems, we design the NWD metric as loss function by:</p><formula xml:id="formula_8">L N W D = 1 ? N W D (N p , N g ) ,<label>(9)</label></formula><p>where N p is the Gaussian distribution model of prediction box P , N g is the Gaussian distribution model of gt box G. According to the introduction in Sec. 3.2, NWD-based loss can provide gradient even in both cases |P ? G| = 0 and |P ? G| = P or G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed method on AI-TOD <ref type="bibr" target="#b28">[29]</ref> and VisDrone2019 <ref type="bibr" target="#b3">[4]</ref> datasets. The ablation study is conducted on AI-TOD, which is a challenging dataset designed for tiny object detection. It comes with eight categories, 700, 621 object instances across 28, 036 aerial images with 800 ? 800 pixels. The mean absolute size of AI-TOD is only 12.8 pixels, which is much smaller than other object detection dataset like PASCAL VOC (156.6 pixels) <ref type="bibr" target="#b4">[5]</ref>, MS COCO (99.5 pixels) <ref type="bibr" target="#b15">[16]</ref>, and DOTA (55.3 pixels) <ref type="bibr" target="#b30">[31]</ref>. In addition, VisDrone2019 <ref type="bibr" target="#b3">[4]</ref> is an UAV dataset for object detection. It consists of 10,209 images with 10 categories. VisDrone2019 has many complex scenes and large numbers of tiny objects since images are captured in different places at different height.</p><p>We adopt the same evaluation metric as AI-TOD <ref type="bibr" target="#b28">[29]</ref> dataset, including AP, AP 0.5 , AP 0.75 , AP vt , AP t , AP s and AP m . Specifically, AP is averaged mAP across different IoU thresholds IoU={0.5, 0.55, ? ? ? , 0.95}, AP 0.5 and AP 0.75 are APs at IoU threshold of 0.5 and 0.75, respectively. In addition, AP vt , AP t , AP s and AP m are for very tiny (2-8 pixels), tiny (8-16 pixels), small (16-32 pixels) and medium (32-64 pixels) scale evaluation in AI-TOD <ref type="bibr" target="#b28">[29]</ref>.</p><p>We conduct all the experiments on a computer with 4 NVIDIA Titan X GPUs, and the codes are used for our experiments are based on MMdetection <ref type="bibr" target="#b2">[3]</ref> code library. The ImageNet <ref type="bibr" target="#b22">[23]</ref> pretrained ResNet-50 <ref type="bibr" target="#b7">[8]</ref> with FPN <ref type="bibr" target="#b13">[14]</ref> is used as the backbone, unless specified otherwise. All models are trained using the SGD optimizer for 12 epochs with 0.9 momentum, 0.0001 weight decay and 8 batch size. We set the initial learning rate as 0.01 and decay it at epoch 8 and 11 by a factor of 0.1. Besides, the batch size of RPN and Fast R-CNN are set to 256 and 512, respectively, and the sampling ratio of positive and negative samples is set to 1/3. The number of proposals generated by RPN is set to 3000.</p><p>In the inference stage, we use the preset score 0.05 to filter out background bounding boxes, and NMS is applied with the IoU threshold 0.5. The above training and inference parameters are used in all experiments, unless specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with Other Metrics based IoU</head><p>There are some IoU-based metrics can be used to measure the similarity between bounding boxes as mentioned in Sec. 2. In this work, we re-implement the aforementioned four metrics (i.e.IoU, GIoU, CIoU and DIoU) and our proposed NWD on the same basic network (i.e.Faster R-CNN) to compare their performance on tiny objects. Specifically, they are applied in label assignment, NMS and loss function, respectively. Experimental results on AI-TOD dataset are shown in Tab. 1.</p><p>Comparison in label assignment. Note that the metric in assigning modules of RPN and R-CNN are both modified. It can be seen that NWD achieves the highest AP of 16.1% and improves 9.6% on AP t when comparing with IoU metric, revealing that the NWD-based label assignment can provide more high quality training samples for tiny objects. In addition, to analyze the essential of the improvement, We make a group of statistical experiment. Specifically, we respectively calculate the average number of positive anchors matched by each gt box when using IoU, GIoU, DIoU, CIoU and NWD under the same default threshold, the number is 0.72, 0.71, 0.19, 0.19 and 1.05 respectively. It can be found that only NWD can ensure a considerable number of positive training samples. Moreover, although simply lowering the threshold of IoU-based metrics can provide more positive anchors for training, the performance of IoU-based tiny object detector after threshold fine-tuning is not better than the performance of NWD-based detector, which will be further discussed in supplementary materials. It attributes to the fact that NWD can solve the sensitivity of IoU to tiny object location deviation.</p><p>Comparison in NMS. We only modify the NMS module of RPN in this experiment since only the NMS in RPN can directly affect the training processing of detector. It can be seen that using different metrics to filter out redundant predictions during training can also affect the detection performance. Concretely, NWD achieves the best AP of 11.9%, which is 0.8% higher than the commonly used IoU. This implies that the NWD is a better metric for filtering out redundant bounding boxes when detecting tiny objects.  Comparison in loss function. Note that we modify the loss function both in RPN and R-CNN, which can both affect the convergence of the detector. It can also be seen that NWD-based loss function achieves the highest AP of 12.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In this section, Faster R-CNN <ref type="bibr" target="#b20">[21]</ref> are used as the baseline, and it consists of two stages: RPN and R-CNN. Our proposed method can both be applied in the label assignment, NMS, loss function module of RPN and R-CNN, therefore there are totally six modules that can be switched from IoU metric to NWD metric. To verify the effectiveness of our proposed method in different modules, we make the following two groups of ablation study: comparison of applying NWD into one of the six modules and comparison of applying NWD into all modules in RPN or R-CNN.</p><p>Applying NWD into single module. Experimental results are shown in Tab. 2. Compared to baseline method, NWD-based assigning module in RPN and R-CNN respectively achieves the highest and second-highest AP improvement of 6.2% and 3.2%, which indicates that the problem of tiny object training label assignment resulting from IoU is the most noticeable, and our proposed NWD-based assignment strategy greatly improves assignment quality. It can also be observed that our proposed method improves the performance in 5 out of 6 modules, which significantly verifies the effectiveness of our NWD-based method. And the performance drop in NMS of R-CNN may owe to the fact that the default NMS threshold is sub-optimal, and it needs fine-tuning to boost the performance.</p><p>Applying NWD into multiple modules. Tab. 3 lists the experimental results. When training for 12 epochs, the detection performance all achieves significant improvement when using NWD in RPN, R-CNN or all modules. And the best performance of 17.8% is achieved when we apply NWD into all three modules of RPN. However, we find that when using NWD in all six modules, the AP has a drop of 2.6% compared with merely using NWD in RPN. In order to analyze the reason for performance drop, we add a group of experiments and train the network for 24 epochs. It can be seen that the performance gap decreases from 2.6% to 0.9%, which reveals that the network needs more time to converge when using NWD in R-CNN. Therefore, we only use NWD in RPN to achieve a considerable performance improvement with less time in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>To reveal the effectiveness of NWD on TOD, we conduct experiments on tiny object detection datasets AI-TOD <ref type="bibr" target="#b28">[29]</ref> and VisDrone2019 <ref type="bibr" target="#b3">[4]</ref>.</p><p>Main results on AI-TOD. To verify that NWD can be applied into any anchor-based detector and boost TOD performance, we select five baseline detectors, including one-stage anchor-based detectors  (i.e., RetinaNet <ref type="bibr" target="#b14">[15]</ref>, ATSS <ref type="bibr" target="#b35">[36]</ref>) and multi-stage anchor-based detectors (i.e., Faster R-CNN <ref type="bibr" target="#b20">[21]</ref>, Cascade R-CNN <ref type="bibr" target="#b1">[2]</ref>, DetectoRS <ref type="bibr" target="#b19">[20]</ref>). Experimental results are shown in Tab. 4. It can be seen that AP vt of current state-of-the-art detectors is extremely low and close to zero, that means they cannot produce satisfactory results on tiny objects. In addition, our proposed NWD-based detectors improve AP metric of RetinaNet, ATSS, Faster R-CNN, Cascade R-CNN and DetectoRS by 4.5%, 0.7%, 6.7%, 4.9% and 6.0%, respectively. The performance improvement is even more obvious when objects are extremely tiny. It is worth noticing that NWD-based DetectoRS achieves state-of-the-art performance (20.8% AP) on AI-TOD. Some visualization results using IoU-based detector (first row) and NWD-based detector (second row) on AI-TOD dataset are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We can observe that NWD can significantly reduce false negative (FN) compared with IoU.</p><p>Main results on Visdrone. Besides AI-TOD, we use VisDrone2019 <ref type="bibr" target="#b3">[4]</ref> which contains many tiny objects with different scenarios to verify the generalization of NWD-based detectors. The results are shown in Tab. 5. It can be seen that NWD-based anchor-based detectors all achieve considerable improvements over their baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we observe that IoU-based metrics is sensitive to the location deviation of tiny objects, which drastically deteriorates the tiny object detection performance. To handle this problem, we propose a new metric dubbed Normalized Wasserstein Distance (NWD) to measure the similarity between bounding boxes for tiny objects. Based on that, we further present a novel NWD-based tiny object detector by embedding NWD into label assignment, non-maximum suppression, and loss function of anchor-based detectors to replace original IoU metric. Experimental results show that our proposed method can improve the tiny object detection performance by a large margin and achieve state-of-the-art on AI-TOD dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A comparison between IoU-Deviation Curve and NWD-Deviation Curve in two different scenarios. The abscissa value denotes the number of pixels deviation between the center points of A and B, the ordinate value denotes the corresponding metric value. Note that the location of bounding box can only change discretely, the Value-Deviation curve is presented in the form of scatter diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of detection results using IoU-based detector (first row) and NWD-based detector (second row) of AI-TOD dataset. The green, blue and red boxes denote true positive (TP), false positive (FP) and false negative (FN) predictions, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different metrics in label assignment, NMS and loss function. AP t AP AP 0.5 AP t AP AP 0.5 AP t</figDesc><table><row><cell cols="3">Assigning AP AP 0.5 DIoU Metric 5.4 11.3</cell><cell>3.9</cell><cell>11.2</cell><cell>NMS 26.8</cell><cell>7.8</cell><cell>10.7</cell><cell>Loss 25.1</cell><cell>6.7</cell></row><row><cell>CIoU</cell><cell>5.9</cell><cell>12.5</cell><cell>4.4</cell><cell>10.9</cell><cell>25.7</cell><cell>7.2</cell><cell>10.6</cell><cell>24.9</cell><cell>6.8</cell></row><row><cell cols="2">GIoU 11.0</cell><cell>26.5</cell><cell>7.7</cell><cell>11.5</cell><cell>26.5</cell><cell>7.6</cell><cell>10.9</cell><cell>25.1</cell><cell>6.9</cell></row><row><cell>IoU</cell><cell>11.1</cell><cell>26.5</cell><cell>7.8</cell><cell>11.1</cell><cell>26.5</cell><cell>7.8</cell><cell>10.8</cell><cell>25.3</cell><cell>7.1</cell></row><row><cell cols="2">NWD 16.1</cell><cell>43.8</cell><cell cols="2">17.4 11.9</cell><cell>27.5</cell><cell>8.0</cell><cell>12.1</cell><cell>27.5</cell><cell>8.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation experiments when NWD is applied to single module.</figDesc><table><row><cell>Method</cell><cell>Assigning RPN R-CNN RPN R-CNN RPN R-CNN NMS Loss</cell><cell>AP</cell></row><row><cell>Baseline</cell><cell></cell><cell>11.1</cell></row><row><cell></cell><cell></cell><cell>17.3</cell></row><row><cell></cell><cell></cell><cell>14.3</cell></row><row><cell>NWD</cell><cell></cell><cell>11.9 10.8</cell></row><row><cell></cell><cell></cell><cell>12.1</cell></row><row><cell></cell><cell></cell><cell>12.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments when NWD is applied to multiple modules.</figDesc><table><row><cell>Method</cell><cell cols="2">Assigning RPN R-CNN RPN R-CNN RPN R-CNN 12 epochs 24 epochs NMS Loss AP AP</cell></row><row><cell>Baseline</cell><cell>11.1</cell><cell>12.6</cell></row><row><cell></cell><cell>17.8</cell><cell>19.7</cell></row><row><cell>NWD</cell><cell>13.8</cell><cell>16.8</cell></row><row><cell></cell><cell>15.2</cell><cell>18.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison of the baselines and NWD (with *) on AI-TOD test set.MethodBackbone AP AP 0.5 AP 0.75 AP vt AP t AP s AP m</figDesc><table><row><cell>SSD-512 [18]</cell><cell cols="2">ResNet-50 7.0</cell><cell>21.7</cell><cell>2.8</cell><cell>1.0</cell><cell cols="3">4.7 11.5 13.5</cell></row><row><cell>TridentNet [13]</cell><cell cols="2">ResNet-50 7.5</cell><cell>20.9</cell><cell>3.6</cell><cell>1.0</cell><cell cols="3">5.8 12.6 14.0</cell></row><row><cell>FoveaBox [11]</cell><cell cols="2">ResNet-50 8.1</cell><cell>19.8</cell><cell>5.1</cell><cell>0.9</cell><cell cols="3">5.8 13.4 15.9</cell></row><row><cell>PepPonits [33]</cell><cell cols="2">ResNet-50 9.2</cell><cell>23.6</cell><cell>5.3</cell><cell>2.5</cell><cell cols="3">9.2 12.9 14.4</cell></row><row><cell>FCOS [27]</cell><cell cols="2">ResNet-50 9.8</cell><cell>24.1</cell><cell>5.9</cell><cell>1.4</cell><cell cols="3">8.0 15.1 17.4</cell></row><row><cell>CenterNet [39]</cell><cell>DLA-34</cell><cell>13.4</cell><cell>39.2</cell><cell>5.0</cell><cell>3.8</cell><cell cols="3">12.1 17.7 18.9</cell></row><row><cell>M-CenterNet [29]</cell><cell>DLA-34</cell><cell>14.5</cell><cell>40.7</cell><cell>6.4</cell><cell>6.1</cell><cell cols="3">15.0 19.4 20.4</cell></row><row><cell>RetinaNet [15]</cell><cell cols="2">ResNet-50 4.7</cell><cell>13.6</cell><cell>2.1</cell><cell>2.0</cell><cell>5.4</cell><cell>6.3</cell><cell>7.6</cell></row><row><cell>RetinaNet*</cell><cell cols="2">ResNet-50 9.2</cell><cell>24.9</cell><cell>5.0</cell><cell>3.2</cell><cell cols="3">10.0 13.1 16.9</cell></row><row><cell>ATSS [36]</cell><cell cols="2">ResNet-50 12.8</cell><cell>30.6</cell><cell>8.5</cell><cell>1.9</cell><cell cols="3">11.6 19.5 29.2</cell></row><row><cell>ATSS*</cell><cell cols="2">ResNet-50 13.5</cell><cell>33.2</cell><cell>8.6</cell><cell>2.1</cell><cell cols="3">11.1 20.9 31.9</cell></row><row><cell>Faster R-CNN [21]</cell><cell cols="2">ResNet-50 11.1</cell><cell>26.3</cell><cell>7.6</cell><cell>0.0</cell><cell cols="3">7.2 23.3 33.6</cell></row><row><cell>Faster R-CNN*</cell><cell cols="2">ResNet-50 17.8</cell><cell>43.8</cell><cell>11.0</cell><cell>2.5</cell><cell cols="3">17.0 26.1 34.3</cell></row><row><cell cols="3">Cascade R-CNN [2] ResNet-50 13.8</cell><cell>30.8</cell><cell>10.5</cell><cell>0.0</cell><cell cols="3">10.6 25.5 36.6</cell></row><row><cell>Cascade R-CNN*</cell><cell cols="2">ResNet-50 18.7</cell><cell>44.2</cell><cell>12.9</cell><cell>3.6</cell><cell cols="3">17.4 26.5 35.6</cell></row><row><cell>DetectoRS [20]</cell><cell cols="2">ResNet-50 14.8</cell><cell>32.8</cell><cell>11.4</cell><cell>0.0</cell><cell cols="3">10.8 28.3 38.0</cell></row><row><cell>DetectoRS*</cell><cell cols="2">ResNet-50 20.8</cell><cell>49.3</cell><cell>14.3</cell><cell>6.4</cell><cell cols="3">19.7 29.6 38.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Quantitative comparison of the baselines and NWD (with *) on VisDrone2019 val set. Method Faster R-CNN Faster R-CNN* Cascade R-CNN Cascade R-CNN* AP</figDesc><table><row><cell>0.5</cell><cell>38.0</cell><cell>38.5</cell><cell>38.5</cell><cell>40.3</cell></row><row><cell>AP vt</cell><cell>0.1</cell><cell>3.8</cell><cell>0.5</cell><cell>2.9</cell></row><row><cell>AP t</cell><cell>6.2</cell><cell>10.2</cell><cell>6.8</cell><cell>11.1</cell></row><row><cell>AP s</cell><cell>20.0</cell><cell>21.4</cell><cell>21.4</cell><cell>22.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sod-mtgan: Small object detection via multi-task generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">CoRR, abs/</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visdrone-det2019: The vision meets drone object detection in image challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ota</surname></persName>
		</author>
		<title level="m">Optimal transport assignment for object detection. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="355" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">San: Learning relationship between convolutional features for multi-scale object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bong-Nam</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daijin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7389" to="7398" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual generative adversarial networks for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6054" to="6063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Better to follow, follow to be better: Towards precise supervision of feature super-resolution for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyug</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonhee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhwan</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9725" to="9734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02334</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9310" to="9320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recent advances in small object detection based on deep learning: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiquan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">103910</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tiny object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3791" to="3798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning center probability map for detecting objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4307" to="4323" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking rotated object detection with gaussian wasserstein distance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scale match for tiny person detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshops on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1257" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">M2det: A single-shot object detector based on multi-level feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9259" to="9266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distance-iou loss: Faster and better learning for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongguang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwei</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12993" to="13000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points. CoRR, abs</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
