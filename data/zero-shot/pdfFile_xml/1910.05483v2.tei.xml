<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frustum VoxNet for 3D object detection from RGB-D or Depth images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Shen</surname></persName>
							<email>xshen@gradcenter.cuny.edu</email>
							<affiliation key="aff0">
								<orgName type="department">The Graduate Center</orgName>
								<orgName type="institution">CUNY New York City</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Ioannis Stamos Hunter College &amp; The Graduate Center</orgName>
								<orgName type="institution">CUNY</orgName>
								<address>
									<settlement>New York City</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Frustum VoxNet for 3D object detection from RGB-D or Depth images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, there have been a plethora of classification and detection systems from RGB as well as 3D images. In this work, we describe a new 3D object detection system from an RGB-D or depth-only point cloud. Our system first detects objects in 2D (either RGB, or pseudo-RGB constructed from depth). The next step is to detect 3D objects within the 3D frustums these 2D detections define. This is achieved by voxelizing parts of the frustums (since frustums can be really large), instead of using the whole frustums as done in earlier work. The main novelty of our system has to do with determining which parts (3D proposals) of the frustums to voxelize, thus allowing us to provide high resolution representations around the objects of interest. It also allows our system to have reduced memory requirements. These 3D proposals are fed to an efficient ResNet-based 3D Fully Convolutional Network (FCN). Our 3D detection system is fast, and can be integrated into a robotics platform. With respect to systems that do not perform voxelization (such as PointNet), our methods can operate without the requirement of subsampling of the datasets. We have also introduced a pipelining approach that further improves the efficiency of our system. Results on SUN RGB-D dataset show that our system, which is based on a small network, can process 20 frames per second with comparable detection results to the state-of-the-art [16], achieving a 2? speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1910.05483v2 [cs.CV] 6 Feb 2020</head><p>IoI 3D = IoI XY * IoI Z</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Classification and object detection are significant problems in the fields of computer vision and robotics. 2D object detection systems from RGB images have been significantly improved in recent years due to the emergence of deep neural networks and large labeled image datasets. For applications related to robotics though, such as autonomous navigation, grasping, etc., a 2D object detection system is not adequate. Thus 3D object detection systems have been developed, with input coming from RGB-D or depth-only sensors. In this paper we describe a new 3D object detection A DHS image is a pseudo-RGB image generated by a depth image (see text). Bottom: The final 3D detected objects from the associated 3D range image. The 3D detection not only provides an amodal bounding box but also an orientation. The red point is the center of the bounding box and the green one is the front center. The detected 2D bounding boxes from either and RGB or DHS image, generate 3D frustums (which are prisms having as apex the sensor location and extend through the 2D bounding boxes to the 3D space). They are then fed to our Frustum VoxNet network, which produces the 3D detections. system that incorporates mature 2D object detection methods as a first step. The 2D detector can run on an input RGB image, or pseuso-RGB image generated from a 3D point cloud. That 2D detection generates a 3D frustum (defined by the sensor and the 2D detected bounding box) where a search for a 3D object is performed. Our main contribution is the 3D object detection within such as frustum. Our method involves 3D voxelization, not of the whole frustum, but of a learned part of it. That allows for a higher resolution voxelization, lower memory requirements, and a more efficient detection. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the overview of our system. In the upper left we see a 2D RGB image, along with the 2D detected bounded boxes (a chair and a desk). On the upper right we see a 2D pseudo-RGB image that was generated from the associated 3D range image (see <ref type="bibr" target="#b27">[28]</ref>), along with similarly detected 2D bounded boxes. We call this pseudo-RGB image a DHS image, where D stands for Depth, H for Height, and S for Signed angle. The depth is a normalized distance of the associated 3D point, height is a normalized height of the 3D point, and the signed angle is a normalized approximation of the normal at the 3D point (see <ref type="bibr" target="#b27">[28]</ref>). We can apply traditional 2D detectors on this pseudo-RGB image, making our method applicable even when no RGB information is available. 3D frustums are then extracted from these 2D detections. A 3D frustum is a prism having as apex the sensor location and extending through the 2D bounding boxes into the 3D space. Learned parts of the 3D frustum are being voxelized. These voxelizations are fed to Frustum VoxNet, which is a 3D Fully Convolutional Neural Network (FCN).</p><p>The key contributions of our work can be summarized as follows:</p><p>? We demonstrate the power of using a 3D FCN approach based on volumetric data to achieve accurate 3D detection results efficiently.</p><p>? We provide a novel method for learning the parts of 3D space to voxelize. This allow us to provide high resolution representations around the objects of interest. It also allows our system to have reduced memory requirements and leads to its efficiency.</p><p>? Compared to systems that do not perform voxelization (such as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>), our methods can operate without the requirement of subsampling the datasets. Also, our approach is more efficient and can be used in robotics applications.</p><p>? Compared to systems that do voxelize (such as <ref type="bibr" target="#b28">[29]</ref>), our system does not voxelize the whole space, and thus allows a higher-resolution object representation.</p><p>? We compare the 3D detection performance of using different input channels (RGB or DHS).</p><p>? We provide a more efficient variation of our method that involves pipelining, geared to robotics applications.</p><p>? The parameters of our network are much smaller than leading methods. That results in faster inference time.</p><p>We start by reviewing related work and then proceed with the description of our 3D detection system along with our experimental results. Since our final goal is indoor robotic navigation, our current system has been evaluated based on an indoor SUN-RGBD dataset <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>2D methods RGB-based approaches can be summarized as two-stage frameworks (proposal and detection stages) and one-stage frameworks (proposal and detection in parallel). Generally speaking, two-stage methods such as R-CNN <ref type="bibr" target="#b3">[4]</ref>, Fast RCNN <ref type="bibr" target="#b2">[3]</ref>, Faster RCNN <ref type="bibr" target="#b19">[20]</ref>, FPN <ref type="bibr" target="#b10">[11]</ref> and mask R-CNN <ref type="bibr" target="#b5">[6]</ref> can achieve a better detection performance while one-stage systems such as YOLO <ref type="bibr" target="#b17">[18]</ref>, YOLO9000 <ref type="bibr" target="#b18">[19]</ref> and RetinaNet <ref type="bibr" target="#b11">[12]</ref> are faster at the cost of reduced accuracy. For deep learning based systems, as the size of network is increased, larger datasets are required. Labeled datasets such as PASCAL VOC dataset <ref type="bibr" target="#b1">[2]</ref> and COCO (Common Objects in Context) <ref type="bibr" target="#b12">[13]</ref> have played important roles in the continuous improvement of 2D detection systems.</p><p>3D methods Compared with detection based on 2D images, the detection based on 3D data is more challenging due to several reasons <ref type="bibr" target="#b21">[22]</ref>: 1) Data representation itself is more complicated. 3D images can be represented by point clouds, meshes, or volumes, while 2D images have pixel grid representations. 2) Due to the extra dimension, there are increased computation and memory resource requirements. 3) 3D data is generally sparser and of lower resolution compared with the dense 2D images, making 3D objects more difficult to identify. Finally, 4) large sized labeled datasets, which are extremely important for supervised based algorithms, are still inferior compared with well-built 2D datasets. Below we summarize the basic approaches.</p><p>Project 3D data to 2D and then employ 2D methods There are different ways to project 3D data to 2D features. HHA was proposed in <ref type="bibr" target="#b4">[5]</ref> where the depth image is encoded with three channels: Horizontal disparity, Height above ground, and the Angle of each pixels local surface normal with gravity direction. The signed angle feature described in <ref type="bibr" target="#b25">[26]</ref> measures the elevation of the vector formed by two consecutive points and indicates the convexity or concavity of three consecutive points. Input features converted from depth images of normalized depth(D), normalized relative height(H), angle with up-axis(A), signed angle(S), and missing mask(M) were used in <ref type="bibr" target="#b27">[28]</ref>. We are using DHS in this work to project 3D depth image to 2D since as shown in <ref type="bibr" target="#b27">[28]</ref> adding more channels did not affect classification accuracy significantly. Keeping the number of total channels to three, allow us to use networks with pre-trained weights for starting our training.</p><p>2D-Driven 3D Object Detection from RGB-D Data Our proposed framework is mainly inspired by 2D-driven 3D object detection approaches as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>. First a 2D detector is used to generate 2D detections. The differences of our work with <ref type="bibr" target="#b9">[10]</ref> are: 1) the 2D detector in <ref type="bibr" target="#b9">[10]</ref> is only based on RGB images and our proposed system explores both RGB-D and Depth only data. 2) 3D detection in <ref type="bibr" target="#b9">[10]</ref> uses a MLP regressor to regress the object boundaries based on histograms of points along x, y, and z directions. Converting raw point clouds to histograms results in a loss of information. The main differences of our system to Frustum PointNets <ref type="bibr" target="#b15">[16]</ref> are the following: 1) in the 2D detection part, Frustum PointNets is based on RGB inputs, while our system can support both RGB-D and depth-only sensing. 2) in the 3D detection part, our system is using voxelized data, while Frustum PointNets is consuming raw point clouds via PointNet <ref type="bibr" target="#b16">[17]</ref>. PointNet uses a fully connected neural network and max pooling, so it cannot support convolution/deconvolution operations well. We believe 3D convolution/deconvolution can play important roles in both 3D semantic segmentation and object detection. 3) Point-Net's computation complexity is increased if more points are available as the framework's input is N ? K where N is the number of points and K is the number of channels. 4) Random sampling is required in PointNet, but is not needed in our voxelization approach.</p><p>A recent method <ref type="bibr" target="#b14">[15]</ref> that is based on PointNet and Hough Voting, achieves improved detection results without the use of RGB images. Our method is still more efficient in inference time, and thus more appropriate for robotics application. Also, our approach does not need to subsample the 3D point cloud as required by <ref type="bibr" target="#b14">[15]</ref>.</p><p>3D CNNs VoxelNet <ref type="bibr" target="#b28">[29]</ref> uses 3D LiDAR data to detect 3D objects based on the KITTI outdoor dataset, and utilizes bird's eye view (BEV) features (such as MV3D <ref type="bibr" target="#b0">[1]</ref> and AVOD <ref type="bibr" target="#b8">[9]</ref>)). The use of BEV is not helpful in indoor applications. Also, the use of the whole range image for voxelization lowers the resolution (and therefore the scale) of the objects of interest. Early influential 3D detection systems used two-stage approaches. The first stage generates proposals, while the second stage performs 3D detection. DeepSliding Shape <ref type="bibr" target="#b23">[24]</ref> detects 3D objects based on the SUNRGB-D dataset and it uses directional Truncated Signed Distance Function (TSDF) to encode 3D shapes. The 3D space is divided into 3D voxels and the value in each voxel is defined to be the shortest distance between the voxel center and the surface from the input depth map. A fully convolutional 3D network extracts 3D proposals at two scales corresponding to large size objects and small size objects. For the final 3D detection, this method fuses the 3D voxel data and RGB image data by using 3D and 2D CNNs. Our approach, one the other hand, first focuses on the frustum to voxelize, and then selects the part to be voxelized based on training. That allows us to achieve higher resolution around the objects of interest.</p><p>We refer readers to <ref type="bibr" target="#b21">[22]</ref> for latest, comprehensive comparisons of different 3D detection systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>We are focusing on the indoor SUN RGBD dataset <ref type="bibr" target="#b22">[23]</ref>. SUN RGBD dataset splits the data into a training set which contains 5285 images and a testing set which contains 5050 images. For the training set, it further splits into a training only, which contains 2666 images and a validation set, which contains 2619 images. Similar to <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10]</ref>, we are training our model based on the training only set and evaluate our system based on the validation set. We call the only training dataset as train2666 in the future description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Frustum VoxNet System Overview</head><p>First, 2D detections on RGB or DHS image generate 2D bounding boxes of objects. The 2D detections generate 3D frustums (defined by the sensor and the 2D detected bounding box) where a search for a 3D object is performed. For each such frustum we know the class of the object to be detected by the 2D detection. Our system accurately localizes the amodal 3D bounding box and the orientation of the detected 3D object. To achieve this, we perform 3D voxelization, not of the whole frustum, but a learned part of it. That allows for a higher resolution voxelization, lower memory requirements, and a more efficient detection. We explain first how we decide which part of the frustum to use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Frustum Voxelization</head><p>Given a 3D frustum (defined as a 3D prism from the sensor and the 2D detected bounding box into the 3D space), our goal is to voxelize only a part of it. We define that part as axis-aligned 3D bounding boxes enclosed in the frustum. We call that bounding box a 3D Cropped Box (3DCB for short). Given a specific object class (for instance a table), an ideal 3DCB will be big enough to contain all the 3D points belonging to the object, but also small enough to achieve high resolution voxelization. In order to quantify the ability of a given 3DCB to tightly contain a given 3D object, we define the metric 3D Intersection over Itself (IoI). Suppose the object of interest lies in a bounding box 3DBBOX. Then the IoI of the 3DBBOX wrt to a given 3DCB is defined as the volume of intersection of the 3D bounding box with the 3DCB over the volume of the 3D bounding box itself. Therefore an IoI of 1.0 means that the 3DCB is perfectly enclosing the object in 3DBBOX, while as this number tends to 0.0 more and more empty space is included in the 3DCB.</p><p>The formula for 3D IoI is:</p><formula xml:id="formula_0">IoI 3D = volume 3DBBOX ? volume 3DCB</formula><p>volume 3DBBOX From the definition, it is trivial to show that: where IoI XY is the IoI in the XY plane and IoI Z is the IoI along the Z axis.</p><formula xml:id="formula_1">IoI XY = area 3DBBOX XY ? area 3DCB XY area 3DBBOX XY IoI Z = length 3DBBOX Z ? length 3DCB Z length 3DBBOX Z</formula><p>3DBBOX XY and 3DCB XY are 2D projections of 3D bounding box and 3DCB onto the XY plane. 3DBBOX Z and 3DCB Z are 1D projections of 3D bounding box and 3DCB onto the Z axis. We use this metric to choose the optimal 3DCB size. A 2D example in <ref type="figure">Figure 2</ref> is used to show the difference between IoI and IoU (Intersection over Union). From this example, box A is totally contained in 2DCB(XY plane projection of a 3DCB) while only half of box B is covered by 2DCB. If we use 2D IoU, we will get 0.11 for box A with 2DCB and 0.18 for box B with 2DCB. Generating 3DCBs using an IoI metric During training, given a ground truth 2D bounding box of an object of a given class (for example table) and given the ground truth 3D bounding box of the same object, we would like to calculate the optimal 3DCB box. The 3DCB is represented by its center, and width, depth, and height. We are adding the constraint that width and depth are the same. This makes sure that the object can freely rotate within the 3DCB along the vertical axis. We proceed by equally dividing the 2D bounding box along the Row and Column into F R ? F C 2D boxes. Then we have F R ? F C subFfrustums. We will generate F R ? F C candidate centers of 3DCBs in that case. The center of each 3DCB is the centroid of the respective frustum. One example of 3 ? 3 subfrustums of a desk is shown in <ref type="figure">Figure 3</ref>. If we set F R = F C = 1, then there is only one 3D frustum to consider (and therefore one 3DCB center). Our goal is to calculate the optimal sizes of respective 3DCBs for each object category.</p><p>A ground truth 3D bounding box will be recalled (i.e. enclosed into the 3DCB) if the 3D IoI of this box is greater than a threshold. Formally, we define this recall as recall volume :</p><formula xml:id="formula_2">recall volume = |3DCB positive | |3DCB|</formula><p>where |3DCB positive | is the cardinality of positive 3DCBs and |3DCB| is the cardinality of all 3DCBs. A 3DCB is positive when IoI 3D = IoI XY * IoI Z ? threshold.</p><p>To make the parameter setting simple, we are exploring the recall of XY plane and Z axis separately. Similar to recall volume , recall XY and recall Z are defined as:</p><formula xml:id="formula_3">recall XY = |3DCB positive XY | |3DCB| , recall Z = |3DCB positive Z | |3DCB| , where |3DCB positive XY | is the cardinality of positive 3DCBs in XY plane, |3DCB positive Z</formula><p>| is the cardinality of positive 3DCBs in Z axis and |3DCB| is the cardinality of all 3DCBs. A 3DCB is positive in XY plane when IoI XY ? threshold XY and a 3DCB is positive in Z axis when IoI Z ? threshold Z . Although, we can NOT naively have recall volume = recall XY * recall Z , we have a nice inequality to guarantee a lower bound of recall volume :</p><formula xml:id="formula_4">recall volume ? max(0, recall XY + recall Z ? 1) (1)</formula><p>The proof of this inequality is given in the appendix.</p><p>Both of threshold XY and threshold Z are set as 0.90. We are generating both the average center and median center from subfrustums and pick up the best one from these F R ? F C candidates to calculate the recall. The average recall based on different setups of width/depth and height are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. From the results, we can observe: 1) the performance of the average center based 3DCB is better especially when 1 ? 1 subfrustums are used compared with the median center. The reason for this might be the range of indoor depth sensor is limited and outliers will not have too much influence to the results. 2) The 3DCB generated from 1 ? 1 is better than 3 ? 3 and 5 ? 5 ones. Based on these observations, we are choosing both 1 ? 1 and 3 ? 3 during training to generate more samples and make the training robust to the inaccurate bounding box predictions. During inference, 1 ? 1 subfrustum  . IoI XY and IoI Z with the widths/depths and heights. 3DCB are generated from average/median center based on F R ? F C subfrustums with different widths/depths and heights. In this plot, average/median m n corresponds to recall based on average/median center in m ? n subfrustums.</p><p>based 3DCB is used to speed up and get better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Double Frustum Method</head><p>To increase the accuracy of the center calculations, we developed a double Frustum framework. We use a smaller 2D bounding box to generate a smaller frustum for the calculation of the 3DCB center. The estimated center should now be more accurate since it will concentrate on the central part of the object and thus will avoid the use of other background objects. A 3DCB is then selected from a larger frustum in order to contain background context points and possible false negative points. The larger frustum is generated from a larger 2D bounding box. During training, we generate large frustum by randomly increasing the 2D bounding box width and height by 0% to 15% independently. For the small frustum, we randomly decrease the 2D bounding box width and height by 0% to 10% independently. During inference, the large frustum is generated by increasing the 2D bounding box width and height by 5%. Original 2D detection bounding boxes are used to calculate the 3DCB center.</p><p>Multiple Scale Networks In <ref type="bibr" target="#b23">[24]</ref>, two scales network were used for different categories concerning the 3D physical size. We are using 4 scales networks to voxelize the 3D objects corresponding to the average physical size of average height, maximum of average width and depth. The mapping of 3D object categories to different scales is shown in <ref type="table" target="#tab_1">Table 1</ref>. We are calculating the recall XY and recall Z for different objects with the different setups for width/depth and heights. The curves of recall XY with width/depth and recall Z with height are plotted for four classes based on 3 ? 3 subfrustums (sofa is from large short scale, chair is from medium short scale toilet is from small short scale and bookshelf is from median tall scale) are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. From these curves, we can find out that   medium tall scale category needs greater height and both the large short and medium short categories need more width/depth. We are selecting the minimum width/depth and height which can guarantee all objects within that scale network can meet the requirements of recall XY ? 0.90 and recall Z ? 0.95. This is based on 3 ? 3 subfrustums. From the equation 1, we can have the lower bound of the recall volume of 0.85. Although 0.85 is not high enough, when based on 1 ? 1 subfrustums, the lower bound of the recall volume can achieve 0.94 as recall XY ? 0.95 and recall Z ? 0.99 for 3DCBs generating from 1 ? 1. Since we are using both 3DCBs from 1 ? 1 and 3 ? 3 subfrustums, the recall is good enough to support the training. The physical sizes(width/depth/height) of 4 scale networks are shown in <ref type="table" target="#tab_2">Table 2</ref> based on the principles described above. 3DCB are further voxalized(counting the number of cloud points within each voxel) into a 3D tensor with the shape of W ? D ? H. The W ? D ? H for each scale network are selected to make it having a better resolution as compared with <ref type="bibr" target="#b23">[24]</ref>. The comparison of physical size, resolution, tensor shape of the RPN and detection networks of <ref type="bibr" target="#b23">[24]</ref> and ours are also shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D Object Detection</head><p>3D Bounding Box Encoding Similar to <ref type="bibr" target="#b23">[24]</ref>, we are using the orientation, center, width, depth and height to encode the 3D bounding box.</p><p>Network architecture We are using 3D FCN networks to build the 3D detection network by adapting the network structure of ResNet <ref type="bibr" target="#b6">[7]</ref> and Fully Convolutional Network(FCN) <ref type="bibr" target="#b13">[14]</ref>. We propose a fast 6 layer fully convolutional 3D CNN model as shown in <ref type="figure">Figure 6</ref>. <ref type="figure">Figure 6</ref>. ResnetFCN6 architecture (used for large short scale). Every 3D CNN layer will be followed by a dropout layer. The tensor shape shown here is the output shape of each block. It provides the (width, depth, height, channel) information of the network. The rest three scale networks have the same structure with different input size as shown in <ref type="table" target="#tab_2">Table 2</ref>. The architecture of ResnetFCN35 will be provided in the Supplementary Material.</p><p>Inputs of our networks are voxelized images. Our network will have C * 7 outputs, where C is the number of classes within the corresponding scale network, and 7 is the orientation, center xyz and size(width/depth/height) predictions. The 2D prediction info is implicitly encoded in the system since the prediction is based on each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>We are generating loss function for detection by adjusting the loss function from YOLO9000 <ref type="bibr" target="#b18">[19]</ref>. Similar to <ref type="bibr" target="#b18">[19]</ref>, we use simple L2 distance instead of KullbackLeibler divergence to evaluate the difference of predited category probability distributions and the ground truth distributions. For the regression part, for centers, we normalize the x, y, z values to 0 and 1 and then use a sigmoid function to make the prediction. For width(w), depth(d) and height(h), we use anchor to support the prediction. For each category, we set the anchor as the average value of the train2666 samples for objects within this category. The ratio of the bounding box to the related anchors are used to drive the network to make the correct prediction. The formal definition of the loss is given in the formulas below. </p><formula xml:id="formula_5">L 3D detection = ? 1 L orientation + ? 2 L xyz + ? 3 L wdh Where L xyz = L x + L y + L z , L wdh = L w + L d + L h , L x = (x ? x ) 2 , L y = (y ? y ) 2 , L z = (z ? z ) 2 , L w = (log w aw ? log w aw ) 2 , L d = (log d a d ? log d a d ) 2 , L h = (log h a h ?log h a h ) 2 . a w , a d ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training Process</head><p>For the 2D detection, we are using ResNet <ref type="bibr" target="#b6">[7]</ref> 101 layer as the backbone and using the feature pyramid layers proposed by <ref type="bibr" target="#b10">[11]</ref> which is based on Faster RCNN <ref type="bibr" target="#b19">[20]</ref> approach. The loss is the same as <ref type="bibr" target="#b10">[11]</ref>. For the 2D detection, the network is pretrained on COCO dataset. Then it is retrained on SUN-RGBD dataset based on RGB or DHS images. Although, the DHS images are different to the RGB images, we find the pretrained weights can still speed up the whole training process and improve the detection results. Data is augmented by adding gaussian blur, random cropping and image translating up to 10% of the original images.</p><p>For the 3D detection, we use the stochastic gradient descent(SGD) with learning rate of 0.01 and a scheduled decay of 0.00001. For regulation we use batch normalization <ref type="bibr" target="#b7">[8]</ref>. The cloud points are randomly rotated around z-axis and jittered during the voxelization process before feeding them to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Efficiency boost by Pipelining</head><p>Pipelining instructions is a technology used in central processing units to speed up the computing. An instruction pipeline reads an instruction from the memory while previous instructions are being executed in other steps of the pipeline. Thus multiple instructions can be executed simultaneously. Pipelining can be perfectly used in our system as we have two stages, one is 2D detection and one is 3D detection. In the 3D detection, instead of using the 2D detection of frame n, we can use the 2D detection results of frame n-1 and generate frustums based on that. By using pipelining, our system can be sped up from t 2D + t 3D to max(t 2D , t 3D ), where t 2D and t 3D are the 2D and 3D detection time, respectively. The disadvantage of using pipelining is frustums generated from the previous 2D image maybe not accurate under the fast movement of the sensor of an object of interest. However, our system will not suffer significantly as our results show, due to robustness on frustum location. We use multiple candidates with different centers during training to make it robust. Meanwhile, the double frustum method used in our system makes our 3D detections robust to slightly moved 2D detections. The illustration of the pipelining method is shown in <ref type="figure" target="#fig_6">figure 7</ref>. By using pipelining, our system can be sped up to 48 ms (this is about 2.5? speedup to the state-of-the-art <ref type="bibr" target="#b15">[16]</ref>) when use YOLO v3 and ResNetFCN6. It can achieve 21 frames per second which can well support real time 3D object detection. <ref type="bibr" target="#b7">[8]</ref>, Group Normalization <ref type="bibr" target="#b26">[27]</ref> and Dropout <ref type="bibr" target="#b24">[25]</ref> Dropout is a powerful tool to prevent neural networks from overfitting. Batch Normalization(BN) <ref type="bibr" target="#b7">[8]</ref> is another method we can use to speed up the training and prevent overfitting. However, BN performs better when the batch size is large enough. Since Frustum VoxNet is using 3D CNNs, large batch sizes are not well supported when single GPU is used. Some new technologies are introduced to address the small batch size problem such as Group Normalization(GN) <ref type="bibr" target="#b26">[27]</ref>. We explore the performance of different combinations of these methods by evaluating the performance of center and orientation predictions. Results are shown in <ref type="figure" target="#fig_7">Figure 8</ref>. We do not use BN as our batch size is small and the using of BN will lead to inconsistencies between training and inference. Although when using the GN, there are no inconsistencies between training and inference, the performance of center prediction is worse compared with not using any normalization. Therefore, our final model does not use any normalization. However, dropout is used in our final model as the performance of center prediction is improved.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Effects of Batch Normalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Evaluation of the whole system</head><p>First we evaluate the 2D detector in <ref type="table" target="#tab_6">Table 3</ref>. The evaluation is based on the standard mAP metric with IoU threshold of 0.5. Comparing our RGB-based and depth-based (DHS image) 2D detection, we see that in most cases RGB performs better, but the depth-based 2D detector is competitive. For few classes such as bathtubs, DHS results are slightly better. The reason might be that some classes such as bathtubs have special geometric shapes and they are easier to be detected by depth sensors. Comparing with stateof-the-art methods, our 2D detector performs better in some categories, and we are also introducing new categories. We are on par with most other categories, except for bathtub, desk, and bookshelf.</p><p>Full 3D detection results are shown in <ref type="table">Table 4</ref>. We provide various variations in our system. First two variations include RGB 2D detector, and the last two include depth only (DHS) 2D detector. In all cases, we use a FPN for the 2D detector. For the 3D detection we have experimented with ResNetFCN6 and ResNetFCN35. As in the 2D case, our 3D detector is on par in most categories with the stateof-the-art, and we have also incorporated more classes. Looking at the computational performance of the 3D detector only, we see that our implementation using ResNet-FCN6 provides significant improvements on inference time. Since the architecture is modular (i.e. we can swap out our 2D detector with one from the reported as state-of-the-art),   <ref type="table">Table 4</ref>: 3D detection results on SUN-RGBD validation set. Evaluation metric is average precision with IoU threshold of 0.25 as proposed by <ref type="bibr" target="#b22">[23]</ref>. Both COG <ref type="bibr" target="#b20">[21]</ref> and 2D-driven <ref type="bibr" target="#b9">[10]</ref> are using room layout context to boost performance while ours, DSS <ref type="bibr" target="#b23">[24]</ref> and Frustum PointNets <ref type="bibr" target="#b15">[16]</ref> are not. Frustum PointNets <ref type="bibr" target="#b15">[16]</ref> is using the 3D segmentation information to train the network to boost the 3D detection, while our system and DSS <ref type="bibr" target="#b23">[24]</ref> are not.</p><p>we see that our approach can lead to significant efficiency improvements, without a significant drop in detection accuracy. That will lead to a system geared to real-time robotics applications.</p><p>We have also evaluated the efficiency and accuracy of our system when a very fast 2D detector (Yolo v3) is being used. <ref type="table" target="#tab_8">Table 5</ref> shows the decrease in detection accuracy as expected. Finally <ref type="table" target="#tab_10">Table 6</ref> provides a detailed analysis of multiple network combinations in terms of efficiency, along with the number of parameters to tune. As mentioned before we can achieve faster inference times in 3D detection, and can thus lead to a faster system overall if we swap our 2D detector with the ones reported as state-of-the-art. Using Yolo and pipelining approach, we can provide a significant boost in total efficiency, with accuracy loss though.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion and Future Work</head><p>We presented a 2D-based 3D detection system by using 2D/3D CNNs. Our method can operate in both Depth only and RGB-D sensor modalities. We provide comparable re-  sults to state-of-the-art, but with significantly more efficient 3D detection. This is due to the use of networks with fewer number of parameters than competing methods. It is also due to our ability to voxelize only parts of the 3D frustums. This leads to decreased memory requirements and improved resolution around the objects of interest. In future work we will be integrating segmentation that we believe will further boost the detection accuracy of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgement</head><p>This work was partially supported by NSF Award CNS-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Visualizations of 2D and 3D detection results</head><p>A Visualizations of both 2D and 3D detection results are shown in <ref type="figure" target="#fig_12">Figure 9</ref> and <ref type="figure" target="#fig_0">Figure 10</ref> for both the based on RGB-D system and based on depth image only system. From <ref type="figure" target="#fig_12">Figure 9</ref> we can see that the 3D detection system works well for both the based on RGB-D and based on Depth only systems. The RGB-D based 3D detection system will generate some false positive 3D detections as it has more false positive detection during 2D detection stage. We can also find out that our system can detect objects which were not labeled during the data annotation. In <ref type="figure" target="#fig_0">Figure 10</ref>, in the left two images, our system can successfully detects unlabelled objects. On the right two images of <ref type="figure" target="#fig_0">Figure 10</ref>, we can see we have some false negative detections as there are too few points within the object. Detail explanations are given in captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Proof of equation 1</head><p>Proof. Define the threshold used for positive 3DCB as threshold 3D , and a 3DCB is positive when</p><formula xml:id="formula_6">IoI 3D = IoI XY * IoI Z ? threshold 3D .</formula><p>The recall volume recall XY , recall Z , threshold XY and threshold Z are defined in the main article. We set the threshold 3D = threshold XY * threshold Z . As if threshold XY = 0.9, threshold Z = 2.9, we can get threshold 3D = 0.81.</p><p>A 3DCB with IoI XY = 1.0, IoI Z = 0.82 will be an element of set 3DCB positive XY ? 3DCB nonpositive Z . Also it is a positive 3DCB. From above arguments, we can conclude the following relation: </p><p>From equations 3, 5, we can get:</p><formula xml:id="formula_8">|3DCB positive | ? |3DCB positive XY | ? (|3DCB| ? |3DCB positive Z |) = |3DCB positive XY | + |3DCB positive Z | ? |3DCB|<label>(6)</label></formula><p>From equation 6, we can get:</p><formula xml:id="formula_9">|3DCB positive | |3DCB| ? |3DCB positive XY | + |3DCB positive Z | ? |3DCB| |3DCB|<label>(7)</label></formula><p>Equation 7 can be rewritten as:</p><formula xml:id="formula_10">recall volume ? recall XY + recall Z ? 1<label>(8)</label></formula><p>Since recall volume is supposed to be greater or equal to 0, we get:</p><p>recall volume ? max(0, recall XY + recall Z ? 1) (9)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Evaluate Frustum VoxNet Results based on</head><p>Ground Truth 2D Bounding Box</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1">Evaluation Metrics</head><p>We are using following metrics to evaluate the predictions based on ground truth 2D bounding boxes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.2">Evaluation Results</head><p>We compare the center prediction based on the frustum average center and the prediction from our Frustum VoxNet system. <ref type="table">Table 7</ref> provides the average distance between predicted and ground truth centers by using these two methods.</p><p>As expected, the Frustum VoxNet prediction is better than the average center from frustum. Evaluation results for the performance of Frustum Voxnet based on frustums generated from ground truth bounding boxes are shown in <ref type="table">Table  8</ref>. Histograms of the dot product between predicted orientations and GT orientations for each category are shown in <ref type="figure" target="#fig_0">Figure 11</ref>. Histograms of 3D detection IoU for each category are shown in <ref type="figure" target="#fig_0">Figure 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">ResNetFCN35 network structure</head><p>ResNetFCN35 network structure is shown in <ref type="figure" target="#fig_0">Figure 13</ref>.  <ref type="table">Table 7</ref>: Result comparison predicted between frustum center and predicted center from Frustum VoxNet.  <ref type="table">Table 8</ref>: Detail evaluation results. Frustum VoxNet is evaluated based on SUN-RGBD validation set. Frustums used to finalize detection are generated from ground truth 2D bounding boxes. The 3D IoU threshold used for 3D recall is 0.25. For the first image in the first row, our system can perfectly detect the chair. For the desk, the orientation is off as the frustum generated by the 2D bounding box contains some cloud points from the chair. For the second image, we can see that the based on RGB image system detect more false positive objects in the 2D stage and hence more 3D false positive objects will be detected. For the first image of the second row, our system successfully detect the unlabelled table. For the last image, the sofa's orientation is off as there are too many points are missing for the sofa. <ref type="figure" target="#fig_0">Figure 10</ref>. Visualizations of 2D and 3D detection results part 2. This visualization contains four images. Please read the caption of <ref type="figure" target="#fig_12">Figure  9</ref> to get an explanation about how to understand the visualization. On the left part, our system can successfully detect unlabelled object such as garbage bin and table. On the top right image, our system fails to detect one table in 2D detection stage as it is partially observed. For the last one, one night stand is undetected as it is blocked by bed.  </p><formula xml:id="formula_11">x ? x * y ? y * z ? z * Dxyz</formula><formula xml:id="formula_12">category instance number D x D y D z D xyz D w D d D h D wdh |o * ? o| average 3D IoU</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the whole system. Upper left: RGB image and detected 2D bounding boxes. Upper right: DHS (Depth Height and Signed angle) image, and detected 2D bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>An example of 2DCB with two objects box A and box B. All these boxes are square. A has length 1, B has length 2 and 2DCB has length 3. Half of B is overlapped with 2DCB An example of equally subdividing a whole frustum into 3 ? 3 subfrustums (best viewed in color). In this example, the object is a desk. The upper one shows the 2D bounding box of desk is equally divided into 9 small boxes. From each small box, a subfrustum is generated as shown in the bottom image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4. IoI XY and IoI Z with the widths/depths and heights. 3DCB are generated from average/median center based on F R ? F C subfrustums with different widths/depths and heights. In this plot, average/median m n corresponds to recall based on average/median center in m ? n subfrustums.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>XY plane recall and Z axis recall for bed, chair bookshelf and toilet with the widths/depths and heights based on train2666 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>a h are width/depth/height of anchors. ? 1 , ? 2 , ? 3 are used to balance losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Illustration of using pipelining to speedup the whole detection framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Performance comparison of different combinations on using BN, GN and dropout. "gn w/o dropout" means using GN without dropout. "no bn no gn w/o dropout" means using none. "no bn no gn with dropout" means not using BN/GN, however, the Dropout is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>implies</head><label></label><figDesc>IoI XY ? threshold XY and IoI Z ? threshold Z . We can further get IoI 3D = IoI XY * IoI Z ? threshold XY * threshold Z = threshold 3D , which implies 3DCBs in the set of 3DCB positive XY ? 3DCB positive Z are positive. Meanwhile, we can show from an example that the set of 3DCB positive can possibly be obtained from 3DCB positive XY ? 3DCB nonpositive Z , where 3DCB nonpositive Z is a complement set of 3DCB positive Z :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>From equation 2 ,| ( 3 )</head><label>23</label><figDesc>we can get: |3DCB positive | ? |3DCB positive XY ? 3DCB positive Z We can also rewrite right part of equation 2 as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>D</head><label></label><figDesc>x = |x * ? x|, D y = |y * ? y|, D z = |z * ? z| D w = |w * ? w|, D d = |d * ? d|, D h = |h * ? h| D xyz = (x * ? x) 2 + (y * ? y) * + (z * ? z)| D wdh = (w * ? w) 2 + (d * ? d) * + (h * ? h)|x * , y * , z * are the predicted center and x, y, z are ground truth. w * , d * , h * are the predicted width/depth/height and w, d, h are ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Visualizations of 2D and 3D detection results part 1. This visualization contains four images. For each image, upper left shows 2D detection based on RGB image. Upper right shows the corresponding 3D detection results (light green ones are the 3D ground truth boxes and orange-colored boxes are predictions) based on frustums generated from RGB image 2D detections (to have a better visualization, RGB colors are projected back to the cloud points). Lower left shows 2D detection based on DHS image. Lower right shows the corresponding 3D detection results (light green ones are the 3D ground truth boxes and orange-colored boxes are predictions) based on frustums generated from DHS image 2D detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Histogram of the dot product between predicted orientation and GT orientation. Histograms are not normalized. Histogram of 3D IoU. Histograms are not normalized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>ResNetFCN35 network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Objects are classified into 4 categories based on there average physical size. Voxelization is processed based on each category.</figDesc><table><row><cell>plane @IoI=0.90 Recall of xy</cell><cell>0.00 0.25 0.50 0.75 1.00</cell><cell>1</cell><cell>2</cell><cell>3 Width/Depth(m) of 3DCB 4</cell><cell>5</cell><cell>bed chair bookshelf toilet</cell><cell>6</cell></row><row><cell>axis @IoI=0.90</cell><cell>0.50 0.75 1.00</cell><cell>bed chair bookshelf toilet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall of z</cell><cell>0.00 0.25</cell><cell>0.5</cell><cell>1.0</cell><cell>1.5 Height(m) of 3DCB 2.0</cell><cell>2.5</cell><cell>3.0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>? 2.5 ? 2.5 208 ? 208 ? 100 5.2 ? 6.0 ? 2.5 Detection (bed) 6.7 ? 6.7 ? 3.2 30 ? 30 ? 30 2.0 ? 2.0 ? 0.95 Detection (trash can) 1.0 ? 1.0 ? 1.2 30 ? 30 ? 30 0.3 ? 0.3 ? 0.5 ? 1.6 ? 1.5 198 ? 198 ? 102 0.8 ? 0.8 ? 1.5 medium short 3.2 ? 3.2 ? 1.7 198 ? 198 ? 102 1.6 ? 1.6 ? 1.7 large short 4.8 ? 4.8 ? 2.2 198 ? 198 ? 102 2.4 ? 2.4 ? 2.2 medium tall 2.8 ? 2.8 ? 3.0 134 ? 134 ? 134 2.1 ? 2.1 ? 2.2</figDesc><table><row><cell></cell><cell></cell><cell>3DCB</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">physical size</cell><cell>3DCB</cell><cell>Resolution</cell></row><row><cell>Method</cell><cell>Network</cell><cell>(m)</cell><cell>Shape</cell><cell>(cm)</cell></row><row><cell cols="2">DSS [24] 2.5 Ours RPN small short 1.6</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Resolution and shape comparison between Deep-Sliding Shape<ref type="bibr" target="#b23">[24]</ref> and ours. Anchors of the bed and trash can from<ref type="bibr" target="#b23">[24]</ref> are used as examples of proposal's physical size to make the comparison with ours.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>2D detection results based on SUN-RGBD validation set. Evaluation metric is average precision with 2D IoU threshold of 0.5.</figDesc><table><row><cell></cell><cell></cell><cell>night</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sofa</cell><cell>garbage</cell><cell>frustum proposal</cell><cell>3D detection</cell><cell>Total</cell></row><row><cell></cell><cell>bed toilet</cell><cell cols="6">stand bathtub chair dresser sofa table desk bookshelf</cell><cell>chair</cell><cell>bin</cell><cell>runtime</cell><cell>runtime</cell><cell>runtime</cell></row><row><cell>DSS[24](RGB-D)</cell><cell>78.8 78.9</cell><cell>15.4</cell><cell>44.2</cell><cell>61.2</cell><cell>6.4</cell><cell>53.5 50.3 20.5</cell><cell>11.9</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>19.55s</cell></row><row><cell>COG[21](RGB-D)</cell><cell>63.7 70.1</cell><cell>27.4</cell><cell>58.3</cell><cell>62.2</cell><cell>15.5</cell><cell>51.0 51.3 45.2</cell><cell>31.8</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>10-30min</cell></row><row><cell>2D-driven[10](RGB-D)</cell><cell>64.5 80.4</cell><cell>41.9</cell><cell>43.5</cell><cell>48.3</cell><cell>15.5</cell><cell>50.4 37.0 27.9</cell><cell>31.4</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>4.15s</cell></row><row><cell>Frustum PointNets[16](RGB-D)</cell><cell>81.1 90.0</cell><cell>58.1</cell><cell>43.3</cell><cell>64.2</cell><cell>32.0</cell><cell>61.1 51.1 24.7</cell><cell>33.3</cell><cell>N/A</cell><cell>N/A</cell><cell>60ms</cell><cell>60ms</cell><cell>0.12s</cell></row><row><cell>OURS RGB-D (FPN+3D ResNetFCN6)</cell><cell>78.5 84.5</cell><cell>34.5</cell><cell>42.4</cell><cell>47.2</cell><cell>18.2</cell><cell>40.3 30.4 12.4</cell><cell>18.0</cell><cell>47.1</cell><cell>47.6</cell><cell>110ms</cell><cell>48ms</cell><cell>0.16s</cell></row><row><cell>OURS RGB-D (FPN+3D ResNetFCN35)</cell><cell>79.5 84.6</cell><cell>36.2</cell><cell>44.6</cell><cell>49.1</cell><cell>19.6</cell><cell>40.8 27.5 12.5</cell><cell>19.1</cell><cell>47.9</cell><cell>48.2</cell><cell>110ms</cell><cell>128ms</cell><cell>0.24s</cell></row><row><cell>OURS Depth only (FPN+3D ResNetFCN6)</cell><cell>77.1 76.1</cell><cell>32.4</cell><cell>42.0</cell><cell>45.9</cell><cell>14.1</cell><cell>35.8 25.3 11.7</cell><cell>16.8</cell><cell>48.5</cell><cell>35.0</cell><cell>110ms</cell><cell>48ms</cell><cell>0.16s</cell></row><row><cell cols="2">OURS Depth only (FPN+3D ResNetFCN35) 77.4 76.8</cell><cell>33.1</cell><cell>43.7</cell><cell>45.8</cell><cell>15.2</cell><cell>37.3 25.5 11.8</cell><cell>17.4</cell><cell>48.8</cell><cell>35.4</cell><cell>110ms</cell><cell>148ms</cell><cell>0.24s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>2D/3D detection results based on YOLO v3 V.S. FPN. 2D detection is based on RGB images. 3D detection is based on RGB-D images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Number of parameters and inference time comparison between Frustum Pointnet and our system. For YOLO v3, input resolution is 416 by 416 and the model FLOPS is 65.86 Bn.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table Frustum</head><label>Frustum</label><figDesc></figDesc><table><row><cell></cell><cell>Average Center</cell><cell>-0.005</cell><cell>-0.233</cell><cell>0.075</cell><cell>0.522</cell></row><row><cell></cell><cell>Predicted from Frustum VoxNet</cell><cell>0.014</cell><cell>-0.040</cell><cell>0.030</cell><cell>0.395</cell></row><row><cell>Desk</cell><cell>Frustum Average Center Predicted from Frustum VoxNet</cell><cell>-0.010 0.028</cell><cell>-0.198 -0.040</cell><cell>0.109 0.048</cell><cell>0.428 0.319</cell></row><row><cell>Sofa</cell><cell>Frustum Average Center Predicted from Frustum VoxNet</cell><cell>-0.015 0.007</cell><cell>-0.168 0.041</cell><cell>0.010 0.013</cell><cell>0.516 0.444</cell></row><row><cell>Bed</cell><cell>Frustum Average Center Predicted from Frustum VoxNet</cell><cell>0.031 -0.009</cell><cell>-0.195 0.010</cell><cell>0.013 -0.012</cell><cell>0.573 0.354</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1625843 and Google Faculty Research Award 2017 (special thanks to Aleksey Golovinskiy, Tilman Reinhardt and Steve Hsu for attending to all of our needs). We acknowledge the support of NVIDIA with the donation of the Titan-X GPU used for this work. We thank Jaspal Singh for data preparation and earlier discussion. We also would like to thank Allan Zelener, James Kluz, Jaime Canizales and Bradley Custer for helpful comments and advice.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno>abs/1611.07759</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1311.2524</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1407.5736</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Joint 3D Proposal Generation and Object Detection from View Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">2d-driven 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno>abs/1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft COCO: common objects in context. CoRR, abs/1405.0312</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1711.08488</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1612.08242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1525" to="1533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12683</idno>
		<title level="m">A survey of Object Classification and Detection based on 2D/3D data. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep sliding shapes for amodal 3d object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno>abs/1511.02300</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online algorithms for classification of urban objects in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hadjiliadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Second International Conference on 3D Imaging, Modeling, Processing, Visualization Transmission</title>
		<imprint>
			<date type="published" when="2012-10" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1803.08494</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cnn-based object segmentation in urban lidar with missing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zelener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="417" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
