<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection and Classification of Acoustic Scenes and Events 2020 THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION Technical Report</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuma</forename><surname>Koizumi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Takeuchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasunori</forename><surname>Ohishi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noboru</forename><surname>Harada</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunio</forename><surname>Kashino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detection and Classification of Acoustic Scenes and Events 2020 THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION Technical Report</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Challenge</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Audio captioning</term>
					<term>sequence-to-sequence model</term>
					<term>keyword estimation</term>
					<term>acoustic event/scene estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning <ref type="bibr">[1]</ref>. Automated audio captioning (AAC) is an intermodal translation task when translating an input audio into its description using natural language <ref type="bibr" target="#b0">[2]</ref><ref type="bibr" target="#b1">[3]</ref><ref type="bibr" target="#b2">[4]</ref><ref type="bibr" target="#b3">[5]</ref><ref type="bibr" target="#b4">[6]</ref>. In contrast to automatic speech recognition (ASR), which converts a speech to a text, AAC converts environmental sounds to a text. This task potentially raises the level of automatic understanding of sound environment from merely tagging events <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8]</ref> (e.g. alarm), scenes <ref type="bibr" target="#b7">[9]</ref> (e.g. kitchen) and condition <ref type="bibr" target="#b8">[10]</ref> (e.g. normal/anomaly) to higher contextual information, for example, "a digital alarm in the kitchen has gone off three times."</p><p>Our submission focuses on solving the indeterminacy problems in AAC which were tackled in our previous studies <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b9">11]</ref>. This indeterminacy can be broadly divided into the indeterminacy in (i) word selection <ref type="bibr" target="#b9">[11]</ref> and (ii) sentence length <ref type="bibr" target="#b1">[3]</ref>. The first problem is caused by that one acoustic event/scene can be described with several words, such as {car, automobile, vehicle, wheels} and {road, roadway, intersection, street} <ref type="bibr" target="#b9">[11]</ref>. The second one is caused by that a sound can be explained in either short or long sentences, such as "noisy car sounds," or "a lot of cars are driving on the roadway and there are very loud engine noises" <ref type="bibr" target="#b9">[11]</ref>. Such indeterminacy leads to a combinatorial explosion of possible answers, making it almost impossible to estimate the ground-truth and difficulty in training an AAC system.</p><p>Our strategy for solving these problems is to simultaneously estimate keywords and sentence length through multi-task learning framework. <ref type="figure" target="#fig_0">Figure 1</ref> shows the overview of our system. The preprocessing stage involves rule-based keywords and sentence length extraction from the caption and metadata. The captioning DNN has  keyword estimation and a sentence length estimation branches, and estimates the ground-truth caption by integrating these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM DESCRIPTION</head><p>This section describes the detail of our system. Since this paper is a technical report, we focus on describing the detailed implementation of the system. Effectiveness of each modules will be discussed in the workshop paper through ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pre-processing</head><p>Audio pre-processing: As acoustic feature, we used three logmel-spectrograms calculated from the time-domain input audio x. The first one was the log-mel-spectrogram of the input audio S ? R F ?Ts , where F and Ts are the number of mel-filterbanks and time-frames. The second and third ones were that of the harmonicpercussive source separation (HPSS) outputs, H ? R F ?Ts and P ? R F ?Ts . These three spectrograms were concatenated on the channel dimension X ? R 3?F ?Ts .</p><p>The hyper-parameters of the audio pre-processing are as follows. All audio samples were down-sampled at 22.05 kHz. The window-and hop-size of short-time Fourier transform (STFT) were 4096 and 2048 points, respectively. The number of mel-filterbank was F = 64. The hyper-parameters of the HPSS were default one of librosa.decompose.hpss <ref type="bibr">[12]</ref>.</p><p>Caption pre-processing: All captions were tokenized using the word tokenizer of the natural language toolkit (NLTK) <ref type="bibr" target="#b10">[13]</ref> while removing punctuation. All tokens in the development dataset were then counted, and words that appeared more than five times were appended in the word vocabulary. The vocabulary size was C cap = 2144, which includes BOS, EOS, PAD, and UNK tokens. In addition, the sentence length L of each caption was counted. Also, arXiv:2007.00225v1 [eess.AS] 1 Jul 2020</p><formula xml:id="formula_0">caption keywords k cap = {k cap i } Kc i=1</formula><p>was extracted using the keyword vocabulary which is discussed below. Meta pre-processing: Meta keywords were extracted from the file_name and keyword provided in the metadata csv file, using a keyword vocabulary which was manually created beforehand. The procedure of creating the keyword vocabulary is as follows. First, file_name and keyword were split at places at space and punctuation. Next, words that seem to be nouns, verbs, adjectives, and adverbs were converted to its lemma. Finally, all lemmas were counted, and lemmas that appeared more than ten times were appended in the keyword vocabulary, which is a hash table that maps the original word to its lemma. The vocabulary size was C key = 421. The keyword vocabulary was used to extract meta keyword m = {m k } Km k=1 and caption keyword c. Note that the procedure for creating the keyword vocabulary can be automated by using the part-of-speech (POS)-tagger and the WordNet Lemmatizer of the NLTK, however, we did this manually because their use is prohibited in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data augmentation</head><p>TF-IDF-based sample selection and data augmentation: Since the target metrics of this challenge is SPIDEr, we need to accurately predict captions which include low frequent words and topics. To deal with word and topic bias in the training dataset, we adopted two tricks for training sample selection based on inverse document frequency (IDF), and one trick for data augmentation based on term frequency (TF)-IDF <ref type="bibr" target="#b12">[15]</ref>.</p><p>The first trick is for selecting an audio sample x from the training dataset. First, we concatenated the five ground-truth captions corresponding to each x in the training dataset, and used as a "sentence". Then, we calculated IDFs for all words in all sentences, and calculated the average IDF of each sentence. Finally, each average IDF was normalized by the sum of the average IDF. We regarded the normalized IDF as the parameter of the Categorical distribution, and selected x based on this probability.</p><p>The second trick is for selecting a ground-truth caption w from five captions corresponding to the selected x. The basic strategy was the same as the first trick. First, we calculated IDFs of all words in the five captions. Here, note that the document was the five captions in contrast to the first trick. Then, we calculated the normalized IDF and used as the parameter of Categorical distribution, and selected the target caption w based on this probability.</p><p>Finally, we adopted the third trick which is the TF-IDF based word replacement <ref type="bibr" target="#b12">[15]</ref> to augment text data. Random data cropping: To train our captioning DNN using minibatches, we adjusted the input length of audio sequence and text sequence using random cropping and padding. We set the input length of audio to 20 seconds (T = 216), and the number of words is N = 20. Thus, the inputs of the captioning DNN were X ? R 3?F ?T and w = (w1, ..., wN ). For X whose Ts was greater than T , a random crop was performed so that the time-length was T , and shorter ones were applied zero-padding. Similarly, if the sentence length was greater than N , words after the N -th word were cropped, and PAD tokens were added for shorter ones. Mix-up: After adjusting input length T and N , we used the mixup data augmentation. First, we drew a mixing parameter ? from a beta distribution as ? ? Beta(0.4, 0.4) where ? is sample drawing from the right-hand distribution. Then, two audio samples were mixed by multiplying ? and (1 ? ?), respectively. Since text inputs  a set of class labels, direct mixing of w is not suitable. Thus, we mixed the embedded word tokens by multiplying the mixing parameters. <ref type="figure" target="#fig_1">Figure 2</ref> shows the network architecture of the captioning DNN. The pink area in <ref type="figure" target="#fig_1">Fig. 2</ref> is a basic sequence-to-sequence (Sec2Sec)based captioning model <ref type="bibr" target="#b11">[14]</ref> using bidirectional long short-term memory (BLSTM)-LSTM. The encoder BLSTMs outputted the initial hidden state h ? R D and cell states c ? R D of the decoder LSTM, where D = 120 was the hidden dimension of the whole network. Then, the decoder LSTM estimated posterior probability of n-th word given the audio signal x and 1st to (n ? 1)-th words p(wn|x, w1,...,n?1) by using embedded word tokens. In our submission, to solve the indeterminacy problems in AAC, we additionally used sub-blocks for keyword and sentence length estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model description</head><p>The following describes these sub-locks in detail.</p><p>Audio embedding block A: The input audio X was first passed to this block. This block embeded X into a feature space as A = A(X) ? R D?T a . As shown in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>, this block consisted of three convolutional neural network (CNN)-blocks and two fully connected (FC)-blocks. The kernel size, stride, padding, and number of output channels of CNN were 3, 1, 1, and 64 for all CNN layers, respectively. The kernel size and stride of the 2D maxpooling were 2 and 2, respectively. Then, the output of CNN-blocks 64 ? Fa ? Ta was reshaped in to 64Fa ? Ta, where Fa = F 2 3 = 8</p><p>and Ta = T 2 3 = 27, respectively. The reshaped output was passed to the first FC layer which converts R 64Fa?Ta to R D?Ta . Finally, the second FC-layer outputted A ? R D?T a .</p><p>Caption keyword estimation block C: This block estimated caption keyword probabilities of each keyword p cap ? [0, 1] C key from A as p cap = C(A). We expected that this block guides the audio embedding block so that its output includes information of the keywords of the ground-truth caption. As shown in <ref type="figure" target="#fig_1">Fig. 2 (d)</ref>, this block consisted of a muti-head self-attention (MHSA) layer and a FC layer. The number of heads of MHSA was 4. The output shape of MHSA and FC layer were D ? Ta and C key ? Ta, respectively. Since the caption keyword has no time-labels, we aggregated the output by taking a maximum value in the time direction and outputted caption keyword probabilities of each keyword p cap .</p><p>Meta keyword estimation block M: This block estimated meta keyword probabilities p meta ? [0, 1] C key and its embedding M ? R D?Km from A as {p meta , M } = M(A). As shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (e), the base architecture was the same as the caption keyword estimation block, which consisted of MHSA and FC layers. The base architecture outputted meta keyword probabilities of each keyword p meta . To embed the estimated meta keywords into the feature space, first, we used the argsort function which returns the index on which p meta sorts in descending order. Then the top Km = 15 indices were selected as the estimated meta keyword {m k ? N} Km k=1 . Finally, these indexes were passed to the embedding layer to obtain the estimated meta keyword embedding M . After this block, A and M were concatenated as (M , A, M ), and it was passed to BLSTMs.</p><p>Sentence length estimation block L: This block estimated the sentence length probability p len ? [0, 1] L max and its embedding l ? R D l as {p len , l} = L(h, c), where L max = 20 is the maximum sentence length that we assumed. First, h and c were concatenated, and the first FC layer estemited p len from the concatenated feature. Finally, p len was passed to the second FC layer, and outputted l. After this block, l was concatenated to h and c, and used as the initial hidden and cell state of the decoder LSTM.</p><p>Attention block: Before calculating p(wn|x, w1,...,n?1) using the final FC layer, this block integrated the output of the LSTM H ? R (D+D l )?N and M and outputs M ? R (D+D l )?N by using three FC-layers, like an MHSA with a single head. Then the tanh activation was applied to M . Finally, it is added to H as H +tanh(M ) and passed to the final FC-layer to estimate p(wn|x, w1,...,n?1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Loss functions</head><p>In order to train encoder/decoder and sub-blocks simultaneously, we designed loss function as a sum of multiple losses functions. In addition, since the input audio and text were augmented by the mix-up, the cost function was also calculated using the mix-up; each loss was calculated for each of the two original label data and mixed using mixing-parameters ? and (1 ? ?). The following describes these loss functions in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word estimation loss:</head><p>For word prediction, we used the crossentropy loss between wn and p(wn|x, w1,...,n?1). To avoid overfitting, we used label smoothing where smoothing factor was 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caption/meta keyword estimation loss:</head><p>The weighted binarycross entropy was used as the loss function for both caption/meta keyword estimation block as</p><formula xml:id="formula_1">? 1 C key C key i=1 ?izi ln pi + ?i(1 ? zi) ln(1 ? pi).</formula><p>(1)</p><p>Note that for all variables, we omitted the superscripts cap and meta which indicate whether the variable belongs to caption keyword or meta keyword. Here, the meanings of each variable are followings: zi is 1 when ground-truth keyword set includes i-th word and 0 otherwise, pi is the i-th value of the estimated posterior vector p, and ?i and ?i are the weight for i-th keyword as ?i = (p(zi)) ?1 and ?i = (1 ? p(zi)) ?1 , respectively, where p(zi) is the prior probability of the i-th keyword calculated by</p><formula xml:id="formula_2">p(zi) = # of c-th keyword in training samples # of training samples .<label>(2)</label></formula><p>To balance this loss and other losses, we multiplied a weight (1 ? 10 ?4 ) s to this loss, where s is the number of training steps. Sentence length estimation loss: We used the softmax cross entropy between L and p len as the loss for the sentence length estimation block. To balance this loss and other losses, we multiplied a weight 10 ?2 to this loss. Keyword co-occurrence loss: In order to prevent the decoder outputs the words which are obviously not related to the meta keywords, we used the keyword co-occurrence loss between words in a caption and its meta keywords. For example, when meta keywords are {car, sing, bird}, words not related to the keywords such as {people, children, talking, talk, speak} may not be included in the correct caption. To prevent the decoder outputs such words, we adopted a penalty based on the decoder outputs p(wn|x, w1,...,n?1). Before training, we created a hash-table of the co-occurrence lists; the keys of the hash-table are all keywords in the keyword vocabulary, and the element of each key is a list of the words that have co-occurred with the keyword in the training dataset. For example, in the case of meta keywords are {car, sing, bird} and ground-truth captions are {Cars are driving and birds are singing, A car passes by while birds are chirping and singing}, {cars, are, driving, and, birds, singing, a, passes, by, while, chirping} are added to the cooccurrence lists of car, sing, and bird. Then, in the training step, we added penalties of the decoder outputs to the whole loss value as</p><formula xml:id="formula_3">1 C cap N n=1 C cap i=1 |bi ? p(wn = i|x, w1,...,n?1)|,<label>(3)</label></formula><p>where bi ? {0, 1} a binary mask where bi = 1 when none of all co-occurrence lists of the ground-truth meta keywords includes the i-th word, and otherwise bi = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Beam search and test time augmentation</head><p>We used the beam search decoding for the word decision process from p(wn|x, w1,...,n?1). The beam size was 5, and n-gram blocking size was 2, i.e. a hypothesis in a beam was discarded if there was a bi-gram that appeared more than once within it. In addition, we used test time augmentation (TTA) for audio input. This is because the audio input was randomly cropped for limiting the time-length as T = 216 in training phase. If the length of audio input is changed in testing phase, it may have a bad influence on the batch normalization layers. Therefore, in testing-phase, we also randomly cropped and zero-padded the audio input so as to T = 216. We generated five input audios by this process, and took the average of five outputs of the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Training hyper-parameters</head><p>We used the AdamW <ref type="bibr" target="#b14">[17]</ref> optimier with a constant learning rate 10 ?4 . The minibatch-size was 48. We randomly splitted 2893 + 1045 samples in the development dataset into 3842 training samples and 96 validation samples. We used a DNN whose validation score was the best while 300 epochs training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Submitted systems</head><p>We used a model ensemble to output the final results; each model in the ensemble outputted ln p(wn|x, w1,...,n?1), and we took the average of all log-probabilities in the beamsearch phase. The four submitted results were four types of different combinations of following models. where single means we did not used the HPSS (i.e. X = S), and param2 means two additional modification: (i) before adding the meta keyword estimation loss, we multiplied 0.8 to it as a loss weight. (ii) we did not used the second trick in minibatch sample selection, i.e. the target caption was selected with equal probability from the five ground-truth caption of an audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATION ON DEV-TEST DATASET</head><p>To give a sense of the accuracy of the submitted system, we tested a simplified Submission 1 on the development-test dataset of the Challenge. First, we conducted three unit tests for Model1, Model2, Model3, and Model4, and then evaluated the ensemble model as Ensemble. Although Ensemble is simpler than actual our challenge submissions, it should be useful for testing the performance of each model and the effectiveness of the ensemble. <ref type="table" target="#tab_2">Table 1</ref> shows the evaluation results. All models significantly outperformed the baseline system, and with these ensembles model achieved the SPIDEr score 20.7. Our model consists of a complex combination of various sub-blocks and cost functions. As a future work, we will conduct ablation studies to determine how each blocks/cost functions has affected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>This technical report described the system participating to the DCASE 2020 Challenge Task 6 [1]. Our submission focused on solving the indeterminacy problems in word selection and sentence length. We simultaneously solved the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. The SPIDEr score of our submission on the development-testing dataset was 20.7. Since our model consisted of a complex combination of various sub-blocks and cost functions, as a future work, we will conduct ablation studies for these modules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>System overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Network architecture of captioning DNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on development-testing dataset.</figDesc><table><row><cell>Model</cell><cell>B-1</cell><cell>B-2</cell><cell>B-3</cell><cell>B-4</cell><cell cols="5">CIDEr METEOR ROUGE-L SPICE SPIDEr</cell></row><row><cell>Baseline</cell><cell>38.9</cell><cell>13.6</cell><cell>5.5</cell><cell>1.5</cell><cell>7.4</cell><cell>8.4</cell><cell>26.2</cell><cell>3.3</cell><cell>5.4</cell></row><row><cell>Model1</cell><cell>52.6</cell><cell>33.5</cell><cell>22.4</cell><cell>14.6</cell><cell>30.1</cell><cell>14.7</cell><cell>34.7</cell><cell>9.0</cell><cell>19.5</cell></row><row><cell>Model2</cell><cell>51.2</cell><cell>32.1</cell><cell>21.3</cell><cell>14.1</cell><cell>29.7</cell><cell>14.5</cell><cell>33.9</cell><cell>9.1</cell><cell>19.4</cell></row><row><cell>Model3</cell><cell>51.7</cell><cell>33.0</cell><cell>22.0</cell><cell>14.5</cell><cell>30.0</cell><cell>14.7</cell><cell>34.3</cell><cell>8.6</cell><cell>19.3</cell></row><row><cell>Model4</cell><cell>53.0</cell><cell>33.7</cell><cell>22.4</cell><cell>14.5</cell><cell>30.2</cell><cell>14.8</cell><cell>35.2</cell><cell>9.1</cell><cell>19.6</cell></row><row><cell cols="5">Ensemble 53.7 34.8 23.5 15.6</cell><cell>31.9</cell><cell>15.2</cell><cell>35.9</cell><cell>9.4</cell><cell>20.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Model1 The base model described in Sec. 2.3. Model2 Modified model of Model1. A FC layer was used instead of the MHSA layer in the caption keyword estimation block. Model3 Modified model of Model2. The meta keyword estimation block in the encoder and the attention block in the decoder were removed. Model5 Modified model of Model1. The audio embedding block consists of one CNN block, the reshape block, one FC layer for changing the hidden dimension to D = 120, and one shared Transformer encoder block [16] with time-direction sub-sampling operation. The Transformer encoder block and sub-sampling operation were used twice, with the subsampling operation thinning out the one time-frame every two time-frames. Model6 Modified model of Model1. The encoder has only one BLSTM layer, and D = 160. Ensemble of 20 models. This model consists of two Model1, two Model1single, two Model1param2, two Model2, two Model3, two Model3param2, two Model4, two Model4single, and four Model4param2. The number of trainable parameters was 33.0M. Submission 2 Ensemble of 50 models. This model consists of five Model1, five Model1single, five Model1param2, five Model2, five Model3, five Model3param2, five Model4, five Model4single, and ten Model4param2. The number of trainable parameters was 82.5M. Submission 3 Ensemble of 12 models. This model consists of two Model1, two Model3, four Model4, two Model5single, and two Model6single. The number of trainable parameters was 20.7M. Submission 4 Ensemble of 30 models. This model consists of five Model1, five Model3, ten Model4, five Model5single, and five Model6single. The number of trainable parameters was 51.7M.</figDesc><table><row><cell>Model4 Modified model of Model1. Mix-up augmentation for text</cell></row><row><cell>was removed.</cell></row><row><cell>The details of four submitted systems are followings:</cell></row><row><cell>Submission 1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated Audio Captioning with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Drossos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Workshop on Application of Signal Process. to Audio and Acoust. (WASPAA)</title>
		<meeting>of IEEE Workshop on Application of Signal ess. to Audio and Acoust. (WASPAA)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Audio Captioning based on Conditional Sequence-to-Sequence Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Detection and Classification of Acoust. Scenes and Events Workshop (DCASE)</title>
		<meeting>of the Detection and Classification of Acoust. Scenes and Events Workshop (DCASE)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audio Caption: Listen and Tell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dinkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Acoust., Speech, and Signal Process. (ICASSP)</title>
		<meeting>of Int&apos;l Conf. on Acoust., Speech, and Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AudioCaps: Generating Captions for Audios in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the North American Chapter of the Association for Computational Linguistics: Human Lang. Tech. (NAACL-HLT)</title>
		<meeting>of the North American Chapter of the Association for Computational Linguistics: Human Lang. Tech. (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clotho: An Audio Captioning Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Drossos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Acoust., Speech, and Signal Process. (ICASSP)</title>
		<meeting>of Int&apos;l Conf. on Acoust., Speech, and Signal ess. (ICASSP)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Acoustic Event Detection in Real Life Recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eronen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Euro. Signal Process. Conf. (EUSIPCO)</title>
		<meeting>of Euro. Signal ess. Conf. (EUSIPCO)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sound Event Detection By Multitask Learning of Sound Events and Scenes with Soft Scene Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Imoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tonami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamashita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Acoust., Speech, and Signal Process. (ICASSP)</title>
		<meeting>of Int&apos;l Conf. on Acoust., Speech, and Signal ess. (ICASSP)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Acoustic Scene Classification: Classifying Environments from the Sounds they Produce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barchiesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giannoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised Detection of Anomalous Sound based on Deep Learning and the Neyman-Pearson Lemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Uematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kawachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Tran. on Audio</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Transformer-based Audio Captioning Model with Keyword Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Masumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>submitted to Interspeech, 2020</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>O&apos;Reilly Media Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Process. Systems (NIPS)</title>
		<meeting>of Advances in Neural Information ess. Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised Data Augmentation for Consistency Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Neural Information Processing Systems (NIPS)</title>
		<meeting>of Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Learning Representations (ICLR)</title>
		<meeting>of Int&apos;l Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
