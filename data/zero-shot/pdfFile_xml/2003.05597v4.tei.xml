<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Arbitrary-Oriented Object Detection: Classification based Approaches Revisited</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">On the Arbitrary-Oriented Object Detection: Classification based Approaches Revisited</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Arbitrary-Oriented Object Detection</term>
					<term>Boundary Problem</term>
					<term>Circular Smooth Label</term>
					<term>Densely Coded Labels</term>
					<term>Object Heading Detection !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Arbitrary-oriented object detection has been a building block for rotation sensitive tasks. We first show that the boundary problem suffered in existing dominant regression-based rotation detectors, is caused by angular periodicity or corner ordering, according to the parameterization protocol. We also show that the root cause is that the ideal predictions can be out of the defined range. Accordingly, we transform the angular prediction task from a regression problem to a classification one. For the resulting circularly distributed angle classification problem, we first devise a Circular Smooth Label technique to handle the periodicity of angle and increase the error tolerance to adjacent angles. To reduce the excessive model parameters by Circular Smooth Label, we further design a Densely Coded Labels, which greatly reduces the length of the encoding. Finally, we further develop an object heading detection module, which can be useful when the exact heading orientation information is needed e.g. for ship and plane heading detection. We release our OHD-SJTU dataset and OHDet detector for heading detection. Extensive experimental results on three large-scale public datasets for aerial images i.e. DOTA, HRSC2016, OHD-SJTU, and face dataset FDDB, as well as scene text dataset ICDAR2015 and MLT, show the effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O BJECT detection has been a standing task in computer vision. Recently, rotation detection has played an emerging and vital role in processing and understanding visual information from aerial images <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, scene text <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and face <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. The rotation detector can provide accurate orientation and scale information, which will be helpful in applications such as object change detection in aerial images and recognition of sequential characters for multi-oriented scene texts.</p><p>Recently, a line of advanced rotation detectors evolved from classic detection algorithms <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> have been developed. Among these methods, detectors based on region regression take the dominance, and the representation of multi-oriented object is achieved by rotated bounding box or quadrangles. Although these rotation detectors have achieved promising results, there are still some fundamental problems. Specifically, we note both the fiveparameter regression <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref> and the eight-parameter regression <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> methods suffer the issue of discontinuous boundaries, as often caused by angular periodicity or corner ordering, depending on the choice of parameterization protocol. However, the root reasons are not limited to the particular representation of the bounding box. In this paper, we argue that the root cause of boundary problems based on regression methods is that the ideal predictions are beyond the defined range. Thus, the model's loss value suddenly increases at the boundary situation so that the model cannot obtain the prediction result in the simplest and most direct way, and additional complicated treatment is often needed. Therefore, these detectors often have difficulty in boundary conditions. For detection using rotated bounding boxes, the accuracy of angle prediction is critical. A slight angle deviation can lead to notable Intersection-over-Union (IoU) drop, resulting in inaccurate detection, especially for large aspect ratios.</p><p>There have been some works addressing the boundary problem. For example, IoU-smooth L1 loss <ref type="bibr" target="#b2">[3]</ref> introduces the IoU factor, and modular rotation loss <ref type="bibr" target="#b20">[21]</ref> increases the boundary constraint to eliminate the sudden increase in boundary loss and reduce the difficulty of model learning. However, these regression-based detection methods still have not solved the root cause as mentioned above.</p><p>In this paper, we aim to devise a more fundamental rotation detection baseline to solve the boundary problem. Specifically, we consider object angle prediction as a classification problem to better limit the prediction results, and then we design a Circular Smooth Label (CSL) to address angle periodicity and to increase the tolerance between adjacent angles. We show that the rotation accuracy error due to the conversion from continuous prediction to discrete bins can be negligible by a fine-granularity. We also introduce four window functions in CSL and explore the effect of different window radius sizes on detection performance. We further design two Densely Coded Labels (DCL), which greatly reduce the length of the encoding while ensuring the angle prediction accuracy does not sacrifice. In order to eliminate the theoretical prediction errors caused by angle dispersion, We also propose an angle fine-tuning mechanism. Finally, we implement object heading detection on the basis of rotation detection, namely OHDet, and release a new dataset OHD-SJTU to the community. Through experiments and visual analysis, we show that CSL-based and arXiv:2003.05597v4 [cs.CV] 23 Mar 2022 <ref type="figure">Fig. 1</ref>. Architecture of the proposed detector (RetinaNet <ref type="bibr" target="#b14">[15]</ref> as an embodiment). 'W' and 'H' refer to the width and height respectively. 'C' and 'T' in red represents the number of object and encoding length of the angle, respectively. Object heading detection refers to further finding the head of the object based on the orientation information obtained by the rotation detection. Note that the CSL/DCL module on the right refers to the classification based prediction function either based on a Circular Smooth Label or the size reduced version Densely Coded Label.</p><p>DCL-based rotation detection algorithms are indeed better baseline choices than the angle regression-based protocol on different detectors and datasets. Note the regressionbased, CSL-based and DCL-based protocols mentioned in the rest of the paper are named according to the prediction form of the angle. As a byproduct, we show how the object head can be effectively identified after detecting the rotating bounding box.</p><p>The preliminary content of this paper has partially appeared in ECCV 2020 <ref type="bibr" target="#b22">[23]</ref> and CVPR 2021 <ref type="bibr" target="#b23">[24]</ref>  <ref type="bibr" target="#b0">1</ref> . The overall contributions of this extended journal version can be summarized as: <ref type="bibr" target="#b0">1</ref>. To obtain a more thorough analysis and comprehensive results, the conference versions <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> have been significantly extended and improved in this journal version, especially in the following aspects: i) We explore the relationship between the angle discrete representation granularity denoted by ? and the detection performance. It shows that discrete granularity ? can be approximated as a CSL technique with a rectangular window function, which has a certain tolerance in the divided angle interval. The difference is that CSL smooths between adjacent angle intervals. See <ref type="table" target="#tab_7">Table 7</ref> in Section 4.2; ii) We use a specific calculation example to explain why the code length has such a large impact on the amount of detection model parameters and calculations, see Section 3.5; iii) As for the angle prediction of the regression branch, we use two forms as the baseline to be compared, include direct regression and indirect regression, see Section 3.8; iv) We verify our approach on additional more challenging datasets, including FDDB, and DOTA-v1.5/v2.0, see <ref type="table" target="#tab_11">Table 10 and Table 11</ref>. Among them DOTA-v1.5/v2.0 contain more data and tiny object (less than 10 pixels) than DOTA-v1.0; v) We propose an angle fine-tuning mechanism to eliminate the theoretical prediction errors caused by angle dispersion which has been a common issue in whatever CSL and DCL, see Section 3.6; vi) As a common function for downstream applications, we develop a classification-based object heading detector in Section 3.7. To verify its usefulness, we annotate and release a new dataset for this purpose and perform detection evaluation for both rotation and heading with a considerable amount, and more stringent evaluation indicators are used, as detailed in Section 4.1. To our best knowledge, this is the first public benchmark for multiple-category heading detection, especially at a considerable scale. Finally, we also release the full version of the source code. <ref type="bibr">?</ref> We characterize the boundary problems encountered in different regression-based rotation detection methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and show the root cause is that the ideal predictions are beyond the defined range.</p><p>? To effectively dismiss the boundary problem, we design a novel classification based rotation detection paradigm, in contrast to the dominant regression based methods in existing works. The incurred accuracy error is negligible thanks to the devised fine-grained angle discretization (less than 1 degree) which to our best knowledge, has not been developed yet in the literature, and a coarse classification (around 10-degree) model is dated back to the 1990s for face detection <ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr">?</ref> We develop the Circular Smooth Label (CSL) technique as an independent module. It can be readily reused in existing regression based methods by replacing the regression component with classification, to enhance angular prediction in face of boundary conditions and objects with large aspect ratio. We further design a Densely Coded Label (DCL) to solve the problem of excessive model parameters induced by CSL. DCL can greatly reduce the length of the encoding while maintaining a high angle prediction accuracy. We also propose an angle fine-tuning mechanism to eliminate the theoretical prediction errors caused by angle dispersion which has been a common issue in both CSL and DCL.</p><p>? On the basis of rotation detection, we further develop an object heading detector, namely OHDet, to identify the heading of object. In addition, we annotate and release a dataset called OHD-SJTU 2 , which can be used for both rotation detection and object heading detection tasks. <ref type="bibr" target="#b1">2</ref>. https://yangxue0827.github.io/OHD-SJTU.html.</p><p>? Extensive experimental results on HRSC2016 and DOTA-v1.0 show the state-of-the-art performance of our detector, and the efficacy of the CSL and DCL technique as independent components have been verified across different detectors. The source code <ref type="bibr" target="#b24">[25]</ref> is publicly available <ref type="bibr" target="#b2">3</ref> .</p><p>The paper is organized as follows. Section 2 introduces the related work. Section 3 presents the main approach in this paper and the experiments are conducted in Section 4. Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we discuss the related work, including classic region object detection and rotated object detection. Specifically, we discuss some relevant techniques regarding classification based orientation estimation, as well as the recent works on object heading detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Horizontal Region Object Detection</head><p>Classic object detection aims to detect general objects with horizontal bounding boxes, and many high performance general-purpose object detectors have been proposed. R-CNN <ref type="bibr" target="#b25">[26]</ref> pioneers a method based on CNN detection. Subsequently, region-based models such as Fast R-CNN <ref type="bibr" target="#b15">[16]</ref>, Faster R-CNN <ref type="bibr" target="#b16">[17]</ref>, and R-FCN <ref type="bibr" target="#b18">[19]</ref> are proposed, which improve the detection speed while reducing computational storage. FPN <ref type="bibr" target="#b17">[18]</ref> focuses on the scale variance of objects in images and propose feature pyramid network to handle objects at different scales. SSD <ref type="bibr" target="#b26">[27]</ref>, YOLO <ref type="bibr" target="#b27">[28]</ref> and RetinaNet <ref type="bibr" target="#b14">[15]</ref> are representative single-stage methods, and their single-stage structure leads to higher detection speeds. Compared to anchor-based protocols, many anchor-free have become extremely popular in recent years. CornerNet <ref type="bibr" target="#b28">[29]</ref>, CenterNet <ref type="bibr" target="#b29">[30]</ref> and ExtremeNet <ref type="bibr" target="#b30">[31]</ref> attempt to predict some keypoints of objects such as corners or extreme points, which are then grouped into bounding boxes. However, horizontal detector does not provide accurate orientation and scale information, which poses problem in real applications such as object change detection in aerial images and recognition of sequential characters for multi-oriented scene texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Arbitrary-oriented Object Detection</head><p>Aerial images and scene text are the main application scenarios of the rotation detector. Recent advances in multioriented object detection are mainly driven by adaption of classical object detection methods using rotated bounding boxes or quadrangles to represent multi-oriented objects. Due to the complexity of the remote sensing image scene and the large number of small, cluttered and rotated objects, multi-stage rotation detectors are still dominant for their robustness. Among them, ICN <ref type="bibr" target="#b3">[4]</ref>, ROI-Transformer <ref type="bibr" target="#b1">[2]</ref>, SCRDet <ref type="bibr" target="#b2">[3]</ref>, R 3 Det [1] are state-of-the-art detectors. Gliding Vertex <ref type="bibr" target="#b21">[22]</ref> and RSDet <ref type="bibr" target="#b20">[21]</ref> achieve more accurate object detection through quadrilateral regression prediction. For scene text detection, RRPN <ref type="bibr" target="#b8">[9]</ref> employs rotated RPN to generate rotated proposals and further performs rotated 3. https://github.com/yangxue0827/RotationDetection bounding box regression. TextBoxes++ <ref type="bibr" target="#b10">[11]</ref> adopts vertex regression on SSD. RRD <ref type="bibr" target="#b9">[10]</ref> further improves TextBoxes++ by decoupling classification and bounding box regression on rotation-invariant and rotation sensitive features, respectively. In fact, these mainstream regression-based methods often suffer the boundary problems due to the predictions beyond the defined range.</p><p>The idea of segmentation is an effective way of solving boundary problem. For example, segmentation-based protocols <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> are popular in the area of scene text detection. However, these methods are not practical for aerial images, which often contain a large number of densely arranged small objects in multiple categories. In contrast, instance segmentation is more suitable, such as Mask R-CNN <ref type="bibr" target="#b33">[34]</ref>, SOLO <ref type="bibr" target="#b34">[35]</ref>, and CondInst <ref type="bibr" target="#b35">[36]</ref>, but there are also many limitations.</p><p>First, Considering that instance segmentation requires a lot of labeling workload, a more straightforward solution is to convert the rotated boxes into binary masks <ref type="bibr" target="#b36">[37]</ref>. However, such conversion will introduce many background areas, which will reduce the classification accuracy of pixels and affect the accuracy of the final prediction box. Besides, for the top-down methods (e.g. Mask RCNN), dense scenes will limit the detection of horizontal boxes because of the excessive suppression of dense horizontal overlapping bounding boxes due to non-maximum suppression (NMS), thereby affecting subsequent segmentation. Last but not least, the bottom-up methods, such as SOLO and CondInst, assign different instances to different channels, so they are not suitable for aerial images, which often show large scale scenes with a large number of dense and small objects. Take the parking lot scene in DOTA-v1.5 <ref type="bibr" target="#b37">[38]</ref> dataset as an example, a sub-image with a size of 450 ? 600 will contain up to 2,000 vehicles, which are often less than 10 pixels in size and are densely arranged.</p><p>The above reasons may help explain why angle-based rotation detection algorithms still dominate in aerial imagery which is an important application area. Therefore, we design a new rotation detection baseline, which basically eliminates the boundary problem by transforming angle prediction from a regression problem to a classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification for Orientation Information</head><p>Early works have been developed for multi-view face detection with arbitrary rotation-in-plane (RIP) angles, by obtaining orientation information via classification. Specifically, the divide-and-conquer technique is adopted in <ref type="bibr" target="#b12">[13]</ref>, which uses several small neural networks to deal with a small range of face appearance variations individually. In <ref type="bibr" target="#b13">[14]</ref>, a router network is firstly used to estimate each face candidate's RIP angle. PCN <ref type="bibr" target="#b11">[12]</ref> progressively calibrates the RIP orientation of each face candidate and shrinks the RIP range by half in early stages. Finally, PCN makes the accurate final decision for each face candidate to determine whether it is a face and predict the precise RIP angle. In other research areas, <ref type="bibr" target="#b38">[39]</ref> adopts ordinal regression for effective future motion classification. <ref type="bibr" target="#b39">[40]</ref> obtains the orientation information of the ship by classifying the four sides. The above methods all obtain the approximate orientation range through classification, but cannot be directly applied to (a) Five-parameter method with 90 ? angular range <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>. (b) Fiveparameter method with 180 ? angular range <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>. (c) Ordered quadrilateral representation <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>scenarios that require precise orientation information such as aerial images and scene text. They also do not suffer the boundary problem due to the PoA and EoE, because their prediction granularity is very rough and the aspect ratio of object is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Object Heading Detection</head><p>DLR 3K <ref type="bibr" target="#b40">[41]</ref> is an aerial image dataset that can be used for car head detection. There are 20 images in total, including 3,418 cars and 54 trucks. For car head detection, the authors first use a sliding window strategy with binary classification to detect cars, and then perform a rough estimation of the head through classification, with 16 classes (22.5 ? rotation difference between adjacent sample groups, respectively). This method needs to train multiple detectors and cannot perform high-precision angle prediction. DRBox <ref type="bibr" target="#b41">[42]</ref> and DRBox-v2 <ref type="bibr" target="#b42">[43]</ref> define the rotation bounding box according to the head of the object, and the angle predicted by regression can be used to determine the direction of the object head. However, the bounding box of this definition protocol has a relatively large angular range, at [0 ? , 360 ? ), which is challenging. EAGLE <ref type="bibr" target="#b43">[44]</ref> is a large-scale dataset for vehicle detection in aerial imagery, which has still not been released so far. This paper releases a new dataset called OHD-SJTU with labeled heading information, and it covers more categories of objects. In summary, this work is dedicated to proposing a general detection method that can be used for multi-class highprecision rotation detection and object heading detection. <ref type="figure">Figure 1</ref> gives an overview of our method. The embodiment is a single-stage rotation detector based on the Reti-naNet <ref type="bibr" target="#b14">[15]</ref>. The figure shows a multi-tasking pipeline, including classification branch, rotation detection branch and object heading detection branch. Among them, rotation detection branch contains regression based prediction and CSL-based prediction, to facilitate the comparison of the performance of the two methods. It can be seen from the figure that CSL-based protocol is more accurate for learning the orientation and scale information of the object. The DCLbased protocol maintains consistent performance with CSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head><p>Note that the method proposed in this paper is applicable to most regression-based protocols by replacing the regression module with our classification one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Regression-based Rotation Detection Method</head><p>Parametric regression is currently a popular method for rotation object detection, mainly including five-parameter regression-based protocols <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and eightparameter regression-based protocols <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The commonly used five-parameter regression-based protocols realize arbitrary-oriented bounding box detection by adding an additional angle parameter ?. <ref type="figure" target="#fig_0">Figure 2</ref>(a) shows one of the rectangular definition (x, y, w, h, ?) with 90 ? angular range <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, ? denotes the acute angle to the x-axis, and for the other side we refer it as w. It should be distinguished from another definition (x, y, h, w, ?) illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>(b), with 180 ? angular range <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, whose ? is determined by the long side (h) of the rectangle and x-axis. The eight-parameter regression-based detectors directly regress the four corners (x 1 , y 1 , x 2 , y 2 , x 3 , y 3 , x 4 , y 4 ) of the object, so the prediction is a quadrilateral. The key step to the quadrilateral regression is to sort the four corner points in advance, which avoids large loss even if the prediction is correct, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Boundary Problem of Regression Method</head><p>Although the parametric regression-based rotation detection method has achieved competitive performance in visual detection, these methods essentially suffer the discontinuous boundaries problem <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[21]</ref>. On the surface, boundary discontinuity problems are often caused by angular periodicity under the five-parameter protocol, or the corner ordering in the eight-parameter setting. While there exists a more fundamental root cause behind the representation choice of the bounding box.</p><p>The boundary discontinuity can cause the loss value suddenly increase at the boundary situation. Thus methods have to resort to particular and often complex tricks to mitigate this issue. Therefore, these detection methods are often inaccurate in boundary conditions. We describe the boundary problem in three typical categories of regressionbased protocols according to their different representation forms (the first two refer to the five-parameter methods):</p><p>? The 90 ? -regression-based protocol, as sketched in <ref type="figure" target="#fig_2">Figure 3</ref>(a). It shows that an ideal form of regression (the blue box rotates counterclockwise to the red box), but the loss of this situation is very large due to the periodicity of angular (PoA) and exchangeability of edges (EoE), see the example in <ref type="figure" target="#fig_2">Figure 3</ref>(a) and Equation 13, 12, 17 for detail. Therefore, the model has to be regressed in other complex forms (such as the blue box rotating clockwise to the gray box while scaling w and h), increasing the difficulty of regression. It should be noted that the prediction box and ground truth in the ideal regression way do have a high IoU value in visual perception, but the prediction box at this time has exceeded our defined range so that we cannot calculate the accurate IoU if no additional judgment processing is performed.  Similarly, this method also suffers the issue of sharp increase of loss caused by the PoA at the boundary. The model will eventually choose to rotate the proposal a large angle clockwise to get the final predicted bounding box. </p><formula xml:id="formula_0">is {(a ? a), (b ? b), (c ? c), (d ? d)}.</formula><p>In fact, this situation also belongs to PoA. By contrast, the actual and ideal regression of the blue to red bounding boxes is consistent.</p><p>Some approaches have been proposed to solve these problems based on the above analysis. For example, IoUsmooth L1 <ref type="bibr" target="#b2">[3]</ref> loss introduces the IoU factor, and modular rotation loss <ref type="bibr" target="#b20">[21]</ref> increases the boundary constraint to elim-inate the sudden increase in boundary loss and reduce the difficulty of model learning. However, these methods are still regression-based detection methods, and no solution is given from the root cause.</p><p>In this paper, we will start from a new perspective and replace regression with classification to achieve better and more robust rotation detectors. We reproduce some classic rotation detectors based on regression and compare them visually under boundary conditions, as shown in <ref type="figure" target="#fig_4">Figure  4</ref>(a) to <ref type="figure" target="#fig_4">Figure 4(d)</ref>. In contrast, CSL-based and DCL-based protocols have no boundary problem, as shown in <ref type="figure" target="#fig_4">Figure  4</ref>(i) and 4(e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Vanilla Angular Classification</head><p>The main cause of boundary problems based on regression methods is that the ideal predictions are beyond the defined range. Therefore, we consider the prediction of object angle as a classification task to restrict the prediction range. One simple solution is to use the object angle as its category label, and the number of categories is related to the angle range. <ref type="figure" target="#fig_5">Figure 5</ref>(a) shows the label setting for a vanilla classification problem (one-hot label encoding). The conversion from regression to classification can cause certain accuracy error. Taking the five-parameter method with 180 ? angle range as an example: ? (default ? = 1 ? ) degree per interval refers to a category for labeling. It calculates the maximum accuracy error M ax(error) and the expected accuracy error E(error):</p><formula xml:id="formula_1">M ax(error) = ? 2 E(error) = b a x b ? a dx = ? 2 0 x ? 2 ? 0 dx = ? 4<label>(1)</label></formula><p>where ? = AR/C ? indicates the angle discretization granularity. AR and C ? represents angle range (the default value is 180) and the number of angle categories, respectively. Based on the above equations, one can see the error is slight for a rotation detector with small enough angle discrete granularity ?. For example, when two rectangles with a 1 : 9 aspect ratio differ by 0.25 ? and 0.5 ? (default expected and maximum accuracy error), the Intersection over Union (IoU) between them only decreases by 0.02 and 0.05.</p><p>The discrete equation and prediction equation of the angle are as follows:</p><formula xml:id="formula_2">Encode: One-Hot(?Round((? gt ? 90)/?)) Decode: 90 ? ? (0.5 + Argmax(Sigmoid(logits)))<label>(2)</label></formula><p>where ? gt presents the angle decimal label. Applying vanilla classification methods to prediction of angles is appeared earlier in the field of face detection <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. As these works only need approximate orientation range, e.g.10-degree (? = 10) in <ref type="bibr" target="#b13">[14]</ref>. We find that vanilla angular classification methods are difficult to deal with objects with multiple categories and large aspect ratio, e.g. DOTA dataset. In contrast, the small aspect ratio and single category characteristics in face detection make it unnecessary for high-precision angle prediction, so highprecision angle classification still has not been solved. </p><formula xml:id="formula_3">(a) RetinaNet-H [1] (b) FPN-H [18] (c) R 3 Det [1] (d) IoU-Smooth L1 [3] (e) 180 ? -DCL (f) 180 ? -CSL-Pulse (g) 180 ? -CSL-Rect. (h) 180 ? -CSL-Triangle (i) 180 ? -CSL-Gaussian (j) 90 ? -CSL-Gaussian</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Circular Smooth Label for Angular Classification</head><p>In our analysis, there are two reasons why vanilla classification methods cannot obtain high-precision angle prediction for rotation detection: Reason i) The EoE problem still exists when the bounding box uses the 90 ? -based protocol, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>(j). Moreover, 90 ? -based protocol has two different border cases (vertical and horizontal), while 180 ? -based protocol has only vertical border cases.</p><p>Reason ii) Note vanilla classification loss is agnostic to the angle distance between the predicted label and ground truth label, thus it is inappropriate for the nature of the angle prediction problem. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>(a), when the ground truth is 0 ? and the prediction results of the classifier are 1 ? and ?90 ? respectively, their prediction losses are the same, but the prediction results close to ground truth should be allowed from a detection perspective.</p><p>Therefore, Circular Smooth Label (CSL) technique is designed to obtain more robust angular prediction through classification without suffering boundary conditions, including EoE and PoA. It should be noted that CSL can only solve the PoA, and the EoE problem can be solved by the 180 ? angular definition method. It can be clearly seen from <ref type="figure" target="#fig_5">Figure 5</ref>(b) that CSL involves a circular label encoding with periodicity, and the assigned label value is smooth with a (a) One-hot label.</p><p>(b) Circle smooth label. certain tolerance. The expression of CSL is as follows:</p><formula xml:id="formula_4">CSL(x) = g(x), ? ? r &lt; x &lt; ? + r 0, otherwise<label>(3)</label></formula><p>where g(x) is a window function. r is the radius of the window function. ? represents the angle of the current bounding box. An ideal window function g(x) is required to hold the following properties:   <ref type="figure">6</ref>. The relationship between the various angle encoding methods.</p><formula xml:id="formula_5">? Symmetry: 0 ? g(? + ?) = g(? ? ?) ? 1, |?| &lt; r. ? is the center of symmetry. ? Maximum: g(?) = 1. ? Monotonic: 0 ? g(???) ? g(???) ? 1, |?| &lt; |?| &lt; r.</formula><p>The function presents a monotonous non-increasing trend from the center point to both sides <ref type="figure" target="#fig_5">Figure 5</ref>(b) shows four efficient window functions that meet the above four properties: pulse functions, rectangular functions, triangle functions, and Gaussian functions. Note that the label value is continuous at the boundary and there is no arbitrary accuracy error due to the periodicity of CSL. In addition, one-hot label (vanilla classification) is equivalent to CSL when the window function is a pulse function or the radius of the window function is very small. Equation 4 describes the angle prediction process in CSL:</p><formula xml:id="formula_6">Encode: CSL(?Round((? gt ? 90)/?)) Decode: 90 ? ?(Argmax(Sigmoid(logits)) + 0.5)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Densely Coded Label for Angular Classification</head><p>The CSL-based detectors adopt the so-called Sparsely Coded Label (SCL) encoding technique. Although CSL has many good properties for angle prediction, the The design of CSL will cause the prediction layer to introduce too much parameter and calculation, resulting in inefficiency of the detector. Specifically, the One-Hot and CSL described above are sparse angle encoding methods, which often leads to excessively long angle encoding length:</p><formula xml:id="formula_7">L one?hot = L csl = AR/?<label>(5)</label></formula><p>Binary Coded Label (BCL) <ref type="bibr" target="#b44">[45]</ref> and Gray Coded Label (GCL) <ref type="bibr" target="#b45">[46]</ref> are two Densely Coded Label (DCL) methods commonly used in the field of electronic communication. Their advantage is that they can represent a larger range of values with less coding length. Thus, they can effectively solve the problem of excessively long coding length in CSL and One-Hot based methods. BCL processes the angle by binarization to obtain a string of codes represented by multiple '0' and '1'. In the encoding of a group of numbers, if any two adjacent codes differ only by one binary number, then this kind of encoding is called Gray Code. In addition, because only one digit is different between the maximum number and the minimum number, it is also called Cyclic Code. The encoding forms between adjacent angles are not much different, which makes GCL also have a certain classification tolerance. The cycle characteristics of GCL are also consistent with circular design idea of CSL. <ref type="table">Table 1   TABLE 1</ref> The three-digit binary code and gray code corresponding to the decimal number.  compares the coding results of BCL and GCL and <ref type="figure">Figure  6</ref> shows the relationship between various angle encoding methods.</p><p>The code length of DCL is:</p><formula xml:id="formula_8">L dcl = log 2 (AR/?)<label>(6)</label></formula><p>We will use a specific calculation example to explain why the code length has such a large impact on the amount of detection model parameters and calculations. Take RetinaNet as an example, the number of prediction layer channels can be calculated as follows:</p><formula xml:id="formula_9">C o = A ? L<label>(7)</label></formula><p>where A represents the number of anchors, and A = scale num ? ratio num ? angle num. In this paper scale num = 3, ratio num = 7, angle num = 1.</p><p>For all prediction layers {P3, P4, P5, P6, P7}, the total Flops and Params are calculated as follows:</p><formula xml:id="formula_10">F lops = 7 i=3 F lops i = 7 i=3 2C i i K 2 i H i W i C i o P arams =C i K 2 C o<label>(8)</label></formula><p>where K i , H i and W i denote the convolution kernel size of the i-th level prediction layer, and the height and width of the input feature map for the i-th level prediction layer. C i and C o represent the number of input and output channels of the i-th level prediction layer. It should be noted that the parameters of different levels of prediction layers are shared. According to the default settings of our paper, the input image size is 800, C i = 256, C o = AL = 21L, K=3. Then, taking AR = 180, w = 1 as an example, the code length required by CSL and One-Hot are L onehot = L csl = 180, while the code length of DCL is only L dcl = 8. The total Flops and Params of all prediction layers are 1, 291, 175, 424L and 48, 384L. The huge base make the prediction layer occupy the main calculations and parameters of the model. Therefore, the shortening of the code length is necessary and important. Finally, we have counted the total parameters and calculations of three different models, as shown in <ref type="table" target="#tab_1">Table 2</ref>. From the perspective of GFlops and Params, detectors based on CSL have increased by about 82.96% and 45.63%, respectively. In contrast, DCL-based protocol only increases by 3.24% and 0.92%. The training and testing time of RetinaNet-DCL is about 3 times and 2 times faster than RetinaNet-CSL, respectively.</p><p>In the DCL-based method, only the number of categories is a power of 2 to ensure that each coding corresponds to a valid angle. For example, if the 180 degree range is divided into 2 8 = 256 categories, then the range of each division interval is ? = 180/256 = 0.703125 ? . According to the M ax(error) = ?/2 and E(error) = ?/4, the maximum and expected accuracy error are only 0.3515625 ? and 0.17578125 ? , whose influence on final detecton accuracy can be negligible. However, the above condition is not necessary. We find that even with some redundant invalid codes, there is no significant drop in final performance. Equation 9 specifies the encoding and decoding process of DCL (take BCL as an example):</p><p>Encode: Bin(?Round((? gt ? 90)/?)) Decode: 90 ? ?Int(Round(Sigmoid(logits))) (9) <ref type="figure" target="#fig_8">Figure 7</ref> gives the examples of encoding and decoding process of One-Hot, CSL-Gaussian and BCL for angle prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Angle Fine-Tuning</head><p>A larger angle discrete granularity ? can alleviate the problem of too many parameters in the prediction layer to a certain extent, and reduce the dependence on the classification ability of the model angle. However, the theoretical angle error in Eq. 1 cannot be ignored at this time. To solve this problem, we predict a smaller angle ? to make up for the error of accuracy caused by angle encoding.</p><formula xml:id="formula_11">? disc =Decode(Encode(? gt )) ? gt =? gt ? ? disc , ? gt ? ? ? 2 , ? 2<label>(10)</label></formula><p>Taking RetinaNet in <ref type="figure">Figure 1</ref> as an example, we will add an extra one-dimensional output at the prediction layer, denoted as ? logits , and then fine-tune the angle by:</p><formula xml:id="formula_12">? pred = (Sigmoid(? logits ) ? 0.5) * ? ? pred = min(max(? pred + ? pred , ?90 ? ), 90 ? )<label>(11)</label></formula><p>where ? pred and ? pred respectively represent the predicted angle before and after fine-tuning the angle. The min and max operations are to avoid PoA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Object Heading Detector</head><p>Compared with rotation detection, object heading detection is a more fine-grained detection task, which aims to determine the head of the object. Although the rotation detection retains the orientation information of the object, it still cannot determine the accurate head of the object based on the angle of the rotating bounding box alone. By a carefully study, we find that the head of the object must be located in the four sides of the rotating bounding box. Inspired by this discovery, we only need to perform an additional simple four-category to predict the head. <ref type="figure" target="#fig_10">Figure 8</ref> shows how the object heading label is defined. One essential prerequisite for realizing object heading detection is an accurate rotation detector, which is expected to satisfy the following two characteristics:</p><p>? In non-boundary situations, the detector can output high-precision rotating bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>In boundary situations, the detector is not sensitive to boundary problem. Therefore, cascade multi-stage strategy and angle classification technique can be combined to solve the above problems. Based on the above analysis, we have adjusted the entire detector as follows:</p><p>? The number of anchors plays a vital role in the performance of the detector, which can be calculated by num scales?num ratios?num angles. Therefore, we do not use any angle classification technique (e.g. CSL or DCL) in the first stage, so that an appropriate number of anchors can be set in the first stage to provide high-quality initial candidate boxes.</p><p>? After using regression prediction to obtain the refined anchor in the first stage, each feature point on the feature map only retains the refined anchor with the highest confidence. The filtering of the refined anchor makes the parameter A is always equal to 1 at each refined stage, which means that we can use angle classification technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We use 90-degree angle definition method, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(a), to reduce AR to a minimum. At the same time, in order to solve the unprocessed boundary problem in the first stage and the EoE problem in the refinement stage, we use the IoU-Smooth L1 loss function <ref type="bibr" target="#b2">[3]</ref> in each stage.</p><p>? Experimental results show that proper adjacent angle prediction fault tolerance can improve the performance of the detector. When the evaluation standard is not too strict (e.g. DOTA uses AP 50 as the evaluation metric), we can appropriately increase w and adjust the radius r of the window function to relieve the pressure on the prediction layer.</p><p>Combining the head prediction strategy and the above procedures, an efficient object heading detection method is fulfilled, which is called OHDet 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Loss Function Design</head><p>Our multi-tasking pipeline contains regression-based prediction branch and angle classification-based prediction branch, to facilitate the performance comparison of the two methods on an equal footing. The model mainly outputs four items for location and size:</p><formula xml:id="formula_13">t x = (x ? x a )/w a , t y = (y ? y a )/h a , t w = log(w /w a ), t h = log(h /h a )<label>(12)</label></formula><p>to match the four targets from the ground truth:</p><formula xml:id="formula_14">t x = (x ? x a )/w a , t y = (y ? y a )/h a , t w = log(w/w a ), t h = log(h/h a )<label>(13)</label></formula><p>where x, y, w, h denote the box's center coordinates, width, height and angle, respectively. Variables x, x a , x are for the ground truth box, anchor box, and predicted box, respectively (likewise for y, w, h).</p><p>As for the angle prediction of the regression branch, we use two forms as the baseline to be compared:</p><p>? Direct regression (Reg.). The model directly predicts the angle offset t ? :</p><formula xml:id="formula_15">t ? =(? ? ? a ) ? ?/180, t ? =(? ? ? a ) ? ?/180<label>(14)</label></formula><p>? Indirect regression (Reg. * ). The model predicts two vectors (t sin ? and t cos ? ) to match the two targets from the ground truth (t sin ? and t cos ? ):</p><p>t sin ? = sin (? ? ?/180), t cos ? = cos (? ? ?/180), t sin ? = sin (? ? ?/180), t cos ? = cos (? ? ?/180)</p><p>To ensure that t 2 sin ? + t 2 cos ? = 1 is satisfied, we will perform the following normalization processing: Indirect regression is a simpler way to avoid boundary problems. The multi-task loss is defined as follows:</p><formula xml:id="formula_17">t sin ? = t sin ? t 2 sin ? + t 2 cos ? , t cos ? = t cos ? t 2 sin ? + t 2 cos ?<label>(16)</label></formula><formula xml:id="formula_18">L = ? 1 N pos Npos n=1 L reg (t n , t n ) + ? 2 N pos Npos n=1 L angle cls (? n , ? n ) + ? 3 N pos Npos n=1 L reg (? gt , ? pred ) + ? 4 N pos Npos n=1 L head (h n , h n ) + ? 5 N N n=1 L cls (p n , l n )<label>(17)</label></formula><p>where N and N pos indicate the total number of samples and positive samples, respectively. t n denotes the predicted vectors, t n is the targets vector of ground truth. ? n and ? n denote the label and prediction of angle, respectively. ? gt and ? pred represent the theoretical angle error and the predicted angle error. h n and h n represent the head of ground truth and prediction bounding box, respectively. l n represents the label of object, p n is the probability distribution of various classes calculated by sigmoid function.</p><p>The hyper-parameter ? k (k = 1, 2, ..5) control the trade-off and are set to {1, 0.5, 20, 0.1, 1} by default. The classification loss L cls , L head and L angle cls are focal loss <ref type="bibr" target="#b14">[15]</ref> or sigmoid cross-entropy loss depends on the detector. The regression loss L reg is smooth L1 loss as used in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We use Tensorflow <ref type="bibr" target="#b46">[47]</ref> to implement the proposed methods on a server with GeForce RTX 2080 Ti and 11G memory. The experiments in this article are initialized by ResNet50 <ref type="bibr" target="#b47">[48]</ref> by default unless otherwise specified. We perform experiments on both aerial benchmarks and scene text benchmarks to verify the generality of our techniques. Weight decay and momentum are set 0.0001 and 0.9, respectively. We employ MomentumOptimizer over 4 GPUs with a total of 4 images per minibatch (1 image per GPU). At each pyramid level, we use anchors at seven aspect ratios {1, 1/2, 2, 1/4, 4, 1/6, 6}, and the settings of the remaining anchor numbers are the same as the original RetinaNet and FPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmarks and Protocls</head><p>DOTA <ref type="bibr" target="#b37">[38]</ref> is a complex aerial image dataset for object detection, which contains objects exhibiting a wide variety of scales, orientations, and shapes. DOTA-v1.0 contains 2,806 aerial images and 15 common object categories from different sensors and platforms. The fully annotated DOTA-v1.0 benchmark contains 188,282 instances, each of which is labeled by an arbitrary quadrilateral. There are two detection tasks for DOTA: horizontal bounding boxes (HBB) and oriented bounding boxes (OBB). The training set, validation set, and test set account for 1/2, 1/6, 1/3 of the entire data set, respectively. In contrast, DOTA-v1.5 uses the same images as DOTA-v1.0, but extremely small instances (less than 10 pixels) are also annotated. Moreover, a new category, containing 402,089 instances in total is added in this version. While DOTA-v2.0 contains 18 common categories, 11,268   <ref type="bibr" target="#b48">[49]</ref> is the Challenge 4 of ICDAR 2015 Robust Reading Competition, which is commonly used for oriented scene text detection and spotting. This dataset includes 1,000 training images and 500 testing images. In training, we first train our model using 9,000 images from ICDAR 2017 MLT training and validation datasets, then we use 1,000 training images to fine-tune our model. ICDAR 2017 MLT <ref type="bibr" target="#b49">[50]</ref> is a multi-lingual text dataset. It includes 7,200 training images, 1,800 validation images and 9,000 testing images. The dataset is composed of complete scene images in 9 languages, and text regions can be in arbitrary orientations, being more diverse and challenging.</p><p>HRSC2016 <ref type="bibr" target="#b50">[51]</ref> contains images from two scenarios: ships on sea and ships close inshore. All images are collected from six harbors around the world. The training, validation and test set include 436, 181 and 444 images, respectively.</p><p>FDDB <ref type="bibr" target="#b51">[52]</ref> is a dataset designed for unconstrained face detection, in which faces have a wide variability of face scales, poses, and appearance. This dataset contains annotations for 5,171 faces in a set of 2,845 images taken from the faces in the Wild dataset <ref type="bibr" target="#b52">[53]</ref>. In our paper, we manually use 70% as the training set and the rest as the validation set.</p><p>OHD-SJTU is our newly collected and public dataset for rotation detection and object heading detection. OHD-SJTU contains two different scale datasets, called OHD-SJTU-S and OHD-SJTU-L. OHD-SJTU-S is collected pub-   licly from Google Earth with 43 large scene images sized 10, 000 ? 10, 000 pixels and 16, 000 ? 16, 000. It contains two object categories (ship and plane) and 4,125 instances (3,343 ships and 782 planes). Each object is labeled by an arbitrary quadrilateral, and the first marked point is the head position of the object to facilitate head prediction. We randomly select 30 original images as the training and validation set, and 13 images as the testing set. <ref type="figure">Figure 9</ref> shows some samples of annotated subimages in OHD-SJTU-S. The scenes cover a decent variety of road scenes and typical: cloud occlusion, seamless dense arrangement, strong changes in illumination/exposure, mixed sea and land scenes and large number of interfering objects. In contrast, OHD-SJTU-L adds more categories and instances, such as small vehicle, large vehicle, harbor, and helicopter. The additional data comes from DOTA-v1.0, but we reprocess the annotations and add the annotations of the object head. According to statistics, OHD-SJTU-L contains six object categories and 113,435 instances. The statistical details are shown in <ref type="table" target="#tab_3">Table  3</ref>. Compared with the AP 50 used by DOTA as the evaluation indicator, OHD-SJTU uses a more stringent AP 50:95 to measure the performance of the method, which poses a further challenge to the high accuracy of the detector. We divide the training and validation images into 600 ? 600 subimages with an overlap of 150 pixels and scale it to 800 ? 800. In the process of cropping the image with the sliding window, objects whose center point is in the subimage are kept. All the used datasets are trained by 20 epochs in total, and the learning rate is reduced tenfold at 12 epochs and 16 epochs, respectively. The initial learning rates for Reti- <ref type="figure">Fig. 9</ref>. Samples of annotated subimages in our collected OHD-SJTU-S. Each object is labeled by an arbitrary quadrilateral, and the first marked point is the head position of the object to facilitate head prediction.  </p><formula xml:id="formula_19">(a) radius=0 (b) radius=2 (c) radius=4 (d) radius=6 (e) radius=8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Comparison of classification and regression. Comparison of four window functions in CSL method. <ref type="table" target="#tab_5">Table 5</ref> shows the performance comparison of the four window functions on the DOTA-v1.0 dataset. It also details the accuracy of the five categories with larger aspect ratio and more border cases in the dataset. We believe that these categories can better reflect the advantages of our method. In general, the Gaussian window function performs best, while the pulse function performs worst because it has not learned any orientation and scale information.  <ref type="figure" target="#fig_4">Figure 4</ref> shows the consistent results with the those in <ref type="table" target="#tab_5">Table  5</ref>. Suitable window radius in CSL method. The Gaussian window form has shown best performance, while here we study the effect of radius of the window function. When the radius is too small, the window function tends to a pulse function. Conversely, the discrimination of all predictable results becomes smaller when the radius is too large. Therefore, we choose a suitable radius range from 0 to 8, <ref type="table" target="#tab_6">Table 6</ref> shows the performance of the two detectors in this range. Although both detectors achieve the best performance with a radius of 6, the single-stage detection method is more sensitive to radius. We speculate that the instancelevel feature extraction capability (like RoI Pooling <ref type="bibr" target="#b15">[16]</ref> and RoI Align <ref type="bibr" target="#b33">[34]</ref>) in the two-stage detector is stronger than the image-level in the single-stage detector. Therefore, the two-stage detection method can distinguish the difference   between the two approaching angles. <ref type="figure" target="#fig_11">Figure 10</ref> compares visualizations using different window radius. When the radius is 0, the detector cannot learn any orientation and scale information, which is consistent with the performance of the pulse function above. As the radius becomes larger and more optimal, the detector can learn the angle in any direction. Please refer to the enlarged part of the figure.</p><formula xml:id="formula_20">(a) ? csl =1 (b) ? csl =3 (c) ? csl =10 (d) ? csl =18 (e) ? csl =30 (f) ? dcl = 180/4 (g) ? dcl = 180/32 (h) ? dcl = 180/128 (i) ? dcl = 180/256</formula><p>Angle discretization granularity ?. In general, the smaller the angle discrete granularity ?, the more accurate the angle predicted by the model, as shown in <ref type="figure" target="#fig_13">Figure 11</ref>. However, considering that the criterion for judging the object to be detected is that IoU is greater than 0.5 (such as the DOTA-v1.0 dataset). Therefore, a proper ? can make the model have a certain degree of fault tolerance, and can achieve better detection performance. It can be seen from <ref type="table" target="#tab_7">Table 7</ref> that when the discrete granularity is 10, the CSL-based protocol can achieve the highest performance on the DOTA-v1.0 dataset, and when ? is 30, the excessive angular prediction error makes the performance of the model drop sharply. Discrete granularity ? can be approximated as a CSL technique with a rectangular window function, which has a certain tolerance in the divided angle interval. The difference between them is that CSL smooths between adjacent angle intervals. Similar conclusions can still be obtained in the DCL method, as shown in <ref type="table" target="#tab_7">Table 7</ref> and <ref type="table" target="#tab_8">Table 8</ref>. In general, the smaller ?, the higher theoretical upper bound of the model's performance. However, the decrease of ? will lead to an increase in the number of angle categories, which poses a challenge to the angle classification performance of the model. Therefore, we need to explore the impact of ? on the detection performance under different IoU thresholds, and find a suitable range of ?. In order to get the performance indicators under different IoU threshold, we conduct experiments on the DOTA-v1.0 validation set, and the number of image iterations per epoch is 40k. According to <ref type="table" target="#tab_8">Table  8</ref>, when the number of angle categories is between 32 and 128, the performance of the model reaches its peak. If the number of categories is too small, the theoretical accuracy loss is too large, resulting in a sharp drop in performance; if the number of categories is too large, the angle classification network of the model cannot be effectively processed and the performance will decrease slightly. <ref type="figure" target="#fig_13">Figure 11</ref> shows the comparison of angle estimates under different ?. Compared with CSL, DCL can set a smaller w (e.g. w &lt; 1) without too much parameter overhead, and no need to adjust the window function radius at the same time. Redundant invalid coding. To make each code have a corresponding different angle value, the number of categories must be a power of 2 in the DCL-based method. However, this is not required. When we only set 180 categories, about 76 codings are invalid, but BCL-based method can still achieve good performance, at 36.35% as shown in <ref type="table" target="#tab_8">Table  8</ref>. We also artificially increase the length based on the theoretical shortest code length to increase the proportion of invalid codes, and the performance is only slightly reduced. <ref type="table" target="#tab_10">Table 9</ref>, including RetinaNet-H, RetinaNet-R, R 3 Det and FPN-H, are used to compare the performance differences among CSL-based, DCL-based and regressionbased protocols. The former two are single-stage detectors, whose anchor format is different. One of the remaining two is a cascade multi-stage strategy based method and the other is a classic two-stage detection method. It can be clearly seen that CSL and DCL have better detection ability for objects with large aspect ratios and more boundary conditions. It also should be noted that CSL and DCL are designed to solve the boundary problem, whose proportion in the entire dataset is relatively small, so the overall performance (mAP) is not as obvious as the five categories listed (5-mAP). Overall, the CSL-based and DCL-based rotation detection algorithms are indeed better baseline choices than the angle Comparison between classification-based and regression-based protocols on the text dataset ICDAR2015, MLT, aerial dataset HRSC2016, and face dataset FDDB. Note 2007 and 2012 in bracket means using the 2007 and 2012 evaluation metric, respectively. Except for FDDB's baseline is RetinaNet <ref type="bibr" target="#b14">[15]</ref>, the others are FPN <ref type="bibr" target="#b17">[18]</ref>.   regression-based protocol.</p><formula xml:id="formula_21">(a) bin=90 (b) bin=15 (c) bin=9 (d) bin=6</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of CSL and DCL on other detectors. Four detectors in</head><p>Performance of CSL and DCL on other datasets. To further verify that CSL-based and DCL-based protocols are better than baseline, we have also verified it in other datasets, including the text dataset ICDAR2015, MLT, and another remote sensing dataset HRSC2016. These three datasets are single-class object detection datasets, whose objects have a large aspect ratio. Although boundary conditions still account for a small proportion of these datasets, CSL and DCL still shows stronger performance advantage. As shown in <ref type="table" target="#tab_11">Table 10</ref>, compared with the regression-based protocol, the CSL-based protocol is improved by 1.21%, 0.56%, and 1.29% (1.4%) respectively under the same experimental configuration. The same improvement is also reflected in the DCL-based protocol. For face dataset FDDB <ref type="figure" target="#fig_0">(Figure 12</ref>), AP 50 cannot better reflect the advantages of the proposed technique due to the aspect ratio of face is small. In contrast, CSL/GCL shows a great performance improvement on AP 75 , about 17.41%/18.25%. For more challenging datasets (e.g. DOTA-v1.5, DOTA-v2.0 in <ref type="table" target="#tab_13">Table 11</ref>), CSL/DCL still has a steady improvement. These experimental results provide strong support for demonstrating the versatility of the CSLbased and DCL-based protocols. Angle Fine-Tuning. <ref type="table" target="#tab_1">Table 12</ref> compares the performance before and after using angle fine-tuning under different sizes of ? on the HRSC2016. The low-precision indicator (mAP 50 ) has a certain tolerance for angle errors. Take two objects with the same scale and the same center as an example, when their aspect ratio is 1:9, their IoU is still close to 0.7 when the angle deviation is 5 ? , which shows that mAP 50 cannot better reflect the advantages of the angle fine-tuning mechanism when ? is small. When ? csl = 1, ? gcl = 180/256, the maximum angle that can be fine-tuned is only 0.5 ? and 0.35 ? according to Eq. 10, so the effectiveness of fine-  tuning technique on mAP 50 is not significant in <ref type="table" target="#tab_1">Table 12</ref>. In contrast, the high-precision indicator (mAP 75 ) well reflects the advantages of the angle fine-tuning technique, and the larger the ?, the more significant its advantages. When ? becomes larger that the theoretical accuracy exceeds the tolerance of mAP 50 , angle fine-tuning technique can effectively adjust the prediction angle to reduce the negative impact of theoretical angle errors and improve both mAP 50 and mAP <ref type="bibr" target="#b74">75</ref> . Visual analysis of angular features. By zooming in on part of <ref type="figure" target="#fig_4">Figure 4</ref>(i), we show that the prediction of the boundary conditions become continuous (for example, two large vehicle in the same direction predicted 90 ? and ?88 ? , respectively). This phenomenon reflects the purpose of de- signing the CSL: the labels are periodic (circular) and the prediction of adjacent angles has a certain tolerance. In order to confirm that the angle classifier has indeed learned this property, we visually analyze the angular features of each region of interest (RoI) in the FPN detector by principal component analysis (PCA) <ref type="bibr" target="#b85">[86]</ref>, as shown in <ref type="figure" target="#fig_2">Figure 13</ref>. The detector does not learn the orientation information well when the pulse window function is used. It can be seen from the first row of <ref type="figure" target="#fig_2">Figure 13</ref> that the feature distribution of RoI is relatively random, and the prediction results of some angles occupy the vast majority. For the Gaussian function, the feature distribution is obvious a ring structures, and the features of adjacent angles are close to each other and have a certain overlap. It is this property that helps CSLbased detectors to eliminate boundary problems and accurately obtain the orientation and scale information. We also show the visualization results when the number of angle categories are 4 and 8, as shown in <ref type="figure" target="#fig_4">Figure 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State-of-the-Art Methods</head><p>Results on HRSC2016. The HRSC2016 contains lots of large aspect ratio ship instances with arbitrary orientation, which poses a huge challenge to the positioning accuracy of the detector.   <ref type="bibr" target="#b86">[87]</ref> or COCO <ref type="bibr" target="#b87">[88]</ref> (e.g. SBD <ref type="bibr" target="#b19">[20]</ref>, SegLink <ref type="bibr" target="#b88">[89]</ref>, TextField <ref type="bibr" target="#b61">[62]</ref>, DBNet <ref type="bibr" target="#b32">[33]</ref>, PAN++ <ref type="bibr" target="#b63">[64]</ref>, PolarMask++ <ref type="bibr" target="#b64">[65]</ref>), model ensemble (e.g. Inceptext <ref type="bibr" target="#b89">[90]</ref>). so we are refrained to over compare with state-of-the-art text detection models tailored to texts, and become more focused on the proposed components themselves which we do not want to couple with other factors. <ref type="table" target="#tab_4">Table 14</ref> shows the comparative experiments on the ICDAR2015, and our methods obtain the competitive F-measure without using external data or powerful pretrained weight. Results on DOTA-v1.0. We choose DOTA-v1.0 as the main validation dataset due to the complexity of the remote sensing and the large number of small, cluttered and rotated objects in the dataset. The used data augmentation include random horizontal, vertical flipping, random graying, and random rotation. Training and testing scale is set to [400, 600, 720, 800, 1000, 1100]. As shown in <ref type="table" target="#tab_5">Table 15</ref>, CSLbased, GCL-based, BCL-based protocols show competitive performance, at the accuracy of 77.26%, 76.97% and 77.37%, respectively. Results on OHD-SJTU. We assess the performance of stateof-the-art rotation object detection methods on OHD-SJTU,  mainly include R 2 CNN, RRPN, RetinaNet, R 3 Det. All experiments are based on the same setting, using ResNet101 as the backbone. Except for data augmentation (include random horizontal, vertical flipping, random graying, and random rotation) is used in OHD-SJTU-S, no other tricks are used. As shown in <ref type="table" target="#tab_6">Table 16</ref>, the large number of dense ship with large aspect ratio in the OHD-SJTU-S brings huge challenges to the high-precision detection capabilities of the detector. RetinaNet-R and R 3 Det use rotating anchor and cascade structure respectively, which makes them stand out in highprecision indicators, such as AP <ref type="bibr" target="#b74">75</ref> . OHDet uses CSL and IoU smooth L1 in combination, which not only reduces the amount of model parameters, but also avoids the side effects of boundary problems. Then OHDet is further combined with the cascade structure to achieve the best detection performance, at about 63.94%. Especially in AP 75 and AP 50:95 , our method is 3.93% and 2.08% higher than the secondbest method. Similar conclusions can also be obtained from OHD-SJTU-L. As shown in <ref type="table" target="#tab_7">Table 17</ref>, OHDet achieves a notable advantage in high-precision indicators and achieves the best performance (about 41.29%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Object Heading Detection Experiment</head><p>For rotation object detection, the expected output includes the object category and the IoU between the predicted bounding box and ground truth, which is often compared with a certain threshold, e.g. 0.5. In contrast, object heading detection additionally outputs the head prediction results. Three indicators are used to measure the performance: OBB mAP, OHD mAP and Head Accuracy. OBB mAP is a detection metric without considering object head prediction, which is consistent with rotation detection. While OHD mAP additionally considers the accuracy of object head prediction. Therefore, for the same model, the upper bound of OHD mAP is OBB mAP when the head prediction accuracy is 100%. The Head Accuracy indicates the accuracy of head prediction in all the detection boxes judged as true positive (TP). <ref type="figure" target="#fig_5">Figure 15</ref> visualizes the object heading detection on different categories. <ref type="table" target="#tab_8">Table 18</ref> shows the performance of OHDet on the two sub-data sets of OHD-SJTU. Due to the high image resolution and clear objects in OHD-SJTU-S, a very high head prediction accuracy can be achieved: 94.25%. However, head prediction still faces challenges in complex environments. For example, the head prediction accuracy is only 64.12% on OHD-SJTU-L dataset, and the detection performance drops from 37.66% to 22.97%. It can be seen that the addition of head prediction conditions will greatly reduce the detection performance. The main difficulties of inaccurate head prediction are as follows: extremely similar head and tail (e.g. LV, SH, SV), fuzzy small object (e.g. SV, SH), densely arranged (e.g. LV, SH, SV), and small sample size (e.g. HC). <ref type="figure" target="#fig_17">Figure 16</ref> shows some bad cases. We hope that the open source of the OHD-SJTU can promote the research of related methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have particularly identified the boundary problems as faced by different regression-based rotation detection methods. The main cause of boundary problems based on regression methods is that the ideal predictions are beyond the defined range. Therefore, considering the prediction of the object angle as a classification problem to better limit the prediction results, and then we design a Circular Smooth Label (CSL) to adapt to the periodicity of the angle and increase the tolerance of classification between adjacent angles with little accuracy error. We also introduce four window functions in CSL and explore the effect of different window radius sizes on detection performance.</p><p>To reduce the excessive model parameters caused by CSL, we further design a Densely Coded Label (DCL), which greatly reduces the length of the encoding while ensuring that the angle prediction accuracy is not reduced. An angle fine-tuning mechanism is also devised to eliminate the theoretical prediction errors caused by angle dispersion which has been a common issue in whatever CSL and DCL. Our devised angle high-precision classification is also the first application in rotation detection.</p><p>We further fulfill the function of object heading detection, called OHDet, which is used to find the head of object. We annotate and release a dataset for rotation detection and object heading detection, called OHD-SJTU. Extensive experiments and visual analysis on different detectors and datasets show the effectiveness of our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Popular definitions of bounding boxes in existing literature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) The 90 ? -regression-based protocol (b) The 180 ? -regression-based protocol (c) Point-regression-based protocol</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration for the boundary problem which persistently exists over three popular categories of regression based protocols. The red solid arrow indicates the actual regression process, and the red dotted shows the ideal regression process. ? The 180 ? -regression-based protocol, as illustrated in Figure 3(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>Point-regression-based protocol, as shown inFigure 3(c). Through further analysis, the boundary discontinuity problem still exists in the eight-parameter regression method due to the advance ordering of corner points. Consider the situation of an eightparameter regression in the boundary case, the ideal regression process should be {(a ? b), (b ? c), (c ? d), (d ? a)}, but the actual regression process from the blue reference box to the green ground truth box</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of four regression-based rotation detection methods and angle classification-based protocols in the boundary case. 'H' and 'R' represent the horizontal and rotating anchors. Red dotted circles indicate some bad cases. Figures 4(f)-4(i) show that the Gaussian window function performs best, while the pulse function performs worst because it has not learned any orientation and scale information. According to Figure 4(i) and Figure 4(j), the 180 ? -CSL-based protocol obviously has better boundary prediction due to the EoE problem still exists in the 90 ? -CSL-based protocol. In general, CSL-based and DCL-based protocols have no boundary problem, as shown in Figure 4(i) and 4(e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Two kinds of labels for angular classification. FL means using the focal loss function<ref type="bibr" target="#b14">[15]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?</head><label></label><figDesc>Periodicity: g(x) = g(x + kT ), k ? N . T = 180/? represents the number of bins into which the angle is divided, and the default value is 180.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.</head><label></label><figDesc>Fig. 6. The relationship between the various angle encoding methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Examples of encoding and decoding process of One-Hot, CSL-Gaussian and BCL for angle prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( a )</head><label>a</label><figDesc>Heading label is 0. (b) Heading label is 1.(c) Heading label is 0.(d) Heading label is 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Definition of object heading label. The green bounding box shows the rotation detection. The red dot denotes the head of the object. The green dot indicates the center of the rotating bounding box. The yellow dotted line shows the object's head orientation, and the coordinate quadrant pointed by the yellow dotted line is the heading label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Visualization of detection results (RetinaNet-H CSL-Based) under different window function radius. The red bounding box indicates that no orientation and scale information has been learned, and the green bounding box is the correct detection result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Figures 4(f)-4(i) show the visualization of the four window functions. According to Figure 4(i) and Figure 4(j), the 180 ? -CSL-based protocol obviously has better boundary prediction due to the EoE problem still exists in the 90 ? -CSL-based protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Visualization of detection results (RetinaNet-Based) under different angle discrete granularity ?. For CSL, The red bounding box indicates that there is a large angle prediction error, and the orange bounding box indicates an acceptable angle prediction error. For DCL, the red and green box indicate ground truth and prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Visual comparison between classification-based and regression-based protocols on the FDDB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 13 . 8 Fig. 14 .</head><label>13814</label><figDesc>Angular feature visualization of the 90-CSL-FPN detector on the DOTA-v1.0 dataset. First, we divide the entire angular range into several bins, and bins are different between columns. The two rows show two-dimensional feature visualizations of pulse and Gaussian function, respectively. Each point represents an RoI of the test set with an index of the bin it belongs to.(a) ? = 180/4 (b) ? = 180/Angular feature visualization of the RetinaNet-DCL. The red dotted lines divide the categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 15 .</head><label>15</label><figDesc>Detection examples of our method in large-scale scenarios on OHD-SJTU. Our method can both effectively handle the dense and rotating cases. The blue border in the bounding box denotes the predicted head of the object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 16 .</head><label>16</label><figDesc>This work was partly supported by National Key Research and Development Program of China (2020AAA0107600), Illustration for the main failure cases of head prediction: extremely similar head and tail, fuzzy small object, densely arranged, and small sample size, etc.Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (U20B2068, 61972250). Xue Yang is partly supported by Wu Wen Jun Honorary Doctoral Scholarship, AI Institute, Shanghai Jiao Tong University.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Comparison of GFlops and Param over rotation detectors, under the same setting and hyperparameters. The baseline is RetinaNet.</figDesc><table><row><cell cols="2">Method ?</cell><cell cols="2">GFlops ?GFlops</cell><cell>Params</cell><cell cols="3">?Params Training Inference</cell></row><row><cell>Reg.</cell><cell>-</cell><cell>139.35</cell><cell cols="2">-36.97 M</cell><cell>-</cell><cell>-</cell></row><row><cell>CSL</cell><cell>1</cell><cell>254.96</cell><cell cols="2">+82.96% 45.63 M</cell><cell>+23.42%</cell><cell>?1/3x</cell><cell>?1/2x</cell></row><row><cell>GCL</cell><cell>1</cell><cell>143.87</cell><cell cols="2">+3.24% 37.31 M</cell><cell>+0.92%</cell><cell>?1x</cell><cell>?1x</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Statistics on the categories and quantities of OHD-SJTU datasets. Abbreviations of the object categories are in brackets.</figDesc><table><row><cell>Tag</cell><cell>Plane (PL)</cell><cell>Ship (SH)</cell><cell>Small vehicle (SV)</cell><cell>Large vehicle (LV)</cell><cell>Harbor (HA)</cell><cell>Helicopter (HC)</cell></row><row><cell>L train</cell><cell>8,614</cell><cell>30,386</cell><cell>26,126</cell><cell>16,969</cell><cell>5,983</cell><cell>630</cell></row><row><cell>L val</cell><cell>2,754</cell><cell>9,985</cell><cell>5,438</cell><cell>4,387</cell><cell>2,090</cell><cell>73</cell></row><row><cell>S train</cell><cell>559</cell><cell>2,318</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>S val</cell><cell>223</cell><cell>1,025</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Comparison between different angle prediction methods on DOTA-v1.0 validation set. For CSL-based protocol, the label mode and windows function radius are set to Gaussian and 6, respectively. The baseline is RetinaNet. The angle range is [?90 ? , 90 ? ).</figDesc><table><row><cell>Angle Pred.</cell><cell>?</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP 50:95</cell></row><row><cell>Reg. (??) Reg.  *  (sin ?, cos ?)</cell><cell>--</cell><cell>62.21 63.23</cell><cell>26.06 30.63</cell><cell>31.49 33.19</cell></row><row><cell>Cls. CSL</cell><cell>1</cell><cell>64.40</cell><cell>32.58</cell><cell>35.04</cell></row><row><cell>Cls. BCL</cell><cell>180/256</cell><cell>65.93</cell><cell>35.66</cell><cell>36.71</cell></row><row><cell>Cls. GCL</cell><cell>180/256</cell><cell>66.13</cell><cell>33.65</cell><cell>36.34</cell></row></table><note>images and 1,793,658 instances. Compared to DOTA-v1.5, it includes the new categories. The 11,268 images in DOTA- v2.0 are split into training, validation, test-dev, and test- challenge sets. We divide the images into 600 ? 600 subim- ages with an overlap of 150 pixels and scale it to 800 ? 800. With all these processes, we obtain about 27,000 patches. The short names for categories are defined as (abbreviation- full name): PL-Plane, BD-Baseball diamond, BR-Bridge, GTF-Ground field track, SV-Small vehicle, LV-Large ve- hicle, SH-Ship, TC-Tennis court, BC-Basketball court, ST- Storage tank, SBF-Soccer-ball field, RA-Roundabout, HA- Harbor, SP-Swimming pool, HC-Helicopter, CC-container crane, AP-airport and HP-helipad. ICDAR2015</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Comparison of the four window functions (with radius set to 6) on the DOTA-v1.0 test set. 5-mAP refers to the mean average precision of the five categories with large aspect ratio. mAP means mean average precision of all 15 categories. The base method is RetinaNet-CSL. Note the EoE problem exists for the angle range [?90 ? , 0 ? ).</figDesc><table><row><cell>Angle Range</cell><cell>Label Mode</cell><cell>BR</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>HA</cell><cell>5-mAP</cell><cell>mAP</cell></row><row><cell></cell><cell>Pulse</cell><cell>9.80</cell><cell>28.04</cell><cell>11.42</cell><cell>18.43</cell><cell>23.35</cell><cell>18.21</cell><cell>39.52</cell></row><row><cell>[?90 ? , 0 ? )</cell><cell>Rectangular Triangle</cell><cell>37.62 37.25</cell><cell>54.28 54.45</cell><cell>48.97 44.01</cell><cell>62.59 60.03</cell><cell>50.26 52.20</cell><cell>50.74 49.59</cell><cell>58.86 60.15</cell></row><row><cell></cell><cell>Gaussian</cell><cell>41.03</cell><cell>59.63</cell><cell>52.57</cell><cell>64.56</cell><cell>54.64</cell><cell>54.49</cell><cell>63.51</cell></row><row><cell></cell><cell>Pulse</cell><cell>13.95</cell><cell>16.79</cell><cell>6.50</cell><cell>16.80</cell><cell>22.48</cell><cell>15.30</cell><cell>42.06</cell></row><row><cell>[?90 ? , 90 ? )</cell><cell>Rectangular Triangle</cell><cell>36.14 32.69</cell><cell>60.80 47.25</cell><cell>50.01 44.39</cell><cell>65.75 54.11</cell><cell>53.17 41.90</cell><cell>53.17 44.07</cell><cell>61.98 57.94</cell></row><row><cell></cell><cell>Gaussian</cell><cell>40.55</cell><cell>66.77</cell><cell>51.50</cell><cell>73.60</cell><cell>46.05</cell><cell>55.69</cell><cell>65.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Comparison of detection mAP under different radius on the DOTA-v1.0 test set. The angle range, label mode and ? are set to [?90 ? , 90 ? ), Gaussian and 1, respectively.</figDesc><table><row><cell>Method</cell><cell>r=0</cell><cell>r=2</cell><cell>r=4</cell><cell>r=6</cell><cell>r=8</cell></row><row><cell>RetinaNet-CSL</cell><cell>40.78</cell><cell>59.23</cell><cell>62.12</cell><cell>65.69</cell><cell>63.99</cell></row><row><cell>FPN-CSL</cell><cell>48.08</cell><cell>70.18</cell><cell>70.09</cell><cell>70.92</cell><cell>69.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Comparison of detection results under different angle discrete granularity ? on the DOTA-v1.0 test set. The angle range is [?90 ? , 90 ? ). For CSL-based protocol, the label mode and windows function radius are set to Gaussian and 1, respectively.</figDesc><table><row><cell>Granularity</cell><cell>?=30</cell><cell>?=18</cell><cell>?=10</cell><cell>?=3</cell><cell>?=1</cell></row><row><cell>RetinaNet-CSL</cell><cell>40.81</cell><cell>66.10</cell><cell>67.38</cell><cell>64.81</cell><cell>58.92</cell></row><row><cell>Granularity</cell><cell>?=180/4</cell><cell>?=180/32</cell><cell>?=180/64</cell><cell>?=180/128</cell><cell>?=180/256</cell></row><row><cell>RetinaNet-GCL</cell><cell>62.38</cell><cell>65.59</cell><cell>67.02</cell><cell>65.14</cell><cell>64.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8</head><label>8</label><figDesc>Comparison of detection results under different angle discretization granularities denoted by ? on the DOTA-v1.0 validation set.</figDesc><table><row><cell>Method</cell><cell>?</cell><cell>BR</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>HA</cell><cell>5-mAP50</cell><cell>mAP50</cell><cell>mAP75</cell><cell>mAP50:95</cell></row><row><cell>Reg</cell><cell>-</cell><cell cols="5">34.52 51.42 50.32 73.37 55.93</cell><cell>53.12</cell><cell>62.21</cell><cell>26.07</cell><cell>31.49</cell></row><row><cell>CSL</cell><cell>180/180</cell><cell cols="5">35.94 53.42 61.06 81.81 62.14</cell><cell>58.87</cell><cell>64.40</cell><cell>32.58</cell><cell>35.04</cell></row><row><cell></cell><cell>180/4</cell><cell cols="5">30.74 40.54 50.98 72.07 59.54</cell><cell>50.77</cell><cell>62.38</cell><cell>24.88</cell><cell>31.01</cell></row><row><cell></cell><cell>180/8</cell><cell cols="5">36.65 52.58 60.46 82.24 61.60</cell><cell>58.71</cell><cell>66.17</cell><cell>33.14</cell><cell>35.77</cell></row><row><cell></cell><cell>180/32</cell><cell cols="5">39.83 54.41 60.62 80.81 60.32</cell><cell>59.20</cell><cell>65.93</cell><cell>35.66</cell><cell>36.71</cell></row><row><cell>BCL</cell><cell>180/64 180/128</cell><cell cols="5">38.22 54.70 60.16 80.75 60.11 36.76 53.73 61.35 82.52 58.42</cell><cell>58.79 58.56</cell><cell>65.00 65.14</cell><cell>34.31 34.28</cell><cell>36.00 35.69</cell></row><row><cell></cell><cell>180/180</cell><cell cols="5">37.42 53.72 58.70 80.73 63.31</cell><cell>58.78</cell><cell>65.83</cell><cell>33.94</cell><cell>36.35</cell></row><row><cell></cell><cell>180/256</cell><cell cols="5">37.66 53.83 60.66 80.43 60.74</cell><cell>58.66</cell><cell>64.97</cell><cell>33.52</cell><cell>35.21</cell></row><row><cell></cell><cell>180/512</cell><cell cols="5">37.93 53.85 58.52 80.04 60.87</cell><cell>58.24</cell><cell>64.88</cell><cell>33.09</cell><cell>34.99</cell></row><row><cell></cell><cell>180/4</cell><cell cols="5">30.90 41.20 48.30 72.93 60.16</cell><cell>50.70</cell><cell>62.98</cell><cell>23.83</cell><cell>30.81</cell></row><row><cell></cell><cell>180/8</cell><cell cols="5">36.88 51.10 59.81 82.40 61.57</cell><cell>58.35</cell><cell>65.23</cell><cell>33.92</cell><cell>35.29</cell></row><row><cell></cell><cell>180/32</cell><cell cols="5">38.04 54.77 60.88 82.75 61.24</cell><cell>59.54</cell><cell>65.11</cell><cell>34.67</cell><cell>36.15</cell></row><row><cell>GCL</cell><cell>180/64</cell><cell cols="5">38.05 54.36 60.59 81.84 60.39</cell><cell>59.05</cell><cell>64.78</cell><cell>33.23</cell><cell>35.67</cell></row><row><cell></cell><cell>180/128</cell><cell cols="5">37.74 54.36 59.43 81.15 60.51</cell><cell>58.64</cell><cell>66.13</cell><cell>33.65</cell><cell>36.34</cell></row><row><cell></cell><cell>180/256</cell><cell cols="5">35.81 53.78 58.35 81.45 59.84</cell><cell>57.85</cell><cell>64.87</cell><cell>33.77</cell><cell>35.97</cell></row><row><cell></cell><cell>180/512</cell><cell cols="5">37.99 54.23 61.61 80.84 62.13</cell><cell>59.36</cell><cell>64.34</cell><cell>34.08</cell><cell>35.92</cell></row><row><cell cols="11">naNet and FPN are 5e-4 and 1e-3 respectively. The number</cell></row><row><cell cols="11">of image iterations per epoch for DOTA-v1.0, DOTA-v1.5,</cell></row><row><cell cols="11">DOTA-v2.0, ICDAR2015, MLT, HRSC2016, FDDB, OHD-</cell></row><row><cell cols="11">SJTU-S and OHD-SJTU-L are 54k, 64k, 80k, 10k, 10k, 5k, 4k,</cell></row><row><cell cols="11">5k and 10k respectively, and doubled if data augmentation</cell></row><row><cell cols="7">and multi-scale training are used.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>com- pares four different angle prediction methods on DOTA-v1.0 validation set, two of which are based on regression and the other two are via classification. Among them, the indirect regression (Reg.* ) is a relatively simple way to eliminate boundary problem, so it has a better performance than the direct regression (Reg.). In contrast, the classification- based prediction methods CSL, BCL and GCL outperform, achieving 35.04%, 36.71% and 36.34% on the DOTA-v1.0 validation set, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Comparison between classification-based and regression-based protocols on the DOTA-v1.0 test set. 'H' and 'R' represent the horizontal and rotating anchors, respectively.</figDesc><table><row><cell>Baseline</cell><cell>Angle Range</cell><cell>Angle Pred.</cell><cell>PoA</cell><cell>EoE</cell><cell>Label Mode</cell><cell>BR</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>HA</cell><cell>5-mAP</cell><cell>mAP</cell></row><row><cell></cell><cell>[?90 ? , 0 ? )</cell><cell>Reg. (??) Cls.: CSL</cell><cell></cell><cell></cell><cell>-Gaussian</cell><cell>41.15 41.03</cell><cell>53.75 59.63</cell><cell>48.30 52.57</cell><cell>55.92 64.56</cell><cell>55.77 54.64</cell><cell>50.98 54.49 (+3.51)</cell><cell>63.18 63.51 (+0.33)</cell></row><row><cell></cell><cell></cell><cell>Cls.: GCL</cell><cell></cell><cell></cell><cell>-</cell><cell>42.49</cell><cell>62.38</cell><cell>49.13</cell><cell>69.06</cell><cell>56.40</cell><cell>55.89 (+4.09)</cell><cell>64.94 (+1.76)</cell></row><row><cell>RetinaNet-H</cell><cell>[?90 ? , 90 ? )</cell><cell>Reg. (??) Reg.  *  (sin ?, cos ?) Cls.: CSL</cell><cell></cell><cell></cell><cell>-Gaussian</cell><cell>38.31 41.52 42.25</cell><cell>60.48 63.94 68.28</cell><cell>49.77 44.95 54.51</cell><cell>68.29 71.18 72.85</cell><cell>51.28 53.22 53.10</cell><cell>53.63 54.96 (+1.33) 58.20 (+4.57)</cell><cell>64.17 65.78 (+1.61) 67.38 (+3.21)</cell></row><row><cell></cell><cell></cell><cell>Cls.: GCL</cell><cell></cell><cell></cell><cell>-</cell><cell>39.78</cell><cell>67.20</cell><cell>56.02</cell><cell>74.10</cell><cell>53.82</cell><cell>58.18 (+4.55)</cell><cell>67.02 (+2.85)</cell></row><row><cell></cell><cell></cell><cell>Cls.: BCL</cell><cell></cell><cell></cell><cell>-</cell><cell>41.40</cell><cell>65.82</cell><cell>56.27</cell><cell>73.80</cell><cell>54.30</cell><cell>58.32 (+4.69)</cell><cell>67.39 (+3.22)</cell></row><row><cell>RetinaNet-R</cell><cell>[?90 ? , 0 ? )</cell><cell>Reg. (??) Cls.: CSL</cell><cell></cell><cell></cell><cell>-Gaussian</cell><cell>32.27 35.14</cell><cell>64.64 63.21</cell><cell>71.01 73.92</cell><cell>68.62 69.49</cell><cell>53.52 55.53</cell><cell>58.01 59.46 (+1.45)</cell><cell>62.76 65.45 (+2.69)</cell></row><row><cell>R 3 Det</cell><cell>[?90 ? , 0 ? )</cell><cell>Reg. (??) Cls.: BCL</cell><cell></cell><cell></cell><cell>-Gaussian</cell><cell>44.15 46.84</cell><cell>75.09 74.87</cell><cell>72.88 74.96</cell><cell>86.04 85.70</cell><cell>61.01 57.72</cell><cell>67.83 68.02 (+0.19)</cell><cell>70.66 71.21 (+0.55)</cell></row><row><cell></cell><cell>[?90 ? , 0 ? )</cell><cell>Reg. (??) Cls.: CSL</cell><cell></cell><cell></cell><cell>-Gaussian</cell><cell>44.78 45.46</cell><cell>70.25 70.22</cell><cell>71.13 71.96</cell><cell>68.80 76.06</cell><cell>54.27 54.84</cell><cell>61.85 63.71 (+1.86)</cell><cell>68.25 69.02 (+0.77)</cell></row><row><cell>FPN-H</cell><cell>[?90 ? , 90 ? )</cell><cell>Reg. (??) Cls.: CSL</cell><cell></cell><cell></cell><cell>-Gaussian</cell><cell>45.88 47.90</cell><cell>69.37 69.66</cell><cell>72.06 74.30</cell><cell>72.96 77.06</cell><cell>62.31 64.59</cell><cell>64.52 66.70 (+2.18)</cell><cell>69.45 70.92 (+1.47)</cell></row><row><cell></cell><cell></cell><cell>Cls.: GCL</cell><cell></cell><cell></cell><cell>-</cell><cell>47.56</cell><cell>69.81</cell><cell>74.03</cell><cell>76.56</cell><cell>64.29</cell><cell>66.45 (+1.93)</cell><cell>70.83 (+1.38)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 10</head><label>10</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 11</head><label>11</label><figDesc></figDesc><table><row><cell cols="5">Ablative study by accuracy (%) of CSL and DCL on the OBB task of</cell></row><row><cell></cell><cell cols="3">DOTA-v1.0/v1.5/v2.0.</cell><cell></cell></row><row><cell>Method</cell><cell>Angle Pred.</cell><cell>DOTA-v1.0</cell><cell>DOTA-v1.5</cell><cell>DOTA-v2.0</cell></row><row><cell></cell><cell>Reg. (??)</cell><cell>64.17</cell><cell>56.10</cell><cell>43.06</cell></row><row><cell>RetinaNet-H</cell><cell>Cls: CSL</cell><cell>67.38</cell><cell>58.55</cell><cell>43.34</cell></row><row><cell></cell><cell>Cls: BCL</cell><cell>67.39</cell><cell>59.38</cell><cell>45.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 12</head><label>12</label><figDesc>Ablation experiment of our proposed angle fine-tuning technique on the HRSC2016 dataset. The baseline is RetinaNet.</figDesc><table><row><cell>Method Fine-Tune</cell><cell cols="2">?csl=18, ?gcl=180/8 AP50 AP75</cell><cell cols="2">?csl=10, ?gcl=180/128 AP50 AP75</cell><cell cols="2">?csl=1, ?gcl=180/256 AP50 AP75</cell></row><row><cell>CSL</cell><cell>46.72 76.17</cell><cell>3.68 20.47</cell><cell>74.07 81.41</cell><cell>16.89 32.57</cell><cell>81.17 81.37</cell><cell>44.75 46.51</cell></row><row><cell>GCL</cell><cell>57.34 66.70</cell><cell>5.36 28.27</cell><cell>77.77 78.07</cell><cell>30.35 32.62</cell><cell>76.60 76.56</cell><cell>34.20 34.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 13</head><label>13</label><figDesc></figDesc><table><row><cell cols="4">Accuracy and speed on HRSC2016. Here (07) and (12) means using</cell></row><row><cell cols="3">the 2007 and 2012 evaluation metric, respectively.</cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">mAP (07) mAP (12)</cell></row><row><cell>R 2 CNN [8]</cell><cell>ResNet101</cell><cell>73.07</cell><cell>79.73</cell></row><row><cell>RC1 &amp; RC2 [51]</cell><cell>VGG16</cell><cell>75.70</cell><cell>-</cell></row><row><cell>RRPN [9]</cell><cell>ResNet101</cell><cell>79.08</cell><cell>85.64</cell></row><row><cell>R 2 PN [54]</cell><cell>VGG16</cell><cell>79.60</cell><cell>-</cell></row><row><cell>RetinaNet-H [1]</cell><cell>ResNet101</cell><cell>82.89</cell><cell>89.27</cell></row><row><cell>RRD [10]</cell><cell>VGG16</cell><cell>84.30</cell><cell>-</cell></row><row><cell>RoI-Transformer [2]</cell><cell>ResNet101</cell><cell>86.20</cell><cell>-</cell></row><row><cell>Gliding Vertex [22]</cell><cell>ResNet101</cell><cell>88.20</cell><cell>-</cell></row><row><cell>BBAVectors [55]</cell><cell>ResNet101</cell><cell>88.60</cell><cell>-</cell></row><row><cell>DRN [56]</cell><cell>Hourglass104</cell><cell>-</cell><cell>92.70</cell></row><row><cell>CenterMap OBB [57]</cell><cell>ResNet50</cell><cell>-</cell><cell>92.80</cell></row><row><cell>SBD [20]</cell><cell>ResNet50</cell><cell>-</cell><cell>93.70</cell></row><row><cell>RetinaNet-R [1]</cell><cell>ResNet101</cell><cell>89.18</cell><cell>95.21</cell></row><row><cell>R 3 Det [1]</cell><cell>ResNet101</cell><cell>89.26</cell><cell>96.01</cell></row><row><cell>CSL</cell><cell>ResNet101</cell><cell>89.62</cell><cell>96.10</cell></row><row><cell>GCL</cell><cell>ResNet101</cell><cell>89.56</cell><cell>96.02</cell></row><row><cell>BCL</cell><cell>ResNet101</cell><cell>89.46</cell><cell>96.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 14</head><label>14</label><figDesc>Detection accuracy on ICDAR2015. ? and ? denote that the method uses external data and stronger pre-trained weight, respectively.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>Backbone</cell><cell cols="3">Precison Recall F-measure</cell></row><row><cell>CTPN [58]</cell><cell>ECCV'16</cell><cell>VGG16</cell><cell>74.2</cell><cell>51.5</cell><cell>60.8</cell></row><row><cell>EAST  ? [6]</cell><cell>CVPR'17</cell><cell>VGG16</cell><cell>83.5</cell><cell>73.4</cell><cell>78.2</cell></row><row><cell>DeepReg [59]</cell><cell>ICCV'17</cell><cell>VGG16</cell><cell>82.0</cell><cell>80.0</cell><cell>81.0</cell></row><row><cell>RRPN  ? [9]</cell><cell>TMM'18</cell><cell>VGG16</cell><cell>82.0</cell><cell>73.0</cell><cell>77.0</cell></row><row><cell>PixelLink [60]</cell><cell>AAAI'18</cell><cell>VGG16</cell><cell>82.9</cell><cell>81.7</cell><cell>82.3</cell></row><row><cell>PAN  ? [61]</cell><cell>ICCV'19</cell><cell>ResNet18</cell><cell>82.9</cell><cell>77.8</cell><cell>80.3</cell></row><row><cell>TextField  ? ? [62]</cell><cell>TIP'19</cell><cell>VGG16</cell><cell>84.3</cell><cell>80.5</cell><cell>82.4</cell></row><row><cell>TextDragon  ? ? [63]</cell><cell>ICCV'19</cell><cell>VGG16</cell><cell>84.8</cell><cell>81.8</cell><cell>83.1</cell></row><row><cell>DBNet(736)  ? [33]</cell><cell>AAAI'20</cell><cell>ResNet18</cell><cell>86.8</cell><cell>78.4</cell><cell>82.3</cell></row><row><cell>DBNet(1,152)  ? [33]</cell><cell>AAAI'20</cell><cell>ResNet50</cell><cell>91.8</cell><cell>83.2</cell><cell>87.3</cell></row><row><cell>PAN++  ? [64]</cell><cell>TPAMI'21</cell><cell>ResNet18</cell><cell>86.7</cell><cell>78.4</cell><cell>82.3</cell></row><row><cell>PAN++  ? ? [64]</cell><cell>TPAMI'21</cell><cell>ResNet50</cell><cell>91.4</cell><cell>83.9</cell><cell>87.5</cell></row><row><cell>PolarMask++  ? [65]</cell><cell>TPAMI'21</cell><cell>ResNet50</cell><cell>86.2</cell><cell>80.0</cell><cell>83.4</cell></row><row><cell>CSL(800) (FPN based)</cell><cell>-</cell><cell>ResNet50</cell><cell>84.3</cell><cell>83.0</cell><cell>83.7</cell></row><row><cell>GCL(800) (FPN based)</cell><cell>-</cell><cell>ResNet50</cell><cell>84.7</cell><cell>82.6</cell><cell>83.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 15</head><label>15</label><figDesc>Detection accuracy (AP) on each category of object and overall performance (mAP) on the DOTA-v1.0 test set, using different backbones.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>FR-O [38]</cell><cell>ResNet101</cell><cell>79.09</cell><cell>69.12</cell><cell>17.17</cell><cell>63.49</cell><cell>34.20</cell><cell>37.16</cell><cell>36.20</cell><cell>89.19</cell><cell>69.60</cell><cell>58.96</cell><cell>49.4</cell><cell>52.52</cell><cell>46.69</cell><cell>44.80</cell><cell>46.30</cell><cell>52.93</cell></row><row><cell>IENet [66]</cell><cell>ResNet101</cell><cell>80.20</cell><cell>64.54</cell><cell>39.82</cell><cell>32.07</cell><cell>49.71</cell><cell>65.01</cell><cell>52.58</cell><cell>81.45</cell><cell>44.66</cell><cell>78.51</cell><cell>46.54</cell><cell>56.73</cell><cell>64.40</cell><cell>64.24</cell><cell>36.75</cell><cell>57.14</cell></row><row><cell>R-DFPN [5]</cell><cell>ResNet101</cell><cell>80.92</cell><cell>65.82</cell><cell>33.77</cell><cell>58.94</cell><cell>55.77</cell><cell>50.94</cell><cell>54.78</cell><cell>90.33</cell><cell>66.34</cell><cell>68.66</cell><cell>48.73</cell><cell>51.76</cell><cell>55.10</cell><cell>51.32</cell><cell>35.88</cell><cell>57.94</cell></row><row><cell>TOSO [67]</cell><cell>ResNet101</cell><cell>80.17</cell><cell>65.59</cell><cell>39.82</cell><cell>39.95</cell><cell>49.71</cell><cell>65.01</cell><cell>53.58</cell><cell>81.45</cell><cell>44.66</cell><cell>78.51</cell><cell>48.85</cell><cell>56.73</cell><cell>64.40</cell><cell>64.24</cell><cell>36.75</cell><cell>57.92</cell></row><row><cell>PIoU [68]</cell><cell>DLA-34 [69]</cell><cell>80.9</cell><cell>69.7</cell><cell>24.1</cell><cell>60.2</cell><cell>38.3</cell><cell>64.4</cell><cell>64.8</cell><cell>90.9</cell><cell>77.2</cell><cell>70.4</cell><cell>46.5</cell><cell>37.1</cell><cell>57.1</cell><cell>61.9</cell><cell>64.0</cell><cell>60.5</cell></row><row><cell>R 2 CNN [8]</cell><cell>ResNet101</cell><cell>80.94</cell><cell>65.67</cell><cell>35.34</cell><cell>67.44</cell><cell>59.92</cell><cell>50.91</cell><cell>55.81</cell><cell>90.67</cell><cell>66.92</cell><cell>72.39</cell><cell>55.06</cell><cell>52.23</cell><cell>55.14</cell><cell>53.35</cell><cell>48.22</cell><cell>60.67</cell></row><row><cell>RRPN [9]</cell><cell>ResNet101</cell><cell>88.52</cell><cell>71.20</cell><cell>31.66</cell><cell>59.30</cell><cell>51.85</cell><cell>56.19</cell><cell>57.25</cell><cell>90.81</cell><cell>72.84</cell><cell>67.38</cell><cell>56.69</cell><cell>52.84</cell><cell>53.08</cell><cell>51.94</cell><cell>53.58</cell><cell>61.01</cell></row><row><cell>Axis Learning [70]</cell><cell>ResNet101</cell><cell>79.53</cell><cell>77.15</cell><cell>38.59</cell><cell>61.15</cell><cell>67.53</cell><cell>70.49</cell><cell>76.30</cell><cell>89.66</cell><cell>79.07</cell><cell>83.53</cell><cell>47.27</cell><cell>61.01</cell><cell>56.28</cell><cell>66.06</cell><cell>36.05</cell><cell>65.98</cell></row><row><cell>ICN [4]</cell><cell>ResNet101</cell><cell>81.40</cell><cell>74.30</cell><cell>47.70</cell><cell>70.30</cell><cell>64.90</cell><cell>67.80</cell><cell>70.00</cell><cell>90.80</cell><cell>79.10</cell><cell>78.20</cell><cell>53.60</cell><cell>62.90</cell><cell>67.00</cell><cell>64.20</cell><cell>50.20</cell><cell>68.20</cell></row><row><cell>RADet [71]</cell><cell>ResNeXt101 [72]</cell><cell>79.45</cell><cell>76.99</cell><cell>48.05</cell><cell>65.83</cell><cell>65.46</cell><cell>74.40</cell><cell>68.86</cell><cell>89.70</cell><cell>78.14</cell><cell>74.97</cell><cell>49.92</cell><cell>64.63</cell><cell>66.14</cell><cell>71.58</cell><cell>62.16</cell><cell>69.09</cell></row><row><cell>RoI-Transformer [2]</cell><cell>ResNet101</cell><cell>88.64</cell><cell>78.52</cell><cell>43.44</cell><cell>75.92</cell><cell>68.81</cell><cell>73.68</cell><cell>83.59</cell><cell>90.74</cell><cell>77.27</cell><cell>81.46</cell><cell>58.39</cell><cell>53.54</cell><cell>62.83</cell><cell>58.93</cell><cell>47.67</cell><cell>69.56</cell></row><row><cell>P-RSDet [73]</cell><cell>ResNet101</cell><cell>89.02</cell><cell>73.65</cell><cell>47.33</cell><cell>72.03</cell><cell>70.58</cell><cell>73.71</cell><cell>72.76</cell><cell>90.82</cell><cell>80.12</cell><cell>81.32</cell><cell>59.45</cell><cell>57.87</cell><cell>60.79</cell><cell>65.21</cell><cell>52.59</cell><cell>69.82</cell></row><row><cell>CAD-Net [74]</cell><cell>ResNet101</cell><cell>87.8</cell><cell>82.4</cell><cell>49.4</cell><cell>73.5</cell><cell>71.1</cell><cell>63.5</cell><cell>76.7</cell><cell>90.9</cell><cell>79.2</cell><cell>73.3</cell><cell>48.4</cell><cell>60.9</cell><cell>62.0</cell><cell>67.0</cell><cell>62.2</cell><cell>69.9</cell></row><row><cell>O 2 -DNet [75]</cell><cell>Hourglass104 [76]</cell><cell>89.31</cell><cell>82.14</cell><cell>47.33</cell><cell>61.21</cell><cell>71.32</cell><cell>74.03</cell><cell>78.62</cell><cell>90.76</cell><cell>82.23</cell><cell>81.36</cell><cell>60.93</cell><cell>60.17</cell><cell>58.21</cell><cell>66.98</cell><cell>61.03</cell><cell>71.04</cell></row><row><cell>AOOD [77]</cell><cell>DPN [78]</cell><cell>89.99</cell><cell>81.25</cell><cell>44.50</cell><cell>73.20</cell><cell>68.90</cell><cell>60.33</cell><cell>66.86</cell><cell>90.89</cell><cell>80.99</cell><cell>86.23</cell><cell>64.98</cell><cell>63.88</cell><cell>65.24</cell><cell>68.36</cell><cell>62.13</cell><cell>71.18</cell></row><row><cell>Cascade-FF [79]</cell><cell>ResNet152</cell><cell>89.9</cell><cell>80.4</cell><cell>51.7</cell><cell>77.4</cell><cell>68.2</cell><cell>75.2</cell><cell>75.6</cell><cell>90.8</cell><cell>78.8</cell><cell>84.4</cell><cell>62.3</cell><cell>64.6</cell><cell>57.7</cell><cell>69.4</cell><cell>50.1</cell><cell>71.8</cell></row><row><cell>BBAVectors [55]</cell><cell>ResNet101</cell><cell>88.35</cell><cell>79.96</cell><cell>50.69</cell><cell>62.18</cell><cell>78.43</cell><cell>78.98</cell><cell>87.94</cell><cell>90.85</cell><cell>83.58</cell><cell>84.35</cell><cell>54.13</cell><cell>60.24</cell><cell>65.22</cell><cell>64.28</cell><cell>55.70</cell><cell>72.32</cell></row><row><cell>SCRDet [3]</cell><cell>ResNet101</cell><cell>89.98</cell><cell>80.65</cell><cell>52.09</cell><cell>68.36</cell><cell>68.36</cell><cell>60.32</cell><cell>72.41</cell><cell>90.85</cell><cell>87.94</cell><cell>86.86</cell><cell>65.02</cell><cell>66.68</cell><cell>66.25</cell><cell>68.24</cell><cell>65.21</cell><cell>72.61</cell></row><row><cell>SARD [80]</cell><cell>ResNet101</cell><cell>89.93</cell><cell>84.11</cell><cell>54.19</cell><cell>72.04</cell><cell>68.41</cell><cell>61.18</cell><cell>66.00</cell><cell>90.82</cell><cell>87.79</cell><cell>86.59</cell><cell>65.65</cell><cell>64.04</cell><cell>66.68</cell><cell>68.84</cell><cell>68.03</cell><cell>72.95</cell></row><row><cell>GLS-Net [81]</cell><cell>ResNet101</cell><cell>88.65</cell><cell>77.40</cell><cell>51.20</cell><cell>71.03</cell><cell>73.30</cell><cell>72.16</cell><cell>84.68</cell><cell>90.87</cell><cell>80.43</cell><cell>85.38</cell><cell>58.33</cell><cell>62.27</cell><cell>67.58</cell><cell>70.69</cell><cell>60.42</cell><cell>72.96</cell></row><row><cell>DRN [56]</cell><cell>Hourglass104</cell><cell>89.71</cell><cell>82.34</cell><cell>47.22</cell><cell>64.10</cell><cell>76.22</cell><cell>74.43</cell><cell>85.84</cell><cell>90.57</cell><cell>86.18</cell><cell>84.89</cell><cell>57.65</cell><cell>61.93</cell><cell>69.30</cell><cell>69.63</cell><cell>58.48</cell><cell>73.23</cell></row><row><cell>FADet [82]</cell><cell>ResNet101</cell><cell>90.21</cell><cell>79.58</cell><cell>45.49</cell><cell>76.41</cell><cell>73.18</cell><cell>68.27</cell><cell>79.56</cell><cell>90.83</cell><cell>83.40</cell><cell>84.68</cell><cell>53.40</cell><cell>65.42</cell><cell>74.17</cell><cell>69.69</cell><cell>64.86</cell><cell>73.28</cell></row><row><cell>MFIAR-Net [83]</cell><cell>ResNet152</cell><cell>89.62</cell><cell>84.03</cell><cell>52.41</cell><cell>70.30</cell><cell>70.13</cell><cell>67.64</cell><cell>77.81</cell><cell>90.85</cell><cell>85.40</cell><cell>86.22</cell><cell>63.21</cell><cell>64.14</cell><cell>68.31</cell><cell>70.21</cell><cell>62.11</cell><cell>73.49</cell></row><row><cell>R 3 Det [1]</cell><cell>ResNet152</cell><cell>89.49</cell><cell>81.17</cell><cell>50.53</cell><cell>66.10</cell><cell>70.92</cell><cell>78.66</cell><cell>78.21</cell><cell>90.81</cell><cell>85.26</cell><cell>84.23</cell><cell>61.81</cell><cell>63.77</cell><cell>68.16</cell><cell>69.83</cell><cell>67.17</cell><cell>73.74</cell></row><row><cell>RSDet [21]</cell><cell>ResNet152</cell><cell>90.1</cell><cell>82.0</cell><cell>53.8</cell><cell>68.5</cell><cell>70.2</cell><cell>78.7</cell><cell>73.6</cell><cell>91.2</cell><cell>87.1</cell><cell>84.7</cell><cell>64.3</cell><cell>68.2</cell><cell>66.1</cell><cell>69.3</cell><cell>63.7</cell><cell>74.1</cell></row><row><cell>Gliding Vertex [22]</cell><cell>ResNet101</cell><cell>89.64</cell><cell>85.00</cell><cell>52.26</cell><cell>77.34</cell><cell>73.01</cell><cell>73.14</cell><cell>86.82</cell><cell>90.74</cell><cell>79.02</cell><cell>86.81</cell><cell>59.55</cell><cell>70.91</cell><cell>72.94</cell><cell>70.86</cell><cell>57.32</cell><cell>75.02</cell></row><row><cell>Mask OBB [37]</cell><cell>ResNeXt-101</cell><cell>89.56</cell><cell>85.95</cell><cell>54.21</cell><cell>72.90</cell><cell>76.52</cell><cell>74.16</cell><cell>85.63</cell><cell>89.85</cell><cell>83.81</cell><cell>86.48</cell><cell>54.89</cell><cell>69.64</cell><cell>73.94</cell><cell>69.06</cell><cell>63.32</cell><cell>75.33</cell></row><row><cell>FFA [84]</cell><cell>ResNet101</cell><cell>90.1</cell><cell>82.7</cell><cell>54.2</cell><cell>75.2</cell><cell>71.0</cell><cell>79.9</cell><cell>83.5</cell><cell>90.7</cell><cell>83.9</cell><cell>84.6</cell><cell>61.2</cell><cell>68.0</cell><cell>70.7</cell><cell>76.0</cell><cell>63.7</cell><cell>75.7</cell></row><row><cell>APE [85]</cell><cell>ResNeXt-101</cell><cell>89.96</cell><cell>83.62</cell><cell>53.42</cell><cell>76.03</cell><cell>74.01</cell><cell>77.16</cell><cell>79.45</cell><cell>90.83</cell><cell>87.15</cell><cell>84.51</cell><cell>67.72</cell><cell>60.33</cell><cell>74.61</cell><cell>71.84</cell><cell>65.55</cell><cell>75.75</cell></row><row><cell>CenterMap OBB [57]</cell><cell>ResNet101</cell><cell>89.83</cell><cell>84.41</cell><cell>54.60</cell><cell>70.25</cell><cell>77.66</cell><cell>78.32</cell><cell>87.19</cell><cell>90.66</cell><cell>84.89</cell><cell>85.27</cell><cell>56.46</cell><cell>69.23</cell><cell>74.13</cell><cell>71.56</cell><cell>66.06</cell><cell>76.03</cell></row><row><cell>CSL</cell><cell>ResNet152</cell><cell>89.33</cell><cell>84.88</cell><cell>53.70</cell><cell>75.68</cell><cell>77.57</cell><cell>80.21</cell><cell>84.18</cell><cell>89.80</cell><cell>86.57</cell><cell>86.22</cell><cell>71.86</cell><cell>64.48</cell><cell>73.48</cell><cell>74.84</cell><cell>66.05</cell><cell>77.26</cell></row><row><cell>GCL</cell><cell>ResNet152</cell><cell>89.26</cell><cell>83.59</cell><cell>53.05</cell><cell>72.76</cell><cell>78.13</cell><cell>81.97</cell><cell>86.94</cell><cell>90.36</cell><cell>85.98</cell><cell>86.94</cell><cell>66.19</cell><cell>65.56</cell><cell>73.29</cell><cell>70.56</cell><cell>69.99</cell><cell>76.97</cell></row><row><cell>BCL</cell><cell>ResNet152</cell><cell>89.32</cell><cell>83.54</cell><cell>53.60</cell><cell>72.70</cell><cell>78.94</cell><cell>82.66</cell><cell>87.27</cell><cell>90.69</cell><cell>86.61</cell><cell>87.98</cell><cell>66.49</cell><cell>66.97</cell><cell>73.20</cell><cell>70.65</cell><cell>69.90</cell><cell>77.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13</head><label>13</label><figDesc>shows that our models achieve superior performances, about 89.62% (96.10%), 89.56% (96.02%) and 89.46% (96.41%), for CSL, GCL and BCL respectively.</figDesc><table /><note>Results on ICDAR2015. Scene text detection has been a well-studied field and many advanced techniques are specifically designated to texts while our proposed model is general for rotation detection. Moreover, to achieve com- petitive performance in text detection, it often further in- volves non-trivial processing and like using external data, such as RRPN [9], PAN [61], TextField [62], FOTS [7] and TextDragon [63], powerful pre-trained weights on SynthText</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 16</head><label>16</label><figDesc>Detection accuracy on each object (AP) and overall performance (mAP) on OHD-SJTU-S. 'H' and 'R' represent the horizontal and rotating anchors, respectively. Here the numbers in the subscript of AP i.e. 50, 75, 95 represent the threshold of IoU. Note our model's performance gain is even pronounced on the challenging AP 75 and AP 50:95 metrics.</figDesc><table><row><cell>Method</cell><cell cols="2">PL (AP 50 ) SH (AP 50 )</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP 50:95</cell></row><row><cell>R 2 CNN [8]</cell><cell>90.91</cell><cell>77.66</cell><cell>84.28</cell><cell>55.00</cell><cell>52.80</cell></row><row><cell>RRPN [9]</cell><cell>90.14</cell><cell>76.13</cell><cell>83.13</cell><cell>27.87</cell><cell>40.74</cell></row><row><cell>RetinaNet-H [1]</cell><cell>90.86</cell><cell>66.32</cell><cell>78.59</cell><cell>58.45</cell><cell>53.07</cell></row><row><cell>RetinaNet-R [1]</cell><cell>90.82</cell><cell>88.14</cell><cell>89.48</cell><cell>74.62</cell><cell>61.86</cell></row><row><cell>R 3 Det [1]</cell><cell>90.82</cell><cell>85.59</cell><cell>88.21</cell><cell>67.13</cell><cell>56.19</cell></row><row><cell>OHDet (ours)</cell><cell>90.74</cell><cell>87.59</cell><cell>89.06</cell><cell>78.55</cell><cell>63.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE 17</head><label>17</label><figDesc>Detection accuracy on each object (AP) and overall performance (mAP) on OHD-SJTU-L. Note our model's performance gain is even pronounced on the challenging AP 75 and AP 50:95 metrics.</figDesc><table><row><cell>Method</cell><cell>PL</cell><cell>SH</cell><cell>SV</cell><cell>LV</cell><cell>HA</cell><cell>HC</cell><cell>AP50</cell><cell>AP75</cell><cell>AP50:95</cell></row><row><cell>R 2 CNN [8]</cell><cell cols="6">90.02 80.83 63.07 64.16 66.36 55.94</cell><cell>70.06</cell><cell>32.70</cell><cell>35.44</cell></row><row><cell>RRPN [9]</cell><cell cols="6">89.55 82.60 57.36 72.26 63.01 45.27</cell><cell>68.34</cell><cell>22.03</cell><cell>31.12</cell></row><row><cell>RetinaNet-H [1]</cell><cell cols="6">90.22 80.04 63.32 63.49 63.73 53.77</cell><cell>69.10</cell><cell>35.90</cell><cell>36.89</cell></row><row><cell>RetinaNet-R [1]</cell><cell cols="6">90.00 86.90 63.24 86.90 62.85 52.35</cell><cell>72.78</cell><cell>40.13</cell><cell>40.58</cell></row><row><cell>R 3 Det [1]</cell><cell cols="6">89.89 87.69 65.20 78.95 57.06 53.50</cell><cell>72.05</cell><cell>36.51</cell><cell>38.57</cell></row><row><cell>OHDet (ours)</cell><cell cols="6">89.73 86.63 61.37 78.80 63.76 54.62</cell><cell>72.49</cell><cell>43.60</cell><cell>41.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 18</head><label>18</label><figDesc>Results of object heading detection on OHD-SJTU dataset, covering six categories under three different settings of IoU. OBB 89.62 85.58 48.45 76.55 61.43 33.87 65.92 38.80 37.66 OHD 59.93 47.57 26.59 35.32 41.29 17.53 38.04 24.86 22.97 Head 74.43 68.39 60.15 57.79 76.66 49.06 64.41 65.17 64.12</figDesc><table><row><cell>Tag</cell><cell>Task</cell><cell>PL</cell><cell>SH</cell><cell>SV</cell><cell>LV</cell><cell>HA</cell><cell>HC</cell><cell>IoU50</cell><cell>IoU75</cell><cell>IoU50:95</cell></row><row><cell></cell><cell>OBB</cell><cell cols="2">90.73 88.59</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.66</cell><cell>75.62</cell><cell>61.49</cell></row><row><cell>S</cell><cell>OHD</cell><cell cols="2">76.89 86.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.65</cell><cell>65.51</cell><cell>55.09</cell></row><row><cell></cell><cell>Head</cell><cell cols="2">90.91 94.87</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92.89</cell><cell>93.81</cell><cell>94.25</cell></row><row><cell>L</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3163" to="3171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fots: Fast oriented text spotting with a unified network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5676" to="5685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time rotationinvariant face detection with progressive calibration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2295" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">High-performance rotation invariant multiview face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="671" to="686" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rotation invariant neural network-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 98CB36231)</title>
		<meeting>1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 98CB36231)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="38" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Omnidirectional scene text detection with sequential-free box discretization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning modulated loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2458" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1452" to="1459" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection with circular smooth label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="677" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dense label encoding for boundary discontinuity free rotation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Alpharotate: A rotation detection benchmark using tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06677</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning shape-aware embedding for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4234" to="4243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time scene text detection with differentiable binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">481</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="649" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mask obb: A semantic attention-based mask oriented bounding box representation for multi-category object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">2930</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Instancelevel future motion estimation in a single image based on ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-G</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Position detection and direction prediction for arbitrary-oriented ships via multitask rotation region convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="50" to="839" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast multiclass vehicle detection on aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mattyus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1938" to="1942" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning a rotation invariant detector with rotatable bounding box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09405</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Drbox-v2: An improved detector with rotatable boxes for target detection in sar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8333" to="8349" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Eagle: Largescale vehicle detection dataset in real-world scenarios using aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kurz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6920" to="6927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Origins of the binary code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="76" to="83" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pulse code communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="page">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>Icdar 2015 competition on robust reading</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the International Conference on Pattern Recognition Applications and Methods</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Who&apos;s in the picture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1745" to="1749" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Oriented object detection in aerial images with box boundary-aware vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2150" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning center probability map for detecting objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4307" to="4323" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="745" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient and accurate arbitrary-shaped text detection with pixel aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8440" to="8449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Textfield: Learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5566" to="5579" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Textdragon: An end-to-end framework for arbitrary shaped text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9076" to="9085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pan++: Towards efficient and accurate end-to-end spotting of arbitrarily-shaped text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Polar-mask++: Enhanced polar representation for single-shot instance segmentation and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mingyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ruimao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Ienet: Interacting embranchment one stage anchor free detector for orientation aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00969</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Toso: Student&apos;st distribution aided one-stage orientation target detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4057" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Piou loss: Towards accurate oriented object detection in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="195" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Axis learning for orientated objects detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">908</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Radet: Refine feature pyramid network and multi-layer attention network for arbitrary-oriented object detection of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">389</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection in remote sensing images based on polar coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="223" to="373" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cad-net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Oriented objects as pairs of middle lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="268" to="279" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection via dense feature fusion and attention model for remote sensing super-resolution image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4467" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Cascade detector with feature fusion for arbitrary-oriented objects in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Sard: Towards scale-aware rotated object detection in aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="173" to="855" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Object detection based on global-local saliency constraint in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1435</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Featureattentioned object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3886" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Multi-scale feature integrated attention-based rotation network for object detection in vhr aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1686</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Rotationaware and multi-scale convolutional neural network for object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="294" to="308" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Adaptive period embedding for representing oriented objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7247" to="7257" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geladi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chemometrics and intelligent laboratory systems</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2550" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Inceptext: a new inception-text module with deformable psroi pooling for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1071" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
