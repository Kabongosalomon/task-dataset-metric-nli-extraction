<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anomaly Detection in Video via Self-Supervised and Multi-Task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>B?rb?l?u</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MBZ University of Artificial Intelligence</orgName>
								<address>
									<addrLine>Abu Dhabi 3 SecurifAI</addrLine>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anomaly Detection in Video via Self-Supervised and Multi-Task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection in video is a challenging computer vision problem. Due to the lack of anomalous events at training time, anomaly detection requires the design of learning methods without full supervision. In this paper, we approach anomalous event detection in video through selfsupervised and multi-task learning at the object level. We first utilize a pre-trained detector to detect objects. Then, we train a 3D convolutional neural network to produce discriminative anomaly-specific information by jointly learning multiple proxy tasks: three self-supervised and one based on knowledge distillation. The self-supervised tasks are: (i) discrimination of forward/backward moving objects (arrow of time), (ii) discrimination of objects in consecutive/intermittent frames (motion irregularity) and (iii) reconstruction of object-specific appearance information. The knowledge distillation task takes into account both classification and detection information, generating large prediction discrepancies between teacher and student models when anomalies occur. To the best of our knowledge, we are the first to approach anomalous event detection in video as a multi-task learning problem, integrating multiple self-supervised and knowledge distillation proxy tasks in a single architecture. Our lightweight architecture outperforms the state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. Additionally, we perform an ablation study demonstrating the importance of integrating self-supervised learning and normality-specific distillation in a multi-task learning setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, a growing interest has been dedicated to the task of detecting anomalous events in video <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>. An anomalous event is commonly defined as an unfamiliar or unexpected event in a given context. For example, a person crossing the road can be viewed as anomalous if the event does not happen on the crosswalk. This example shows that context plays a key role in the definition of anomalous events and, consequently, in the problem formulation. Indeed, the reliance on context, coupled with the large variety of unexpected events, makes it extremely difficult to collect anomalous events for training. Hence, the anomaly detection problem is typically regarded as an outlier detection task. Then, a normality model is fit on normal training data, labeling events that deviate from the model as anomalous. Without being able to employ standard supervision, researchers have proposed alternative approaches ranging from distance-based <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59]</ref> and reconstruction-based strategies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref> to probabilistic <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b57">58]</ref> and change detection methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>In lieu of learning to discriminate directly between normal and anomalous events, related methods approach a different yet connected task. For example, in the pioneering work of Liu et al. <ref type="bibr" target="#b26">[27]</ref>, a neural network learns to predict future video frames. During inference, an event is labeled as anomalous if the predicted future frame exhibits a high reconstruction error. Although the state-of-the-art methods attain impressive results, addressing anomaly detection through a single proxy task is suboptimal, since the proxy task is not well aligned with anomaly detection. For instance, a car stopped in a pedestrian area should be labeled as an anomaly, yet the car is trivial to reconstruct in a future frame (since it is standing still). We therefore propose to perform anomaly detection by training a model jointly on multiple proxy tasks. Following a series of recent methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b60">61]</ref>, we also employ an object detector, subsequently performing anomaly detection at the object level. However, these recent methods take into account a single proxy task. Different from <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b60">61]</ref>, we propose a novel anomaly detection approach that jointly learns a set of multiple proxy tasks through a single object-centric architecture.</p><p>As discussed above, we devise an object-centric approach comprising a 3D convolutional neural network (CNN) that jointly learns the following proxy tasks: (i) predicting the arrow of time (discriminating between forward and backward moving objects), (ii) predicting the irregularity of motion (discriminating between objects cap- <ref type="bibr">Figure 1</ref>. Our anomaly detection framework based on self-supervised and multi-task learning. First, we detect the objects in video with the help of an object detector (YOLOv3). For each object, we devise three self-supervised tasks (learning the arrow of time, predicting motion irregularity and predicting the object appearance in the middle box) and a knowledge distillation task (using YOLOv3 and ResNet-50 as teachers). A 3D convolutional neural network is trained jointly on the four tasks. Models represented with dashed lines are pre-trained. Best viewed in color. tured in consecutive frames versus objects captured in intermittent frames), (iii) reconstructing the appearance of objects (given their appearance in preceding and succeeding frames), (iv) estimating normality-specific class probabilities by distilling pre-trained classification (ImageNet <ref type="bibr" target="#b42">[43]</ref>) and detection (MS COCO <ref type="bibr" target="#b25">[26]</ref>) teachers. To jointly address these self-supervised and knowledge distillation tasks, we integrate a prediction head for each corresponding task, as illustrated in <ref type="figure">Figure 1</ref>. To our knowledge, we are the first to propose a multi-task learning approach that integrates a set of novel self-supervised and knowledge distillation proxy tasks in a single object-centric architecture for anomaly detection in video.</p><p>We perform comprehensive experiments on three benchmarks, namely Avenue <ref type="bibr" target="#b28">[29]</ref>, ShanghaiTech <ref type="bibr" target="#b30">[31]</ref> and UCSD Ped2 <ref type="bibr" target="#b31">[32]</ref>. Our approach outperforms the state-of-the-art methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref> on all three data sets, achieving frame-level AUC scores of 92.8% on Avenue, 90.2% on ShanghaiTech and 99.8% on UCSD Ped2. Additionally, we present empirical evidence confirming that a jointly optimized model on the proposed proxy tasks outperforms single models optimized on individual tasks, thus indicating that modeling anomaly detection through a single proxy task is suboptimal.</p><p>In summary, our contribution is multifold:</p><p>? We introduce learning the arrow of time as a proxy task for anomaly detection. ? We introduce motion irregularity prediction as a proxy task for anomaly detection. ? We introduce model distillation as a proxy task for anomaly detection in video. ? We pose anomaly detection in video as a multi-task learning problem, integrating multiple self-supervised and knowledge distillation tasks into a single model. ? We conduct experiments showing that our approach attains superior results compared to the state-of-the-art methods on three benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>While the early works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b57">58]</ref> on video anomaly detection relied heavily on handcrafted appearance and motion features, the recent literature is abundant in deep learning methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>. For instance, Xu et al. <ref type="bibr" target="#b58">[59]</ref> proposed the use of stacked denoising auto-encoders to automatically learn both appearance and motion features, which are further used as input for multiple one-class SVM models. Hasan et al. <ref type="bibr" target="#b13">[14]</ref> diverged from using auto-encoders simply as feature extractors for subsequent models, leveraging the reconstruction error as an estimator for abnormality. More recently, Wang et al. <ref type="bibr" target="#b53">[54]</ref> proposed a further improvement by combining CNNs with LSTMs, forming a spatiotemporal auto-encoder able to better account for the temporal evolution of spatial features. Wang et al. <ref type="bibr" target="#b53">[54]</ref> rely on the assumption that anomalous events will cause significant discrepancies between future and past frames. Employing generative networks for video anomaly detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref> is another significant line of research that relies on the same principle, that is, synthesizing future frames will prove to be significantly more challenging when an anomalous event occurs than in a normal situation. To this end, Liu et al. <ref type="bibr" target="#b26">[27]</ref> employed a generative model to predict future frames, considering the reconstruction error as an indicator of abnormality. In another similar framework, Lee et al. <ref type="bibr" target="#b23">[24]</ref> proposed to predict the middle frame, considering a bidirectional approach that learns from both past and future frames. Similar to future frame <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref> or middle frame <ref type="bibr" target="#b23">[24]</ref> prediction frameworks, we propose a framework that incorporates middle frame prediction. Different from methods such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b53">54]</ref>, we study middle frame prediction at the object level, enabling the accurate localization of anomalies. Moreover, middle frame prediction is just one of our four proxy tasks. To our knowledge, we are the first to propose learning the arrow of time, motion irregularity prediction and model distillation as proxy tasks for anomaly detection in video. We note that model distillation has been studied as a single task for anomaly detection in still images <ref type="bibr" target="#b2">[3]</ref>. However, our ablation results show that model distillation alone is not sufficient for anomaly detection in video.</p><p>Aside from the direction relying on reconstruction errors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref>, other recent works, such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38]</ref>, tackle the problem from completely different angles. For example, Ramachandra et al. <ref type="bibr" target="#b37">[38]</ref> employed a Siamese network to learn a metric between spatio-temporal video patches. In this scenario, the dissimilarity between patches provides the means to estimate the level of abnormality.</p><p>In addition, anomalous event detection approaches can be divided with respect to the level of analysis. While some frameworks, such as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47]</ref>, approach the problem from a global (frame-level) perspective, methods such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b63">64]</ref> extract features at a local level, e.g. by considering spatio-temporal cubes. In some cases, the detection of anomalous events is explored with multi-level frameworks, a recent example being the work of Lee et al. <ref type="bibr" target="#b23">[24]</ref>. Aside from these mainstream perspectives, Ionescu et al. <ref type="bibr" target="#b16">[17]</ref> introduced a novel object-centric framework, employing a single-shot object detector on each frame, before applying convolutional autoencoders to learn deep unsupervised representations as part of a one-versus-rest classification approach based on clustering training samples into normality clusters. A few recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b60">61]</ref> further explored the same line of research, proposing alternative object-centric frameworks. Similar to object-centric frameworks such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b60">61]</ref>, we employ an object detector, focusing our analysis on the detected objects. Unlike <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b60">61]</ref>, we perform the analysis through a series of proxy self-supervised and model distillation tasks, proposing a novel anomaly detection framework based on multi-task learning. Hence, the only common aspect with the other object-centric methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b60">61]</ref> is the use of an object detector.</p><p>The related methods presented so far follow the mainstream formulation of anomalous event detection, which implies that an anomalous event is an unfamiliar event in a known context. In the mainstream formulation, anomalous events are not available at training time, as it is considered too difficult to collect a sufficiently wide variety of anomalous events. Although our study adopts the mainstream formulation, we acknowledge the recent effort of Sultani et al. <ref type="bibr" target="#b47">[48]</ref>, which considers anomalous events that do not depend on the context. By eliminating the reliance on context, they are able to collect and use anomalous events at training. In their formulation, anomalous event detection becomes equivalent to action recognition in video. We thus consider the line of research initiated by Sultani et al. <ref type="bibr" target="#b47">[48]</ref> and continued by others <ref type="bibr" target="#b64">[65]</ref> less related to our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation and Overview</head><p>Motivation. Modeling anomalous event detection through a single proxy task, e.g. future frame prediction <ref type="bibr" target="#b26">[27]</ref>, is suboptimal due to the lack of perfect alignment between the proxy task and the actual (anomaly detection) task. To reduce the non-alignment of the model with respect to the anomaly detection task, we propose to train the model by jointly optimizing it on multiple proxy tasks. Training. Our framework based on self-supervised and multi-task learning is illustrated in <ref type="figure">Figure 1</ref>. First, we detect the objects in each frame using a pre-trained YOLOv3 <ref type="bibr" target="#b41">[42]</ref> detector, obtaining a list of bounding boxes. For each detected object in the frame i, we create an object-centric temporal sequence by simply cropping the corresponding bounding box from frames {i?t, ..., i?1, i, i+1, ...., i+t}</p><formula xml:id="formula_0">width ? ????????????????????????????????????? ? depth ???????????????????????????????????? 3 ? 3 ? 3 conv 16 3 ? 3 ? 3 conv 32 1 ? 2 ? 2 max-pooling 1 ? 2 ? 2 max-pooling 3 ? 3 ? 3 conv 32 3 ? 3 ? 3 conv 64 1 ? 2 ? 2 max-pooling 1 ? 2 ? 2 max-pooling 3 ? 3 ? 3 conv 32 3 ? 3 ? 3 conv 64 : ?2 ? 2 max-pooling : ?2 ? 2 max-pooling 3 ? 3 ? 3 conv 16 3 ? 3 ? 3 conv 32 3 ? 3 ? 3 conv 16 3 ? 3 ? 3 conv 32 1 ? 2 ? 2 max-pooling 1 ? 2 ? 2 max-pooling 3 ? 3 ? 3 conv 32 3 ? 3 ? 3 conv 64 3 ? 3 ? 3 conv 32 3 ? 3 ? 3 conv 64 1 ? 2 ? 2 max-pooling 1 ? 2 ? 2 max-pooling 3 ? 3 ? 3 conv 32 3 ? 3 ? 3 conv 64 1 ? 2 ? 2 max-pooling 1 ? 2 ? 2 max-pooling 3 ? 3 ? 3 conv 32 3 ? 3 ? 3 conv 64 : ?2 ? 2 max-pooling</formula><p>: ?2 ? 2 max-pooling <ref type="table">Table 1</ref>. Alternative architectures considered for the 3D CNN included in our anomaly detection framework. Global temporal pooling is denoted by ":".</p><p>(without performing any object tracking), resizing each cropped image to 64 ? 64 pixels. For illustration purposes, we set t = 2 in <ref type="figure">Figure 1</ref>. The resulting object-centric sequence is the input of our 3D CNN. Our architecture is formed of the shared 3D CNN followed by four branches (prediction heads), one for each proxy task.</p><p>Inference. During inference, the anomaly score is computed by averaging the scores predicted for each task. For the arrow of time and motion irregularity tasks, we take the probability of the temporal sequence to move backward and the probability of the temporal sequence to be intermittent. For the middle frame prediction task, we consider the mean absolute difference between the ground-truth and the reconstructed object. The last component of the anomaly score is the difference between the class probabilities predicted by YOLOv3 and the corresponding class probabilities predicted by our knowledge distillation branch. We do not include ResNet-50 predictions at inference time to preserve the real-time processing of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Neural Architectures</head><p>Our architecture is composed of a shared CNN and four independent prediction heads. The shared CNN uses 3D convolutions (conv) to model temporal dependencies, while individual branches use only 2D convolutions. When considering the proxy tasks one at a time, we observed accurate results using a relatively shallow and narrow neural architecture formed of three conv layers. When we moved to jointly optimizing our model on multiple proxy tasks, we observed the need to increase the width and depth of our neural network to accommodate for the increased complexity of the multi-task learning problem. We therefore employ a set of four neural architectures considering all possible combinations of shallow, deep, narrow and wide architectures. These are: shallow+narrow, shallow+wide, deep+narrow and deep+wide. The detailed configuration of each 3D CNN architecture is presented in <ref type="table">Table 1</ref>.</p><p>For each network configuration, the spatial size of the RGB input is 64 ? 64 pixels. The 3D conv layers use filters of 3 ? 3 ? 3. Each conv layer is followed by a batch normalization layer and a ReLU activation. Our shallow+narrow 3D CNN is formed of three 3D conv layers and three 3D max-pooling layers. Its first 3D conv layer is composed of 16 filters and the next two conv layers are composed of 32 filters each. The padding is set to "same" and the stride is set to 1. We perform only spatial pooling for the first two 3D max-pooling layers. The pooling size and the stride are both set to 2. The last 3D max-pooling layer performs global temporal pooling, keeping the same configuration as the first two pooling layers at the spatial level. Using temporal pooling only once (in the last pooling layer) enables us to employ a different temporal size for each proxy task. In the shallow+wide configuration, we change the 3D CNN by doubling the number of filters in each conv layer. For the deep+narrow architecture, we increase the number of 3D conv layers from three to six. Finally, in the deep+wide configuration, we double the number of layers as well as the number of filters in each conv layer with respect to the shallow+narrow model.</p><p>In the middle object prediction head, we incorporate a decoder formed of upsampling and 2D conv layers based on 3 ? 3 filters. The number of upsampling operations is always equal to the number of max-pooling layers in the 3D CNN. Similarly, the number of 2D conv layers in the decoder matches the number of 3D conv layers in the 3D CNN. Each upsampling operation is based on nearest neighbor interpolation, increasing the spatial support by a factor of 2?. The last conv layer in the decoder has only three filters in order to reconstruct the RGB input.</p><p>The other three prediction heads share the same configuration, having a 2D conv layer with 32 filters and a maxpooling layer with a spatial support of 2 ? 2. The last layer is a fully-connected layer with either two units to predict the arrow of time and motion irregularity or 1080 units to predict the teachers' output scores for the 1000 ImageNet <ref type="bibr" target="#b42">[43]</ref> classes and the 80 MS COCO <ref type="bibr" target="#b25">[26]</ref> categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proxy Tasks and Joint Learning</head><p>Task 1: Arrow of time. To predict the arrow of time <ref type="bibr" target="#b55">[56]</ref> at the object level, we generate two labeled training samples from each object-centric sequence. The first sample takes the frames in their temporal order, namely (i ? t, ..., i ? 1, i, i + 1, ..., i + t), thus being labeled as forward motion (class 1). The second sample takes the frames in reversed order, namely (i+t, ..., i+1, i, i?1, ..., i?t), being labeled as backward motion (class 2). During inference, we expect the arrow of time to be harder to predict for objects with anomalous motion. Let f be the shared 3D CNN and h T1 be the arrow of time head. Let X (T1) be a forward or backward object-centric sequence of size (2 ? t + 1) ? 64 ? 64 ? 3. We use the cross-entropy loss to train the arrow of time head:</p><formula xml:id="formula_1">L T1 X (T1) , Y (T1) = ? 2 k=1 Y (T1) k log ? (T1) k ,<label>(1)</label></formula><formula xml:id="formula_2">where? (T1) = sof tmax h T1 f (X (T1) and Y (T1)</formula><p>is the one-hot encoding of the ground-truth label for X (T1) . Task 2: Motion irregularity. Assuming that some anomalies can be identified through irregular motion patterns, we train our model to predict if an object-centric sequence has consecutive or intermittent frames (some frames being skipped). To learn motion irregularity, we generate two labeled training samples from each object-centric sequence. The first example captures an object in consecutive frames from i ? t to i + t, the corresponding class being regular motion (class 1). The intermittent object-centric sequence is created by retaining the frame i, then gradually adding t randomly chosen previous frames and t randomly chosen succeeding frames. The intermittent frames are chosen by skipping frames using random gaps in the range {1, 2, 3, 4}. The intermittent object-centric sequence is labeled as irregular motion (class 2). Let h T2 be the irregular motion head and X (T2) be a regular or irregular object-centric sequence of size (2?t+1)?64?64?3. We employ the cross-entropy loss to train the motion irregularity head:</p><formula xml:id="formula_3">L T2 X (T2) , Y (T2) = ? 2 k=1 Y (T2) k log ? (T2) k ,<label>(2)</label></formula><p>where? (T2) = sof tmax h T2 f (X (T2) and Y (T2) is the one-hot encoding of the ground-truth label for X (T2) . Task 3: Middle bounding box prediction. Our 3D CNN model also learns to reconstruct objects detected in the normal training videos. From each object-centric sequence, we select the image samples cropped from frames {i ? t, ..., i ? 1, i + 1, ..., i + t}, forming the input object-centric sequence</p><formula xml:id="formula_4">X (T3) of size (2 ? t) ? 64 ? 64 ? 3.</formula><p>The middle image, corresponding to the bounding box in frame i, represents the target output Y (T3) of size 64 ? 64 ? 3. When we encounter an anomaly with unusual motion, such as a person running, the input object-centric sequence of that person will not contain enough information for the model to accurately reconstruct the middle bounding box, thus being labeled as anomalous. Let h T3 be the middle bounding box prediction head. We use the L 1 loss to learn the middle bounding box prediction task:</p><formula xml:id="formula_5">L T3 X (T3) ,Y (T3) = 1 h?w?c h j=1 w k=1 c l=1 Y (T3) jkl ?? (T3) jkl ,<label>(3)</label></formula><p>where? (T3) = h T3 f X (T3) and h ? w ? c is the size of the output, i.e. h = 64, w = 64 and c = 3.</p><p>Task 4: Model distillation. On the one hand, our 3D CNN model learns to predict the features from the last layer (just before softmax) of a ResNet-50 <ref type="bibr" target="#b14">[15]</ref>, which is pre-trained on ImageNet. On the other hand, our 3D CNN model learns to predict the class probabilities predicted by YOLOv3 <ref type="bibr" target="#b41">[42]</ref>, which is pre-trained on MS COCO. During distillation, our model learns the predictive behavior of the teachers on normal data. During inference, we expect high prediction discrepancies between our student and the YOLOv3 teacher when we encounter an object with unusual appearance or that belongs to an object category not seen during training. We refrain from using ResNet-50 during inference in order to save valuable computational time. We note that YOLOv3 is applied only once on each frame i, the corresponding class probabilities for each detected object being already available during model distillation. During training, we still need to pass each object to ResNet-50 to extract the pre-softmax features. In order to distill the knowledge from the YOLOv3 and ResNet-50 teachers, our student 3D CNN model receives the same input as ResNet-50 and learns to predict the pre-softmax features Y YOLO predicted by YOLOv3. Let X (T4) be the input image comprising a detected object and h T4 be the knowledge distillation head. The model distillation task is learned by minimizing the L 1 loss function:</p><formula xml:id="formula_6">L T4 X (T4) , Y (T4) = 1 n n j=1 Y (T4) j ?? (T4) j ,<label>(4)</label></formula><formula xml:id="formula_7">where? (T4) = h T4 f X (T4) and Y (T4) = Y (T4) ResNet ? Y (T4) YOLO</formula><p>is the concatenation of the 1000 ResNet-50 pre-softmax features and the 80 YOLOv3 class probabilities, resulting in a vector of n = 1080 components. Joint loss. Our 3D CNN model is trained by jointly optimizing it on the four proxy tasks described above. Hence, the model is training using a joint loss function:</p><formula xml:id="formula_8">L total = L T1 + L T2 + L T3 + ? ? L T4 ,<label>(5)</label></formula><p>where ? ? (0, 1] is a weight that regulates the importance of the knowledge distillation task. We empirically observed that L T4 has a typically higher magnitude than the other loss functions, dominating the joint loss without a regularization term. In our experiments, we fine-tune ? with respect to the validation values of the joint loss, before ever applying our framework on the anomaly detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference</head><p>During inference, we utilize YOLOv3 to detect objects in each frame i. For each object, we extract the corresponding object-centric sequence X by cropping the bounding box from the frames {i ? t, ..., i ? 1, i, i + 1, ..., i + t}. We pass each object-centric sequence through our neural model, obtaining the outputs? (T1) ,? (T2) ,? (T3) and Y (T4) , respectively. For the arrow of time proxy task, we take the probability of the temporal sequence to move backward as the anomaly score. For the motion irregularity task, we consider the probability of the gapless test sequence X to be intermittent as a good abnormality indicator. We interpret the mean absolute error between the reconstructed and the ground-truth middle object as the anomaly score provided by the middle bounding box prediction head. For the knowledge distillation task, we consider the absolute difference between the class probabilities predicted by YOLOv3 and those predicted by our model. We compute the final anomaly score of an object as the average of the anomaly scores given by each prediction head:</p><formula xml:id="formula_9">score(X) = 1 4 ? (T1) 2 +? (T2) 2 + avg Y (T3) ?? (T3) + avg Y (T4) YOLO ?? (T4) YOLO .<label>(6)</label></formula><p>Next, we reassemble the detected objects in a pixel-level anomaly map for each frame. Therefore, we can easily localize the anomalous regions in any given frame. To create a smooth pixel-level anomaly map, we apply a 3D mean filter. The anomaly score for a certain frame is given by the maximum score in the corresponding anomaly map. The final frame-level anomaly scores are obtained by applying a temporal Gaussian filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Object-Level versus Frame-Level Detection</head><p>Although performing anomaly detection at the object level enables the accurate localization of anomalies, the downside is that the detection failures of YOLOv3 (due to a limited set of object categories or poor performance) are translated into false negatives. In order to address this limitation, we can apply our framework at the frame level, eliminating YOLOv3 from the pipeline and keeping the other components in place. By fusing the frame-level and objectlevel anomaly scores at a late stage, we can recover some of the false negatives of our object-centric framework. In our experiments, we report the results of our framework based on late fusion, as well as the results at the object level and at the frame level, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Sets</head><p>We perform experiments on three benchmark data sets: Avenue <ref type="bibr" target="#b28">[29]</ref>, ShanghaiTech <ref type="bibr" target="#b30">[31]</ref> and UCSD Ped2 <ref type="bibr" target="#b31">[32]</ref>. Each data set has pre-defined training and test sets, anomalous events being included only at test time.</p><p>Avenue. The Avenue <ref type="bibr" target="#b28">[29]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Setup and Implementation Details</head><p>Evaluation measures. As our main evaluation metric, we consider the area under the curve (AUC) computed with respect to the ground-truth frame-level annotations. The frame-level AUC metric is the most commonly used metric in related works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b61">62]</ref>. Many related works also report the pixel-level AUC for the UCSD Ped2 data set. As explained by Ramachandra et al. <ref type="bibr" target="#b36">[37]</ref>, the pixel-level AUC is a flawed evaluation metric. We thus report our performance on UCSD Ped2 in terms of the region-based detection criterion (RBDC) and the trackbased detection criterion (TBDC). These metrics were recently introduced by Ramachandra et al. <ref type="bibr" target="#b36">[37]</ref> to replace the commonly used pixel-level and frame-level AUC metrics. Parameter tuning. The first step of our framework is object detection based on YOLOv3 <ref type="bibr" target="#b41">[42]</ref>. For Avenue and ShanghaiTech, we keep the detections with a confidence higher than 0.8. Because UCSD Ped2 has a lower resolution, we set the detection confidence to 0.5. We use the same confidence threshold during training and inference.</p><p>We use the first 85% of the frames in each training video to train our models on the proxy tasks, keeping the last 15% to validate the models on each proxy task. We fine-tune the parameters t and ? on our validation sets, before making the transition to anomaly detection. For t, we considered values in the set {1, 2, 3, 4}. As we obtained optimal results with t = 3, we use this value throughout all the anomaly detection experiments. Hence, an object-centric temporal sequence is a tensor of 7 ? 64 ? 64 ? 3 components. We fine-tune the parameter ? controlling the importance of L T4 in Equation <ref type="formula" target="#formula_8">(5)</ref>, considering values in the set {0.1, 0.2, 0.5, 1.0}. We obtained optimal results with ? = 0.5 on UCSD Ped2 and ? = 0.2 on Avenue and Shang-haiTech, respectively. We therefore report anomaly detection results with these optimal settings.</p><p>Each neural network is trained for 30 epochs using the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with a learning rate of 10 ?3 , keeping the default values for the other parameters of Adam. We trained the models using mini-batches of 256 samples for the shallow+narrow architecture, 128 samples for the deep+narrow and shallow+wide architectures and 64 samples for the deep+wide architecture, being limited by our computational resources. For each model, we select the checkpoint with the lowest validation error on the proxy  <ref type="table">Table 2</ref>. Frame-level AUC scores (in %) of the state-of-the-art methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref> versus our deep+wide architecture trained on four proxy tasks at the object level, at the frame level or based on late fusion. The top two results are shown in red and blue.</p><p>tasks to perform anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Anomaly Detection Results</head><p>In <ref type="table">Table 2</ref>, we present the comparative results of our object-level, frame-level and late fusion frameworks versus the state-of-the-art methods, reporting the frame-level AUC scores (whenever available) on the following three bench-    <ref type="table">Table 3</ref>. Frame-level AUC, RBDC and TBDC scores (in %) of two state-of-the-art methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> versus our object-level framework. The best results are highlighted in red. mark data sets: Avenue, ShanghaiTech and UCSD Ped2. Results on Avenue. There are only two methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref> that surpass the 90% threshold on Avenue. Our framework applied at the object level obtains a frame-level AUC of 91.9%, surpassing the state-of-the-art method [17] by 1.5%. When we apply our framework at the frame level, our performance drops considerably, but the method is still able to outperform some recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b50">51]</ref>. When we fuse the object-level anomaly scores with the framelevel anomaly scores, our performance improves, reaching a new state-of-the-art frame-level AUC of 92.8%. In <ref type="figure">Figure</ref>  Remarkably, we are the first to reach a frame-level AUC score of over 90% on ShanghaiTech. Aside from <ref type="bibr" target="#b16">[17]</ref>, our method surpasses all other state-of-the-art approaches by a margin of at least 10.9%. In <ref type="figure" target="#fig_2">Figure 3</ref>, we present some Number</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D CNN Level</head><p>Avenue <ref type="table" target="#tab_4">UCSD Ped2  of  Accuracy  MAE  AUC  Accuracy  MAE  AUC  Tasks  Task 1 Task 2 Task 3 Task 4  Task 1 Task 2 Task 3 Task 4  1</ref> shallow+narrow object 84. anomaly localization examples along with the frame-level anomaly scores for test video 03 0035. Our approach correlates well with the ground-truth annotations.</p><p>Results on UCSD Ped2. UCSD Ped2 is one of the most popular video anomaly detection benchmarks, resulting in 23 works reporting frame-level AUC scores of over 90%.</p><p>The current state-of-the-art method <ref type="bibr" target="#b52">[53]</ref> reports a framelevel AUC of 99.2%. Nevertheless, our method still manages to surpass all previous works, reaching a new state-ofthe-art frame-level AUC of 99.8% on UCSD Ped2. Since RBDC and TBDC are part of a very recent evaluation protocol, there are only two methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> that we can compare with in <ref type="table">Table 3</ref>. We outperform the first method <ref type="bibr" target="#b36">[37]</ref> by significant margins in terms of all metrics. We also surpass the second method by 1.9% in terms of TBDC and by 5.8% in terms of frame-level AUC, our RBDC score being slightly lower. These results show that our approach can accurately localize anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We perform an ablation study on Avenue and UCSD Ped2 to assess the benefit of including each proxy task in our joint multi-task framework. The corresponding results are presented in <ref type="table" target="#tab_4">Table 4</ref>. Along with the anomaly detection performance, we report the performance levels for each proxy task on our validation sets. Considering the individual tasks, we observe that the arrow of time produces the highest frame-level AUC (83.6%) on Avenue, likely because anomalies are caused by unusual actions, e.g. people running. The most suitable tasks for UCSD Ped2 seem to be middle bounding box prediction and knowledge distil-lation, probably because anomalies are caused by objects with unusual appearance, e.g. bikes or cars. We observe increasingly better anomaly detection results as we gradually add more proxy tasks in our joint optimization framework.</p><p>While increasing the number of proxy tasks, we also aim to assess the effect of increasing the width and depth of our neural architecture. We observe performance improvements as we add more layers and filters to our 3D CNN, especially when we jointly optimize on three or four tasks. Hence, we conclude that it is beneficial to increase the learning capacity of the 3D CNN along with the number of proxy tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have proposed a novel anomaly detection method based on self-supervised and multi-task learning, presenting comprehensive results on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. To our knowledge, our method is the first and only to exceed the 90% threshold on all three benchmarks. Additionally, we performed an ablation study showing the benefits of jointly learning multiple proxy tasks for anomaly detection in video. In future work, we will consider exploring additional proxy tasks to further boost the performance of our multi-task framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary</head><p>In the supplementary, we include additional examples of frame-level scores predicted by our object-centric framework. Along with the frame-level scores, we also show anomaly localization examples in specific frames. Besides showing correct detections, we also include a set of false positive and false negative examples. Moreover, the supplementary provides details about the running time and a discussion about the reliance on object detectors and the chosen proxy tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Qualitative Results</head><p>The supplementary results are structured as follows. Avenue. Our framework reaches a state-of-the-art framelevel AUC performance of 92.8% on the Avenue data set, being able to detect anomalies such as: (i) the two, mostly overlapped, individuals dressed in white preforming a dance on one side of the scene, (ii) the child dressed in red that was moving very close to the camera and (iii) the man running on the main alley, all shown in <ref type="figure" target="#fig_4">Figure 4</ref> (top row). Aside from these true positive detections, we present a false positive example of two people that act strangely. In this specific instance, the security agent that took a stance in front of the main alley was wrongly labeled as anomalous, probably because this behavior is not observed during training. Finally, due to the detection failure of the object detector, our framework is not able to label the backpack thrown in the air as an anomaly, generating the false negative illustrated in <ref type="figure" target="#fig_4">Figure 4 (top row)</ref>. This deficiency is compensated by recognizing that the gesture of throwing a backpack into the air performed by the human is indeed anomalous. Figure 5 illustrates how our framework is able to capture the gesture of throwing, labeling the individual as anomalous. Our framework reaches an almost perfect frame-level AUC performance of 99.88% on the fifth test video from the Avenue data set. Additionally, <ref type="figure" target="#fig_6">Figure 6</ref> showcases how our framework is able to detect other object-related anomalies. In this instance, our anomaly score starts to increase as the bike appears in the scene. Our method reports it as a clear anomalous occurrence as it becomes fully visible and moves towards the camera.</p><p>ShanghaiTech. On ShanghaiTech, our framework is able to correctly identify most vehicle-related anomalies. As show in <ref type="figure" target="#fig_4">Figure 4</ref> (second row), objects such as cars and bicycles are regularly labeled as anomalies. However, in the specific scenario presented as false negative in <ref type="figure" target="#fig_4">Figure 4</ref> (second row), a bicycle that was used by two individuals simultaneously managed to pass as a normal event. Aside from vehicles, our framework also labels strange (meaning not previously seen) objects as anomalies when encountered. Accordingly, in the false positive example, the umbrella was detected and labeled as anomalous. <ref type="figure" target="#fig_7">Figures  7 and 8</ref> showcase our anomaly score predictions together with the frame-level ground-truth labels for test videos 06 0144 and 12 0149 from ShanghaiTech, respectively. In the first instance, our method correctly identifies the car as an anomaly, reaching a frame-level AUC of 98.97%, while in the second instance, our framework accurately identifies the individual running behind the group as abnormal, reaching a frame-level AUC of 98.51%. UCSD Ped2. On UCSD Ped2, our method reaches a frame-level AUC of 99.8%, accurately and almost perfectly capturing all anomalous events such as people riding bicycles among the crowd or vehicles making an appearance in the pedestrian area. Objects are missed only in very few particular frames, such as when the bike did not completely enter the scene (being truncated), shown as the false negative example from UCSD Ped2 in <ref type="figure" target="#fig_4">Figure 4</ref> (bottom row). In addition, the individual featured as the false positive leaving the alley through the camera-facing exit is also wrongly labeled as an anomaly. <ref type="figure" target="#fig_9">Figures 9 and 10</ref> showcase the general performance of our method on the UCSD Ped2 data set, reaching perfect frame-level AUC scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Running Time</head><p>Our lightweight model infers the anomaly score of a single object in 6 milliseconds (ms). The YOLOv3 model takes 26 ms per frame to detect the objects. Reassembling the anomaly map from the object-level anomaly scores takes less than 1 ms. With all components in place, our framework runs at 23 FPS with an average of 5 objects per   frame. The reported time includes only the object-level inference, which is the most heavy part (due to the object detector). When we add the frame-level inference, the speed decreases by a small margin, from 23 FPS to 21 FPS. The FPS rates are measured on a single GeForce GTX 1080Ti GPU with 11GB of VRAM.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Discussion</head><p>Dependence on object detector. We note that objectcentric methods are influenced by the quality of object de-  tectors. For example, on Avenue, we observed that our object-centric method does not detect papers (paper is not in the COCO set of classes) or backpacks thrown in the air (backpack is in the COCO set of classes, but the detector fails due to motion blur). Despite not explicitly detecting papers or backpacks, the detector detects the person throwing these objects and our framework labels the respective person as abnormal. The same can happen in the case of fire or explosion, if there is a person nearby that runs away from the fire or that is thrown on the ground by the blast. A pure object-centric framework is expected to increase the number of false negatives due to detection failures, but, in the same time, it significantly reduces the number of false positives (as the framework is focused on objects). Our results show that the object-centric pipeline attains significantly better results compared to its frame-level counterpart. Thus, the benefits of the object detector outweigh its limitations. Moreover, our final framework combines both object-centric and frame-level streams, alleviating the limitations of a pure object-centric method and improving the overall performance. Indeed, the frame-level pipeline can detect all anomaly types. The frame-level framework can localize anomalies by considering the magnitude of reconstruction errors in the output of the middle frame prediction head, just as other reconstruction-based approaches.</p><p>Generating object-centric temporal sequences. We take the bounding box of an object x in frame i and apply the same bounding box in preceding or subsequent frames to form an object-centric temporal sequence. If the object x is detected in another frame, say i + 1, we will use the respective bounding box to generate another object-centric temporal sequence. Although we may end up with multiple slightly different sequences for the same object, this is better than applying an object tracker (which increases time and introduces errors).</p><p>Notes on the chosen proxy tasks. We underline that anomalies can be caused by both abnormal motion and abnormal appearance. Our multi-task framework can detect both anomaly types, since the first two proxy tasks (arrow of time, motion irregularity) focus on motion anomalies, while the last two tasks (middle box prediction, knowledge distillation) focus on appearance anomalies. Although our framework is simple, it is based on careful design thinking and significant effort in formulating the proxy tasks, in a single architecture, to be beneficial for anomaly detection. We believe that its simplicity coupled with its effectiveness in anomaly detection is interesting and compelling. Nevertheless, in future work, additional or alternative proxy tasks can be considered while seeking to further improve the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Frame-level scores and anomaly localization examples for test video 04 from Avenue. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Frame-level scores and anomaly localization examples for test video 03 0035 from ShanghaiTech. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4 illustrates a set of true positive, false positive and false negative examples extracted from our runs on the benchmark data sets. Figures 5 and 6 showcase the overlap between our frame-level anomaly predictions and the groundtruth labels for two videos from Avenue. Similarly, Figures 7 and 8 illustrate the overlap between our frame-level anomaly predictions and the ground-truth labels for two ShanghaiTech videos. Finally, Figures 9, 10 and 11 showcase our frame-level performance for three UCSD Ped2 videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>True positive, false positive and false negative examples from Avenue (top row), ShanghaiTech (second row) and UCSD Ped2 (bottom row). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Frame-level scores and anomaly localization examples for test video 05 from Avenue. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Frame-level scores and anomaly localization examples for test video 16 from Avenue. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Frame-level scores and anomaly localization examples for test video 06 0144 from ShanghaiTech. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Frame-level scores and anomaly localization examples for test video 12 0149 from ShanghaiTech. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Frame-level scores and anomaly localization examples for test video 02 from UCSD Ped2. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Frame-level scores and anomaly localization examples for test video 04 from UCSD Ped2. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Frame-level scores and anomaly localization examples for test video 06 from UCSD Ped2. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>data set contains 16 training videos with normal activity and 21 test videos. Examples of anomalous events in Avenue are related to people running, throwing objects or walking in the wrong direction. The resolution of each video is 360 ? 640 pixels. ShanghaiTech. ShanghaiTech [31] is one of the largest data sets for anomaly detection in video. It consists of 330 train-ing videos and 107 test videos. The training videos contain only normal events, while the test videos contain normal and abnormal sequences. Examples of anomalous events are: robbing, jumping, fighting and riding bikes in pedestrian areas. The resolution of each video is 480?856 pixels. UCSD Ped2. UCSD Ped2 [32] contains 16 training videos with normal activity and 12 test videos. Examples of abnormal events are bikers, skaters and cars in a pedestrian area. The resolution of each video is 240 ? 360 pixels.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.6</cell><cell>98.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.4</cell></row><row><cell>1</cell><cell cols="2">shallow+narrow object</cell><cell>-</cell><cell>91.8</cell><cell>-</cell><cell>-</cell><cell>83.4</cell><cell>-</cell><cell>99.3</cell><cell>-</cell><cell>-</cell><cell>94.9</cell></row><row><cell>1</cell><cell cols="2">shallow+narrow object</cell><cell>-</cell><cell>-</cell><cell>0.0001</cell><cell>-</cell><cell>83.5</cell><cell>-</cell><cell>-</cell><cell>0.0001</cell><cell>-</cell><cell>97.1</cell></row><row><cell>1</cell><cell cols="2">shallow+narrow object</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.0014 73.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.0014 97.1</cell></row><row><cell>2</cell><cell cols="2">shallow+narrow object</cell><cell>80.5</cell><cell>-</cell><cell>0.0315</cell><cell>-</cell><cell>87.7</cell><cell>98.7</cell><cell>-</cell><cell>0.0408</cell><cell>-</cell><cell>97.0</cell></row><row><cell>2</cell><cell>deep+narrow</cell><cell>object</cell><cell>82.6</cell><cell>-</cell><cell>0.0428</cell><cell>-</cell><cell>83.7</cell><cell>95.3</cell><cell>-</cell><cell>0.0520</cell><cell>-</cell><cell>97.2</cell></row><row><cell>2</cell><cell>shallow+wide</cell><cell>object</cell><cell>81.9</cell><cell>-</cell><cell>0.0283</cell><cell>-</cell><cell>83.7</cell><cell>98.9</cell><cell>-</cell><cell>0.0300</cell><cell>-</cell><cell>96.7</cell></row><row><cell>2</cell><cell>deep+wide</cell><cell>object</cell><cell>82.4</cell><cell>-</cell><cell>0.0383</cell><cell>-</cell><cell>84.2</cell><cell>98.5</cell><cell>-</cell><cell>0.0554</cell><cell>-</cell><cell>97.7</cell></row><row><cell>3</cell><cell cols="2">shallow+narrow object</cell><cell>79.6</cell><cell>89.6</cell><cell>0.0350</cell><cell>-</cell><cell>89.1</cell><cell>98.0</cell><cell>98.9</cell><cell>0.0400</cell><cell>-</cell><cell>97.5</cell></row><row><cell>3</cell><cell>deep+narrow</cell><cell>object</cell><cell>89.9</cell><cell>94.4</cell><cell>0.0425</cell><cell>-</cell><cell>91.6</cell><cell>98.8</cell><cell>99.7</cell><cell>0.0501</cell><cell>-</cell><cell>98.6</cell></row><row><cell>3</cell><cell>shallow+wide</cell><cell>object</cell><cell>87.4</cell><cell>93.3</cell><cell>0.0305</cell><cell>-</cell><cell>90.1</cell><cell>98.8</cell><cell>98.4</cell><cell>0.0385</cell><cell>-</cell><cell>97.5</cell></row><row><cell>3</cell><cell>deep+wide</cell><cell>object</cell><cell>90.0</cell><cell>95.2</cell><cell>0.0410</cell><cell>-</cell><cell>90.7</cell><cell>98.9</cell><cell>99.3</cell><cell>0.0433</cell><cell>-</cell><cell>98.8</cell></row><row><cell>4</cell><cell cols="2">shallow+narrow object</cell><cell>81.6</cell><cell>92.2</cell><cell cols="3">0.0337 0.3898 89.6</cell><cell>98.7</cell><cell>99.3</cell><cell cols="3">0.0565 0.3568 99.1</cell></row><row><cell>4</cell><cell>deep+narrow</cell><cell>object</cell><cell>89.6</cell><cell>93.7</cell><cell cols="3">0.0438 0.3952 91.5</cell><cell>99.1</cell><cell>98.4</cell><cell cols="3">0.0499 0.3807 99.0</cell></row><row><cell>4</cell><cell>shallow+wide</cell><cell>object</cell><cell>82.9</cell><cell>91.0</cell><cell cols="3">0.0313 0.3767 89.4</cell><cell>98.8</cell><cell>99.4</cell><cell cols="3">0.0604 0.3575 97.8</cell></row><row><cell>4</cell><cell>deep+wide</cell><cell>object</cell><cell>92.2</cell><cell>95.3</cell><cell cols="3">0.0398 0.3709 91.9</cell><cell>99.0</cell><cell>98.7</cell><cell cols="3">0.0408 0.3576 99.8</cell></row><row><cell>4</cell><cell>deep+wide</cell><cell>frame</cell><cell>92.8</cell><cell>96.1</cell><cell cols="3">0.0199 0.5608 86.9</cell><cell>99.9</cell><cell>99.6</cell><cell cols="3">0.0104 0.4979 92.4</cell></row></table><note>. Accuracy rates for Task 1 (arrow of time) and Task 2 (motion irregularity), mean absolute errors (MAE) for Task 3 (middle box prediction) and Task 4 (model distillation), and frame-level AUC scores (in %) for anomaly detection obtained by adding one proxy task at a time. The best frame-level AUC scores are highlighted in red.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research leading to these results has received funding from the EEA Grants 2014-2021, under Project contract no. EEA-RO-NO-2018-0496. This article has also benefited from the support of the Romanian Young Academy, which is funded by Stiftung Mercator and the Alexander von Humboldt Foundation for the period 2020-2022.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilan</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daviv</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video parsing for abnormality detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borislav</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2415" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yie-Tarng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsien</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2909" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Abnormal event detection in crowded scenes using sparse representation. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-07" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1851" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Discriminative Framework for Anomaly Detection in Large Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dual Discriminator Generative Adversarial Network for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiushan</forename><surname>Nie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE Access</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="88170" to="88176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Any-Shot Sequential Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keval</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="934" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Continual Learning for Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keval</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="254" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online Detection of Abnormal Events Using Incremental Coding Length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jayanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonny</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3755" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep event models for crowd anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachuang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="548" to="556" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton Van Den Hengel. Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object-Centric Auto-Encoders and Dummy Anomalies for Abnormal Event Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2895" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detecting abnormal events in video using Narrowed Normality Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TAM-Net: Temporal Enhanced Appearance-to-Motion Generative Network for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangli</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuesheng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A space-time MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">STAN: Spatio-temporal adversarial networks for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Hak Gu Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1323" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BMAN: Bidirectional Multi-Scale Aggregation Networks for Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Hak Gu Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2395" to="2408" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Future Frame Prediction for Anomaly Detection -A New Baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classifier Two-Sample Test for Video Anomaly Detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusha</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnaba?s</forename><surname>P?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection at 150 FPS in MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Few-Shot Scene-Adaptive Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="125" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Wei-Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viral</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Trong-Nguyen Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-trained Deep Ordinal Regression for End-to-End Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12173" to="12182" />
		</imprint>
	</monogr>
	<note>Anton van den Hengel, and Xiao Bai</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Memory-guided Normality for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoun</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14372" to="14381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Street Scene: A new dataset and evaluation protocol for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharathkumar</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a distance function with a Siamese network to localize anomalies in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharathkumar</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranga</forename><surname>Vatsavai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2598" to="2607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A Survey of Single-Scene Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharathkumar</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranga Raju</forename><surname>Vatsavai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05993</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1689" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection in Videos using Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep-Cascade: Cascading 3D Deep Neural Networks for Fast Anomaly Detection and Localization in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1992" to="2004" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep-anomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2112" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep Appearance Features for Abnormal Behavior Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIAP</title>
		<meeting>ICIAP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10485</biblScope>
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Real-World Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scene-Aware Context Reasoning for Unsupervised Abnormal Event Detection in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="184" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Online growing neural gas for anomaly detection in changing surveillance scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Integrating prediction and reconstruction for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="123" to="130" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Anomaly Detection using a Convolutional Winner-Take-All Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Robust Anomaly Detection in Videos Using Multilevel Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Tu Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5216" to="5223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection in Videos Using Hybrid Spatio-Temporal Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2276" to="2280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cluster Attention Contrast for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2463" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning and Using the Arrow of Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8052" to="8060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A Deep One-Class Neural Network for Anomalous Event Detection in Complex Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2609" to="2622" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Chaotic Invariants of Lagrangian Particle Trajectories for Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shandong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2054" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning Deep Representations of Appearance and Motion for Anomalous Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>pages 8.1-8.12</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Detecting Anomalous Events in Videos by Learning Deep Representations of Appearance and Motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cloze Test Helps: Effective Video Anomaly Detection via Learning to Complete Video Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanfu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Old is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ha</forename><surname>Muhammad Zaigham Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Ik</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14183" to="14193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Video Anomaly Detection and Localization using Motion-field Shape Description and Homogeneity Testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiulong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weishan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107394</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on locality sensitive hashing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="302" to="311" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Graph Convolutional Label Noise Cleaner: Train a Plug-And-Play Action Classifier for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1237" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
