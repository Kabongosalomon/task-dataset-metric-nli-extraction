<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
							<email>chuxiangxiang@xiaomi.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Lab</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaomi</surname></persName>
						</author>
						<title level="a" type="main">Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the most successful variant and improvement for Trust Region Policy Optimization (TRPO), proximal policy optimization (PPO) has been widely applied across various domains with several advantages: efficient data utilization, easy implementation, and good parallelism. In this paper, a firstorder gradient reinforcement learning algorithm called Policy Optimization with Penalized Point Probability Distance (POP3D), which is a lower bound to the square of total variance divergence is proposed as another powerful variant. Firstly, we talk about the shortcomings of several commonly used algorithms, by which our method is partly motivated. Secondly, we address to overcome these shortcomings by applying POP3D. Thirdly, we dive into its mechanism from the perspective of solution manifold. Finally, we make quantitative comparisons among several state-of-the-art algorithms based on common benchmarks. Simulation results show that POP3D is highly competitive compared with PPO. Besides, our code is released in https://github.com/paperwithcode/pop3d.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the development of deep reinforcement learning, lots of impressive results have been produced in a wide range of fields such as playing Atari game <ref type="bibr" target="#b4">[Mnih et al., 2015;</ref>, controlling robotics , <ref type="bibr">Go [Silver et al., 2017]</ref>, neural architecture search <ref type="bibr" target="#b5">[Tan et al., 2018;</ref><ref type="bibr" target="#b4">Pham et al., 2018]</ref>.</p><p>The basis of a reinforcement learning algorithm is generalized policy iteration <ref type="bibr" target="#b4">[Sutton and Barto, 2018]</ref>, which states two essential iterative steps: policy evaluation and improvement. Among various algorithms, policy gradient is an active branch of reinforcement learning whose foundations are P olicy Gradient T heorem and the most classical algorithm <ref type="bibr">REINFORCEMENT [Sutton and Barto, 2018]</ref>. Since then, handfuls of policy gradient variants have been proposed, such as Deep Deterministic Policy Gradient (DDPG) , Asynchronous Advantage Actor Critic (A3C) <ref type="bibr" target="#b4">[Mnih et al., 2016]</ref>, Actor Critic using Kronecker-factored Trust Region (ACKTR) <ref type="bibr" target="#b6">[Wu et al., 2017]</ref>, Proximal Policy Optimization (PPO) .</p><p>Improving the strategy monotonically had been nontrivial before the trust region policy optimization(TRPO) was proposed <ref type="bibr" target="#b4">[Schulman et al., 2015a]</ref>. Hessian-free strategy: Fisher vector product is utilized to cut down the computing burden. Specifically, Kullback-Leibler divergence(KLD) acts as a hard constraint in place of objective, because its corresponding coefficient is difficult to set for different problems. However, TRPO still has several drawbacks: too complicated, inefficient data usage. Quite a lot of efforts have been devoted to improving TRPO since then and the most distinguished one is PPO.</p><p>PPO can be regarded as a first-order variant of TRPO and have obvious improvements in several facets. In particular, a pessimistic clipped surrogate objective is proposed where TRPO's hard constraint is replaced by the clipped action probability ratio. In such a way, it constructs an unconstrained optimization problem so that any first-order stochastic gradient optimizer can be directly applied. Besides, it's easier to be implemented, more robust against various problems and achieves an impressive result on Atari games <ref type="bibr" target="#b0">[Brockman et al., 2016]</ref>.</p><p>Unfortunately, no further analysis of PPO's success is provided in that paper. In fact, the pessimistic surrogate objective which is the most critical component of PPO still has some limitations.</p><p>As another potential improvement for TRPO and alternative to PPO, this paper focuses on a policy optimization algorithm, where its contributions are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">It proposes a simple variant of TRPO called POP3D</head><p>along with a new surrogate objective containing a point probability penalty item, which is symmetric lower bound to the square of the total variance divergence for two policy distributions. Specifically, it helps to stabilize the learning process and encourage exploration. Furthermore, it escapes from penalty item setting headache along with penalized version TRPO, where is arduous to select one fixed value for various environments.</p><p>2. It achieves state-of-the-art results with a clear margin on 49 Atari games within 40 million frame steps based on two shared metrics. Moreover, it also achieves competitive results compared with PPO in the continuous do-main. 3. It dives into the mechanism of PPO's improvement over TRPO by the perspective of solution manifold, which also plays an important role in our method. 4. It enjoys almost all PPO's advantages such as easy implementation, fast learning ability.</p><p>2 Background and Related Works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Policy Gradient</head><p>Agents interact with the environment and receive rewards which are used to adjust their policy in turn. At state s t , one agent takes strategy ? and transfers to a new state s t+1 , rewarded r t by the environment. Maximizing discounted return (accumulated rewards) R t is its objective. In particular, given a policy ?, R t is defined as</p><formula xml:id="formula_0">R t = ? n=0 (r t + ?r t+1 + ? 2 r t+2 + ... + ? n r t+n ).<label>(1)</label></formula><p>? is the discounted coefficient to control future rewards, which lies in the range (0, 1). Regarding a neural network with parameter ?, the policy ? ? (a|s) can be learned by maximizing Equation 1 using the back-propagation algorithm. Particularly, given Q(s, a) which represents the agent's return in state s after taking action a, the objective function can be written as max ? E s,a log ? ? (a|s)Q(s, a).</p><p>(2) Equation 2 lays the foundation for handfuls of policy gradient based algorithms, Another variant can be deduced by using A(s, a) = Q(s, a) ? V (s) (3) to replace Q(s, a) in Equation 2 equivalently, V (s) can be any function so long as V depends on s but not a. In most cases, state value function is used for V , which not only helps to reduce variations but has clear physical meaning. Formally, it can be written as max ? E s,a log ? ? (a|s)A(s, a).</p><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Advantage Estimate</head><p>One commonly used method for advantage calculation is one step estimation, which estimates</p><formula xml:id="formula_1">A(s t , a) = Q(s t , a) ? V (s t ) = r t + ?V (s t+1 ) ? V (s t ).<label>(5)</label></formula><p>A better estimate for advantage called generalized advantage estimation is proposed in <ref type="bibr">[Schulman et al., 2015b]</ref>, where one, two, three, up to ? time step estimate are combined and summarized using ? based weights, which helps to estimate more accurately. The generalized advantage estimator is defined a?</p><formula xml:id="formula_2">A GAE(?,?) t = ? l=0 (??) l ? V t+l ? V t+l = r t+l + ?V (s t+l+1 ) ? V (s t+l ).<label>(6)</label></formula><p>The parameter ? meets 0 ? ? ? 1, which controls the trade-off between bias and variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Trust Region Policy Optimization</head><p>Schulman et al. propose TRPO to update the policy monotonically. In particular, its mathematical form is</p><formula xml:id="formula_3">max ? E t [ ? ? (a t |s t ) ? ? old (a t |s t )? t ] ?CE t [KL[? ? old (?|s t ), ? ? (?|s t )]] C = 2 ? (1 ? ?) 2 = max s E a?? ? (a|s) [A ? ? old (s, a)]),<label>(7)</label></formula><p>where C is the penalty coefficient.</p><p>In practice, the policy update steps would be too small if C is valued as Equation 7. In fact, it's intractable to calculate C beforehand since it requires traversing all states to reach the maximum. Moreover, inevitable bias and variance will be introduced by estimating the advantages of old policy while training. Instead, a surrogate objective is maximized based on the KLD constraint between the old and new policy, which can be written as below,</p><formula xml:id="formula_4">max ? E t [ ? ? (a t |s t ) ? ? old (a t |s t )? t ] s.t. E t [KL[? ? old (?|s t ), ? ? (?|s t )]] ? ?<label>(8)</label></formula><p>where ? is the KLD upper limitation. In addition, the conjugate gradient algorithm is applied to solve Equation 8 more efficiently. Two major problems have yet to be addressed: one is its complexity even using the conjugate gradient approach, another is compatibility with architectures that involve noise or parameter sharing tricks .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Proximal Policy Optimization</head><p>To overcome the shortcomings of TRPO, PPO replaces the original constrained problem with a pessimistic clipped surrogate objective where KL constraint is implicitly imposed. The loss function can be written as</p><formula xml:id="formula_5">L CLIP (?) = E t [min(r t (?)? t , clip(r t (?), 1 ? , 1 + )? t )] r t (?) = ? ? (a t |s t ) ? ? old (a t |s t ) ,<label>(9)</label></formula><p>where is a hyper-parameter to control the clipping ratio. Except for the clipped PPO version, KL penalty versions including fixed and adaptive KLD. Besides, their simulation results convince that clipped PPO performs best with an obvious margin across various domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Disadvantages of Kullback-Leibler Divergence</head><p>According to Theorem 1 in TRPO paper, the following inequality holds.</p><formula xml:id="formula_6">?(? ? ) ? L ? ? old (? ? ) + 2 ? (1 ? ?) 2 ? 2 ? = D max T V (? ? old , ? ? ) D max T V (? ? old , ? ? ) = max s D T V (? ? old ||? ? )<label>(10)</label></formula><p>TRPO replaces the square of total variation divergence</p><formula xml:id="formula_7">D max T V (? ? old , ? ? ) by D max KL (? ? old , ? ? ) = max s D KL (? ? old ||? ? ).</formula><p>Given a discrete distribution p and q, their total variation divergence</p><formula xml:id="formula_8">D T V (p||q) is de- fined by 1 2 i |p i ? q i |. Obviously, D T V is symmetric by definition, while KLD is asymmetric.</formula><p>Formally, given state s, KLD of ? ? old (?|s) for ? ? (?|s) can be written as</p><formula xml:id="formula_9">D KL (? ? old (?|s)||? ? (?|s)) = a ? ? old (a|s) ln ? ? old (a|s) ? ? (a|s) .<label>(11)</label></formula><p>Similarly, KLD in the continuous domain can be defined simply by replacing summation with integration.</p><p>The consequence of KLD's asymmetry leads to a nonnegligible difference of whether choose D KL (? ? old ||? ? ) or D KL (? ? ||? ? old ). Sometimes, those two choices result in quite different solutions. Robert compared the forward and reverse KL on a distribution, one solution matches only one of the modes, and another covers both modes <ref type="bibr" target="#b4">[Robert, 2014]</ref>. Therefore, KLD is not an ideal bound or approximation for the expected discounted cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discussion about Pessimistic Proximal Policy</head><p>In fact, PPO is called pessimistic proximal policy optimization 1 in the meaning of its objective construction style.</p><p>Without loss of generality, supposing A t &gt; 0 for given state s t and action a t , and the optimal choice is a t . When a t = a t , a good update policy is to increase the probability of action to a relatively high value a t by adjusting ?. However, the clipped item clip(r t (?), 1? , 1+ )? t will fully contribute to the loss function by the minimum operation, which ignores further reward by zero gradients even though it's the optimal action. Other situation with A t &lt; 0 can be analyzed in the same manner.</p><p>However, if the pessimistic limitation is removed, PPO's performance decreases dramatically , which is again confirmed by our preliminary experiments. In a word, the pessimistic mechanism plays a very critical role for PPO by a relatively weak preference for good action decision for a given state, which in turn affects learning efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Restricted Solution Manifold</head><p>To be simple, we don't take the model identifiability issues along with deep neural network into account here because 1 The word "pessimistic" is used by the PPO paper. they don't affect the following discussion much <ref type="bibr" target="#b2">[Goodfellow et al., 2016]</ref>. Suppose ? ? is the optimal solution for a given environment, in most cases, more than one parameter set for ? can generate the ideal policy, especially when ? ? is learned by a deep neural network. In other words, the relationship between ? and ? ? is many to one. On the other hand, when agents interact with the environment using policy represented by neural networks, the action is taken approximately strongly corrected with the highest probability value. Although some strategies of enhancing exploration are applied, they don't affect the policy much in the meaning of expectation.</p><p>Taking the Atari-Pong game for example, when an agent sees a Pong ball is coming nearly, its optimal policy is moving the racket to the right position. The probability of this action is a relatively high value such as 0.95 and it's near to impossible that this value is 1.0 since it's produced by a softmax operation on the several discrete actions. In fact, we hardly obtained the optimal solution accurately, instead, our goal is a good enough answer. Namely, ? 1 outputting a probability 0.95 and ? 2 with 0.9 for the right action are both good answers. During the training process, these similar events occur frequently.</p><p>Using a penalty such as KLD cannot handle it effectively, because it involves all of the actions' probabilities. Moreover, it doesn't stop penalizing unless two distributions become exactly indifferent or the advantage item is large enough to compensate for the KLD cost. Therefore, even if ? outputs ? old the same high probability for the right action, it's still penalized owing to probabilities mismatch for other uncritical actions. Indeed, when a person is asked to make the choice, corresponding action will be taken only if the probability is above a threshold. From the perspective of the manifold, if the optimal parameters constitute a solution manifold. The KLD penalty will act until ? exactly locates in the solution if possible. However, if the agent concentrates only on critical actions like a human, it's much easier to approach the manifold, which in fact, expands the solution manifold at least one dimension such as curves to surfaces and surfaces to spheres at best.</p><p>Besides, since mini-batch is a commonly used trick for training neural networks, removing this unexpected penalty helps to decrease penalty noises, which are reflected by the corresponding gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Exploration</head><p>One shared highlight in reinforcement learning is the balance between exploitation and exploration. For a policy-gradient algorithm, entropy is added in the total loss to encourage exploration in most case. When included in loss function, KLD penalizes the old and new policy probability mismatch for all possible actions as Equation 11. This strict punishment for every action's probability mismatch, which discourages exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Point Probability Distance</head><p>To overcome the above-mentioned shortcomings, we propose a surrogate objective with the point probability distance penalty, which is symmetric and more optimistic than PPO. In the discrete domain, when the agent takes action a, the point probability distance between ? ? old (?|s) and ? ? (?|s) is defined by D pp (? ? old (?|s), ? ? (?|s)) = (? ? old (a|s) ? ? ? (a|s)) 2 .</p><p>Attention should be paid to the penalty definition item, the distance is measured by the point probability, which emphasizes its mismatch for the sampled actions for a state. Undoubtedly, D pp is symmetry by definition. Furthermore, it can be proved that D pp is indeed a lower bound for the total variance divergence D T V . As a special case, it can be easily proved that for binary distribution, D 2 T V (p||q) = D pp (p||q). Theorem 1 For two discrete probability distributions p and q with K values, then D 2 T V (p||q) ? D pp (p||q) holds. Proof 1 Let a = p l , b = q l for any l, and suppose a ? b without loss of generalization. So,</p><formula xml:id="formula_11">D 2 T V (p||q) = ( 1 2 K i=1 |p i ? q i |) 2 = ( 1 2 K i=1,i =l |p i ? q i | + 1 2 |p l ? q l |) 2 ? ( 1 2 | K i=1,i =l p i ? q i | + 1 2 (a ? b)) 2 = ( 1 2 |1 ? a ? (1 ? b)| + 1 2 (a ? b)) 2 = ( 1 2 (a ? b) + 1 2 (a ? b)) 2 = D pp (p||q)</formula><p>Q.E.D.</p><p>Since 0 ? ? ? (a|s) ? 1 holds for discrete action space, D pp has a lower and upper boundary: 0 ? D pp ? 1. Moreover, D pp is less sensitive to action space dimension than KLD, which has a similar effect as PPO's clipped ratio to increase robustness and enhance stability.</p><p>Equation 12 stays unchanged for the continuous domain, and the only difference is ? ? (a|s) represents point probability density instead of probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">POP3D</head><p>After we have defined the point probability distance, we use a new surrogate objective for POP3D, which can be written as</p><formula xml:id="formula_12">max ? E t [ ? ? (a t |s t ) ? ? old (a t |s t )? t ? ?D pp (? ? old (?|s), ? ? (?|s))],<label>(13)</label></formula><p>where ? is the penalized coefficient. These combined advantages lead to considerable performance improvement, which escapes from the dilemma of choosing preferable penalty coefficient. Besides, we use generalized advantage estimates to for epoch = 1 to K do 8:</p><p>Optimized loss objective wrt ? with mini-batch size M ? N T , then update ? old ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>end for 10: end for calculate? t . Algorithm 1 shows the complete iteration process of POP3D. Moreover, it possesses the same computing cost and data efficiency as PPO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Relationship With PPO</head><p>To conclude this section, we take some time to see why PPO works by taking the above viewpoints into account.</p><p>When we pour more attention to Equation 9, the ratio r t (?) only involves the probability for given action a, which is chosen by policy ?. In other words, all other actions' probabilities except a are not activated, which no longer contribute to back-propagation and allow probability mismatch. Obviously, this procedure behaves similarly as POP3D, which expands the restricted solution manifold.</p><p>Above all, POP3D is designed to conform with the regulations for overcoming above mentioned problems, and in the next section experiments from commonly used benchmarks will evaluate its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Controlled Experiments Setup</head><p>OpenAI Gym is a well-known simulation environment to test and evaluate various reinforcement algorithms, which is composed of both discrete (Atari) and continuous (Mujoco) domains <ref type="bibr" target="#b0">[Brockman et al., 2016]</ref>. Most of recent deep reinforcement learning methods such as DQN variants <ref type="bibr" target="#b6">[Van Hasselt et al., 2016;</ref><ref type="bibr" target="#b6">Wang et al., 2015;</ref>, A3C, ACKTR, PPO are evaluated using only one set of hyper-parameters 2 . Therefore, we evaluate POP3D's performance on 49 Atari games(v4, discrete action space ) and 7 Mujoco(v2, continuous space).</p><p>Since PPO is a distinguished RL algorithm which defeats various methods such as A3C, A2C ACKTR, we focus on a detailed quantitative comparison with fine-tuned PPO. And we don't consider large scale distributed algorithms Apex- <ref type="bibr">DQN [Horgan et al., 2018]</ref> and IMPALA <ref type="bibr" target="#b1">[Espeholt et al., 2018]</ref>, because we concentrate on comparable and fair evaluation, while the latter is designed to apply with large scale parallelism. Nevertheless, some orthogonal improvements from those methods have the potentials to improve our method further. Furthermore, we include TRPO to acts as a baseline method. In addition, quantitative comparisons between KLD and point probability penalty helps to convince the critical role of the latter, where the former strategy is named fixed KLD in  and can act as another good baseline in this context, named by BASE-LINE below.</p><p>In particular, we retrained one agent for each game with fine-tuned hyper-parameters 3 . To avoid the problems of reproduction about reinforcement algorithms mentioned in <ref type="bibr">[Henderson et al., 2017]</ref>, we take the following measures:</p><p>? Use the same training steps and make use of the same amount of game frames(40M for Atari game and 10M for Mujoco). ? Use the same neural network structures, which is CNN model with one action head and one value head for Atari game, and fully-connected model with one value head and one action head which produces the mean and standard deviation of diagonal Gaussian distribution as PPO. ? Initialize parameters using the same strategy as PPO.</p><p>? Keep Gym wrappers from Deepmind such as reward clipping and frame stacking unchanged for Atari domain, and enable 30 no-ops at the beginning of each episode. ? Use Adam optimizer [Kingma and Ba, 2014] and decrease ? linearly from 1 to 0 for Atari domain as PPO.</p><p>To facilitate further comparisons with other approaches, we release the seeds and detailed results 4 (across the entire training process for different trials). In addition, we randomly select three seeds from {0, 10, 100, 1000, 10000} for two domains, {10,100,1000} for Atari and {0,10,100} for Mujoco in order to decrease unfavorable subjective bias stated in <ref type="bibr">[Henderson et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>PPO utilizes two score metrics for evaluating agents performance using various RL algorithms. One is the mean score of last 100 episodes Score 100 , which measures how high a strategy can hit eventually. Another is the average score across all episodes Score all , which evaluates how fast an agent learns. In this paper, we conform to this routine and calculate individual metric by averaging three seeds in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discrete Domain Comparisons</head><p>Hyper-parameters We search hyper-parameter four times for the penalty coefficient ? based on four Atari games while keeping other hyper-parameters unchanged as PPO and fix ? = 5.0 to Metric PPO POP3D BASELINE TRPO Score 100 11 32 5 1 Score all 18 20 6 5 <ref type="table">Table 1</ref>: The number of games "won" by each algorithm for Atari game, where the score metric is averaged on three seeds. train all Atari games. For BASELINE, we also search hyperparameter four times on penalty coefficient ? and choose ? = 10.0. To save space, detailed hyper-parameter setting can be found in <ref type="table" target="#tab_3">Table 4</ref>, 5 and 6. This process is not beneficial for POP3D owing to missing optimization for all hyper-parameters. There are two reasons to make this choice. On the one hand, it's the simplest way to make a relatively fair comparison group such as keeping the same iterations and epochs within one loop to our knowledge. On the other hand, this process imposes low search requirements for time and resource. That's to say, we can draw a conclusion that our method is at least competitive to PPO if it performs better on benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Results</head><p>The final score of each game is averaged by three different seeds and the highest is in bold. As <ref type="table">Table 1</ref> shows, POP3D outperforms 32 across 49 Atari games in view of the final score, followed by PPO with 11, BASELINE with 5 and TRPO with 1. Interestingly, for games that POP3D score highest, BASELINE score worse than PPO more often than the other way round, which means that POP3D is not just an approximate version of BASELINE.</p><p>For another metric, POP3D wins 20 out of 49 Atari games which matches PPO with 18, followed by BASELINE with 6, and last ranked by TRPO with 5.</p><p>Detailed performance during the whole training process can be found in <ref type="figure" target="#fig_0">Figure 1</ref>. If we measure the stability of an algorithm by the score variance of different trials, POP3D scores high with good stability across various seeds. And PPO behaves worse in Game Kangaroo and UpNDown. Interestingly, BASELINE shows a large variance for different seeds for several games such as BattleZone, Freeway, Pitfall and Seaquest.</p><p>In sum, POP3D reveals its better capacity to score high and similar fast learning ability in this domain. The detailed metric for each game is listed in <ref type="table" target="#tab_2">Table 3</ref> and 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Continuous Domain Comparisons</head><p>In this section, we focus on comparisons between POP3D and PPO in Mujoco domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters</head><p>For PPO, we use the same hyper-parameter configuration as . Regarding POP3D, we search on two games three times and select 5.0 as the penalty coefficient. More details about hyper-parameters for PPO and POP3D are listed in <ref type="table" target="#tab_7">Table 7</ref> and 8 of Section A.2. Unlike the Atari domain, we we utilize the constant learning rate strategy as  in the continuous domain instead of the linear decrease strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PPO POP3D</head><p>Score 100 2 5 Score all 4 3 <ref type="table">Table 2</ref>: The number of games won by each algorithm for Mujoco game, where the score metric is averaged on three seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Results</head><p>The scores are also averaged on three trials and summarized in <ref type="table">Table 2</ref>. POP3D occupies 5 out of 7 games on Score 100 , and 3 on Score all . Evaluation metrics of both across different games are illustrated in <ref type="table" target="#tab_10">Table 10</ref> and 11. Each algorithm's score performance with iteration steps is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In summary, both metrics indicates that POP3D is competitive to PPO in the continuous domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce a new reinforcement learning algorithm called POP3D (Policy Optimization with Penalized Point Probability Distance), which acts as a TRPO variant like PPO. Compared with KLD that is an upper bound for the square of total variance divergence between two distributions, the penalized point probability distance is a symmetric lower bound. Besides, it equivalently expands the optimal solution manifold effectively while encouraging exploration, which is a similar mechanism implicitly possessed by PPO. The proposed method not only possesses several critical improvements from PPO but outperforms with a clear margin on 49 Atari games from the respective of final scores and meets PPO's match as for fast learning ability.</p><p>More interestingly, it not only suffers less from the penalty item setting headache along with TRPO, where is arduous to select one fixed value for various environments, but outperforms fixed KLD baseline from PPO. In summary, POP3D is highly competitive and an alternative to PPO.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Mujoco</head><p>PPO's and POP3D's hyper-parameters for Mujoco game are respectively listed in <ref type="table" target="#tab_7">Table 7</ref> and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Score Tables and Curves</head><p>Mean scores of various methods for Atari domain are listed in <ref type="table">Table 9</ref>.     <ref type="table">Table 9</ref>: All episodes mean scores of PPO, POP3D, BASELINE and TRPO on Atari games after 40M frames. The results are averaged by three trials.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Score curves of three methods on 49 Atari games within 40 million frame steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Score curves of three methods on 7 Mujoco games within 10 million frame steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input: max iterations L , actors N , epochs K 2: for iteration = 1 to L do</figDesc><table><row><cell cols="2">Algorithm 1 POP3D</cell></row><row><cell>1: 3:</cell><cell>for actor = 1 to N do</cell></row><row><cell>4:</cell><cell>Run policy ? ? old for T time steps</cell></row><row><cell>5:</cell><cell>Compute advantage estimations? 1 , ...,? T</cell></row><row><cell>6:</cell><cell>end for</cell></row><row><cell>7:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean final scores (last 100 episodes) of PPO, POP3D, BASELINE and TRPO on Atari games after 40M frames. The results are averaged on three trials.</figDesc><table><row><cell>A Hyper-parameters</cell><cell></cell></row><row><cell>A.1 Atari</cell><cell></cell></row><row><cell cols="2">PPO's and POP3D's hyper-parameters for Mujoco game are</cell></row><row><cell cols="2">respectively listed in Table 4 and 5.</cell></row><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell>Horizon (T)</cell><cell>128</cell></row><row><cell>Adam step-size</cell><cell>2.5 ?10 ?4 ? ?</cell></row><row><cell>Num epochs</cell><cell>3</cell></row><row><cell>Mini-batch size</cell><cell>32?8</cell></row><row><cell>Discount (?)</cell><cell>0.99</cell></row><row><cell>GAE parameter (?)</cell><cell>0.95</cell></row><row><cell>Number of actors</cell><cell>8</cell></row><row><cell>Clipping parameter</cell><cell>0.1??</cell></row><row><cell>VF coeff.</cell><cell>1</cell></row><row><cell>Entropy coeff.</cell><cell>0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>PPO's hyper-parameters for Atari game.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell>Horizon (T)</cell><cell>128</cell></row><row><cell>Adam step-size</cell><cell>2.5 ?10 ?4 ? ?</cell></row><row><cell>Num epochs</cell><cell>3</cell></row><row><cell>Mini-batch size</cell><cell>32?8</cell></row><row><cell>Discount (?)</cell><cell>0.99</cell></row><row><cell>GAE parameter (?)</cell><cell>0.95</cell></row><row><cell>Number of actors</cell><cell>8</cell></row><row><cell>VF coeff.</cell><cell>1</cell></row><row><cell>Entropy coeff.</cell><cell>0.01</cell></row><row><cell>KL penalty coeff.</cell><cell>5.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>POP3D's hyper-parameters for Atari game.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>BASELINE's hyper-parameters for Atari game.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell>Horizon (T)</cell><cell>2048</cell></row><row><cell>Adam step-size</cell><cell>3 ?10 ?4</cell></row><row><cell>Num epochs</cell><cell>10</cell></row><row><cell>Mini-batch size</cell><cell>64</cell></row><row><cell>Discount (?)</cell><cell>0.99</cell></row><row><cell>GAE parameter (?)</cell><cell>0.95</cell></row><row><cell>Clipping parameter</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>PPO's hyper-parameters for Mujoco game.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell>Horizon (T)</cell><cell>2048</cell></row><row><cell>Adam step-size</cell><cell>3 ?10 ?4</cell></row><row><cell>Num epochs</cell><cell>10</cell></row><row><cell>Mini-batch size</cell><cell>64</cell></row><row><cell>Discount (?)</cell><cell>0.99</cell></row><row><cell>GAE parameter (?)</cell><cell>0.95</cell></row><row><cell>KL penalty coeff.</cell><cell>5.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>POP3D's hyper-parameters for Mujoco game.</figDesc><table><row><cell>game</cell><cell>POP3D</cell><cell>PPO BASELINE</cell><cell>TRPO</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Mean final scores (last 100 episodes) of PPO ,POP3D on Mujoco games after 10M frames. The results are averaged on three trials.</figDesc><table><row><cell>game</cell><cell cols="2">PPO POP3D</cell></row><row><cell>HalfCheetah</cell><cell cols="2">3250.22 2373.30</cell></row><row><cell>Hopper</cell><cell cols="2">1767.14 1257.72</cell></row><row><cell cols="3">InvertedDoublePendulum 3684.92 2561.77</cell></row><row><cell>InvertedPendulum</cell><cell>531.77</cell><cell>552.98</cell></row><row><cell>Reacher</cell><cell>-5.94</cell><cell>-8.05</cell></row><row><cell>Swimmer</cell><cell>94.01</cell><cell>108.27</cell></row><row><cell>Walker2d</cell><cell cols="2">1770.37 2439.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>All episodes mean scores of PPO ,POP3D on Mujoco games after 10M frames. The results are averaged by three trials.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Policy Optimization with Penalized Point Probability DistanceBefore diving into the details of POP3D, we review some drawbacks of several methods, which partly motivate us.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">DQN variants are evaluated in Atari environment since they are designed to solve problems about discrete action space. However, policy gradient based algorithms can handle both continuous and discrete problems.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use OpenAI's PPO and TRPO implementation, which provides a good baseline for various reinforcement learning algorithms: https://github.com/openai/baselines.git. 4 Experiment results can be downloaded from https://drive. google.com/file/d/1c79TqWn74mHXhLjoTWaBKfKaQOsfD2hg/ view?usp=sharing.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bellemare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06887</idno>
		<idno>arXiv:1606.01540</idno>
	</analytic>
	<monogr>
		<title level="m">Jie Tang, and Wojciech Zaremba. Openai gym</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Espeholt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01561</idno>
		<title level="m">Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06560</idno>
		<title level="m">Deep reinforcement learning that matters</title>
		<editor>Henderson et al., 2017] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger</editor>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep learning</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hessel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02298</idno>
		<idno>arXiv:1509.02971</idno>
	</analytic>
	<monogr>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<meeting><address><addrLine>Ba; Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-Dimensional Continuous Control Using Generalized Advantage Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<idno>arXiv:1511.05952</idno>
	</analytic>
	<monogr>
		<title level="m">Proximal Policy Optimization Algorithms. ArXiv e-prints</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page">354</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Nature. Sutton and Barto. Reinforcement learning: An introduction</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Platformaware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable trust-region method for deep reinforcement learning using kroneckerfactored approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasselt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06581</idno>
		<idno>1147.29</idno>
	</analytic>
	<monogr>
		<title level="m">Dueling network architectures for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="5279" to="5288" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
