<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Sensitive Temporal Feature Learning for Gait Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duowang</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botao</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
						</author>
						<title level="a" type="main">Context-Sensitive Temporal Feature Learning for Gait Recognition</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<biblScope unit="page">1</biblScope>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Gait Recognition</term>
					<term>Temporal Modeling</term>
					<term>Spatial Preserving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although gait recognition has drawn increasing research attention recently, it remains challenging to learn discriminative temporal representation, since the silhouette differences are quite subtle in spatial domain. Inspired by the observation that human can distinguish gaits of different subjects by adaptively focusing on temporal clips with different time scales, we propose a context-sensitive temporal feature learning (CSTL) network for gait recognition. CSTL produces temporal features in three scales, and adaptively aggregates them according to the contextual information from local and global perspectives. Specifically, CSTL contains an adaptive temporal aggregation module that subsequently performs local relation modeling and global relation modeling to fuse the multi-scale features. Besides, in order to remedy the spatial feature corruption caused by temporal operations, CSTL incorporates a salient spatial feature learning (SSFL) module to select groups of discriminative spatial features. Particularly, we utilize transformers to implement the global relation modeling and the SSFL module. To the best of our knowledge, this is the first work that adopts transformer in gait recognition. Extensive experiments conducted on three datasets demonstrate the state-of-the-art performance. Concretely, we achieve rank-1 accuracies of 98.7%, 96.2% and 88.7% under normal-walking, bag-carrying and coat-wearing conditions on CASIA-B, 97.5% on OU-MVLP and 50.6% on GREW.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>G AIT recognition is a long-distance biological identification technology, which relies on the walking patterns of human beings, and now reveals great application potential on public security <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and identity recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Although gait recognition has drawn increasing research attention recently, it remains challenging to learn discriminative temporal representation since the silhouette differences in spatial domain are quite subtle.</p><p>Moreover, as mentioned in <ref type="bibr" target="#b7">[8]</ref>, body parts possess diverse motion patterns, thus require temporal modeling to take multi-scale representation into consideration. At present, multi-layer convolutions have been widely used in current methods to model multi-scale temporal information. And they aggregated multi-scale features in a summation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> or a concatenation way <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. However, since the aggregation methods are fixed, these manners are not flexible enough to adapt to variations of complex motion and realistic factors, i.e., self occlusion between body parts and change of camera viewpoints. Consequently, the performance is hindered especially considering gait is a kind of fine-grained motion pattern, whose identification of subjects depends on the diverse expression of customized motion on local body parts.</p><p>It can be seen from life experience that human is able to distinguish gait sequences of different subjects by adap- (a) Two sequences from subject '53' and '119' on CASIA-B can be distinguished relying on short-term temporal clues, e.g., several frames at the beginning.</p><formula xml:id="formula_0">? Xiaohu</formula><p>(b) Two sequences from subject '39' and '77' on CASIA-B, which have to be distinguished relying on long-term temporal clues, e.g., all of the frames. tively focusing on temporal fragments with different time scales. A qualitative illustration is given in <ref type="figure">Figure.</ref> 1, where voting results from 15 volunteers are used to calculate the focus distribution. In <ref type="figure" target="#fig_0">Figure. 1(a)</ref>, the differences between the two gait sequences are so obvious that we can distinguish them by observing fewer frames from the beginning. On the contrary, in <ref type="figure" target="#fig_0">Figure. 1(b)</ref>, differences between two sequences are quite subtle that we have to observe more frames to distinguish them. Therefore, in this situation, short-term clues are not enough to make a distinction between the two subjects, but long-term features need to be considered since they provide richer temporal information. arXiv:2204.03270v2 [cs.CV] 8 Apr 2022 Misalignment <ref type="figure">Fig. 2</ref>. Illustration of the misalignment in gait sequences. Since pixels of the same spatial locations in different frames may correspond to different semantic contents, the utilization of temporal operations could lead to blurred or overlapped appearances.</p><p>Hence, the adaptive adjustment among multi-scale temporal features leads to flexible focus along temporal dimension, which offers a new perspective for gait modeling.</p><p>Motivated by such observation, we propose a contextsensitive temporal feature learning (CSTL) network for gait recognition. The core idea of this method is to integrate multi-scale temporal features according to the contextual information along temporal dimension, which allows information communications among different scales from both local and global perspectives. Here, contextual information is obtained by evaluating the local and global relations among multi-scale temporal features, which reflects diverse motion information existing in context semantics. CSTL produces temporal features for each frame in three scales, i.e., frame-level, short-term and long-term, which are complementary to each other. The frame-level features retain frame characteristics at each time instant. The short-term features capture local temporal contextual clues, which are sensitive to temporal locations and beneficial to model micro motion patterns. The long-term features, on behalf of motion features across all frames, reveal global action periodicity, which are invariant for temporal locations.</p><p>Next, a local relation modeling among these temporal features guides the network to adaptively enhance or suppress temporal features with different scales in each frame. Afterwards, a global relation modeling is involved to interact the multi-scale features along the whole sequence, which constructs the global communications to capture the most discriminative representation. Particularly, a transformer block <ref type="bibr" target="#b13">[14]</ref> is used in global relation modeling, which is natively suited as a long-range reasoning module. Consequently, our method forms a hierarchical framework to aggregate multi-scale features in both local and global aspects adaptively, which provides the possibility of modeling complex motion, and makes it very suitable for gait recognition.</p><p>Further, during the investigation of temporal modeling, we notice the misalignment problem in temporal modeling that has not been investigated in gait recognition yet. As shown in <ref type="figure">Figure.</ref> 2, the same pixel locations from different frames may correspond to different foregrounds and backgrounds. Naturally, the utilization of temporal operations, e.g., temporal convolution and temporal pooling, may result in blurred and corrupted appearances. However, appearance features could provide vital evidences for distinguishing different people. To address such issue, we propose a salient spatial feature learning (SSFL) module to select discriminative local spatial parts across the whole se-quence, which can be considered as supplements to remedy the corruption in appearance features. The discriminative feature selection is based on temporal feature importance evaluation, where we utilize the multi-head self-attention (MHSA) mechanism in transformer to form groups of importance maps globally. Particularly, as explained in <ref type="bibr" target="#b13">[14]</ref>, each head focus on a certain representation subspace, thus multiple heads could generate diverse importance maps, by which we can select groups of discriminative local parts from various aspects.</p><p>The adaptive temporal modeling and salient spatial learning provide complementary properties for each other. On one hand, CSTL mainly considers temporal modeling and SSFL focuses on spatial learning. Specifically, CSTL produces temporal aggregation of multi-scale clues which describes motion patterns, and SSFL selects well-preserved spatial features which involve with still images. On the other hand, CSTL aggregates temporal clues in a soft-attention way and SSFL obtains salient spatial features in a hardattention manner. In a nutshell, by jointly investigating motion learning and spatial mining simultaneously, we achieve outstanding performance over the state-of-the-art methods.</p><p>To the best of our knowledge, this is the first method that explores transformer application in gait recognition. The major contributions of this paper can be summarized as the following three aspects:</p><p>? In this paper, we propose a temporal modeling network CSTL to fuse multi-scale temporal features in an adaptive way from both local and global aspects, which considers the cross-scale contextual information as a guidance for temporal aggregation. ? We propose a salient spatial feature learning (SSFL) module to remedy the feature corruption caused by temporal operations. SSFL constructs global feature importance maps to select salient spatial features across the whole sequence, which provide high quality spatial clues. ? Extensive experiments conducted on three datasets, i.e., CASIA-B <ref type="bibr" target="#b14">[15]</ref>, OU-MVLP <ref type="bibr" target="#b15">[16]</ref> and GREW <ref type="bibr" target="#b16">[17]</ref>, demonstrate the state-of-the-art performance of CSTL. And further ablation experiments prove the effectiveness of the proposed modules. Additional experiments using practical settings reveal the realworld application potentials of CSTL.</p><p>A preliminary conference version of this work was published in ICCV-2021 <ref type="bibr" target="#b17">[18]</ref>. We improve this work in three aspects: <ref type="bibr" target="#b0">(1)</ref> We extend the adaptive multi-scale feature aggregation from a local way to a local-to-global hierarchical manner, which helps improve recognition performance. <ref type="bibr" target="#b1">(2)</ref> The former SSFL constructs importance maps on each frame individually, and only generates a group of salient parts. Instead, the improved SSFL forms the importance maps in a global view by utilizing the multi-head self-attention mechanism, which is capable of generating more groups of salient spatial parts, thus contributes to enhancing spatial mining capacity. (3) We include additional experiments on a recently published large-scale gait dataset in the wild, i.e., GREW <ref type="bibr" target="#b16">[17]</ref>, on which we further validate the effectiveness and robustness of our method. And more ablation experiments are conducted on CASIA-B to analyze the contribution of the proposed modules, explore the network design, and study the performances of our method under real-world settings.</p><p>The rest of the paper is organized as follows. Firstly, the Section 2 describes the related works. Next, the Section. <ref type="bibr" target="#b2">3</ref> gives the details of the network. Afterwards, the Section. 4 provides comprehensive experiments and corresponding analysis. Finally, the Section. 5 concludes the paper and present future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gait Recognition</head><p>Current gait recognition methods could be categorized into two types: model-based and appearance-based.</p><p>Model-based methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> were proposed to model walking patterns and body structures of humans based on extracted key points <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Model-based methods are robust to variations of clothing and camera viewpoints. However, due to (1) inaccurate pose results estimated from low-quality images and (2) the missing of identity-related shape information, model-based methods are usually inferior to appearance-based methods in performance comparison.</p><p>Appearance-based methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> extracted spatio-temporal features on RGB images or binary silhouettes by CNN networks or handcrafted algorithms. Gait Energy Image (GEI) <ref type="bibr" target="#b27">[28]</ref> was generated through projecting a sequence of gait silhouettes into a single image. The GEI-based methods <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> greatly compressed computational cost but lost discriminative expression. In contrast, the video-based approaches <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b32">[33]</ref> processed gait sequences frame by frame, which maintained the framelevel discriminative feature in a large extent, and benefited the networks to learn temporal representation. Our approach belongs to appearance-based method and takes silhouette sequences as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Temporal Modeling</head><p>Current literatures proposed different strategies for gait temporal modeling, including 1D convolutions, LSTMs and 3D convolutions etc. Particularly, GaitSet <ref type="bibr" target="#b24">[25]</ref> and GLN <ref type="bibr" target="#b25">[26]</ref> considered a gait sequence as an unordered set, which mainly focused on spatial modeling and captured interframe dependency implicitly. GaitPart <ref type="bibr" target="#b7">[8]</ref> and Wu et al. <ref type="bibr" target="#b8">[9]</ref> extracted local temporal clues by 1D convolutions and aggregated them in a summation or a concatenation manner. LSTM networks were applied in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> to achieve longshort temporal modeling, which fused temporal clues by temporal accumulation. With the help of stacked 3D blocks, MT3D <ref type="bibr" target="#b11">[12]</ref> and GaitGL <ref type="bibr" target="#b12">[13]</ref> incorporated temporal information with small and large scales, then concatenated or summed these features as outputs. 3DLocal <ref type="bibr" target="#b33">[34]</ref> applied 3D CNN to obtain different local parts, and fused them with feature concatenation. However, current methods have obvious shortcomings in learning flexible and robust multiscale temporal features, which are incapable of satisfying temporal modeling requirements for different body parts of gait motion.</p><p>Recently, transformers were broadly applied in various computer vision tasks <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, but has not been investigated in gait recognition yet. Compared with the prevalent CNNs, the most outstanding strength of transformers is the global reasoning capacity, which empowers models to capture features in the long range. In the domain of videobased tasks, TimeSformer <ref type="bibr" target="#b37">[38]</ref>, ViViT <ref type="bibr" target="#b38">[39]</ref> and Vidtr <ref type="bibr" target="#b39">[40]</ref> proved transformers could extract global temporal features effectively.</p><p>Our method differs from above methods in three aspects: (1) CSTL generates temporal features in three scales, i.e., frame-level, short-term and long-term. Such rich temporal clues enable our network to obtain diverse motion learning ability. (2) For fusing multi-scale temporal features adaptively, CSTL employs both local and global cross-scale relation modelings to obtain the most appropriate temporal expression. (3) The transformer block we use is not for global feature extraction like current methods, but for global feature interaction on multiple time scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Spatial Preserving</head><p>The spatial appearance features provide supplementary cues to recognize people besides motion pattern. GaitNet <ref type="bibr" target="#b10">[11]</ref> proposed a disentanglement-based scheme to obtain the canonical feature to help recognition, whose goal was close to ours. Except for gait recognition, the spatial misalignment also degraded performance in other person-related recognition task, e.g. Person Re-identification. In video-based Person Re-identification, different methods were proposed to maintain the clearness of spatial features. In AP3D <ref type="bibr" target="#b40">[41]</ref>, researchers proposed Appearance Preserving Module (APM) to mitigate the misalignment problem in temporal modeling. APM used a feature similarity calculation strategy to match the foregrounds in continuous frames within a local window based on the color, texture and illumination et al. Chen et al. <ref type="bibr" target="#b41">[42]</ref> proposed a method dubbed Adversarial Feature Augmentation (AFA) to capture motion coherence by a adversarial form. However, AFA only employed motionirrelevant features, but totally abandoned temporal clues.</p><p>Unfortunately, the above spatial preserving methods are not appropriate for gait recognition. GaitNet and APM are both operated on RGB-based appearance features, e.g., color, texture and illumination. But the inputs of our model are sequences of binary silhouettes, which do not include the needed appearance features. Moreover, AFA only incorporated the modeling of motion-irrelevant features but ignored the motion-relevant features, which are vital for gait recognition.</p><p>Different from these strategies, in our approach, SSFL selects discriminative local parts to maintain the spatial characteristics of subjects, which is feasible for binary inputs. And this operation is parallel to temporal modeling, thus would not compromise temporal feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we firstly describe the overall pipeline of our method, then illustrate the detailed structure of key components in the network.  <ref type="figure">Fig. 3</ref>. Overview of CSTL. A sequence of gait silhouettes are firstly fed into a 2D CNN to extract spatial features in each frame. Then, a multi-scale temporal extraction (MSTE) module is utilized to obtain temporal features in three scales. Afterwards, a two-branch architecture composed of an adaptive temporal aggregation (ATA) module and a salient spatial feature learning (SSFL) module is formed to aggregate multi-scale features and select salient spatial parts respectively. Arrows, G, F I , F T , F S and F O denote operations, input gait sequence, features processed by CNN, temporal aggregated features, salient spatial features and output features respectively. L tri and Lce represent triplet loss and cross-entropy loss respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Pipeline</head><p>The overview structure of our method is presented in <ref type="bibr">Figure. 3</ref>. A batch of B gait samples of N frames are fed into the network as input, which is denoted as G ? R B?N ?H?W . H and W denote the height and width of each frame respectively. Firstly, G is passed through a 2D CNN to produce spatial extracted feature F I ? R B?N ?C?H/2?W/2 , where C denotes the number of feature channels. The detailed CNN architecture is given in <ref type="table" target="#tab_3">Table.</ref> 1. Afterwards, we implement a multi-scale temporal extraction (MSTE) module on F I to generate temporal features with three different temporal scales, i.e., frame-level, short-term and long-term, which are denoted as T f , T s and T l respectively. T f , T s and T l are all with size of R B?N ?C?K , where K denotes the number of horizontal divided feature parts that correspond to body parts in some extent. Next, temporal features are fed into adaptive temporal aggregation (ATA) and salient spatial feature learning (SSFL) blocks respectively, through which we obtain temporal aggregated feature F T ? R B?C?K and salient spatial feature F S ? R B?C?K correspondingly. Temporal aggregated feature F T is a temporal summarization of the whole sequence, which represents the discriminative information in temporal domain. Spatial salient feature F S is obtained by selecting groups of salient spatial parts, which maintain rich high-quality silhouette information. Finally, F S and F T are concatenated along channel dimension as outputs F O .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Scale Temporal Extraction</head><p>As discussed in Section. 3.1, we aim to enrich the diversity of temporal features. Firstly, we divide F into K parts vertically, then apply Global Max Pooling (GMP) and Global Average Pooling (GAP) to obtain part-level pooling features P ? R B?N ?C?K , where P b,n represents part-level features of the n-th frame in the b-th sample. As shown in <ref type="figure" target="#fig_2">Figure. 4</ref>, the frame-level features are the duplicate of P , which do not get involved with temporal operation, thus the appearance characteristics of each time instant are well-maintained. In order to capture short-term temporal features, we apply two serial 1D convolutions with kernel size of 3, and add the features after each 1D convolution as T s . Obtaining short-term features enables the network to focus on short period temporal motion patterns and subtle changes with perceptive fields of 3 and 5.</p><p>The long-term feature extraction is based on the combination of all frames. Firstly, a Multi-layer Perceptron (MLP) followed by a Sigmoid function is applied on P to evaluate the importance of different frames. Next, the weighted summation of all frames by the importance scores is utilized as the long-term temporal features T l , which is formulated as: where denotes dot product. It should be noted that, T b l is invariant for all frames in the b-th sample, which describes global motion cues. After that, we obtain temporal features of three levels, e.g., T f , T s and T l , for subsequent ATA and SSFL blocks.</p><formula xml:id="formula_1">T b l = N n=1 Sigmoid(M LP (P b,n )) P b,n N n=1 Sigmoid(M LP (P b,n )) ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive Temporal Aggregation</head><p>In this part, we utilize multi-scale temporal features to explore feature relations, which enable information exchanging among different temporal scales. As discussed in <ref type="bibr" target="#b7">[8]</ref>, different body parts own various motion patterns, which indicates the diverse expressions are needed for temporal modeling. Intuitively, the interaction of different scales of features would effectively enrich the diversity of temporal representation, thus produce suitable motion expression for human body. As stated in Section. 1, the feature interactions are conducted both locally and globally, which are illustrated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Local Relation Modeling</head><p>As shown in <ref type="figure">Figure.</ref> 5, we devise three strategies to achieve local aggregation in each frame. (1) We use a max pooling to obtain the most discriminative clues of the three scales, which is formulated as:</p><formula xml:id="formula_2">T b,n A l = M ax(T b,n f , T b,n s , T b,n l ),<label>(2)</label></formula><p>where T b,n A l ? R C?K represents the local aggregated features in the n-th frame of the b-th sample.</p><p>(2) We firstly concatenate the features from three scales, then take a fully-connected (FC) layer to correlate the channels and obtain the fused output, which is represented as:</p><formula xml:id="formula_3">T b,n A l = F C(T b,n f c T b,n s c T b,n l ),<label>(3)</label></formula><p>where c denotes the concatenation operation.</p><p>(3) We fuse multi-scale features in a more comprehensive way, which produces individual scores for evaluating the importances of different scales in each frame. Specifically,  we firstly apply information flowing among temporal features from top to bottom:</p><formula xml:id="formula_4">T f = T f T s = T f + T s T l = T f + T s + T l .<label>(4)</label></formula><p>Then, we learn temporal importance weight for each scale by considering the contextual information of the three temporal scales, which is implemented with two fully connected layers and a Sigmoid function:</p><formula xml:id="formula_5">W b,n l = Sigmoid(F C(F C( T b,n f c T b,n s c T b,n l ))),<label>(5)</label></formula><p>where W b,n l ? R 3?C?K denote the temporal importance weights of the n-th frame in the b-th sample. W b,n l incorporates importance weights of the three temporal scales, which is denoted as W b,n l,1 , W b,n l,2 and W b,n l,3 respectively. Afterwards, we obtain local attentive temporal features by a soft-attention manner:</p><formula xml:id="formula_6">T b,n A l = T b,n f W b,n l,1 + T b,n s W b,n l,2 + T b,n l W b,n l,3 . (6)</formula><p>Subsequently, the local aggregated feature T A l is utilized as input to the global relation modeling module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Global Relation Modeling</head><p>To achieve global interactions, we use transformer to construct multi-scale relation modeling across the whole sequence. As discussed in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>, position encoding (PE) plays an important role to achieve permutation-variant modeling in transformers. Considering the various lengths of gait sequences, we adopt conditional position encoding <ref type="bibr" target="#b42">[43]</ref> to extract PE, which can fit the sequence length adaptively. As shown in <ref type="figure">Figure.</ref> 6, PE is obtained by applying 1D depth-wise convolutions with kernel size of 3 on T A l , which produces feature T tran ? R B?K?N ?C . Following MHA in <ref type="bibr" target="#b13">[14]</ref>, we conduct three feature transformations with three learnable matrix weights W q , W k and W v , which are formulated as: where W q , W k and W v are all with the dimension of N head ? C?C/N head , and N head denotes the number of heads. Then, the attention matrix is obtained by multiplying T q with T k followed by a Softmax normalization:</p><formula xml:id="formula_7">T q = T tran W q , T k = T tran W k , T v = T tran W v , (7)</formula><formula xml:id="formula_8">A T = Sof tmax(T q T k / ? C),<label>(8)</label></formula><p>where A T ? R B?N head ?K?N ?N and ? C is used to normalize the dot-product scores. Afterwards, we utilize the attention matrix A T to generate attentive feature T a with a shortcut and a normalization layer:</p><formula xml:id="formula_9">T a = N orm(A T T v + T trans ),<label>(9)</label></formula><p>where T a ? R B?K?N ?C . Subsequently, we use a feedforward network (FFN) <ref type="bibr" target="#b13">[14]</ref> and a normalization layer to obtain the globally fused multi-scale feature T Ag :</p><formula xml:id="formula_10">T Ag = N orm(F F N (T a ) + T a )),<label>(10)</label></formula><p>where T Ag ? R B?K?N ?C . Finally, a max pooling along temporal dimension is applied on T Ag to obtain sequencelevel temporal representation F T ? R B?K?C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Salient Spatial Feature Learning</head><p>In this section, we aim to extract salient spatial parts to mitigate the damage in appearance features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Discussion</head><p>Intuitively, in order to remedy the corrupted spatial features, we should select an individual frame as the methods in <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, which is illustrated in <ref type="figure">Figure. 7(a)</ref>. However, due to the various camera viewpoints, motion occlusions (a) Select a discriminative frame across the sequence.</p><p>(b) Select the salient parts individually across the sequnece. <ref type="figure">Fig. 7</ref>. Illustration of two ways to select salient spatial features. Compared with selecting a frame, selecting the salient parts is conducted in a more fine-grained level, thus could obtain more high-quality local features.</p><p>and imperfect segmentations, a single frame is probably incapable of expressing appearance features for all body parts clearly. Actually, the high quality body parts appear and disappear from frame to frame. Therefore, by utilizing such inherent motion characteristics, we select salient body parts across the whole sequence. As shown in <ref type="figure">Figure. 7(b)</ref>, we obtain local discriminative features in different frames instead of directly selecting one frame.</p><p>Since the transformer is able to correlate global information based on the attention matrix adaptively, the weights in the attention matrix could reveal feature importance in some extent. Naturally, a body part contains richer information has a stronger weight than a body part with less information does. Therefore, we consider using the attention matrix as a metric to select salient spatial parts. Besides, due to the diversities in multiple heads <ref type="bibr" target="#b13">[14]</ref>, attention matrices in different heads may focus on informative features from various aspects, which enhances the spatial mining capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Operation</head><p>Considering temporal clues provide contextual information for evaluating the discrimination of each frame, we firstly utilize a FC to fuse the multi-scale features. Then, we construct the attention matrix A s ? R B?N head ?K?N ?N as in Equation. 7 and Equation. 8, but remove the scale operation and Softmax function. Next, we squeeze the attention matrix along the column dimension, which is formulated as: </p><p>where A b,:,:,: s ? R N head ?K?N denotes the part scores of K parts from N head heads in the b-th sample. The values of part scores represent feature importance, thus higher scores indicate clearer spatial representation. In order to supervise the correctness of part scores, we enforce a cross-entropy loss on the weighted summation of T f and A s . Here, the weighted feature F w is obtained by:</p><formula xml:id="formula_12">F b w = N n=1 T b,n f A b,:,:,n s ,<label>(12)</label></formula><p>where F b w ? R N head ?K?C of the b-th sample. Then, a FC layer is utilized to transform F w into classification logics P w ? R B?N head ?K?Ct , where C t denotes the number of training subjects. Afterwards, the cross-entropy loss is applied on P w to produce L ce :</p><formula xml:id="formula_13">L ce = ? B b=1 N head h=1 Ct c=1 y b,c log(Sof tM ax(P b,h w )) c ,<label>(13)</label></formula><p>where y b,c indicates the identity information of the b-th sample, which equals 0 or 1. Next, we obtain part indexes of the highest scores along temporal dimension:</p><formula xml:id="formula_14">x b,h,k = arg max n A b,h,k,n s ,<label>(14)</label></formula><p>where x h,b,k denotes the temporal index of the selected kth part in the h-th head of the b-th sample. By using the index {x b,h,k |k = 1, 2, ..., K} in a hard-attention manner, we obtain the recombinant frame feature F r in the h-th head of the b-th sample:</p><formula xml:id="formula_15">F b,h r = T b,x b,h,1 f c T b,x b,h,2 f ? ? ? c T b,x b,h,k f ,<label>(15)</label></formula><p>where c denotes concatenation along part dimension. In the end, we fuse the recombinant features F r with the weighted features F w in different heads by:</p><formula xml:id="formula_16">F b S = ( N head h=1 F b,h r ) c ( N head h=1 F b,h w ),<label>(16)</label></formula><p>where F b S ? R K?C denotes the obtained salient spatial features of the b-th sample, and c denotes channel concatenation. F S offers supplementary spatial clues for temporal aggregated features F T . Triplet loss <ref type="bibr" target="#b45">[46]</ref> is employed on the combination of F S and F T as metric learning loss function. The overall loss function is presented as following:</p><formula xml:id="formula_17">L = L ce + L tri<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We conduct experiments on three standard datasets, i.e., CASIA-B <ref type="bibr" target="#b14">[15]</ref>, OU-MVLP <ref type="bibr" target="#b15">[16]</ref> and GREW <ref type="bibr" target="#b16">[17]</ref>, to verify the superiority of our method. (3) The number of output channels for FC shown in <ref type="figure">Figure.</ref> 3 is set to 256, 512 and 512 for CASIA-B <ref type="bibr" target="#b14">[15]</ref>, OU-MVLP <ref type="bibr" target="#b15">[16]</ref> and GREW <ref type="bibr" target="#b16">[17]</ref> datasets respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Training Details</head><p>(1) Each frame is aligned as <ref type="bibr" target="#b15">[16]</ref> does, and we resize each frame to the size of 64 ? 44 or 128 ? 88. For each input sequence, we follow the frame sampling strategy as <ref type="bibr" target="#b7">[8]</ref> does.</p><p>(2) We apply separate Batch All (BA + ) triplet loss to train our network. The batch size for training is noted as (p, k), where p denotes the number of sampled subjects and k denotes the number of sampled sequences for each subject. Particularly, (p, k) is set to <ref type="formula">(</ref>  <ref type="table" target="#tab_3">Table.</ref> 1. Since the data amount of OU-MVLP and GREW is much larger than that of CASIA-B, the numbers of output channels of each layer in <ref type="table" target="#tab_3">Table.</ref> 1 are set to 64, 128, 256, 256 on OU-MVLP and GREW datasets, which follows the design in GaitSet <ref type="bibr" target="#b24">[25]</ref>. (4) Totally, we train 120k iterations on CASIA-B and 250k iterations on OU-MVLP and GREW. Morever, our model is optimized by Adam, and the learning rate is started to set as 1e-4 and reduced to 1e-5 at 150k iterations on OU-MVLP and GREW. We implement our models on Pytorch <ref type="bibr" target="#b46">[47]</ref> platform and use four NVIDIA GeForce GTX 3090 GPUs to perform our experiments.   CSTL achieves the lowest performance standard deviation across 11 views under all walking conditions, which proves the robustness against viewpoint variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CASIA-B.</head><p>(3) CSTL also shows robustness to resolution variations of gait sequences. Under different resolution settings of three walking conditions, CSTL all achieves the best mean recognition accuracies over current methods, which reveals that CSTL could adapt to different resolutions flexibly. Based on that, we use resolution setting of 64 ? 44 in the rest of this paper since it achieves better tradeoff between performance and computation cost. Further, we draw the performance curves of the 11 views of CASIA-B in <ref type="figure">Figure.</ref> 8, which illustrates the accuracy fluctuations under cross-view scenarios. We find that: (1) All curves look like saddle-shape, which are roughly symmetric about the 90 ? view. This phenomenon reveals that sequences from symmetric views contain similar spatial-temporal information for recognition, which adheres to human intuitions.  the setting of keeping invalid sequences, CSTL achieves much lower performance standard deviation than 3DLocal (0.9% with 2.0%), which demonstrates the stronger stability against viewpoints. Taking the performances under 0 ? and 180 ? as examples, CSTL marginally outperforms 3DLocal by 2.4% and 3.4% respectively. This phenomenon explains that CSTL has fewer preferences on certain camera viewpoints compared with current methods. Besides, by removing the invalid probe sequences, CSTL achieves much better accuracy compared with other methods. GREW. <ref type="table" target="#tab_3">Table. 4</ref> shows the comparison results between the proposed CSTL and current state-of-the-art methods. CSTL achieves the best performances under all comparison settings, which proves the gait modeling capacity of CSTL under complex real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In order to evaluate the exact effectiveness of CSTL, ablation experiments are conducted on CASIA-B to study the proposed components. Impact of Spatio-Temporal Modeling. The individual effects of spatial and temporal modeling are presented in <ref type="table" target="#tab_3">Table.</ref> 5. The baseline refers to the 4-layer CNN with a feature division, while using a BA+ loss for supervision. Several noteworthy observations can be summarized as: <ref type="bibr" target="#b0">(1)</ref> Compared to spatial modeling network, i.e. GaitSet <ref type="bibr" target="#b24">[25]</ref>, our baseline achieves similar mean accuracy under three conditions (85.4% and 85.7%), which proves the competitive spatial learning capacity of them. However, with the utilization of MSTE, our method achieves significant accuracy improvement over GaitSet <ref type="bibr" target="#b24">[25]</ref> (+3.9%), which indicates the superiority of modeling multi-scale temporal context based on spatial feature extraction.</p><p>(2) Compared with the multiscale temporal modeling method, i.e., MT3D, our method achieves higher performances when applying MSTE with ATA (+0.9%) and MSTE with SSFL (+1.1%), which verifies the effectiveness of adaptive temporal aggregation and salient spatial mining based on multi-scale temporal clues.</p><p>(3) Applying both spatial and temporal modeling achieves the best results, which verifies the complementary properties provided by SSFL and ATA modules. Impact of Multi-Scale Features. We investigate the effects of the temporal features in MSTE module and the results are given in <ref type="table" target="#tab_3">Table.</ref> 6. It can be noticed that: (1) When using features in one single scale, using short-term features outperforms using frame-level features or long-term features, which illustrates that short-term features offer richer finegrained clues to model micro motion and could further help discriminate different subjects. <ref type="formula" target="#formula_2">(2)</ref> The inter-frame modelings, i.e., short-term and long-term, improve recognition performances based on frame-level feature learning, which proves the effectiveness on jointly modeling inter-frame and intra-frame features. Thus, using features in all three levels achieves the best performances. (3) Short-term and longterm features provide improvements for each other, which explains that the two type of features focus on temporal clues in complementary levels. Effectiveness of Local and Global Relation Modeling. <ref type="table" target="#tab_3">Table.</ref> 7 investigates the effectiveness of the proposed local and global relation modeling strategies. We find that, using an FC achieves the best performance over a max pooling operation or an attention subnet, which proves its superiority for fusing multi-scale features locally. Therefore, we take an FC as local relation modeling in the final version. Further, local-with-global joint modeling achieves the best performance, which demonstrates the complementary properties introduced by local and global relation modelings. Comparison of Spatial Selection Strategies In order to investigate the effectiveness of our spatial learning module for supplementing corrupted spatial features, we conduct two more experiments for comparison: <ref type="bibr" target="#b0">(1)</ref> We replace SSFL with a random frame selection to demonstrate that not each frame has good spatial features. <ref type="bibr" target="#b1">(2)</ref> We set the number of selected parts as 1 in SSFL. In this situation, SSFL turns to be a frame-level feature selection instead of part-level feature selection. As shown in <ref type="table" target="#tab_3">Table.</ref> 8, we notice that: SSFL outperforms the other two strategies, which proves the spatial learning capability of our method. On one hand, random frame selection is probably incapable of obtaining high quality spatial features due to the randomness. On the other hand, although frame-level spatial selection achieves better performance than random frame selection, it still limits the diverse discriminative expression of local parts, especially considering the occlusion of motion and change of camera viewpoints. Compared to the above strategies, our SSFL extracts spatial clues in a fine-grained manner and utilizes the inherent motion characteristics to leverage rich visual clues across the sequence. Investigation on the impact factors of SSFL. <ref type="table" target="#tab_11">Table 9</ref> investigates the impact factors in SSFL on CASIA-B dataset. Especially, the first experiment is conducted by using a MLP to produce the part scores of each frame locally, which can only select a group of salient parts. We can notice that: (1) MHSA outperforms MLP when selecting only one group of spatial parts, which illustrates the superiority of constructing the importance map globally. (2) When selecting more groups of spatial parts by MHSA, the recognition performance first improves continually, then drops. This phenomenon explains that the number of high quality salient parts in each sequence is limited, thus we may obtain low-quality parts by selecting redundant groups, which hurts the recognition performance. To achieve the best performance, we select 4 groups in our final version. Averaged Rank-1 Accuracy(%)  <ref type="figure">Fig. 9</ref>. Study on the impact of different part division numbers and frame numbers on CASIA-B <ref type="bibr" target="#b14">[15]</ref> in terms of averaged rank-1 accuracy under NM, BG and CL conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Frames</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study on Part Division Numbers and Frame</head><p>Numbers. In order to investigate the effects of the part division number K and frame number N , we conduct ablation experiments with different number of K and N . Particularly, 32 is the largest number for K, since the output feature dimension is 32 ? 22. And the maximum N is set as the number of all frames in each sequence. As shown in <ref type="figure">Figure.</ref> 9, we see that the accuracy improves continually with the increasing of number of parts and number of frames, which indicates that: (1) More fine-grained part division provides richer clues for modeling spatial local features, which further satisfies the diverse motion expression of different body parts. (2) More frames contain more abundant information for constructing temporal contextual communications, which enables the network to extract more discriminative temporal clues and mine more salient spatial parts. Therefore, to achieve the best performance, we set the K as 32, and use all frames during test stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Practical Scenarios</head><p>In this section, we consider two new experimental settings, which are closer to real-world applications. (1) Because of the possible insufficiency of training data, models may be tested under unseen views in real life. (2) People may walk in arbitrary directions anytime and anywhere, thus one sequence may be composed of frames from different views. Testing Under Unseen Views. As shown in <ref type="figure" target="#fig_0">Figure. 11</ref>, we consider two possible scenarios for testing the view generalization capacity of our method. Specially, scenario A corresponds to the case that the train dataset covers the view range in the test dataset, but does not include some certain views. In contrast, scenario B refers to the case that views in the train dataset and test dataset are in different ranges. Intuitively, scenario B is harder than scenario A.  <ref type="table" target="#tab_1">1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23</ref>  As reported in <ref type="table" target="#tab_3">Table.</ref> 10, the accuracy decreases under unseen views as expected. However, compared with the baseline, our method achieves stronger robustness against unseen scenarios, whose performances degrade 2.5% under scenario A (but 3.5% of the baseline) and 8.1% under scenario B (but 9.1% of the baseline). These results demonstrate the spatial-temporal modeling and view generalization capacities of the proposed modules.  the baseline with all view pairs, which further proves the effectiveness of our method under novel scenarios. Interestingly, we find that: (1) Using frames from different views achieves higher performances than using frames from one single view, which reveals that richer clues could be obtained in different views. (2) Using frames from larger view differences could consistently achieve higher performances, which reflects that more complementary information is provided in larger view-difference pairs. Consequently, we argue that combine frames from different views could facilitate gait recognition in real-world scenarios effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualization</head><p>Salient Spatial Part Selection. In order to better understand the positive effects of SSFL, we give an example in <ref type="figure">Figure.</ref> 10, where we set the number of selected parts as 8 for better visualization. We can notice that: (1) Directly perceived through the senses, SSFL tends to select parts with clear representation and complete appearance features, which are not affected by body overlaps and clothing occlusions, e.g., from frame 8 to frame 14 and from frame 23 to frame 25. In contrast, frames with body overlaps are less selected, e.g., from frame 3 to frame 7 and from frame 19 to frame 21.</p><p>(2) For different groups, the selected local parts are various, which demonstrates the diverse focus in multiple heads of MHSA.</p><p>In this way, we can obtain high quality spatial features, which both remedies the negative influences caused by temporal operations and enhances the robustness of our network against occlusion variations. Attention Visualization. <ref type="figure" target="#fig_0">Figure. 12</ref> illustrates the attention maps from the last layer in the backbone of the baseline and our method. Intuitively, the baseline network mainly pays attention on the most discriminative parts of the human body, i.e., the head and the legs, whose focus is concentrated on certain regions. In contrast, the focus of our method is relatively scattered, which not only notices the parts that the baseline focus on, e.g., the legs, but also attends on the torso of the body that contains supplementary clues to recognize subjects. Therefore, compared with the baseline, our method could extract richer identity-related information thus achieves higher performance. Feature Distribution. We choose ten identities from CASIA-B test dataset to visualize feature distributions by t-SNE <ref type="bibr" target="#b48">[49]</ref>. Comparing the feature distributions of baseline and our method, we notice that, in <ref type="figure" target="#fig_0">Figure. 13(a)</ref>, the feature distributions of different subjects are closer to each other thus identities are harder to distinguish. Differently, in <ref type="figure">Figure.</ref> 13(b), the feature distributions of different subjects are more scattered to each other thus identities are more distinguishable, which proves the discriminant feature learning ability of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a context-sensitive temporal feature learning (CSTL) network for gait recognition. CSTL extracts temporal features with multiple scales and captures salient spatial clues for achieving strong spatio-temporal modeling ability. Specifically, diverse temporal features in three scales are introduced in CSTL, and local-to-global temporal relations are considered based on these temporal information for adaptive temporal aggregation. Besides, discriminative spatial parts are selected across the sequence to remedy the spatial feature corruption. Extensive experiments on three public datasets verify the superiority and the real-world application potentials of our method. In addition, we argue that the insights of learning spatial and temporal features in a supplementary manner could be also applied in other human action-related tasks, e.g., video-based person re-identification. We leave this for future work.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration that humans can distinguish gaits of different subjects by adaptively focusing on temporal fragments with different time scales. Color bar indicates the human focus distribution. Darker color represents more attention paid to corresponding frames. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Detailed architecture of Multi-Scale Temporal Extraction, which produces temporal features in three levels for each frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>An attention subnet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Three designs of local relation modeling in each frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The detailed structure of global relation modeling of multi-scale features using the transformer architecture. Particularly, the depth-wise (DW) conv is used to learn the position encoding conditioned on the input features dynamically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 4 )</head><label>4</label><figDesc>All MLPs follow: FC(c, c/16)-&gt;ReLU()-&gt;FC(c/16, c). The two FCs in ATA are FC(c, c/16) and FC(c/16, c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>8 , 8 )</head><label>88</label><figDesc>on CASIA-B and (32, 8) on OU-MVLP and GREW. (3) The backbone architecture on CASIA-B is given in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( 2 )Fig. 8 .</head><label>28</label><figDesc>Compared with the sequences from side view (90 ? ) and front (0 ? ) or back view (180 ? ), sequences from the squint views (36 ? or 144 ? ) achieve better performances, which may be attributed to the squint views incorporating rich visual clues from both side view and front or back view. OU-MVLP.Table.3 shows the comparison results between the proposed CSTL and current state-of-the-art methods in terms of averaged rank-1 accuracies on OU-MVLP. Our CSTL outperforms the existing methods under mean accuracy comparison, which proves the generalization capacity on a large-scale dataset. Particularly, though 3DLocal achieves competitive performance with CSTL under Multi-view performance comparison on CASIA-B using curve chart, in terms of averaged rank-1 accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Frame 0</head><label>0</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>3 Fig. 10 .Fig. 11 .</head><label>31011</label><figDesc>Illustration of spatial salient feature learning. We select four groups of salient local parts. The red boxes indicate selected parts. Illustration of testing models under unseen views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Illustration of attention heatmaps from the last layer in the backbone. The color bar on the right indicates the attention distribution for different colors. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .</head><label>13</label><figDesc>tSNE visualization examples of the baseline and our proposed model on CASIA-B test dataset. Different numbers with different colors indicate different identities. Best viewed with zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Wenyu Liu (SM' 15 )</head><label>15</label><figDesc>received the B.S. degree in Computer Science from Tsinghua University, Beijing, China, in 1986, and the M.S. and Ph.D. degrees, both in Electronics and Information Engineering, from Huazhong University of Science and Technology (HUST), Wuhan, China, in 1991 and 2001, respectively. He is now a professor and associate dean of the School of Electronic Information and Communications, HUST. His current research areas include computer vision, multimedia, and machine learning. Bin Feng received the B.S. and Ph.D. degrees in School of Electronics and Information Engineering from Huazhong University of Science and Technology (HUST), Wuhan, China, in 2001 and 2006, respectively. He is currently an Associate Professor with the School of Electronic Information and Communications, HUST. His research interests include computer vision and intelligent video analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Structure of the backbone on CASIA-B. C in and Cout denote the input channel and output channel of each layer respectively.</figDesc><table><row><cell>Layer</cell><cell>C in</cell><cell>Cout</cell><cell>Kernel</cell><cell>Pad</cell><cell>Activation</cell></row><row><cell>Conv1</cell><cell>1</cell><cell>32</cell><cell>3</cell><cell>1</cell><cell>Leaky ReLU</cell></row><row><cell>Conv1</cell><cell>32</cell><cell cols="4">64 Max Pooling kernel=(2,2), stride=2 3 1 Leaky ReLU</cell></row><row><cell>Conv3</cell><cell>64</cell><cell>128</cell><cell>3</cell><cell>1</cell><cell>Leaky ReLU</cell></row><row><cell>Conv4</cell><cell>128</cell><cell>128</cell><cell>3</cell><cell>1</cell><cell>Leaky ReLU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>In particular, the sequences with index '01' are regarded as gallery and the sequences with index '02' are regarded as probe set at testing phase. GREW. GREW<ref type="bibr" target="#b15">[16]</ref> captures gait videos under uncontrolled conditions, which is composed of 26345 subjects and, 128671 sequences in total. In particular, 882 cameras are involved to record the videos, where the corresponding views are not predefined like CASIA-B or OU-MVLP. Concretely, the training set includes 102887 sequences of 20000 subjects, the validation set includes 1784 sequences of 345 subjects and the testing set includes 24000 sequences of 6000 subjects.</figDesc><table><row><cell>4.2 Implementation Details</cell></row><row><cell>4.2.1 Hyper-parameters</cell></row><row><cell>(1) Follow the settings in [13], [18], we set the value of</cell></row><row><cell>B (number of training samples in one iteration) as 64,</cell></row><row><cell>256 and 256 on CASIA-B [15], OU-MVLP [16] and GREW</cell></row><row><cell>[17] datasets respectively. (2) The value of N (input frame</cell></row><row><cell>each subject contains</cell></row><row><cell>three walking conditions, i.e., normal (NM) (6 sequences),</cell></row><row><cell>walking with bag (BG) (2 sequences) and walking with coat</cell></row><row><cell>(CL) (2 sequences). For the training and testing stages, we</cell></row><row><cell>follow the protocols in [30]. The samples from the first 74</cell></row><row><cell>subjects are considered as train set, and the remaining 50</cell></row><row><cell>subjects are considered as test set. At testing phase, the first</cell></row><row><cell>4 sequences in NM condition of each subject are regarded as</cell></row></table><note>Further ablation experiments are conducted on CASIA-B to demonstrate the positive impact of each component in our method. Particularly, the source code will be released at https://github.com/OliverHxh/ CSTL. CASIA-B. CASIA-B [15] is composed of 124 subjects, and each subject contains 110 sequences with 11 different cam- era views. Under each camera view,gallery set and the remaining 6 sequences of each subject are used as probe set, including 2 sequences of NM, 2 sequences of BG and 2 sequences of CL. OU-MVLP. OU-MVLP [16] is composed of 10307 subjects. Each subject contains 28 sequences with 14 camera views, thus each subject contains 2 sequences (index '01' and '02') for each view. The first 5153 subjects are used for training, while the remaining 5154 subjects are for testing.number) and K (part division number) are set as 30 and 32.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table .</head><label>.</label><figDesc></figDesc><table><row><cell>2 shows the comparison results between</cell></row><row><cell>the proposed CSTL and current state-of-the-art methods</cell></row><row><cell>in averaged rank-1 accuracies on CASIA-B dataset. Three</cell></row><row><cell>walking conditions (NM, BG, CL) and 11 different camera</cell></row><row><cell>views (0</cell></row></table><note>? ? 180 ? ) are considered into performance eval- uation. Three notable conclusions are summarized as: (1) CSTL outperforms all other methods in mean accuracy comparisons under all cases, which demonstrates the ro- bustness and advantages. (2) It is natural that recognition performances would oscillate under different camera view- points. However, compared with other SOTA approaches,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc>Averaged rank-1 accuracies (%) on CASIA-B, excluding identical-view cases. Std denotes the performance sample standard deviation across 11 views.</figDesc><table><row><cell></cell><cell>Gallery NM Probe</cell><cell>Resolution ?</cell><cell>0 ?</cell><cell>18 ?</cell><cell>36 ?</cell><cell>54 ?</cell><cell>72 ?</cell><cell>0 ? 180 ? 90 ? 108 ?</cell><cell>126 ?</cell><cell>144 ?</cell><cell>162 ?</cell><cell>180 ?</cell><cell>Mean</cell><cell>Std</cell></row><row><cell></cell><cell>GaitSet [25]</cell><cell>64 ? 44</cell><cell cols="6">91.1 99.0 99.9 97.8 95.1 94.5 96.1</cell><cell>98.3</cell><cell>99.2</cell><cell>98.1</cell><cell>88.0</cell><cell>96.1</cell><cell>3.5</cell></row><row><cell></cell><cell>GaitPart [8]</cell><cell>64 ? 44</cell><cell cols="6">94.1 98.6 99.3 98.5 94.0 92.3 95.9</cell><cell>98.4</cell><cell>99.2</cell><cell>97.8</cell><cell>90.4</cell><cell>96.2</cell><cell>3.1</cell></row><row><cell></cell><cell>MT3D [12]</cell><cell>64 ? 44</cell><cell cols="6">95.7 98.2 99.0 97.5 95.1 93.9 96.1</cell><cell>98.6</cell><cell>99.2</cell><cell>98.2</cell><cell>92.0</cell><cell>96.7</cell><cell>2.3</cell></row><row><cell>NM</cell><cell>GaitGL [13]</cell><cell>64 ? 44</cell><cell cols="6">96.0 98.3 99.0 97.9 96.9 95.4 97.0</cell><cell>98.9</cell><cell>99.3</cell><cell>98.8</cell><cell>94.0</cell><cell>97.4</cell><cell>1.7</cell></row><row><cell></cell><cell>3DLocal [33]</cell><cell>64 ? 44</cell><cell cols="6">96.0 99.0 99.5 98.9 97.1 94.2 96.3</cell><cell>99.0</cell><cell>98.8</cell><cell>98.5</cell><cell>95.2</cell><cell>97.5</cell><cell>1.8</cell></row><row><cell></cell><cell>CSTL (Ours)</cell><cell>64 ? 44</cell><cell cols="6">97.7 99.3 99.4 99.1 97.3 96.1 98.5</cell><cell>99.7</cell><cell>99.7</cell><cell>99.2</cell><cell>96.9</cell><cell>98.5</cell><cell>1.2</cell></row><row><cell></cell><cell>GLN [26]</cell><cell>128 ? 88</cell><cell cols="6">93.2 99.3 99.5 98.7 96.1 95.6 97.2</cell><cell>98.1</cell><cell>99.3</cell><cell>98.6</cell><cell>90.1</cell><cell>96.9</cell><cell>3.0</cell></row><row><cell></cell><cell>3DLocal [33]</cell><cell>128 ? 88</cell><cell cols="6">97.8 99.4 99.7 99.3 97.5 96.0 98.3</cell><cell>99.1</cell><cell>99.9</cell><cell>99.2</cell><cell>94.6</cell><cell>98.3</cell><cell>1.7</cell></row><row><cell></cell><cell>CSTL (Ours)</cell><cell>128 ? 88</cell><cell cols="6">97.6 99.4 99.3 98.9 97.9 97.9 98.9</cell><cell>99.8</cell><cell>99.9</cell><cell>99.6</cell><cell>96.7</cell><cell>98.7</cell><cell>1.0</cell></row><row><cell></cell><cell>GaitSet [25]</cell><cell>64 ? 44</cell><cell cols="6">86.7 94.2 95.7 93.4 88.9 85.5 89.0</cell><cell>91.7</cell><cell>94.5</cell><cell>95.9</cell><cell>83.3</cell><cell>90.8</cell><cell>4.4</cell></row><row><cell></cell><cell>GaitPart [8]</cell><cell>64 ? 44</cell><cell cols="6">89.1 94.8 96.7 95.1 88.3 84.9 89.0</cell><cell>93.5</cell><cell>96.1</cell><cell>93.8</cell><cell>85.8</cell><cell>91.5</cell><cell>4.2</cell></row><row><cell></cell><cell>MT3D [12]</cell><cell>64 ? 44</cell><cell cols="6">91.0 95.4 97.5 94.2 92.3 86.9 91.2</cell><cell>95.6</cell><cell>97.3</cell><cell>96.4</cell><cell>86.6</cell><cell>93.0</cell><cell>3.9</cell></row><row><cell>BG</cell><cell>GaitGL [13]</cell><cell>64 ? 44</cell><cell cols="6">92.6 96.6 96.8 95.5 93.5 89.3 92.2</cell><cell>96.5</cell><cell>98.2</cell><cell>96.9</cell><cell>91.5</cell><cell>94.5</cell><cell>2.8</cell></row><row><cell></cell><cell>3DLocal [33]</cell><cell>64 ? 44</cell><cell cols="6">92.9 95.9 97.8 96.2 93.0 87.8 92.7</cell><cell>96.3</cell><cell>97.9</cell><cell>98.0</cell><cell>88.5</cell><cell>94.3</cell><cell>3.5</cell></row><row><cell></cell><cell>CSTL (Ours)</cell><cell>64 ? 44</cell><cell cols="6">94.6 97.5 97.0 95.7 92.0 90.2 91.9</cell><cell>96.2</cell><cell>97.9</cell><cell>97.2</cell><cell>92.5</cell><cell>94.8</cell><cell>2.7</cell></row><row><cell></cell><cell>GLN [26]</cell><cell>128 ? 88</cell><cell cols="6">91.1 97.7 97.8 95.2 92.5 91.2 92.4</cell><cell>96.0</cell><cell>97.5</cell><cell>95.0</cell><cell>88.1</cell><cell>94.0</cell><cell>3.2</cell></row><row><cell></cell><cell>3DLocal [33]</cell><cell>128 ? 88</cell><cell cols="6">94.7 98.7 98.8 97.5 93.3 91.7 92.8</cell><cell>96.5</cell><cell>98.1</cell><cell>97.3</cell><cell>90.7</cell><cell>95.5</cell><cell>2.9</cell></row><row><cell></cell><cell>CSTL (Ours)</cell><cell>128 ? 88</cell><cell cols="6">95.9 97.1 97.8 97.2 95.1 93.0 96.1</cell><cell>97.5</cell><cell>98.0</cell><cell>97.4</cell><cell>93.8</cell><cell>96.2</cell><cell>1.7</cell></row><row><cell></cell><cell>GaitSet [25]</cell><cell>64 ? 44</cell><cell cols="6">59.5 75.0 78.3 74.6 71.4 71.3 70.8</cell><cell>74.1</cell><cell>74.6</cell><cell>69.4</cell><cell>54.1</cell><cell>70.3</cell><cell>7.2</cell></row><row><cell></cell><cell>GaitPart [8]</cell><cell>64 ? 44</cell><cell cols="6">70.7 85.5 86.9 83.3 77.1 72.5 76.9</cell><cell>82.2</cell><cell>83.8</cell><cell>80.2</cell><cell>66.5</cell><cell>78.7</cell><cell>6.6</cell></row><row><cell></cell><cell>MT3D [12]</cell><cell>64 ? 44</cell><cell cols="6">76.0 87.6 89.8 85.0 81.2 75.7 81.0</cell><cell>84.5</cell><cell>85.4</cell><cell>82.2</cell><cell>68.1</cell><cell>81.5</cell><cell>6.2</cell></row><row><cell>CL</cell><cell>GaitGL [13]</cell><cell>64 ? 44</cell><cell cols="6">76.6 90.0 90.3 87.1 84.5 79.0 84.1</cell><cell>87.0</cell><cell>87.3</cell><cell>84.4</cell><cell>69.5</cell><cell>83.6</cell><cell>6.3</cell></row><row><cell></cell><cell>3DLocal [33]</cell><cell>64 ? 44</cell><cell cols="6">78.2 90.2 92.0 87.1 83.0 76.8 83.1</cell><cell>86.6</cell><cell>86.8</cell><cell>84.1</cell><cell>70.9</cell><cell>83.7</cell><cell>6.2</cell></row><row><cell></cell><cell>CSTL (Ours)</cell><cell>64 ? 44</cell><cell cols="6">78.5 90.5 91.6 86.9 84.4 80.8 83.3</cell><cell>87.5</cell><cell>88.0</cell><cell>85.0</cell><cell>73.0</cell><cell>84.5</cell><cell>5.5</cell></row><row><cell></cell><cell>GLN [26]</cell><cell>128 ? 88</cell><cell cols="6">70.6 82.4 85.2 82.7 79.2 76.4 76.2</cell><cell>78.9</cell><cell>77.9</cell><cell>78.7</cell><cell>64.3</cell><cell>77.5</cell><cell>5.8</cell></row><row><cell></cell><cell>3DLocal [33]</cell><cell>128 ? 88</cell><cell cols="6">78.5 88.9 91.0 89.2 83.7 80.5 83.2</cell><cell>84.3</cell><cell>87.9</cell><cell>87.1</cell><cell>74.7</cell><cell>84.5</cell><cell>5.0</cell></row><row><cell></cell><cell>CSTL (Ours)</cell><cell>128 ? 88</cell><cell cols="6">84.2 92.6 93.4 89.9 87.1 85.2 87.4</cell><cell>90.6</cell><cell>92.3</cell><cell>90.8</cell><cell>82.2</cell><cell>88.7</cell><cell>3.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Averaged rank-1 accuracies (%) on OU-MVLP, excluding identical-view cases. Std denotes the performance sample standard deviation across 14 views. The results in the first 6 rows and the last 6 rows are conducted by keeping or removing invalid probe sequences that have no corresponding targets in the gallery set.</figDesc><table><row><cell>Method</cell><cell>0 ?</cell><cell>15 ?</cell><cell>30 ?</cell><cell>45 ?</cell><cell>60 ?</cell><cell>75 ?</cell><cell cols="2">Probe View 90 ? 180 ?</cell><cell>195 ?</cell><cell>210 ?</cell><cell>225 ?</cell><cell>240 ?</cell><cell>255 ?</cell><cell>270 ?</cell><cell>Mean</cell><cell>Std</cell></row><row><cell>GaitSet [25]</cell><cell>81.3</cell><cell>88.6</cell><cell cols="5">90.2 90.7 88.6 89.1 88.3</cell><cell>83.1</cell><cell>87.7</cell><cell>89.4</cell><cell>89.7</cell><cell>87.8</cell><cell>88.3</cell><cell>86.9</cell><cell>87.9</cell><cell>2.6</cell></row><row><cell>GaitPart [8]</cell><cell>82.6</cell><cell>88.9</cell><cell cols="5">90.8 91.0 89.7 89.9 89.5</cell><cell>85.2</cell><cell>88.1</cell><cell>90.0</cell><cell>90.1</cell><cell>89.0</cell><cell>89.1</cell><cell>88.2</cell><cell>88.7</cell><cell>2.3</cell></row><row><cell>GLN [26]</cell><cell>83.8</cell><cell>90.0</cell><cell cols="5">91.0 91.2 90.3 90.0 89.4</cell><cell>85.3</cell><cell>89.1</cell><cell>90.5</cell><cell>90.6</cell><cell>89.6</cell><cell>89.3</cell><cell>88.5</cell><cell>89.2</cell><cell>2.1</cell></row><row><cell>GaitGL [13]</cell><cell>84.9</cell><cell>90.2</cell><cell cols="5">91.1 91.5 91.1 90.8 90.3</cell><cell>88.5</cell><cell>88.6</cell><cell>90.3</cell><cell>90.4</cell><cell>89.6</cell><cell>89.5</cell><cell>88.8</cell><cell>89.7</cell><cell>1.7</cell></row><row><cell>3DLocal [33]</cell><cell>86.1</cell><cell>91.2</cell><cell cols="5">92.6 92.9 92.2 91.3 91.1</cell><cell>86.9</cell><cell>90.8</cell><cell>92.2</cell><cell>92.3</cell><cell>91.3</cell><cell>91.1</cell><cell>90.2</cell><cell>90.9</cell><cell>2.0</cell></row><row><cell>CSTL (Ours)</cell><cell>88.5</cell><cell>91.5</cell><cell cols="5">91.7 92.0 91.8 91.4 91.2</cell><cell>90.3</cell><cell>90.9</cell><cell>91.1</cell><cell>91.2</cell><cell>90.8</cell><cell>90.7</cell><cell>90.4</cell><cell>91.0</cell><cell>0.9</cell></row><row><cell>GaitSet [25]</cell><cell>84.5</cell><cell>93.3</cell><cell cols="5">96.7 96.6 93.5 95.3 94.2</cell><cell>87.0</cell><cell>92.5</cell><cell>96.0</cell><cell>96.0</cell><cell>93.0</cell><cell>94.3</cell><cell>92.7</cell><cell>93.3</cell><cell>3.5</cell></row><row><cell>GaitPart [8]</cell><cell>88.0</cell><cell>94.7</cell><cell cols="5">97.7 97.6 95.5 96.6 96.2</cell><cell>90.6</cell><cell>94.2</cell><cell>97.2</cell><cell>97.1</cell><cell>95.1</cell><cell>96.0</cell><cell>95.0</cell><cell>95.1</cell><cell>2.7</cell></row><row><cell>GLN [26]</cell><cell>89.3</cell><cell>95.8</cell><cell cols="5">97.9 97.8 96.0 96.7 96.1</cell><cell>90.7</cell><cell>95.3</cell><cell>97.7</cell><cell>97.5</cell><cell>95.7</cell><cell>96.2</cell><cell>95.3</cell><cell>95.6</cell><cell>2.5</cell></row><row><cell>GaitGL [13]</cell><cell>90.5</cell><cell>96.1</cell><cell cols="5">98.0 98.1 97.0 97.6 97.1</cell><cell>94.2</cell><cell>94.9</cell><cell>97.4</cell><cell>97.4</cell><cell>95.7</cell><cell>96.5</cell><cell>95.7</cell><cell>96.2</cell><cell>2.0</cell></row><row><cell>3DLocal [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>96.5</cell><cell>-</cell></row><row><cell>CSTL (Ours)</cell><cell>94.3</cell><cell cols="6">97.5 98.7 98.7 97.7 98.3 98.1</cell><cell>96.1</cell><cell>97.3</cell><cell>98.3</cell><cell>98.3</cell><cell>97.1</cell><cell>97.8</cell><cell>97.4</cell><cell>97.5</cell><cell>1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Averaged rank-1, rank-5, rank-10 and rank-20 accuracies (%) on</cell></row><row><cell cols="4">GREW, excluding identical-view cases.</cell><cell></cell></row><row><cell>Method</cell><cell>Rank-1</cell><cell>Rank-5</cell><cell>Rank-10</cell><cell>Rank-20</cell></row><row><cell>GEINet [48]</cell><cell>6.8</cell><cell>13.4</cell><cell>17.0</cell><cell>21.0</cell></row><row><cell>TS-CNN [30]</cell><cell>13.6</cell><cell>24.6</cell><cell>30.2</cell><cell>37.0</cell></row><row><cell>GaitSet [25]</cell><cell>46.3</cell><cell>63.6</cell><cell>70.3</cell><cell>76.8</cell></row><row><cell>GaitPart [8]</cell><cell>44.0</cell><cell>60.7</cell><cell>67.3</cell><cell>73.5</cell></row><row><cell>CSTL (Ours)</cell><cell>50.6</cell><cell>65.9</cell><cell>71.9</cell><cell>76.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Study of the effectiveness of modules in CSTL on CASIA-B in terms of averaged rank-1 accuracy. For the sake of simplicity, we use MSTE to denote multi-scale temporal extraction.</figDesc><table><row><cell>Model</cell><cell>NM</cell><cell cols="3">Rank-1 Accuracy BG CL Mean</cell></row><row><cell>GaitSet [25]</cell><cell>96.1</cell><cell>90.8</cell><cell>70.3</cell><cell>85.7</cell></row><row><cell>MT3D [12]</cell><cell>96.7</cell><cell>93.0</cell><cell>81.5</cell><cell>90.4</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>95.3</cell><cell>88.7</cell><cell>72.1</cell><cell>85.4</cell></row><row><cell>Baseline + MSTE</cell><cell>96.6</cell><cell>91.1</cell><cell>81.0</cell><cell>89.6</cell></row><row><cell>Baseline + MSTE + ATA</cell><cell>98.3</cell><cell>94.0</cell><cell>81.5</cell><cell>91.3</cell></row><row><cell>Baseline + MSTE + SSFL</cell><cell>97.7</cell><cell>93.0</cell><cell>83.8</cell><cell>91.5</cell></row><row><cell>CSTL</cell><cell>98.5</cell><cell>94.8</cell><cell>84.5</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc>Study of the effectiveness of multi-scale temporal features on CASIA-B in terms of averaged rank-1 accuracy.</figDesc><table><row><cell cols="3">Multi-scale Features</cell><cell></cell><cell cols="3">Rank-1 Accuracy</cell></row><row><cell>Frame-level</cell><cell>Short-term</cell><cell>Long-term</cell><cell>NM</cell><cell>BG</cell><cell>CL</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell>98.3</cell><cell>93.9</cell><cell>79.5</cell><cell>90.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>98.1</cell><cell>94.0</cell><cell>84.0</cell><cell>92.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>96.2</cell><cell>91.0</cell><cell>73.4</cell><cell>86.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>98.2</cell><cell>94.3</cell><cell>83.8</cell><cell>92.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>98.2</cell><cell>93.7</cell><cell>80.2</cell><cell>90.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>98.4</cell><cell>94.2</cell><cell>84.4</cell><cell>92.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>98.5</cell><cell>94.8</cell><cell>84.5</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7</head><label>7</label><figDesc>Study on the effectiveness of local and global relation modeling on CASIA-B in terms of averaged rank-1 accuracy.</figDesc><table><row><cell cols="2">Local Relation</cell><cell></cell><cell cols="3">Rank-1 Accuracy</cell></row><row><cell>Max Pooling</cell><cell>FC Attention</cell><cell>Global Relation</cell><cell>NM BG</cell><cell cols="2">CL Mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">97.9 93.8 84.4</cell><cell>92.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">98.1 93.9 84.5</cell><cell>92.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">97.9 93.8 82.8</cell><cell>91.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">98.3 94.0 81.8</cell><cell>91.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">98.5 94.8 84.5</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="5">Comparisons of spatial selection strategies on CASIA-B in terms of</cell></row><row><cell cols="4">averaged rank-1 accuracy.</cell><cell></cell></row><row><cell>Methods</cell><cell>NM</cell><cell cols="3">Rank-1 Accuracy BG CL Mean</cell></row><row><cell>random frame</cell><cell>98.1</cell><cell>94.0</cell><cell>82.9</cell><cell>91.7</cell></row><row><cell>SSFL (frame-level)</cell><cell>98.5</cell><cell>94.5</cell><cell>83.7</cell><cell>92.2</cell></row><row><cell>SSFL</cell><cell>98.5</cell><cell>94.8</cell><cell>84.5</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9</head><label>9</label><figDesc>Study on the impacts of importance evaluation methods and number of selected spatial parts in SSFL on CASIA-B in terms of rank-1 averaged accuracy.</figDesc><table><row><cell>Method</cell><cell>Selected Groups</cell><cell></cell><cell cols="2">Rank-1 Accuracy</cell><cell></cell></row><row><cell>MLP</cell><cell>1</cell><cell>98.1</cell><cell>93.6</cell><cell>83.5</cell><cell>91.7</cell></row><row><cell></cell><cell>1</cell><cell>98.3</cell><cell>93.7</cell><cell>83.6</cell><cell>91.9</cell></row><row><cell>MHSA</cell><cell>2 4</cell><cell>98.3 98.5</cell><cell>93.8 94.8</cell><cell>83.5 84.5</cell><cell>91.8 92.6</cell></row><row><cell></cell><cell>8</cell><cell>98.5</cell><cell>94.2</cell><cell>83.6</cell><cell>92.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 10</head><label>10</label><figDesc>Performance comparison under unseen views on CASIA-B in terms of averaged rank-1 accuracy under all walking conditions, excluding identical-view cases. Particularly, the default setting denotes that test views are in accordance with train views. Frames from Different Views. As shown in Table. 11, we conduct 5 more experiments that combine frames from different views into one sequence. Particularly, each sequence is composed of frames from a pair of views, which are defined by the view difference. Taking the view difference of 18 ? as an example, the corresponding pairs are: 0 ? &amp; 18 ? , 18 ? &amp; 36 ? , 36 ? &amp; 54 ? , 54 ? &amp; 72 ? , 72 ? &amp; 90 ? , 90 ? &amp; 108 ? , 108 ? &amp; 126 ? , 126 ? &amp; 144 ? , 144 ? &amp; 162 ? , 162 ? &amp; 180 ? .And for eliminating the effects of sequence length as far as possible, we only sample half of each sequence from each view. Particularly, the sequences in the probe set are composed of frames from different views, and the sequences in the gallery set are composed of frames in the same view.</figDesc><table><row><cell>Method</cell><cell>Scenario A</cell><cell>Scenario B</cell><cell>Default Setting</cell></row><row><cell>Baseline</cell><cell>81.9</cell><cell>76.3</cell><cell>85.4</cell></row><row><cell>Ours</cell><cell>90.1</cell><cell>84.5</cell><cell>92.6</cell></row></table><note>As given Table. 11, our method marginally outperforms</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 11</head><label>11</label><figDesc></figDesc><table><row><cell cols="7">Performance comparison using frames from different views on</cell></row><row><cell cols="7">CASIA-B in terms of averaged rank-1 accuracy under all walking</cell></row><row><cell cols="6">conditions, excluding identical-view cases.</cell><cell></cell></row><row><cell>View Difference</cell><cell>18 ?</cell><cell>36 ?</cell><cell>54 ?</cell><cell>72 ?</cell><cell>90 ?</cell><cell>Single View</cell></row><row><cell>Baseline</cell><cell>88.1</cell><cell>89.6</cell><cell>89.8</cell><cell>90.2</cell><cell>90.2</cell><cell>85.4</cell></row><row><cell>Ours</cell><cell>93.0</cell><cell>94.6</cell><cell>95.3</cell><cell>95.7</cell><cell>95.9</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Duowang Zhu received the B.S. degree in School of Electronic Information and Communications from Huazhong University of Science and Technology (HUST), Wuhan, China, in 2020. Now, he is pursuing the M.S. degree in School of Electronic Information and Communications from Huazhong University of Science and Technology (HUST), Wuhan, China. His current research areas include computer vision and machine learning. received the B.S. and Ph.D. degrees in Electronics and Information Engineering from Huazhong University of Science and Technology (HUST), Wuhan, China, in 2009 and 2014, respectively. He is currently an Associate Professor with the School of Electronic Information and Communications, HUST. His research interests include computer vision and machine learning. He services as associate editors for Pattern Recognition and Image and Vision Computing journals and an editorial board member of Electronics journal. Wang received the B.S. degree in School of Computer and Information from China Three Gorges University (CTGU), Yichang, China, in 2017. He is currently working toward the Phd degree in School of Electronic Information and Communications from Huazhong University of Science and Technology (HUST), Wuhan, China. His current research areas include scene text spotting and retrieval, multi-modal analysis. Yang received the Master degree in School of mathematics and statistics form Wuhan University,Wuhan, China. He is currently the senior engineer of Wuhan FiberHome Digital Technology Co., Ltd. His research interests include computer vision and data mining. He received the Ph.D. degree in School of Optical and Electronic Information from Huazhong University of Science and Technology (HUST), Wuhan, China. He is currently the deputy general manager of Wuhan FiberHome Digital Technology Co., Ltd. His research interests include computer vision and data mining.</figDesc><table><row><cell>Xinggang Wang (M'17) Hao Bo Botao</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is supported by the NSFC (grants No. 61773176 and No. 61733007).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gait analysis in forensic medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lynnerup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of forensic sciences</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1149" to="1153" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On using gait in forensic biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bouchrika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goffredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of forensic sciences</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="882" to="889" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Influence of velocity on variability in gait kinematics: implications for recognition in forensic science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alkjaer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lynnerup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Simonsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of forensic sciences</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1242" to="1247" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variability and similarity of gait as evaluated by joint angles: implications for forensic gait analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alkjaer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lynnerup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of forensic sciences</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="494" to="504" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Forensic gait analysis and recognition: standards of evidence admissibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Macoveciuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Rando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Borrion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of forensic sciences</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1294" to="1303" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human gait recognition from motion capture data in signature poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balazia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved gait recognition through gait energy image partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Premalatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chandramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1261" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gaitpart: Temporal part-based model for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">233</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Condition-aware comparison scheme for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-view gait recognition by discriminative feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On learning disentangled representations for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gait recognition with multipletemporal-scale 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACMMM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gait recognition via effective globallocal feature representation and local temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="14" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-view large population gait dataset and its performance evaluation for cross-view gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPSJ Transactions on Computer Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gait recognition in the wild: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context-sensitive temporal feature learning for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A model-based gait recognition method with body pose and human prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">107069</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pose-based temporal-spatial network (ptsn) for gait recognition with carrying and clothing variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese conference on biometric recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gaitgraph: Graph convolutional network for skeleton-based gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Teepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>H?rmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11228</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gaitset: Crossview gait recognition through utilizing gait as a deep set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gait lateral network: Learning discriminative and compact representations for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-view gait recognition using 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4165" to="4169" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Individual recognition using gait energy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="316" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-task gans for viewspecific feature learning in gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="113" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A comprehensive study on cross-view gait based human identification with deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Viewinvariant discriminative projection for multi-view gait-based human identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2034" to="2045" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gait recognition via semi-supervised disentangled representation learning to identity and covariate features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">319</biblScope>
			<biblScope unit="page" from="13" to="309" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d local convolutional neural networks for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="920" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d local convolutional neural networks for gait recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="920" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno>pp. 10 012-10 022</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Vidtr: Video transformer without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">587</biblScope>
			<biblScope unit="page" from="13" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Appearance-preserving 3d convolution for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="228" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal coherence or temporal motion: Which is more critical for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Conditional positional encodings for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Smart frame selection for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.10671</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">You only watch once: A unified cnn architecture for real-time spatiotemporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>K?p?kl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06644</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshops</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Geinet: View-invariant gait recognition using a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shiraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 international conference on biometrics (ICB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">His current research areas include computer vision and machine learning</title>
	</analytic>
	<monogr>
		<title level="m">2020. Now, he is pursuing the M.S. degree in School of Electronic Information and Communications from Huazhong University of Science and Technology (HUST)</title>
		<meeting><address><addrLine>Wuhan, China; Wuhan, China</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Xiaohu Huang received the B.S. degree in School of Electronic Information and Communications from Huazhong University of Science and Technology (HUST)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
