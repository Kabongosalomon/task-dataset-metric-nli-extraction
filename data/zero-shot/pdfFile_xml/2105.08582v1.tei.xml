<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vision Transformer for Fast and Efficient Scene Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-18">18 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Electronics Engineering</orgName>
								<orgName type="institution">Institute University of the Philippines</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Vision Transformer for Fast and Efficient Scene Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-18">18 May 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Scene text recognition ? Transformer ? Data augmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text recognition (STR) enables computers to read text in natural scenes such as object labels, road signs and instructions. STR helps machines perform informed decisions such as what object to pick, which direction to go, and what is the next step of action. In the body of work on STR, the focus has always been on recognition accuracy. There is little emphasis placed on speed and computational efficiency which are equally important especially for energy-constrained mobile machines. In this paper we propose ViTSTR, an STR with a simple single stage model architecture built on a compute and parameter efficient vision transformer (ViT). On a comparable strong baseline method such as TRBA with accuracy of 84.3%, our small ViTSTR achieves a competitive accuracy of 82.6% (84.2% with data augmentation) at 2.4? speed up, using only 43.4% of the number of parameters and 42.2% FLOPS. The tiny version of ViTSTR achieves 80.3% accuracy (82.1% with data augmentation), at 2.5? the speed, requiring only 10.9% of the number of parameters and 11.9% FLOPS. With data augmentation, our base ViTSTR outperforms TRBA at 85.2% accuracy (83.7% without augmentation) at 2.3? the speed but requires 73.2% more parameters and 61.5% more FLOPS. In terms of trade-offs, nearly all ViTSTR configurations are at or near the frontiers to maximize accuracy, speed and computational efficiency all at the same time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>STR plays a vital role for machines to understand the human environment.</p><p>We invented text to convey information through labels, signs, instructions and announcements. Therefore, for a computer to take advantage of this visual cue, it must also understand text in natural scenes. For instance, a "Push" signage on a door tells a robot to push it to open. In the kitchen, a label with "Sugar" means that the container has sugar in it. A wearable system that can read "50" or "FIFTY" on a paper bill can greatly enhance the lives of visually impaired people. <ref type="figure">Fig. 1</ref>. Trade-offs between accuracy vs number of parameters, speed and computational load (FLOPS). +Aug uses data augmentation. Almost all versions of ViTSTR are at or near the frontiers to maximize the performance on all metrics. The slope of the line is the accuracy gain as the number of parameters, speed or FLOPS increases. The steeper the slope, the better. Teal line includes ViTSTR with data augmentation.</p><p>STR is related but different from the more developed field of Optical Character Recognition (OCR). In OCR, symbols on a printed front facing document are detected and recognized. In a way, OCR operates in a more structured setting. Meanwhile, the objective of STR is to recognize symbols in varied unconstrained settings such as walls, signboards, product labels, road signs, markers, etc. Therefore, the inputs have many degrees of variation in font style, orientation, shape, size, color, texture and illumination. The inputs are also subject to camera sensor orientation, location and imperfections causing image blur, pixelation, noise, and geometric and radial distortions. Weather disturbances such as glare, shadow, rain, snow and frost can also greatly affect the performance of STR.</p><p>In the body of work on STR, the emphasis has always been on accuracy with little attention paid to speed and computing requirements. In this work, we attempt to put balance on accuracy, speed and efficiency. Accuracy refers to the correctness of recognized text. Speed is measured by how many text images are processed per unit time. Efficiency can be approximated by the number of parameters and computations (eg FLOPS) required to process one image. The number of parameters reflects the memory requirements while FLOPS estimates the number of instructions needed to complete a task. An ideal STR is accurate and fast while requiring only little computing resources.</p><p>In the quest to beat the SOTA, most models are zeroing on accuracy with inadequate discussion on the trade off. In order to instill balance on the importance of accuracy, speed and efficiency, we propose to take advantage of the simplicity and efficiency of vision transformers (ViT) <ref type="bibr" target="#b6">[7]</ref> such as Data-efficient image Transformer (DeiT) <ref type="bibr" target="#b33">[34]</ref>. ViT demonstrated that SOTA results in Ima-geNet <ref type="bibr" target="#b27">[28]</ref> recognition can be achieved using a transformer <ref type="bibr" target="#b34">[35]</ref> encoder only. ViT inherited all the properties of a transformer including its speed and computational efficiency. Using the model weights of DeiT which is simply a ViT trained by knowledge distillation <ref type="bibr" target="#b12">[13]</ref> for better performance, we built an STR that can be trained end-to-end. This resulted to a simple single stage model architecture that is able to maximize accuracy, speed and computational performance. The tiny version of our ViTSTR achieves 80.3% accuracy (82.1% with data augmentation), is fast at 9.3 msec/image, with a small footprint of 5.4M parameters and requires much less computations at 1.3 Giga FLOPS. The small version of ViTSTR achieves a higher accuracy of 82.6% (84.2% with data augmentation), is also fast at 9.5 msec/image while requiring 21.5M parameters and 4.6 Giga FLOPS. With data augmentation, the base version of ViTSTR achieves 85.2% accuracy (83.7% no augmentation) at 9.8 msec/image but requires 85.8M parameters and 17.6 Giga FLOPS. We adopted the reference tiny, small and base to indicate which ViT/DeiT transformer encoder was used in ViTSTR. As shown in <ref type="figure">Figure 1</ref>, almost all versions of our proposed ViTSTR are at or near the frontiers of accuracy vs speed, memory, and computational load indicating optimal trade-offs. To encourage reproducibility, the code of ViTSTR is available at https://github.com/roatienza/deep-text-recognition-benchmark.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For machines, reading text in the human environment is a challenging task due to different possible appearances of symbols. <ref type="figure" target="#fig_1">Figure 2</ref> shows examples of text in the wild affected by curvature, font style, blur, rotation, noise, geometry, illumination, occlusion and resolution. There are many other factors that could affect text images such as weather condition, camera sensor imperfection, motion, lighting, etc.</p><p>Reading text in natural scenes generally requires two stages: 1) text detection and 2) text recognition. Detection determines the bounding box of the region where text can be found. Once the region is known, text recognition reads the symbols in the image. Ideally, a method is able to do both at the same time. However, the performance of SOTA end-to-end text reading models is still far from modern-day OCR systems and remains an open problem <ref type="bibr" target="#b4">[5]</ref>. In this work, our focus is on text recognition of 96 Latin characters (i.e. 0-9, a-Z, etc.).</p><p>STR identifies each character of a text in an image in the correct sequence. Unlike object recognition where usually there is only one category of object, there may be zero or more characters for a given text image. Thus, STR models are more complex. Similar to many vision problems, early methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref> used hand-crafted features resulting to poor performance. Deep learning has dramatically advanced the field of STR. In 2019, Baek et al. <ref type="bibr" target="#b0">[1]</ref> presented a framework that models the design patterns of modern STR. <ref type="figure">Figure 3</ref> shows the four stages or modules of STR. Broadly speaking, even recently proposed methods such as transformer-based models, No-Recurrence sequence-to-sequence Text Recognizer (NRTR) <ref type="bibr" target="#b28">[29]</ref> and Self-Attention Text Recognition Network (SATRN) <ref type="bibr" target="#b17">[18]</ref> can fit into Rectification-Feature Extraction (Backbone)-Sequence Modelling-Prediction framework.</p><p>The Rectification stage removes the distortion from the word image so that the text is horizontal or normalized. This makes it easier for Feature Extraction (Backbone) module to determine invariant features. Thin-Plate-Spline (TPS) <ref type="bibr" target="#b2">[3]</ref> models the distortion by finding and correcting fiducial points. RARE (Robusttext recognizer with Automatic REctification) <ref type="bibr" target="#b30">[31]</ref>, STAR-Net (SpaTial Attention Residue Network) <ref type="bibr" target="#b20">[21]</ref>, and TRBA (TPS-ResNet-BiLSTM-Attention) <ref type="bibr" target="#b0">[1]</ref> use TPS. ESIR (End-to-end trainable Scene text Recognition) <ref type="bibr" target="#b40">[41]</ref> employs an iterative rectification network that significantly boosts the performance of text recognition models. In some cases, no rectification is employed such as in CRNN (Convolutional Recurrent Neural Network) <ref type="bibr" target="#b29">[30]</ref>, R2AM (Recursive Recurrent neural networks with Attention Modeling) <ref type="bibr" target="#b16">[17]</ref>, GCRNN (Gated Recurrent Convolution Neural Network) <ref type="bibr" target="#b35">[36]</ref> and Rosetta <ref type="bibr" target="#b3">[4]</ref>. The role of Feature Extraction (Backbone) stage is to automatically determine the invariant features of each character symbol. STR uses the same feature extractors in object recognition tasks such as VGG <ref type="bibr" target="#b31">[32]</ref>, ResNet <ref type="bibr" target="#b10">[11]</ref>, and a variant of CNN called RCNN <ref type="bibr" target="#b16">[17]</ref>. Rosetta, STAR-Net and TRBA use ResNet. RARE and CRNN extract features using VGG. R2AM and GCRNN build on RCNN. Transformer-based models NRTR and SATRN use customized CNN blocks to extract features for transformer encoder-decoder text recognition.</p><p>Since STR is a multi-class sequence prediction, there is a need to remember long-term dependency. The role of Sequence modelling such as BiLSTM is to make a consistent context between the current character features and the past/future characters features. CRNN, GRCNN, RARE, STAR-Net and TRBA use BiLSTM. Other models such as Rosetta and R2AM do not employ sequence modelling to speed up prediction.</p><p>The Prediction stage examines the features resulting from the Backbone or Sequence modelling to arrive at a sequence of characters prediction. CTC (Connectionist Temporal Classification) <ref type="bibr" target="#b7">[8]</ref> maximizes the likelihood of an output sequence by efficiently summing over all possible input-output sequence align-  The patches are converted into 1D vector embeddings (flattened 2D patches). As input to the encoder, a learnable patch embedding is added together with a position encoding for each embedding. The network is trained end-to-end to predict a sequence of characters.</p><p>[GO] is a pre-defined start of sequence symbol while [s] represents a space or end of a character sequence. ments <ref type="bibr" target="#b4">[5]</ref>. Alternative to CTC is Attention Mechanism <ref type="bibr" target="#b1">[2]</ref> that learns the alignment between the image features and symbols. CRNN, GRCNN, Rosetta and STAR-Net use CTC. R2AM, RARE and TRBA are Attention-based.</p><p>Like in natural language processing (NLP), transformers overcome sequence modelling and prediction by doing parallel self-attention and prediction. This resulted to a fast and efficient model. As shown in <ref type="figure">Figure 3</ref>, current transformerbased STR models still require a Backbone and a Transformer Encoder-Decoder. Recently, ViT <ref type="bibr" target="#b6">[7]</ref> proved that it is possible to beat the performance of deep networks such as ResNet <ref type="bibr" target="#b10">[11]</ref> and EfficientNet <ref type="bibr" target="#b32">[33]</ref> on ImageNet1k <ref type="bibr" target="#b27">[28]</ref> classification by using the transformer encoder only but pre-training it on very large datasets such as ImageNet21k and JFT-300M. DeiT <ref type="bibr" target="#b33">[34]</ref> demonstrated that ViT does not need a large dataset and can even achieve better results but it must be trained using knowledge distillation <ref type="bibr" target="#b12">[13]</ref>. ViT, using pre-trained weights of DeiT, is the basis of our proposed fast and efficient STR called ViTSTR. As shown in <ref type="figure">Figure  3</ref>, ViTSTR is a very simple model with just one stage that can easily halve the number of parameters and FLOPS of a transformer-based STR. The ViT model architecture is similar to the original transformer by Vaswani et al. <ref type="bibr" target="#b34">[35]</ref>. The difference is only the encoder part is utilized. The original transformer was designed for NLP tasks. Instead of word embeddings, each input image x ? R H?W ?C is reshaped into a sequence of flattened 2D patches x p ? R N ?P 2 C . The image dimension is H ? W with C channels while the patch dimension is P ? P . The resulting patch sequence length is N . The transformer encoder uses a constant width D for embedding and features in all its layers. To match this size, each flattened patch is converted to an embedding of size D via linear projection. This is shown as small boxes with teal color in <ref type="figure" target="#fig_2">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Vision Transformer for STR</head><p>A learnable class embedding of the same dimension D is prepended with the sequence. A unique position encoding of the same dimension D is added to each embedding. The resulting vector sum is the input to the encoder. In ViTSTR, a learnable position encoding is used.</p><p>In the original ViT, the output vector corresponding to the learnable class embedding is used for object category prediction. In ViTSTR, this corresponds to the [GO] token. Furthermore, instead of just extracting one output vector, we extract multiple feature vectors from the encoder. The number is equal to the maximum length of text in our dataset plus two for the [GO] and [s] tokens. We use the [GO] token to mark the beginning of the text prediction and [s] to indicate the end or a space. [s] is repeated at the end of each text prediction up to the maximum sequence length to mark that nothing follows after the text characters. <ref type="figure">Figure 5</ref> shows the layers inside one encoder block. Every input goes through Layer Normalization (LN). The Multi-head Self-Attention layer (MSA) determines the relationships between feature vectors. Vaswani et al. <ref type="bibr" target="#b34">[35]</ref> found out that using multiple heads instead of just one allows the model to jointly attend to information from different representation subspaces at different positions. The number of heads is H. The Multilayer Perceptron (MLP) performs feature extraction. Its input is also layer normalized. The MLP is made of 2 layers with GELU activation <ref type="bibr" target="#b11">[12]</ref>. Residual connection is placed between the output of LN and MSA/MLP.</p><p>In summary, the input to the encoder is:</p><formula xml:id="formula_0">z 0 = [x class ; x 1 p E; x 2 p E; ...; x N p E] + E pos ,<label>(1)</label></formula><p>where E ? R P 2 C?D and E pos ? R (N +1)?D . The output of MSA block is:</p><formula xml:id="formula_1">z l = M SA(LN (z l?1 )) + z l?1 ,<label>(2)</label></formula><p>for l = 1...L. L is the depth or the number of encoder blocks. A transformer encoder is made of a stack of L encoder blocks.</p><p>The output of the MLP block is:</p><formula xml:id="formula_2">z l = M LP (LN (z l )) + z l ,<label>(3)</label></formula><p>for l = 1...L. Finally, the head is made of a sequence of linear projections forming the word prediction:</p><formula xml:id="formula_3">y i = Linear(z i L ),<label>(4)</label></formula><p>for i = 1...S. S is the maximum text length plus two for [GO] and [s] tokens. <ref type="table" target="#tab_2">Table 1</ref> summarizes the ViTSTR configurations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results and Discussion</head><p>In order to evaluate different strong baseline STR methods, we used the framework developed by Baek et al. <ref type="bibr" target="#b0">[1]</ref>. A unified framework is important in order to arrive at a fair evaluation of different models. A unified framework ensures consistent train and test conditions are used in the evaluation. Following discussion describes the train and test datasets which have been the point of contention in performance comparisons. Using different train and test datasets can heavily tilt in favor or against a certain performance reporting. After discussing the train and test datasets, we present the evaluation and analysis across different models using the unified framework. Due to the lack of a big dataset of real data, the practice in STR model training is to use synthetic data. Two popular datasets are used: 1) MJSynth (MJ) <ref type="bibr" target="#b13">[14]</ref> or also known as Synth90k and 2) SynthText (ST) <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Train Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MJ ST</head><p>MJSynth (MJ) is a synthetically generated dataset made of 8.9M realistically looking words images. MJSynth was designed to have 3 layers: 1) background, 2) foreground and 3) optional shadow/border. It uses 1,400 different fonts. The font kerning, weight, underline and other properties are varied. MJSynth also utilizes different background effects, border/shadow rendering, base coloring, projective distortion, natural image blending and noise.</p><p>SynthText (ST) is another synthetically generated dataset made of 5.5M word images. SynthText was generated by blending synthetic text on natural images. It uses the scene geometry, texture, and surface normal to naturally blend and distort a text rendering on the surface of an object within the image. Similar to MJSynth, SynthText uses random fonts for its text. The word images were cropped from the natural images embedded with synthetic text.</p><p>In the STR framework, each dataset contributes 50% to the total train dataset. Combining 100% of both datasets resulted to performance deterioration <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure" target="#fig_4">Figure 6</ref> shows sample images from MJ and ST. The test dataset is made of several small publicly available STR datasets of text in natural images. These datasets are generally group into two: 1) Regular and 2) Irregular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Test Dataset</head><p>The regular datasets have text images that are frontal, horizontal and have minimal amount of distortion. IIIT5K-Words <ref type="bibr" target="#b22">[23]</ref>, Street View Text (SVT) <ref type="bibr" target="#b36">[37]</ref>, ICDAR2003 (IC03) <ref type="bibr" target="#b21">[22]</ref> and ICDAR2013 (IC13) <ref type="bibr" target="#b15">[16]</ref> are considered regular datasets. Meanwhile, irregular datasets contain text with challenging appearances such curved, vertical, perspective, low-resolution or distorted. ICDAR2015 (IC15) <ref type="bibr" target="#b14">[15]</ref>, SVT Perspective (SVTP) <ref type="bibr" target="#b24">[25]</ref> and CUTE80 (CT) <ref type="bibr" target="#b26">[27]</ref> belong to irregular datasets. <ref type="figure" target="#fig_5">Figure 7</ref> shows samples from regular and irregular datasets. For both datasets, only the test splits are used for the evaluation. -IC03 contains 1,110 test images from ICDAR2003 Robust Reading Competition. Images were captured from natural scenes. After removing words that are less than 3 characters in length, the result is 860 images. However, 7 additional images were found to be missing. Hence, the framework also contains the 867 test images version. -IC13 is an extension of IC03 and shares similar images. IC13 was created for the ICDAR2013 Robust Reading Competition. In the literature and in the framework, two versions of the test dataset are used: 1) 857 and 2) 1,015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Irregular Dataset</head><p>-IC15 has text images for the ICDAR2015 Robust Reading Competition. Many images are blurry, noisy, rotated, and sometimes of low-resolution since these were captured using Google Glasses with the wearer undergoing unconstrained motion. Two versions are used in the literature and in the framework: 1) 1,811 and 2) 2,077 images. The 2,077 version contains rotated, vertical, perspective-shifted and curved images. -SVTP has 645 test images from Google Street View. Most are images of business signage. -CT focuses on curved text images captured from shirts and product logos.</p><p>The dataset has 288 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Setup</head><p>The recommended training configurations in the framework are listed in <ref type="table" target="#tab_3">Table  2</ref>. We reproduced the results of several strong baseline models: CRNN, R2AM, GCRNN, Rosetta, RARE, STAR-Net and TRBA for a fair comparison with ViTSTR. We trained all models for at least 5 times using different random seeds. The best performing weights on the test datasets are saved to get the mean evaluation scores. For ViTSTR, we used the same train configurations except that the input is resized to 224 ? 224 to match the dimension of the pre-trained DeiT <ref type="bibr" target="#b33">[34]</ref>. The pre-trained weights file of DeiT is automatically downloaded before training ViTSTR. ViTSTR can be trained end-to-end with no parameters frozen. <ref type="table">Tables 3 and 4</ref> show the performance scores of different models. We report the accuracy, speed, number of parameters and FLOPS to get the overall picture of trade-offs as shown in <ref type="figure">Figure 1</ref>. For accuracy, we follow the framework evaluation protocol in most STR models of case sensitive training and case insensitive evaluation. For speed, the reported numbers are based on model run time on a 2080Ti GPU. Unlike in other model benchmarks such as in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, we do not rotate vertical text images (e.g. <ref type="table" target="#tab_5">Table 5</ref> IC15) before evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Data Augmentation</head><p>Using a recipe of data augmentation specifically targeted for STR can significantly boost the accuracy of ViTSTR. In <ref type="figure">Figure 8</ref>, we can see how different data augmentations alter the image but not the meaning of text within. <ref type="table">Table 3</ref> shows that applying RandAugment <ref type="bibr" target="#b5">[6]</ref> on different image transformations such as inversion, curving, blur, noise, distortion, rotation, stretching/compressing, perspective, and shrinking improved the generalization of ViTSTR-Tiny by +1.8%, ViTSTR-Small by +1.6% and ViTSTR-Base by 1.5%. The biggest increase in accuracy is on irregular datasets such as CT (+9.2% tiny, +6.6% small and base), SVTP (+3.8% tiny, +3.3% small, +1.8% base), IC15 1,811 (+2.7% tiny, +2.6% small, +1.7% base) and IC15 2,077 (+2.5% tiny, +2.2% small, +1.5% base).  <ref type="figure">Figure 9</ref> shows the attention map of ViTSTR as it reads out a text image. While the attention is properly focused on each character, ViTSTR also pays attention to neighboring characters. Perhaps, a context is placed during individual symbol prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Performance Penalty</head><p>Every time a stage in an STR model is added, there is a gain in accuracy but at a cost of slower speed and bigger computational requirements. For example, RARE ?TRBA increases the accuracy by 2.2% but requires 38.8M more parameters and slows down the task completion by 4 msec/image. Replacing the CTC stage by Attention like in STAR-Net ?TRBA significantly slows down the computation from 8.8 msec/image to 22.8 msec/image to gain an additional 2.5% in accuracy. In fact, the slowdown due to change from CTC to Attention is &gt; 10? as compared to adding BiLSTM or TPS in the pipeline. In ViTSTR, the transition from tiny to small version requires an increase in embedding size and number of heads. No additional stage is necessary. The performance penalty to gain 2.3% in accuracy is increase in number of parameters by 16.1M. From tiny to base, the performance penalty to gain 3.4% in accuracy is additional 80.4M parameters. In both cases, the speed barely changed since we use the same parallel tensor dot product, softmax and addition operations in MLP and MSA layers of the transformer encoder. Only the tensor dimension is increased resulting to a minimal 0.2 to 0.3 msec/image slowdown in task completion. Unlike in multi-stage STR, an additional module requires additional sequential layers of forward propagation which can not be parallelized resulting into a significant performance penalty. <ref type="table" target="#tab_5">Table 5</ref> shows sample failed predictions by ViTSTR-Small from each test dataset. The main causes of wrong prediction are confusion between similar symbols (e.g. 8 and B, J and I), scripted font (e.g. I in Inc), glare on a character, vertical text, heavily curved text image and partially occluded symbol. Note that in some of these cases, even a human reader can easily make a mistake. However, humans use semantics to resolve ambiguities. Semantics has been used in recent STR methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Failure Cases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>ViTSTR is a simple single stage model architecture that emphasizes balance in accuracy, speed and computational requirements. With data augmentation targeted for STR, ViTSTR can significantly increase the accuracy especially for irregular datasets. When scaled up, ViTSTR stays at the frontiers to balance accuracy, speed and computational requirements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Different variations of text encountered in natural scenes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Network architecture of ViTSTR. An input image is first converted into patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 Fig. 5 .</head><label>45</label><figDesc>shows the model architecture of ViTSTR in detail. The only difference between ViT and ViTSTR is the prediction head. Instead of single object-class recognition, ViTSTR must identify multiple characters with the correct sequence order and length. The prediction is done in parallel. A transformer encoder is a stack of L identical encoder blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Samples from datasets with synthetic images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Samples from datasets with real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>ViTSTR configurations ViTSTR Patch Size Depth Embedding Size No. of Heads Seq Length</figDesc><table><row><cell>Version</cell><cell>P</cell><cell>L</cell><cell>D</cell><cell>H</cell><cell>S</cell></row><row><cell>Tiny</cell><cell>16</cell><cell>12</cell><cell>192</cell><cell>3</cell><cell>27</cell></row><row><cell>Small</cell><cell>16</cell><cell>12</cell><cell>384</cell><cell>6</cell><cell>27</cell></row><row><cell>Base</cell><cell>16</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell>27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Train conditions ,000 images for testing. The images are mostly from street scenes such as sign board, brand logo, house number or street sign. -SVT has 647 images for testing. The text images are cropped from Google Street View images.</figDesc><table><row><cell cols="2">Train dataset: 50%MJ + 50%ST Batch size: 192</cell></row><row><cell>Epochs: 300</cell><cell>Parameter initialization: He [10]</cell></row><row><cell>Optimizer: Adadelta [40]</cell><cell>Learning rate: 1.0</cell></row><row><cell>Adadelta ?: 0.95</cell><cell>Adadelta : 1e ?8</cell></row><row><cell>Loss: Cross-Entropy/CTC</cell><cell>Gradient clipping: 5.0</cell></row><row><cell>Image size: 100 ? 32</cell><cell>Channels: 1 (grayscale)</cell></row><row><cell>Regular Dataset</cell><cell></cell></row><row><cell>-IIIT5K contains 3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Model accuracy. Bold: highest for all, Underscore: highest no augmentation. Model accuracy, speed, and computational requirements on a 2080Ti GPU.Fig. 9. ViTSTR attention as it reads out Nestle text image.</figDesc><table><row><cell>Model</cell><cell>IIIT SVT</cell><cell>IC03</cell><cell>IC13</cell><cell cols="2">IC15 SVTP CT Acc Std</cell></row><row><cell></cell><cell cols="5">3000 647 860 867 857 1015 1811 2077 645 288 %</cell></row><row><cell>CRNN [30]</cell><cell cols="5">81.8 80.1 91.7 91.5 89.4 88.4 65.3 60.4 65.9 61.5 76.7 0.3</cell></row><row><cell>R2AM [17]</cell><cell cols="5">83.1 80.9 91.6 91.2 90.1 88.1 68.5 63.3 70.4 64.6 78.4 0.9</cell></row><row><cell>GCRNN [36]</cell><cell cols="5">82.9 81.1 92.7 92.3 90.0 88.4 68.1 62.9 68.5 65.5 78.3 0.1</cell></row><row><cell>Rosetta [4]</cell><cell cols="5">82.5 82.8 92.6 91.8 90.3 88.7 68.1 62.9 70.3 65.5 78.4 0.4</cell></row><row><cell>RARE [31]</cell><cell cols="5">86.0 85.4 93.5 93.4 92.3 91.0 73.9 68.3 75.4 71.0 82.1 0.3</cell></row><row><cell>STAR-Net [21]</cell><cell cols="5">85.2 84.7 93.4 93.0 91.2 90.5 74.5 68.7 74.7 69.2 81.8 0.1</cell></row><row><cell>TRBA [1]</cell><cell cols="5">87.8 87.6 94.5 94.2 93.4 92.1 77.4 71.7 78.1 75.2 84.3 0.1</cell></row><row><cell>ViTSTR-Tiny</cell><cell cols="5">83.7 83.2 92.8 92.5 90.8 89.3 72.0 66.4 74.5 65.0 80.3 0.2</cell></row><row><cell cols="6">ViTSTR-Tiny+Aug 85.1 85.0 93.4 93.2 90.9 89.7 74.7 68.9 78.3 74.2 82.1 0.1</cell></row><row><cell>ViTSTR-Small</cell><cell cols="5">85.6 85.3 93.9 93.6 91.7 90.6 75.3 69.5 78.1 71.3 82.6 0.3</cell></row><row><cell cols="6">ViTSTR-Small+Aug 86.6 87.3 94.2 94.2 92.1 91.2 77.9 71.7 81.4 77.9 84.2 0.1</cell></row><row><cell>ViTSTR-Base</cell><cell cols="5">86.9 87.2 93.8 93.4 92.1 91.3 76.8 71.1 80.0 74.7 83.7 0.1</cell></row><row><cell cols="6">ViTSTR-Base+Aug 88.4 87.7 94.7 94.3 93.2 92.4 78.5 72.6 81.8 81.3 85.2 0.1</cell></row><row><cell>Model</cell><cell></cell><cell>Accuracy</cell><cell>Speed</cell><cell cols="2">Parameters FLOPS</cell></row><row><cell></cell><cell></cell><cell>%</cell><cell cols="3">msec/image 1 ? 10 6 1 ? 10 9</cell></row><row><cell cols="2">CRNN [30]</cell><cell>76.7</cell><cell>3.7</cell><cell>8.5</cell><cell>1.4</cell></row><row><cell cols="2">R2AM [17]</cell><cell>78.4</cell><cell>22.9</cell><cell>2.9</cell><cell>2.0</cell></row><row><cell cols="2">GRCNN [36]</cell><cell>78.3</cell><cell>11.2</cell><cell>4.8</cell><cell>1.8</cell></row><row><cell cols="2">Rosetta [4]</cell><cell>78.4</cell><cell>5.3</cell><cell>44.3</cell><cell>10.1</cell></row><row><cell cols="2">RARE [31]</cell><cell>82.1</cell><cell>18.8</cell><cell>10.8</cell><cell>2.0</cell></row><row><cell cols="2">STAR-Net [21]</cell><cell>81.8</cell><cell>8.8</cell><cell>48.9</cell><cell>10.7</cell></row><row><cell>TRBA [1]</cell><cell></cell><cell>84.3</cell><cell>22.8</cell><cell>49.6</cell><cell>10.9</cell></row><row><cell cols="2">ViTSTR-Tiny</cell><cell>80.3</cell><cell>9.3</cell><cell>5.4</cell><cell>1.3</cell></row><row><cell cols="2">ViTSTR-Tiny+Aug</cell><cell>82.1</cell><cell>9.3</cell><cell>5.4</cell><cell>1.3</cell></row><row><cell cols="2">ViTSTR-Small</cell><cell>82.6</cell><cell>9.5</cell><cell>21.5</cell><cell>4.6</cell></row><row><cell cols="3">ViTSTR-Small+Aug 84.2</cell><cell>9.5</cell><cell>21.5</cell><cell>4.6</cell></row><row><cell cols="2">ViTSTR-Base</cell><cell>83.7</cell><cell>9.8</cell><cell>85.8</cell><cell>17.6</cell></row><row><cell cols="2">ViTSTR-Base+Aug</cell><cell>85.2</cell><cell>9.8</cell><cell>85.8</cell><cell>17.6</cell></row><row><cell>Original</cell><cell>Invert</cell><cell cols="2">Curve</cell><cell>Blur</cell><cell>Noise</cell></row><row><cell>Distort</cell><cell>Rotate</cell><cell cols="2">Stretch/Comp.</cell><cell>Perspective</cell><cell>Shrink</cell></row><row><cell cols="6">Fig. 8. Illustration of data augmented text images designed for STR.</cell></row><row><cell>N</cell><cell>e</cell><cell>s</cell><cell>t</cell><cell>l</cell><cell>e</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>ViTSTR sample failed prediction from each test dataset. From first to last row: input image, ground truth, prediction, dataset. Wrong symbol prediction in red.</figDesc><table><row><cell>18008091469</cell><cell>INC</cell><cell>JAVA</cell><cell cols="4">Distributed CLASSROOMS BOOKSTORE BRIDGESTONE</cell></row><row><cell>1800B09446Y</cell><cell>Onc</cell><cell>IAVA</cell><cell cols="4">Distribated Io-14DD07 BOOKSTORA Dueeesrreee</cell></row><row><cell>IIIT5K</cell><cell>SVT</cell><cell>IC03</cell><cell>IC13</cell><cell>IC15</cell><cell>SVTP</cell><cell>CUTE80</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was funded by the University of the Philippines ECWRG 2019-2020. GPU machines have been supported by CHED-PCARI AIRSCAN Project and Samsung R&amp;D PH. Special thanks to the people of Computer Networks Laboratory: Roel Ocampo, Vladimir Zurbano, Lope Beltran II, and John Robert Mendoza, who worked tirelessly during the pandemic to ensure that our network and servers are continuously running.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Principal warps: Thin-plate splines and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="567" to="585" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rosetta: Large scale system for text detection and recognition in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Borisyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl Conf on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03492</idno>
		<title level="m">Text recognition in the wild: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Icdar 2015 competition on robust reading</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
		<title level="m">Icdar 2013 robust reading competition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On recognizing texts of arbitrary shapes with 2d self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="546" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8610" to="8617" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scatter: selective context attentional scene text recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11962" to="11972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Star-net: a spatial attention residue network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icdar 2003 robust reading competitions: entries, results, and future directions</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<editor>BMVC. BMVA</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Seed: Semantics enhanced encoder-decoder framework for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13528" to="13537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Nrtr: A no-recurrence sequence-to-sequence model for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="781" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuRIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="6000" to="6010" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gated recurrent convolution neural network for ocr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuRIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="334" to="343" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards accurate scene text recognition with semantic reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12113" to="12122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Esir: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2059" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
