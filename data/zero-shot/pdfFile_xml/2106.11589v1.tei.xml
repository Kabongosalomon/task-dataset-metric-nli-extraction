<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Part-aware Measurement for Robust Multi-View Multi-Human 3D Pose Estimation and Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hau</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Hong</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Chih</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hsien</forename><surname>Hsu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Da</forename><surname>Li</surname></persName>
							<email>jiadali@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Part-aware Measurement for Robust Multi-View Multi-Human 3D Pose Estimation and Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces an approach for multi-human 3D pose estimation and tracking based on calibrated multiview. The main challenge lies in finding the cross-view and temporal correspondences correctly even when several human pose estimations are noisy. Compare to previous solutions that construct 3D poses from multiple views, our approach takes advantage of temporal consistency to match the 2D poses estimated with previously constructed 3D skeletons in every view. Therefore cross-view and temporal associations are accomplished simultaneously. Since the performance suffers from mistaken association and noisy predictions, we design two strategies for aiming better correspondences and 3D reconstruction. Specifically, we propose a part-aware measurement for 2D-3D association and a filter that can cope with 2D outliers during reconstruction. Our approach is efficient and effective comparing to state-of-the-art methods; it achieves competitive results on two benchmarks: 96.8% on Campus and 97.4% on Shelf. Moreover, we extends the length of Campus evaluation frames to be more challenging and our proposal also reach well-performed result. The code will be available at https://git.io/JO4KE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-human 3D pose estimation and tracking based on multi-view streaming videos have many applications, including marker-less motion capture <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b9">12,</ref><ref type="bibr" target="#b32">35,</ref><ref type="bibr" target="#b22">25]</ref>, sports analysis <ref type="bibr">[4]</ref>, and video surveillance <ref type="bibr" target="#b5">[7]</ref>. Recently, plenty of nicely performed 2D pose estimation approaches <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b4">6]</ref> have been developed; they have then been extended to the estimation of 3D poses from a monocular view <ref type="bibr" target="#b11">[14,</ref><ref type="bibr" target="#b13">16]</ref>. Although great progress has been made on inferring the 3D human poses in a single view, the noisy predictions caused by large pose variations and partial occlusions remain to be demanding. To problems, address these issues, constructing 3D human poses from multi-camera views becomes a promising way. However, there are still several challenges to be tackled, such as heavy occlusions, high computational complexity, and noisy predictions.</p><p>In general, given synced multi-view video input, three main tasks should be dealing with properly: 2D skeleton extraction, cross-view association, and temporal association. Human skeletons are estimated for each view initially, which is often achieved via a 2D human pose estimation method with convolution neural networks (CNNs) <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b34">37]</ref>. The 3D skeletons are then reconstructed according to the associated 2D human skeletons in different views. In the last, associations between the 2D or 3D skeletons are established with those in the next frame in video streams.</p><p>Leveraging 2D poses, recent studies [4, <ref type="bibr" target="#b29">32,</ref><ref type="bibr">11,</ref><ref type="bibr" target="#b5">7]</ref> follow an initialization-and-tracking framework for 3D pose inference. The framework assumes a streaming mode, i.e., the outputs of multi-view 3D poses are obtained on-line per input frame, and the previously generated outputs cannot be altered. To perform on-line 3D pose inference, initial 3D human skeletons are computed given the first views. This is often achieved via epi-polar-line distance and/or person reid matching; then, the initial 3D joints are established with multi-view stereo. As for maintaining the flexibility of use, most works <ref type="bibr" target="#b29">[32,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b36">39]</ref> do not need fine-tuning on the data collected in the testing environment; pre-trained person reid models are thus not effective enough due to the testing domain shift in the usual. Hence, epi-polar-line distances are mostly used for initializing the 3D poses. Once initialized, the obtained 3D skeletons of the individuals serve as a guide for human tracking based on the 2D poses of all views detected in future frames. The 3D human skeletons are then updated via multi-view reconstruction according to the new correspondences of the 2D human skeletons obtained when tracking is finished.</p><p>In the processing flow of on-line 3D pose inference, tem-  <ref type="figure">Figure 1</ref>: Overview of our 3D pose tracking mechanism. First, initial 3D poses are given via the reconstruction with crossview 2D correspondences. Then, given new frames as shown in the figure, we apply 2D-3D association to match estimated 2D poses with preciously tracked 3D skeletons. While finishing the association of all views, for each matched 2D poses and 3D skeleton, joints filter is employed to remove 2D outliers of estimated body joints and update the 3D pose.</p><p>poral association plays a main role. Previous approaches such as <ref type="bibr">[11,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b33">36]</ref> construct the 3D skeletons from multiple current views at first, and then the 3D skeletons obtained are smoothed temporally for each individual. However, since matching of C views involves O(C 2 ) pairs of the epi-polar-line distance evaluation, the computation cost could be considerably increased with the number of people and cameras. To alleviate the computation burden, we introduce a 2D-3D matching mechanism to enforce the temporal consistency, which leverages the previously tracked 3D skeletons for finding the correspondences among views. The complexity is O(C) as only the projections of the previous 3D skeletons on the C views is needed. Besides, multi-person interactions in crowded scenes would increase the difficulty of matching and identifying people as individuals across views. To exploit the projected 3D skeletons for finding better correspondences among views, we propose a part-aware measurement to compute the affinity. We also design a greedy algorithm called joints filter to remove noisy points and construct a robust 3D pose by taking advantage of epi-polar constraints. Unlike previous works such as <ref type="bibr" target="#b5">[7]</ref>, the proposed method can handle wrong keypoint estimation caused by occlusion or motion blur during association. We compare our method with previous works and show that our solution achieves competitive results on two benchmark datasets: Campus and Shelf. In the following contents, Section 2 reviews the related approaches of 3D human pose estimation and tracking. Section 3 presents the details of our new approach. Section 4 presents the results comparing the accuracy and efficiency between our approaches and competitive methods on the Campus <ref type="bibr" target="#b2">[3]</ref> and Shelf <ref type="bibr" target="#b2">[3]</ref> datasets. We also demonstrate the design analysis of our approach in ablation studies. Finally, conclusions are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section reviews the previous approaches to 3D human pose estimation and their tracking techniques. In Section 2.1, we review the methods of 3D human pose estimation, which infer the 3D poses based on isolated images. In Section 2.2, we review the techniques of 3D human pose tracking, where temporal consistency in 3D space is utilized for performance enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D Human Pose Estimation</head><p>Depending on the number of input cameras, 3D human pose estimation methods are divided into a monocular camera for taking single-view video <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">23,</ref><ref type="bibr" target="#b28">31,</ref><ref type="bibr" target="#b11">14,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b19">22,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b35">38]</ref> and multiple cameras for taking multi-view videos synchronously <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">13,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr">11,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b36">39,</ref><ref type="bibr" target="#b32">35]</ref>.</p><p>Due to the difficulty of multi-person 3D poses reconstruction in monocular view, most of the single-view approaches are developed to construct a single person's 3D poses <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b28">31,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b35">38]</ref>, where the predicted pose does not include absolute locations in the environment. Therefore, it will limit these approaches applying in different practical surveillance scenarios. Despite a great achievement of multi-human 3D pose estimation in a single view <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">22,</ref><ref type="bibr" target="#b11">14]</ref>, there is still a large deviation when applying these techniques in different practical surveillance scenarios. In particular, the motion blur and occlusions occur in images.</p><p>To retrieve absolute location and handle occlusions, the studies of multi-view 3D pose estimation attract more attention recently. It can be applied in various applications, such as sports analysis, video surveillance, animation, and healthcare. Most previous approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b24">27]</ref> for singleperson 3D pose estimation are developed based on the 3D Pictorial Structure model (3DPS) <ref type="bibr" target="#b3">[5]</ref>, which discretizes the 3D-space as a grid and assigns each joint to one assumed location in the grid. Depending on the cross-view association, 3D pose reconstruction can be solved by minimizing the geometric error <ref type="bibr" target="#b0">[1]</ref> between assumed 3D poses and the 3D poses generated from multi-view 2D images. Since CNN has a great performance in the human detector and 2D human pose estimation, most multi-person 3D pose estimation approaches <ref type="bibr" target="#b10">[13,</ref><ref type="bibr">11,</ref><ref type="bibr" target="#b33">36]</ref> utilize sophisticated CNN-based 2D human pose estimation techniques at the beginning. It detects the human in the image and applies 2D pose estimation in the cropped image to obtain feature maps of present humans in these multi-view images. Feature maps from different views will serve various fusion, matching, or clustering techniques to achieve multi-person 3D poses estimation.</p><p>Ershadi-N. et al. <ref type="bibr" target="#b10">[13]</ref> utilize DeeperCut <ref type="bibr" target="#b12">[15]</ref> to detect the human body's 2D part poses in each image for constructing the initial 3D joints of human body poses in 3D state space. They develop a clustering algorithm to separate all 3D candidate joints into multiple individual 3D human poses before refining these poses using a fully connected pairwise conditional random field (CRF). Dong et al. <ref type="bibr">[11]</ref> utilize Faster R-CNN <ref type="bibr" target="#b26">[29]</ref> with lightweight backbone network and Cascaded Pyramid Network <ref type="bibr" target="#b6">[8]</ref> to detect humans' location and their 2D poses in multi-view images. They develop a multi-way matching method with circle consistency to identify the same person across cameras via human affinity before constructing 3D human poses. The human affinity is calculated from CamStyle <ref type="bibr" target="#b37">[40]</ref> generated features and geometric compatibility. Tu et al. <ref type="bibr" target="#b33">[36]</ref> utilize HRNet <ref type="bibr" target="#b34">[37]</ref> to obtain 2D pose heatmaps in each camera view. To fuse projected 2D pose heatmaps into 3D space, they develop a Cuboid Proposal Network and Pose Regression Network to localize people and regress cuboid proposals to the detailed 3D poses. Their novel solution performs well on both benchmarks, yet retraining the model is necessary when it comes to new scenes. Although these approaches can better perform 3D human pose estimation, the computational cost of cross-view matching and 3D human pose estimation is still too high for real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D Human Pose Tracking</head><p>To process synchronized multiple camera streaming videos, tracking plays an important role in practice by leveraging temporal information to smooth 3D motion capture, handle current pose ambiguity, and accelerate the system's processing time. Recently, Taylor et al. <ref type="bibr" target="#b31">[34]</ref> combine Conditional Restricted Boltzmann Machine <ref type="bibr" target="#b30">[33]</ref> with Bayesian filtering for tracking single-person 3D poses. With an increasing number of people in the scene, dealing with frequent occlusions and ambiguities becomes crucial. Liu et al. <ref type="bibr" target="#b17">[20]</ref> combine image segmentation with articulated template models for accurately capturing and tracking 3D articulated skeleton motion of each person. Like the purpose of Liu et al. <ref type="bibr" target="#b17">[20]</ref>, Elhayek et al. <ref type="bibr" target="#b9">[12]</ref> and Li et al. <ref type="bibr" target="#b16">[19]</ref> combine the CNN-based joint detection method with a model-based motion or spatio-temporal tracking algorithm in a unified 3D pose optimization for tracking and capturing 3D articulated skeleton motion of each person.</p><p>While in larger indoor/outdoor environments with more people and cameras, most approaches [4, 32, 7, 39] focus on reducing the computation cost while obtaining better performance. Tanke et al. <ref type="bibr" target="#b29">[32]</ref> utilize a 2D human pose detector to obtain multiple 2D estimated human poses from multiple views and solve the k-partite matching problem using epipolar geometry to build associations among these multiple 2D estimated human poses across multiple views. They thus construct 3D human pose of each person, followed by a greedy algorithm to match and track iteratively across frames. Like Tanke et al. <ref type="bibr" target="#b29">[32]</ref>, Bridgeman et al.</p><p>[4] applies a greedy fashion to seek the best correspondence between views. Estimated 3D skeletons are also exploited as input to improve the tracking quality. Recently, Chen et al. <ref type="bibr" target="#b5">[7]</ref> develop a novel 3D human pose tracking technique that speeds up tracking tasks in their large-scale camera systems. Compare to Tanke's work <ref type="bibr" target="#b29">[32]</ref>, their cross-view tracking with geometric affinity can track and construct 3D human poses of each person across views and frames synchronously. To attain better performance, Zhang et al. <ref type="bibr" target="#b36">[39]</ref> develop a novel 4D (2D spatial, 1D viewpoint, and 1D temporal) association graph algorithm to enhance the accuracy of finding associations among 2D and 3D human poses from views and frames. Our approach is developed by referring to the inspiration of these approaches <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b29">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Given multiple cameras C from different views in a scene, coupling with their known camera intrinsic K and extrinsic (rotation R and translation o) information, we aim to find out an unknown number of humans' 3D locations in an area. The proposed 3D reconstruction and tracking framework (as shown in <ref type="figure">Figure 1</ref>) can be divided in to three stages: 2D human pose estimation, 2D-3D association and 3D human pose reconstruction. The overall procedure is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">2D Human Pose Estimation</head><p>To reconstruct 3D human pose via multiple views, the potential 2D human poses p in each view are first extracted via an off-the-shelf 2D human pose estimator. It is worth noting that the error of 2D pose estimation may easily affect the following association and 3D pose reconstruction processes, yielding mis-matching or inaccurate 3D pose. We adopt YOLOv3 <ref type="bibr" target="#b25">[28]</ref> coupled with HRNet <ref type="bibr" target="#b34">[37]</ref> as our top-down 2D pose estimator. Since HRNet <ref type="bibr" target="#b34">[37]</ref> conducted multi-scale fusions by fusing high and low resolution representations, leading to a potentially more accurate and spa- tially more precise heatmap predictions, which is different form earlier works that recovered high-resolution representations from lower-resolution representations <ref type="bibr" target="#b7">[9]</ref>. This combination perform a more efficient and accurate estimation. Yet the comparisons of these off-the-shelf 2D pose estimators are beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Part-aware 2D-3D Association</head><p>After extracting 2D human poses p from each view, each 2D pose x t,c ? p is associated with previously reconstructed 3D poses X t ? P, which are initialized across views and updated at t (described in Section 3.3.) The affinity G(x t,c , X t ) of a 2D pose x t,c to a 3D pose X t is measured with the geometric constraints of projection difference between x t,c and re-projecting X t onto view c. The geometry affinity of the n th joint G n is computed by the equation below:</p><formula xml:id="formula_0">G n (x n t,c , X n t ) = (1 ? x n t,c ?x n t ,c ? 2D (t ? t ) ) ? e ??a(t?t ) . (1)</formula><p>wherex n t ,c is the 2D projection of X n t on camera c. ? 2D , ? ? are constants of 2D velocity threshold and penalty rate of time interval respectively.</p><p>Common approaches conduct body-aware affinity measurement to acquire a 2D pose affinity to 3D pose by averaging all the joint errors. However, the potential noisy predictions due to occlusion or motion blur may yield mismatching results. To reduce the effect of noisy estimation of joints, we propose a part-aware measurement approach that can handle those noisy joints (shown in <ref type="figure" target="#fig_3">Figure 4</ref>). In contrast to directly averaging all joint affinities, we only take joints that have positive affinities into consideration, since the outlier joints may yield negative affinities that influence associations.</p><p>Therefore, the affinity G(x t,c , X t ) is the mean of the joint affinities with positive values, indicating the similar-ity of x n t,c and X n t . And for those number of joints with positive affinities smaller than ?, we set G(x t,c , X t ) to 0.</p><p>Once the affinities of all pairs of 2D poses and 3D poses are computed, an affinity matrix A ? R |P|?|p| can be formed to associate 2D poses to 3D skeletons. This becomes a weighted bipartite problem and can easily be solved by Hungarian algorithm <ref type="bibr" target="#b15">[18]</ref>. While finishing the associations of all views, cross-view association is accomplished implicitly, where the corresponding 2D poses across views are those assigned to the same 3D skeleton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Pose Reconstruction</head><p>As long as a tracked 3D skeleton X t is matched with 2D poses after association, the new 3D skeleton can be reconstructed with the associated 2D poses from multiple views. However, the association may sometimes gives only single view of 2D pose thus cannot perform reconstruction from multiple views. Therefore, we gather a set P = {x t c ,c |c ? C; 0 ? t ? t c &lt; ? }, where t c denotes the time of the last matched 2D pose from camera c and t c ? t, to retrieve latest associated 2D pose in each camera within a short time interval ? for triangulation, and we set ? as 75-100ms empirically in practice. Furthermore, we propose joints filter as pre-processing of reconstruction to handle noisy joint estimations. Joints Filter is also a part-aware processing. In some cases, noisy predictions occur because of partial occlusion or motion blur. Epipolar constraint is utilized to remove outliers by computing the distance between epipolar line and the corresponding point. Here we handle each joint independently. Therefore, an epipolar affinity matrix of the n th joint E ? R |P|?|P| can be computed.</p><formula xml:id="formula_1">E i,j (x n i , x n j ) = 1 ? d(x n i , L j ) + d(x n j , L i ) 2? epi .<label>(2)</label></formula><p>where L, d denote the epipolar line and the point-to-line distance function, respectively. ? epi represents a threshold of acceptable distance error. Whenever there is a negative value at E i,j , at least one of x n i and x n j is regarded as an outlier. Benefit from the known 3D skeleton, we can inference its location in 3D space for correlation measurement. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, We design a greedy method to remove it by comparing them to X n t , which is estimated by X n t and motion model (e.g., linear motion, 3D Kalman Filter <ref type="bibr" target="#b27">[30]</ref>.) Afterward, the back-project-ray r of joint x n i can be obtained from the following formula:</p><formula xml:id="formula_2">r n i,c = (KR) + c ? x n i,c + o c<label>(3)</label></formula><p>where K, R ? R 3?3 are intrinsic and rotation matrix, respectively, and + stands for the inverse matrix. o c is the center of camera c with respect to global coordinate system. We then compute the 3D point-to-line distance between X n t and r n i . At last, we remove the joint with larger distance as the outlier of 3D skeleton to get a robust reconstruction. 3D Pose Reconstruction. In our method, we use Direct Linear Transformation (DLT) for triangulation. In some cases, only one 2D pose is associated to the 3D skeleton yet triangulation needs at least two points to process. To avoid this problem, a set P of 2D poses is collected from a small range of time ? . However, traditional triangulation is based on the condition that all the 2D points across views are from the same time, a penalty is set for the time interval, which is referenced from <ref type="bibr" target="#b5">[7]</ref>. But sometimes, all the joints are wrong predicted, often happen at lower arms and legs, we regard it as a missing joint X n t,miss . Here we compensate X n t,miss with X n t . Though joints filter is adopted, slight noisy estimation still remains. We conduct temporal information to smooth the 3D pose trajectory with Gaussian filter for further enhancing the performance. Initialization of 3D pose. After association process in Section 3.2, each extracted 2D pose is labeled as either matched or unmatched. For those unmatched 2D poses from different views possibly forms a new 3D skeleton. Hence, to initialize a new 3D pose, we utilize epipolar constraint to associate unmatched 2D poses across views. Assuming that there is a set U c of unmatched 2D poses in camera c, we measure the correlation of unmatched 2D poses from other views and U c by Eq. 2. The final affinity matrix E = N n=1 E n can also be solved by Hungarian algorithm <ref type="bibr" target="#b15">[18]</ref>. The process is iterative, meaning that we handle cameras one by one. After each association, there might still remain unmatched poses U c that aren't captured in camera c but captured in others. We then add U c into U c . While completing association of all views, the joints filter is also applied to get a robust reconstruction for initial 3D skeleton with a slight difference. Since there is no former 3D skeleton to reference, we have to solve it another way. The epipolar matrix of the n th joint E n is still required, and an Match(X t , xt,c) ? HungarianAlgorithm(A)</p><formula xml:id="formula_3">9 end 10 ? ? T imeInterval / * update 3D skeleton * / 11 foreach X t ? P do 12 P ? {x t c ,c |c ? C; 0 ? t ? t c &lt; ? } 13Xt ? CurrentPoseEstimation(X t ) 14P ? JointsFilter(P,Xt) 15</formula><p>Xt ? 3DReconstruction(P,Xt) <ref type="bibr" target="#b13">16</ref> end outlier exist while E n i,j is a negative value. We then compute the j=0 E n i,j of x n i and x n j , and remove the joint with smaller sum, which has a weaker affinity with others. Finally, the initial 3D pose can be reconstructed via Sec 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark Datasets</head><p>Campus dataset <ref type="bibr" target="#b2">[3]</ref>. A small dataset includes three actors and two pedestrians walking in an outdoor environment and captured by three calibrated cameras. There are 2000 frames in the video. The 350 ? 470 th and 650 ? 750 th frames were used for evaluation; others were used for training. It also provides an evaluation metric, percentage of correct parts (PCP), to quantify the accuracy of detected 3D keypoints on six parts of the human body, i.e. head, torso, upper arm, lower arm, upper leg, and lower leg. We noted that most of the actors in standard evaluation set videos are well captured and insufficient to test the present approach hardly. Therefore, the additional 471 ? 649 th frames were added into the evaluation set. These frames are more demanding while more people were walking around. Shelf dataset <ref type="bibr" target="#b2">[3]</ref>. In comparison with Campus, it's a more complex dataset captured by five calibrated cameras in an indoor scenario where up to four people were dissembling a shelf at the same time. As a result, people will be shaded by the shelf or each other frequently in the video and increase the difficulty of 3D pose reconstruction. The video contains 3200 frames in total. The 300 ? 600 th frames were used  <ref type="table">Table 2</ref>: PCP scores of human parts on different approaches. "U, L, a, l" represent upper, lower, arms and legs. ? stands for our re-implementation of <ref type="bibr" target="#b5">[7]</ref> with same 2D pose estimation method as ours.</p><p>for evaluation; others were used for training. We follow the same evaluation protocol as the Campus dataset to quantify the accuracy of detected 3D keypoints. CMU Panoptic <ref type="bibr" target="#b14">[17]</ref>. In contrast to Campus and Shelf, the Panoptic dataset is a large dataset released by CMU Perceptual Computing Lab. They built an enclosed studio with 480 VGA cameras and 31 HD cameras installed on it. The videos contain multi-person engaging in several social activities. For the lack of the groundtruth of 3D poses, only qualitative video and image results are presented on this dataset. The qualitative results are shown in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with other approaches</head><p>We compare our proposed method with the following approaches. <ref type="bibr">Belagiannis</ref>   [11] develop a multi-way matching with circle consistency to match 2D poses in multiple views optimally after constructing 3D poses. Chen et al. <ref type="bibr" target="#b5">[7]</ref> develop an efficient algorithm by leveraging temporal consistency to find out 2D correspondences across views. An incremental reconstruction that can deal with unsynchronized streaming video and fast-moving limbs is also designed. Tu et al. <ref type="bibr" target="#b33">[36]</ref> develop a cuboid proposal network to localize 3D poses by integrating multi-view projected 2D pose heatmaps. Zhang et al. <ref type="bibr" target="#b36">[39]</ref> develop a 4D association graph to calculate relevance be-tween 2D and 3D joints across views and frames to obtain the best matching connection when constructing 3D poses. <ref type="table" target="#tab_0">Table 1</ref> demonstrates the PCP score of our proposed method and the above approaches evaluated on Campus and Shelf datasets. We reach state-of-the-art on Campus and competitive score on Shelf. In particular, our approach significantly improves Actor1's results on Campus , and as for Actor2 in Shelf, who suffering from severe occlusion, our method can better tackle the condition than most previous works. A further comparison was demonstrated in <ref type="table">Table 2</ref>. We compared four human parts to prove that our proposed reconstruction method is valid. Lower arms that have larger motion amplitudes often suffer from motion blur or occlusion. In most works, the PCP score of lower arms performs under 90%; however, our proposal successfully reach the highest accuracy up to 90.5% on Shelf.</p><p>Furthermore, we also evaluate some approaches [11, 7] on the extended Campus testing set. With three actors and a pedestrian walking around not only enhance the difficulties of associations but also increase the frequency of partial occlusions, Dong et al.</p><p>[11] method slightly degrade the performance(96.36%). On the other hand, ours and Chen et al. <ref type="bibr" target="#b5">[7]</ref>, which is our re-implementation, maintain considerable results on the extended frames(97.47% and 97.27% respectively). We conjecture that with the help of part-aware measurement and joints filter, our approach achieves 0.2% improvement. The result is shown in <ref type="table" target="#tab_2">Table 3</ref>. Thus, we can confirm that our approach can effectively and robustly construct 3D pose of people, who suffer from severe occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>In this section, we give some ablation studies to verify that each component is beneficial to our approach. We compare to the baseline with body-aware measurement in association and without joints filters. Measure in body or part-aware? To verify that measuring in part-aware is effective, we only change the method building associations. The other modules remain the same as the baseline. In some cases, even though most joints have good predictions, there were still seldom joints with an enormous error that significantly affect the affinity and causing unmatched consequence(an example is given in <ref type="figure" target="#fig_3">Figure 4</ref>). As demonstrated in <ref type="table" target="#tab_2">Table 3</ref>, singly applying part-aware measurement enhanced approximately 2.1% on Campus, which is a large improvement. We investigate that measuring in part-aware leads to fewer unmatched results, representing that for each 3D skeleton, there are more matched 2D poses across views. Hence, with more cross-view 2D correspondences, a better 3D skeleton can be constructed. Effectiveness of Joints Filter? As described in Section 3.3, we regard joints filter as an important process to handle noisy points that will degrade the reconstruction accuracy. For comparison, we select the torso, lower arms to verify <ref type="figure">Figure 3</ref>: Ablation study on Campus and Shelf datasets. "jf" means joints filter, and "ours" indicates that part-aware measurement and joints filter are employed. As shown, applying either part-aware measurement or joints filter performs better and by using both of them further improved the PCP result. the impact of the joints filter. Torso performs well (100%) in every method due to the small range of motion. For the lower arms, which are considered the most demanding limb among whole body parts, the joints filter method improves about 0.5?1.0% and PCP score. Some examples on Shelf are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Additionally, to justify that joints filter not only works in our proposal, we add it to the reimplementation of Chen et al. <ref type="bibr" target="#b5">[7]</ref>. We can improve the PCP score even from 97.28% to 97.43%, which is a 0.15% enhancement.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Processing time</head><p>In this section, we report the runtime of our algorithm on different datasets. Considering 2D pose estimation is beyond the scope, only tracking and reconstruction time are demonstrated. Shelf. For the pose estimation, our model takes about 600ms to produce all the 2D poses. Since most of the multiview 3D human pose estimation methods focus on the reconstruction process, we compare the result with other's proposals on this part. Dong et al.</p><p>[11] test their experiment on GeForce 1080Ti GPU, which spent an average of 25ms to compute affinities and 20 ms for finding the crossview association, and 60 ms for reconstruction. In contrast, our implementation runs on Intel CPU 3.70GHz. It takes about 2 ms for finding the 2D-3D association, 8 ms for reconstruction and 1 ms for the cross-view initialization. Campus contains fewer people and cameras, the processing speed is faster than the Shelf. It takes about 1 ms for association, 5 ms for reconstruction, and 0.5 ms for initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose an efficient and robust multi-person 3D pose estimation and tracking from multiple views by directly associating 2D poses estimation to a 3D skeleton in each view and updating 3D poses with a part-aware approach. Our proposed part-aware association is able to achieve better matching in contrast to common body-aware methods. Moreover, joint filter also plays an important role in dealing with noisy joint estimation to get a more accurate reconstruction. In experiments, our approach achieved a competitive result on benchmark datasets without any additional training. And the ablation study verified our part-aware method is effective in partial occlusion.</p><p>6. Supplementary Material 6.1. Implementation Details Paremeters Selection. In this work we have several parameters: ? 2D , ? epi , ? , ? are thresholds of 2D velocity, epipolar distance, time interval and number of positive affinities respectively, ? ? is a penalty rate of time interval. Here we show our empirical selections of parameters for each dataset in <ref type="table" target="#tab_6">Table 5</ref>. ? 2D , ? epi , ? , ? are based on the frame size and the distance between people and camera. For Campus <ref type="bibr" target="#b2">[3]</ref>, the image size is 360 ? 288 and the actors are far from cameras. Therefore, ? 2D and ? epi is set to be smaller number. Yet ? adjust with a strict value since actors are mostly captured completely, and we expect that 2D poses can all be accurately estimated. On the other hand, Shelf <ref type="bibr" target="#b2">[3]</ref> and Panoptic <ref type="bibr" target="#b14">[17]</ref> have larger image size and humans are captured in a small area which is close to cameras. Thus, occlusion and out-of-view often occur. We then define the parameter with a more flexible value. For other two parameters, ? and ? ? basically depends on the fps of video, e.g. the three datates are all captured at 25 fps.  Initialization Procedure. To introduce our 3D pose initialization procedure more clearly, it is detailed in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Qualitative Results</head><p>Here, we demonstrate more qualitative results of our approach on three datasets in <ref type="figure" target="#fig_7">Figure 6</ref>    [11] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2106.11589v1 [cs.CV] 22 Jun 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of joints filter algorithm. (a) The blue and orange lines on image plane of c 2 are epipolar lines of backproject-ray r c1 , r c3 , respectively. The joints in views c 2 and c 3 are considered as outlier candidates due to the large epipolar distance thus be examined in the next step. (b) We remove the outlier with the larger distance between back-project-ray of 2D joint and estimated 3D skeleton X t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 : 5 A</head><label>15</label><figDesc>Estimation and Tracking procedure Input: Cameras c ? C New 2D poses xt ? p Previously tracked 3D skeletons X t ? P at time t Output: Tracked 3D skeletons with new 3D poses Xt ? P at time t 1 Initialization: P ? ?; A ? R |P|?|p| / * cross-view association * / 2 foreach c ? C do 3 foreach i, X t ? P do 4 foreach j, xt,c ? p do (i, j) ? PartAwareAssociation(xt,c, X t )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparison of body-aware and part-ware association on Shelf dataset. (a) a 3D skeleton and its reprojection (b) the corresponding estimated 2D pose. Due to the out-of-view of lower body, the enormous 2D estimation error of ankles leads to weak ankle affinities and affects the affinity of entire body to mis-match in body-aware association. (c) Therefore, we adopt part-aware association by neglecting the few wrong predictions but only comparing affinities of others to reach a robust matching result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>A visualization of ablation studies. From left to right indicate the camera's order, and the last column is the 3D skeleton from c 3 's viewpoint. As shown in red bounding boxes, the black-shirt man's lower body is out of c3's FOV, leading to wrong prediction. (a) body-aware association failed to associate the 2D pose in c3 to the corresponding 3D skeleton. (b) altering association with part-aware measurement succeeds. Yet due to the outliers of his lower body, the legs of 3D skeleton perform weird poses (orange box.) (c) further improves the 3D pose reconstruction of (b) with joint filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, Figure 7 and Figure 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results of Panoptic, three sub-datasets is demonstrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Qualitative result of Campus [10] Yu Cheng, Bo Yang, Bo Wang, Wending Yan, and Robby T Tan. Occlusion-aware networks for 3d human pose estima-Qualitative result of Shelf tion in video. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 723-732, 2019.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>et al. [13] et al. [4] et al. [32] et al. [11] et al. [7] et al. [36] et al. [39] Ours Quantitative comparison on the Campus and Shelf dataset.The results of other methods are taken from respective papers. "red" and "blue" represent the first and second highest scores, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Campus Dataset, PCP(%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Belagiannis Ershadi-N Bridgeman</cell><cell>Tanke</cell><cell>Dong</cell><cell>Chen</cell><cell>Tu</cell><cell>Zhang</cell></row><row><cell>Method</cell><cell>et al. [3]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Actor1</cell><cell>93.5</cell><cell></cell><cell>94.2</cell><cell>91.8</cell><cell></cell><cell>98</cell><cell>97.6</cell><cell>97.1</cell><cell>97.6</cell><cell>-</cell><cell>98.37</cell></row><row><cell>Actor2</cell><cell>75.7</cell><cell></cell><cell>92.9</cell><cell>92.7</cell><cell></cell><cell>91</cell><cell>93.3</cell><cell>94.1</cell><cell>93.8</cell><cell>-</cell><cell>93.76</cell></row><row><cell>Actor3</cell><cell>85.4</cell><cell></cell><cell>84.6</cell><cell>93.2</cell><cell></cell><cell>92.2</cell><cell>98.0</cell><cell>98.6</cell><cell>98.8</cell><cell>-</cell><cell>98.26</cell></row><row><cell>Average</cell><cell>84.5</cell><cell></cell><cell>90.6</cell><cell>92.6</cell><cell></cell><cell>95.7</cell><cell>96.3</cell><cell>96.6</cell><cell>96.7</cell><cell>-</cell><cell>96.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Shelf Dataset, PCP(%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Belagiannis Ershadi-N Bridgeman</cell><cell>Tanke</cell><cell>Dong</cell><cell>Chen</cell><cell>Tu</cell><cell>Zhang</cell></row><row><cell>Method</cell><cell>et al. [3]</cell><cell></cell><cell>et al. [13]</cell><cell>et al. [4]</cell><cell cols="6">et al. [32] et al. [11] et al. [7] et al. [36] et al. [39] Ours</cell></row><row><cell>Actor1</cell><cell>75.3</cell><cell></cell><cell>93.3</cell><cell>99.7</cell><cell></cell><cell>99.8</cell><cell>98.8</cell><cell>99.6</cell><cell>99.3</cell><cell>99.0</cell><cell>99.14</cell></row><row><cell>Actor2</cell><cell>69.7</cell><cell></cell><cell>75.9</cell><cell>92.8</cell><cell></cell><cell>90</cell><cell>94.1</cell><cell>93.2</cell><cell>94.1</cell><cell>96.2</cell><cell>95.41</cell></row><row><cell>Actor3</cell><cell>87.6</cell><cell></cell><cell>94.8</cell><cell>97.7</cell><cell></cell><cell>98</cell><cell>97.8</cell><cell>97.5</cell><cell>97.6</cell><cell>97.6</cell><cell>97.64</cell></row><row><cell>Average</cell><cell>77.5</cell><cell></cell><cell>88.0</cell><cell>96.7</cell><cell></cell><cell>95.9</cell><cell>96.9</cell><cell>96.8</cell><cell>97.0</cell><cell>97.6</cell><cell>97.39</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PCP(%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Campus</cell><cell>Ua</cell><cell>La</cell><cell>Ul</cell><cell>Ul</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Tanke et al. [32] 97.7 84.3 99.3</cell><cell>99.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Chen et al. [7] 98.6 84.6</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">? Chen et al. [7] 99.8 84.2 100.0 100.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">ours 99.8 84.3 100.0 100.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Shelf</cell><cell>Ua</cell><cell>La</cell><cell>Ul</cell><cell>Ll</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Tanke et al. [32] 98.0 86.3 100.0 100.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Chen et al. [7] 98.7 87.7</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">? Chen et al. [7] 98.8 89.9 100.0 99.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">ours 98.8 90.5 100.0 100.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison on the Campus extended testing set. The results of other methods are taken from respective papers. "red" indicates the highest scores.</figDesc><table><row><cell>model with temporal consistency thereafter become a base-</cell></row><row><cell>line on Campus and Shelf datasets. Ershadi-N. et al. [13]</cell></row><row><cell>develop a clustering algorithm to reduce the 3D state space</cell></row><row><cell>by optimally clustering 3D candidate joints. Bridgeman et</cell></row><row><cell>al. [4] develop a 2D pose association algorithm with error</cell></row><row><cell>correction to match 2D poses in multiply views more ro-</cell></row><row><cell>bustly after constructing 3D poses. Tanke et al. [32] de-</cell></row><row><cell>velop an iterative greedy matching to remove outliers and</cell></row><row><cell>fill-in missing joints by temporal 3D pose averaging. Dong</cell></row><row><cell>et al.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Runtime comparison on Shelf. Stages 'Affi.', 'Asso.', 'Recon.' and 'Init.' stand for affinity computation, association, reconstruction and 3D skeleton initialization, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Parameters selection for different datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was partially supported by Ministry of Science and Technology in Taiwan (MOST 110-2634-F-002-04).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Initialization Procedure</head><p>Input: Unmatched 2D poses Uc|c ? C Previously tracked 3D skeletons X t ? P at time t Output: New tracked skeletons {Xt} at time t 1 Initialization: U ? unmatched 2D poses of Camera </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex M</forename><surname>Andrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Kybernetes</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-view tracking for multi-human 3d pose estimation at over 100 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3279" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marconi-convnet-based marker-less motion capture in outdoor and indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Edilson De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="501" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Ershadi-Nasab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erfan</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esmaeil</forename><surname>Sanaei</surname></persName>
		</author>
		<title level="m">Multiple human 3d pose estimation from multiview images. Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="15573" to="15601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>R?za Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="190" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryn</forename><surname>Yaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Quart</title>
		<imprint>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shape and pose estimation for closely interacting persons using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianhong</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="361" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Markerless motion capture of multiple characters using multiview image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2720" to="2735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2823" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6988" to="6997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domes to drones: Self-supervised active triangulation for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksis</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d object tracking using three kalman filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Salih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Symposium on Computers Informatics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="501" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Iterative greedy matching for 3d human pose tracking from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Tanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamical binary latent variable models for 3d human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="631" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lourdes Agapito, and Chris Russell. Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Toso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyue</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inference stage optimization for cross-scenario 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">4d association graph for realtime multi-person motion capture using multiple video cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
