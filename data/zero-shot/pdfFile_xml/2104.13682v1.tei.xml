<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HOTR: End-to-End Human-Object Interaction Detection with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
							<email>bumsoo.brain@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="department">Kakao Brain</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
							<email>kangj@korea.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eun-Sol</forename><surname>Kim</surname></persName>
							<email>eunsol.kim@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="department">Kakao Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
							<email>hyunwoojkim@korea.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HOTR: End-to-End Human-Object Interaction Detection with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-Object Interaction (HOI) detection is a task of identifying "a set of interactions" in an image, which involves the i) localization of the subject (i.e., humans) and target (i.e., objects) of interaction, and ii) the classification of the interaction labels. Most existing methods have indirectly addressed this task by detecting human and object instances and individually inferring every pair of the detected instances. In this paper, we present a novel framework, referred by HOTR, which directly predicts a set of human, object, interaction triplets from an image based on a transformer encoder-decoder architecture. Through the set prediction, our method effectively exploits the inherent semantic relationships in an image and does not require time-consuming post-processing which is the main bottleneck of existing methods. Our proposed algorithm achieves the state-of-the-art performance in two HOI detection benchmarks with an inference time under 1 ms after object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-Object Interaction (HOI) detection has been formally defined in <ref type="bibr" target="#b7">[8]</ref> as the task to predict a set of human, object, interaction triplets within an image. Previous methods have addressed this task in an indirect manner by performing object detection first and associating human, object pairs afterward with separate post-processing steps. Especially, early attempts (i.e., sequential HOI detectors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>) have performed this association with a subsequent neural network, thus being time-consuming and computationally expensive.</p><p>To overcome the redundant inference structure of sequential HOI detectors, recent researches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref> proposed parallel HOI detectors. These works explicitly localize interactions with either interaction boxes (i.e., the tightest box that covers both the center point of an object ? corresponding authors <ref type="figure">Figure 1</ref>. Time vs. Performance analysis for HOI detectors on V-COCO dataset. HOI recognition inference time is measured by subtracting the object detection time from the end-to-end inference time. Blue circle represents sequential HOI detectors, orange circle represents parallel HOI detectors and red star represents ours. Our method achieves an HOI recognition inference time of 0.9ms, being significantly faster than the parallel HOI detectors such as IPNet <ref type="bibr" target="#b29">[30]</ref> or UnionDet <ref type="bibr" target="#b11">[12]</ref> (the comparison between parallel HOI detectors is highlighted in blue). pair) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref> or union boxes (i.e., the tightest box that covers both the box regions of an object pair) <ref type="bibr" target="#b11">[12]</ref>. The localized interactions are associated with object detection results to complete the human, object, interaction triplet. The time-consuming neural network inference is replaced with a simple matching based on heuristics such as distance <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref> or IoU <ref type="bibr" target="#b11">[12]</ref>.</p><p>However, previous works in HOI detection are still limited in two aspects; i) They require additional postprocessing steps like suppressing near-duplicate predictions and heuristic thresholding. ii) Although it has been shown that modeling relations between objects helps object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>, the effectiveness of considering high-level dependency for interactions in HOI detection has not yet been fully explored.</p><p>In this paper, we propose a fast and accurate HOI algorithm named HOTR (Human-Object interaction TRans-former) that predicts a set of human-object interactions in a scene at once with a direct set prediction approach. We design an encoder-decoder architecture based on transformers to predict a set of HOI triplets, which enables the model to overcome both limitations of previous works. First, direct set-level prediction enables us to eliminate hand-crafted post-processing stage. Our model is trained in an endto-end fashion with a set loss function that matches the predicted interactions with ground-truth human, object, interaction triplets. Second, the self-attention mechanisms of transformers makes the model exploit the contextual relationships between human and object and their interactions, encouraging our set-level prediction framework more suitable for high-level scene understanding. We evaluate our model in two HOI detection benchmarks: V-COCO and HICO-DET datasets. Our proposed architecture achieves state-of-the-art performance on two datasets compared to both sequential and parallel HOI detectors. Also, note that our method is much faster than other algorithms as illustrated in <ref type="figure">Figure 1</ref>, by eliminating timeconsuming post-processing through the direct set-level prediction. The contribution of this work can be summarized as the following:</p><p>? We propose HOTR, the first transformer-based set prediction approach in HOI detection. HOTR eliminates the hand-crafted post-processing stage of previous HOI detectors while being able to model the correlations between interactions.</p><p>? We propose various training and inference techniques for HOTR: HO Pointers to associate the outputs of two parallel decoders, a recomposition step to predict a set of final HOI triplets, and a new loss function to enable end-to-end training.</p><p>? HOTR achieves state-of-the-art performance on both benchmark datasets in HOI detection with an inference time under 1 ms, being significantly faster than previous parallel HOI detectors (5?9 ms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human-Object Interaction Detection</head><p>Human-Object Interaction detection has been initially proposed in <ref type="bibr" target="#b7">[8]</ref>, and has been developed in two main streams: sequential methods and parallel methods. In sequential methods, object detection is performed first and every pair of the detected object is inferred with a separate neural network to predict interactions. Parallel HOI detectors perform object detection and interaction prediction in parallel and associates them with simple heuristics such as distance or IoU. Sequential HOI Detectors: InteractNet <ref type="bibr" target="#b5">[6]</ref> extended an existing object detector by introducing an action-specific density map to localize target objects based on the humancentric appearance, and combined features from individual boxes to predict the interaction. Note that interaction detection based on visual cues from individual boxes often suffers from the lack of contextual information. To this end, iCAN <ref type="bibr" target="#b4">[5]</ref> proposed an instance-centric attention module that extracts contextual features complementary to the features from the localized objects/humans. No-Frills HOI detection <ref type="bibr" target="#b8">[9]</ref> propose a training and inference HOI detection pipeline only using simple multi-layer perceptron. Graph-based approaches have proposed frameworks that can explicitly represent HOI structures with graphs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21]</ref>. Deep Contextual Attention <ref type="bibr" target="#b28">[29]</ref> leverages contextual information by a contextual attention framework in HOI. <ref type="bibr" target="#b27">[28]</ref> proposes a heterogeneous graph network that models humans and objects as different kinds of nodes. Various external sources such as linguistic priors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20]</ref> or human pose information <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref> have also been leveraged for further improve performance. Although sequential HOI detectors feature a fairly intuitive pipeline and solid performance, they are time-consuming and computationally expensive because of the additional neural network inference after the object detection phase.</p><p>Parallel HOI Detectors: Attempts for faster HOI detection has been also introduced in recent works as parallel HOI detectors. These works have directly localized interactions with interaction points <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref> or union boxes <ref type="bibr" target="#b11">[12]</ref>, replacing the separate neural network for interaction prediction with a simple heuristic based matching with distance or IoUs. Since they can be parallelized with existing object detectors, they feature fast inference time. However, these works are limited in that they require a hand-crafted postprocessing stage to associate the localized interactions with object detection results. This post-processing step i) requires manual search for the threshold, and ii) generates extra time complexity for matching each object pairs with the localized interactions (5?9 ms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Detection with Transformers</head><p>DETR <ref type="bibr" target="#b1">[2]</ref> has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image. The main loss for DETR produces an optimal bipartite matching between predicted and ground-truth objects. Afterward, the objectspecific losses (for class and bounding box) are optimized. In our recomposition, the interaction representations predicted by the Interaction Decoder are associated with the instance representations to predict a fixed set of HOI triplets (see <ref type="figure" target="#fig_1">Fig.3</ref>). The positional encoding is identical to <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The goal of this paper is to predict a set of human, object, interaction triplets while considering the inherent semantic relationships between the triplets in an end-to-end manner. To achieve this goal, we formulate HOI detection as set prediction. In this section, we first discuss the problems of directly extending the set prediction architecture for object detection <ref type="bibr" target="#b1">[2]</ref> to HOI detection. Then, we propose our architecture HOTR that parallelly predicts a set of object detection and associates the human and object of the interaction, while the self-attention in transformers models the relationships between the interactions. Finally, we present the details of training for our model including Hungarian Matching for HOI detection and our loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Detection as Set Prediction</head><p>We first start from object detection as set prediction with transformers, then show how we extend this architecture to capture HOI detection with transformers.</p><p>Object Detection as Set Prediction. Object Detection has been explored as a set prediction problem by DETR <ref type="bibr" target="#b1">[2]</ref>. Since object detection includes a single classification and a single localization for each object, the transformer encoder-decoder structure in DETR transforms N positional embeddings to a set of N predictions for the object class and bounding box.</p><p>HOI Detection as Set Prediction. Similar to object detection, HOI detection can be defined as a set prediction problem where each prediction includes the localization of a human region (i.e., subject of the interaction), an object region (i.e., target of the interaction) and multi-label classification of the interaction types. One straightforward extension is to modify the MLP heads of DETR to transform each positional embedding to predict a human box, object box, and action classification. However, this architecture poses a problem where the localization for the same object needs to be redundantly predicted with multiple positional embeddings (e.g., if the same person works on a computer while sitting on a chair, two different queries have to infer redundant regression for the same human).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">HOTR architecture</head><p>The overall pipeline of HOTR is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Our architecture features a transformer encoder-decoder structure with a shared encoder and two parallel decoders (i.e., instance decoder and interaction decoder). The results of the two decoders are associated with using our proposed HO Pointers to generate final HOI triplets. We will introduce HO Pointers shortly after discussing the architecture of HOTR.</p><p>Transformer Encoder-Decoder architecture. Similar to DETR <ref type="bibr" target="#b1">[2]</ref>, the global context is extracted from the input image by the backbone CNN and a shared encoder. Afterward, two sets of positional embeddings (i.e., the instance queries and the interaction queries) are fed into the two parallel decoders (i.e., the instance decoder and interaction decoder in <ref type="figure" target="#fig_0">Fig. 2</ref>). The instance decoder transforms the instance queries to instance representations for object detection while the interaction decoder transforms the interaction queries to interaction representations for interaction detection. We apply feed-forward networks (FFNs) to the interaction representation and obtain a Human Pointer, an Object Pointer, and interaction type, see <ref type="figure" target="#fig_1">Fig. 3</ref>. In other words, the interaction representation localizes human and object regions by pointing the relevant instance representations using the Human Pointer and Object Pointer (HO Pointers), instead of directly regressing the bounding box. Our architecture has several advantages compared to the direct regression approach. We found that directly regressing the bounding box has a problem when an object participates in multiple interactions. In the direct regression approach, the localization of the identical object differs across interactions. Our architecture addresses this issue by having separate instance and interaction representations and associating them using HO Pointers. Also, our architecture allows learning the localization more efficiently without the need of learning the localization redundantly for every interaction. Note that our experiments show that our shared encoder is more effective to learn HO Pointers than two separate encoders.</p><p>HO Pointers. A conceptual overview of how HO Pointers associate the parallel predictions from the instance decoder and the interaction decoder is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. HO Pointers (i.e., Human Pointer and Object Pointer) contain the indices of the corresponding instance representations of the human and the object in the interaction. After the in-teraction decoder transforms K interaction queries to K interaction representations, an interaction representation z i is fed into two feed-forward networks FFN h :</p><formula xml:id="formula_0">R d ? R d , FFN o : R d ? R d to obtain vectors v h i and v o i , i.e., v h i = FFN h (z i ) and v o i = FFN o (z i ).</formula><p>Then finally the Human/Object Pointers? h i and? o i , which are the indices of the instance representations with the highest similarity scores, are obtained by?</p><formula xml:id="formula_1">h i = argmax j sim(v h i , ? j ) , c o i = argmax j sim(v o i , ? j ) ,<label>(1)</label></formula><p>where ? j is the j-th instance representation and sim(u, v) = u v/ u v .</p><p>Recomposition for HOI Set Prediction. From the previous steps, we now have the following: i) N instance representations ?, and ii) K interaction representations z and their HO Pointers? h and? o . Given ? interaction classes, our recomposition is to apply the feed-forward networks for bounding box regression and action classification as FFN box : R d ? R 4 , and FFN act : R d ? R ? , respectively. Then, the final HOI prediction for the i-th interaction representation z i is obtained by,</p><formula xml:id="formula_2">b h i = FFN box (??h i ) ? R 4 , b o i = FFN box (??o i ) ? R 4 , a i = FFN act (z i ) ? R ? .<label>(2)</label></formula><p>The final HOI prediction by our HOTR is the set of K triplets,</p><formula xml:id="formula_3">{ bh i ,b o i ,? i } K i=1 .</formula><p>Complexity &amp; Inference time. Previous parallel methods have substituted the costly pair-wise neural network inference with a fast matching of triplets (associating interaction regions with corresponding human regions and object regions based on distance <ref type="bibr" target="#b29">[30]</ref> or IoU <ref type="bibr" target="#b11">[12]</ref>). HOTR further reduces the inference time after object detection by associating K interactions with N instances, resulting in a smaller time complexity O(KN ). By eliminating the post-processing stages in the previous one-stage HOI detectors including NMS for the interaction region and triplet matching, HOTR diminishes the inference time by 4 ? 8ms while showing improvement in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training HOTR</head><p>In this section, we explain the details of HOTR training. We first introduce the cost matrix of Hungarian Matching for unique matching between the ground-truth HOI triplets and HOI set predictions obtained by recomposition. Then, using the matching result, we define the loss for HO Pointers and the final training loss.</p><p>Hungarian Matching for HOI Detection. HOTR predicts K HOI triplets that consist of human box, object box and binary classification for the a types of actions. Each prediction captures a unique human,object pair with one or more interactions. K is set to be larger than the typical number of interacting pairs in an image. We start with the basic cost function that defines an optimal bipartite matching between predicted and ground truth HOI triplets, and then show how we modify this matching cost for our interaction representations.</p><p>Let Y denote the set of ground truth HOI triplets and Y = {? i } K i=1 as the set of K predictions. As K is larger than the number of unique interacting pairs in the image, we consider Y also as a set of size K padded with ? (no interaction). To find a bipartite matching between these two sets we search for a permutation of K elements ? ? S K with the lowest cost:</p><formula xml:id="formula_4">? = argmin ??S K K i C match (y i ,? ?(i) ),<label>(3)</label></formula><p>where C match is a pair-wise matching cost between ground truth y i and a prediction with index ?(i). However, since y i is in the form of hbox,obox,action and? ?(i) is in the form of hidx,oidx,action , we need to modify the cost function to compute the matching cost. Let ? : idx ? box be a mapping function from groundtruth hidx,oidx to ground-truth hbox,obox by optimal assignment for object detection. Using the inverse mapping ? ?1 : box ? idx, we get the ground-truth idx from the ground-truth box.</p><p>Let M ? R d?N be a set of normalized instance representations ? = ?/ ? ? R d , i.e., M = [? 1 . . . ? N ]. We computeP h ? R K?N that is the set of softmax predictions for the H Pointer in (1) given a?</p><formula xml:id="formula_5">P h = K i=1 softmax((v h i ) T M ),<label>(4)</label></formula><p>where K i=1 denotes the vertical stack of row vectors and v h </p><formula xml:id="formula_6">i = v h i /||v h i ||.P o is analogously defined. Given the ground-truth y i = (b h i , b o i , a i ),P h ,</formula><formula xml:id="formula_7">C match (y i ,? ?(i) ) = ???1 {ai =?}P h [?(i), c h i ] ???1 {ai =?}P o [?(i), c o i ] +1 {ai =?} L act (a i ,? ?(i) ),<label>(5)</label></formula><p>whereP [i, j] denotes the element at i-th row and j-th column, and? ?(i) is the predicted action. The action matching cost is calculated as L act (a i ,? ?(i) ) = BCELoss(a i ,? ?(i) ). ? and ? is set as a fixed number to balance the different scales of the cost function for index prediction and action classification.</p><p>Final Set Prediction Loss for HOTR. We then compute the Hungarian loss for all pairs matched above, where the loss for the HOI triplets has the localization loss and the action classification loss as</p><formula xml:id="formula_8">L H = K i=1 L loc (c h i , c o i , z ?(i) ) + L act (a i ,? ?(i) ) .<label>(6)</label></formula><p>The localization loss L loc (c h i , c o i , z ?(i) ) is denoted as</p><formula xml:id="formula_9">L loc = ? log exp(sim(FFN h (z ?(i) ), ? c h i )/? ) N k=1 exp(sim(FFN h (z ?(i) ), ? k )/? ) ? log exp(sim(FFN o (z ?(i) ), ? c o i /? ) N k=1 exp(sim(FFN o (z ?(i) ), ? k )/? ) ,<label>(7)</label></formula><p>where ? is the temperature that controls the smoothness of the loss function. We empirically found that ? = 0.1 is the best value for our experiments.</p><p>Defining No-Interaction with HOTR. In DETR <ref type="bibr" target="#b1">[2]</ref>, maximizing the probability of the no-object class for the softmax output naturally suppresses the probability of other classes. However, in HOI detection the action classification is a multi-label classification where each action is treated as an individual binary classification. Due to the absence of an explicit class that can suppress the redundant predictions, HOTR ends up with multiple predictions for the same human,object pair. Therefore, HOTR sets an explicit class that learns the interactiveness (1 if there is any interaction between the pair, 0 otherwise), and suppresses the predictions for redundant pairs that have a low interactiveness score (defined as No-Interaction class). In our experiment in <ref type="table">Table.</ref> 3, we show that setting an explicit class for interactiveness contributes to the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details.</head><p>We train HOTR with AdamW <ref type="bibr" target="#b21">[22]</ref>. We set the transformer's initial learning rate to 10 ?4 and weight decay to 10 ?4 . All transformer weights are initialized with Xavier init <ref type="bibr" target="#b6">[7]</ref>. For a fair evaluation with baselines, the Backbone, Encoder, and Instance Decoder are pre-trained in MS-COCO and frozen during training. We use the scale augmentation as in DETR <ref type="bibr" target="#b1">[2]</ref>, resizing the input images such that the shortest side is at least 480 and at most 800 pixels while the longest side at most is 1333.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we demonstrate the effectiveness of our model in HOI detection. We first describe the two public datasets that we use as our benchmark: V-COCO and HICO-DET. Next, we show that HOTR successfully captures HOI triplets, by achieving state-of-the-art performance in both mAP and inference time. Then, we provide a detailed ablation study of the HOTR architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To validate the performance of our model, we evaluate our model on two public benchmark datasets: the V-COCO (Verbs in COCO) dataset and HICO-DET dataset. V-COCO is a subset of COCO and has 5,400 trainval images and 4,946 test images. For V-COCO dataset, we report the AP role over 25 interactions in two scenarios AP #1 role and AP #2 role . The two scenarios represent the different scoring ways for object occlusion cases. In Scenario1, the model should correctly predict the bounding box of the occluded object as [0,0,0,0] while predicting human bounding box and actions correctly. In Scenario2, the model does not need to predict about the occluded object. HICO-DET <ref type="bibr" target="#b2">[3]</ref> is a subset of HICO dataset and has more than 150K annotated instances of human-object pairs in 47,051 images (37,536 training and 9,515 testing) and is annotated with 600 verb, object interaction types. For HICO-DET, we report our performance in the Default setting where we evaluate the detection on the full test set. We follow the previous settings and report the mAP over three different category sets: (1) all 600 HOI categories in HICO (Full), (2) 138 HOI categories with less than 10 training instances (Rare), and (3) 462 HOI categories with 10 or more training instances (Non-Rare).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Analysis</head><p>For quantitative analysis, we use the official evaluation code for computing the performance of both V-COCO and HICO-DET. <ref type="table">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> show the comparison of HOTR with the latest HOI detectors including both sequential and parallel methods. For fair comparison, the instance detectors are fixed by the parameters pre-trained in MS-COCO. All results in V-COCO dataset are evaluated with the fixed detector. For the HICO-DET dataset, we provide both results using the fixed detector and the fine-tuned detector following the common evaluation protocol <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Our HOTR achieves a new state-of-the-art performance on both V-COCO and HICO-DET datasets, while being the fastest parallel detector. <ref type="table">Table 1</ref> shows our result in the V-COCO dataset with both Scenario1 and Sce-nario2. HOTR outperforms the state-of-the-art parallel HOI detector <ref type="bibr" target="#b29">[30]</ref> in Scenario1 with a margin of 4.2mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone  <ref type="table" target="#tab_1">Table 2</ref> shows the result in HICO-DET in the Default setting for each Full/Rare/Non-Rare class. Due to the noisy labeling for objects in the HICO-DET dataset, fine-tuning the pre-trained object detector on the HICO-DET train set provides a prior that benefits the overall performance <ref type="bibr" target="#b0">[1]</ref>. Therefore, we evaluate our performance in HICO-DET dataset under two conditions: i) using pre-trained weights from MS-COCO which are frozen during training (denoted as COCO in the Detector column) and ii) performance after fine-tuning the pre-trained detector on the HICO-DET train set (denoted as HICO-DET in the Detector column). Our model outperforms the state-of-the-art parallel HOI detector under both conditions by a margin of 4.1mAP and 4mAP, respectively. Below, we provide a more detailed analysis of our performance.</p><p>HOTR vs Sequential Prediction. In comparative analysis with various HOI methods summarized in <ref type="table">Table 1</ref> and 2, we also compare the experimental results of HOTR with sequential prediction methods. Even though the sequential methods take advantages from additional information while HOTR only utilize visual information, HOTR outperforms the state-of-the-art sequential HOI detector <ref type="bibr" target="#b15">[16]</ref> in both Scenario1 and Scenario2 by 1. Performance on HICO-DET Rare Categories. HOTR shows state-of-the-art performance across both sequential and parallel HOI detectors in the Full evaluation for HICO-DET dataset (see <ref type="table">Table.</ref> 2). However, HOTR underperforms than baseline methods <ref type="bibr" target="#b15">[16]</ref> in the Rare setting. Since this setting deals with the action categories that has less than 10 training instances, it is difficult to achieve accuracy on this setting without the help of external features. Therefore, most of the studies that have shown high performance in Rare settings make use of additional information, such as spatial layouts <ref type="bibr" target="#b4">[5]</ref>, pose information <ref type="bibr" target="#b17">[18]</ref>, linguistic priors <ref type="bibr" target="#b16">[17]</ref>, and coherence patterns between the humans and objects <ref type="bibr" target="#b15">[16]</ref>. In this work, our method is a completely vision-based pipeline but if we include the prior knowledge, we expect further improvement in the Rare setting. Time analysis. Since the inference time of the object detector network (e.g., Faster-RCNN <ref type="bibr" target="#b24">[25]</ref>) can vary depending on benchmark settings (e.g., the library, CUDA, CUDNN version or hyperparameters), the time analysis is based on the pure inference time of the HOI interaction prediction model excluding the time of the object detection phase for fair comparison with our model. For detailed analysis, HOTR takes an average of 36.3ms for the backbone and encoder, 23.8ms for the instance decoder and interaction decoder (note that the two decoders run in parallel), and 0.9ms for the recomposition and final HOI triplet inference. We excluded the i/o times in all models including the time of previous models loading the RoI align features of Faster-RCNN (see <ref type="figure">Figure.</ref>1 for a speed vs time comparison). Note that our HOTR runs ?5 ? ?9 faster compared to the state-of-the-art parallel HOI detectors, since an explicit post-processing stage to assemble the detected objects and interaction regions is replaced with a simple O(KN ) search to infer the HO Pointers. In this section, we explore how each of the components of HOTR contributes to the final performance. <ref type="table">Table 3</ref> shows the final performance in the V-COCO test set after excluding each components of HOTR. We perform all experiments with the most basic R50-C4 backbone, and fix the transformer layers to 6 and attention heads 8 and the feed-forward network dimension to d = 1024 unless otherwise mentioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>With vs Without HO Pointers. In HOTR, the interaction representation localizes human and object region by pointing the relevant instance representations using the Human Pointer and Object Pointer (HO Pointers), instead of directly regressing the bounding box. We pose that our architecture has advantages compared to the direct regression approach, since directly regressing the bounding box for every interaction prediction requires redundant bounding box regression for the same object when an object participates in multiple interactions. Based on the performance gap (55.2 ? 39.3 in V-COCO and 23.5 ? 17.2 in HICO-DET), it can be concluded that using HO Pointers alleviates the issue of direct regression approach.</p><p>Shared Encoder vs Separate Encoders. From the <ref type="figure" target="#fig_0">Fig. 2</ref>, the architecture having separate encoders for each Instance and Interaction Decoder can be considered. In this ablation, we verify the role of the shared encoder of the HOTR. In <ref type="table">Table 3</ref>, it is shown that sharing the encoder outperforms the model with separate encoders by a margin of 21.3mAP and 9.0mAP in V-COCO and HICO-DET, respectively. We suppose the reason is that the shared encoder helps the decoders learn common visual patterns, thus the HO Pointers can share the overall context.</p><p>With vs Without Interactiveness Suppression. Unlike softmax based classification where maximizing the probability for the no-object class can explicitly diminish the probability of other classes, action classification is a multi-label binary classification that treats each class independently. So HOTR sets an explicit class that learns the interactiveness, and suppresses the predictions for redundant pairs that have low probability. <ref type="table">Table 3</ref> shows that setting an explicit class for interactiveness contributes 3mAP to the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present HOTR, the first transformerbased set prediction approach in human-object interaction problem. The set prediction approach of HOTR eliminates the hand-crafted post-processing steps of previous HOI detectors while being able to model the correlations between interactions. We propose various training and inference techniques for HOTR: HOI decomposition with parallel decoders for training, recomposition layer based on similarity for inference, and interactiveness suppression. We develop a novel set-based matching for HOI detection that associates the interaction representations to point at instance representations. Our model achieves state-ofthe-art performance in two benchmark datasets in HOI detection: V-COCO and HICO-DET, with a significant margin to previous parallel HOI detectors. HOTR achieves state-of-the-art performance on both benchmark datasets in HOI detection with an inference time under 1 ms, being significantly faster than previous parallel HOI detectors (5?9 ms).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overall pipeline of our proposed model. The Instance Decoder and Interaction Decoder run in parallel, and share the Encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Conceptual illustration of how HO Pointers associates the interaction representations with instance representations. As instance representations are pre-trained to perform standard object detection, the interaction representation learns localization by predicting the pointer to the index of the instance representations for each human and object boxes. Note that the index pointer prediction is obtained in parallel with instance representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and ,P o , we convert the ground-truth box to indices by c h i = ? ?1 (b h i ) and c o i = ? ?1 (b o i ) and compute our matching cost function written as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>9 mAP and 4 . 1</head><label>41</label><figDesc>mAP in V-COCO while showing comparable performance (with a margin of 0.1?0.52 mAP) in the Default(Full) evaluation of HICO-DET.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>AP #1</figDesc><table><row><cell></cell><cell></cell><cell>role</cell><cell>AP #2 role</cell></row><row><cell cols="2">Models with external features</cell><cell></cell><cell></cell></row><row><cell>TIN (RP D C D ) [18]</cell><cell>R50</cell><cell>47.8</cell><cell></cell></row><row><cell>Verb Embedding [31]</cell><cell>R50</cell><cell>45.9</cell><cell></cell></row><row><cell>RPNN [33]</cell><cell>R50</cell><cell>-</cell><cell>47.5</cell></row><row><cell>PMFNet [27]</cell><cell>R50-FPN</cell><cell>52.0</cell><cell></cell></row><row><cell>PastaNet [17]</cell><cell>R50-FPN</cell><cell>51.0</cell><cell>57.5</cell></row><row><cell>PD-Net [32]</cell><cell>R50</cell><cell>52.0</cell><cell>-</cell></row><row><cell>ACP [13]</cell><cell>R152</cell><cell>53.0</cell><cell></cell></row><row><cell>FCMNet [20]</cell><cell>R50</cell><cell>53.1</cell><cell>-</cell></row><row><cell>ConsNet [21]</cell><cell>R50-FPN</cell><cell>53.2</cell><cell>-</cell></row><row><cell cols="2">Sequential HOI Detectors</cell><cell></cell><cell></cell></row><row><cell>VSRL [8]</cell><cell>R50-FPN</cell><cell>31.8</cell><cell>-</cell></row><row><cell>InteractNet [6]</cell><cell>R50-FPN</cell><cell>40.0</cell><cell>48.0</cell></row><row><cell>BAR-CNN [14]</cell><cell>R50-FPN</cell><cell>43.6</cell><cell>-</cell></row><row><cell>GPNN [24]</cell><cell>R152</cell><cell>44.0</cell><cell>-</cell></row><row><cell>iCAN [5]</cell><cell>R50</cell><cell>45.3</cell><cell>52.4</cell></row><row><cell>TIN (RC D ) [18]</cell><cell>R50</cell><cell>43.2</cell><cell>-</cell></row><row><cell>DCA [29]</cell><cell>R50</cell><cell>47.3</cell><cell>-</cell></row><row><cell>VSGNet [26]</cell><cell>R152</cell><cell>51.8</cell><cell>57.0</cell></row><row><cell>VCL [10]</cell><cell>R50-FPN</cell><cell>48.3</cell><cell></cell></row><row><cell>DRG [4]</cell><cell>R50-FPN</cell><cell>51.0</cell><cell></cell></row><row><cell>IDN [16]</cell><cell>R50</cell><cell>53.3</cell><cell>60.3</cell></row><row><cell>Parallel HOI Detectors</cell><cell></cell><cell></cell><cell></cell></row><row><cell>IPNet [30]</cell><cell>HG104</cell><cell>51.0</cell><cell>-</cell></row><row><cell>UnionDet [12]</cell><cell>R50-FPN</cell><cell>47.5</cell><cell>56.2</cell></row><row><cell>Ours</cell><cell>R50</cell><cell>55.2</cell><cell>64.4</cell></row><row><cell cols="4">Table 1. Comparison of performance on V-COCO test set. AP #1 role , AP #2 role denotes the performance under Scenario1 and Scenario2 in</cell></row><row><cell>V-COCO, respectively.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison in HICO-DET. The Detector column is denoted as 'COCO' for the models that freeze the object detectors with the weights pre-trained in MS-COCO and 'HICO-DET' if the object detector is fine-tuned with the HICO-DET train set.</figDesc><table><row><cell>Default</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sai Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10460" to="10469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ieee winter conference on applications of computer vision (wacv)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="696" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ican: Instancecentric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10437</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9677" to="9685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12407</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Uniondet: Union-level detection towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Detecting human-object interactions with action co-occurrence priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08728</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting visual relationships using box attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10166" to="10175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hoi analysis: Integrating and decomposing human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pastanet: Toward human activity knowledge engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3585" to="3594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Amplifying key cues for human-object-interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Consnet: Learning consistency graph for zero-shot human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4235" to="4243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangalore S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13617" to="13626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9469" to="9478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Contextual heterogeneous graph network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yingbiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10001</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep contextual attention for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laaksonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07721</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4116" to="4125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="843" to="851" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
