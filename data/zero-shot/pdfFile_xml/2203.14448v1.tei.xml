<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupled Multi-task Learning with Cyclical Self-Regulation for Face Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingping</forename><surname>Zheng</surname></persName>
							<email>zhengqingping2018@mail.nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<addrLine>2 Huawei</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
							<email>jiankangdeng@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
							<email>zhengzhu@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<addrLine>2 Huawei</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
							<email>s.zafeiriou@imperial.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Decoupled Multi-task Learning with Cyclical Self-Regulation for Face Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper probes intrinsic factors behind typical failure cases (e.g. spatial inconsistency and boundary confusion) produced by the existing state-of-the-art method in face parsing. To tackle these problems, we propose a novel Decoupled Multi-task Learning with Cyclical Self-Regulation (DML-CSR) for face parsing. Specifically, DML-CSR designs a multi-task model which comprises face parsing, binary edge, and category edge detection. These tasks only share low-level encoder weights without high-level interactions between each other, enabling to decouple auxiliary modules from the whole network at the inference stage. To address spatial inconsistency, we develop a dynamic dual graph convolutional network to capture global contextual information without using any extra pooling operation. To handle boundary confusion in both single and multiple face scenarios, we exploit binary and category edge detection to jointly obtain generic geometric structure and fine-grained semantic clues of human faces. Besides, to prevent noisy labels from degrading model generalization during training, cyclical self-regulation is proposed to self-ensemble several model instances to get a new model and the resulting model then is used to self-distill subsequent models, through alternating iterations.</p><p>Experiments show that our method achieves the new state-of-the-art performance on the Helen, CelebAMask-HQ, and Lapa datasets. The source code is available at https://github.com/deepinsight/ insightface/tree/master/parsing/dml_csr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face parsing, as a fine-grained semantic segmentation task, intends to assign a pixel-wise label for each facial component, e.g., eyes, nose, and mouth. The detailed analysis of semantic facial parts is essential in many high-level applications, such as face swapping <ref type="bibr" target="#b27">[28]</ref>, face editing <ref type="bibr" target="#b14">[15]</ref>, This work is done when Qingping Zheng is an intern at Huawei.  <ref type="figure">Figure 1</ref>. The first three rows show typical failure cases of spatial inconsistency and boundary confusion when applying EARGNet <ref type="bibr" target="#b35">[36]</ref> to face parsing. The last row displays noisy labels on the training datasets. and facial makeup <ref type="bibr" target="#b28">[29]</ref>. Benefit from the learning capacity of deep Convolutional Neural Networks (CNNs) and the labor effort put in pixel-level annotations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>, methods based on Fully Convolutional Networks (FCNs) <ref type="bibr">[7, 10, 18-20, 23, 36, 47, 48]</ref> have achieved a promising performance on the fully supervised face parsing. Nevertheless, the local characteristic of the convolutional kernel prevents FCNs from capturing global contextual information <ref type="bibr" target="#b24">[25]</ref>, which is crucial for semantically parsing facial components in an image.</p><p>To address this issue, most of the region-based face parsing methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b47">47]</ref> integrate CNN features into variant CRFs to learn global information. However, these methods do not consider the correlation among various objects. To this end, Te et al. <ref type="bibr" target="#b35">[36]</ref> proposes the EAGRNet method to model a region-level graph representation over a face image by propagating information across all vertices on the graph. Even though EAGRNet enables reasoning over non-local regions to get global dependencies between distant facial components and achieves state-of-the-art performance, it still faces the problems of spatial inconsistency and boundary confusion. In EAGRNet, PSP module <ref type="bibr" target="#b45">[45]</ref> adopts an average pooling layer <ref type="bibr" target="#b21">[22]</ref> to capture the global context prior, leading to an inconsistent spatial topology. Moreover, EA-GRNet integrates additional clues of binary edges into context embedding to improve the parsing results. However, it is hard for EAGRNet to handle boundaries between highly irregular facial parts (e.g. hair and cloth in <ref type="figure">Figure 1</ref>) and distinguish clear boundaries between different face instances in the crowded scenarios (multi-faces in <ref type="figure">Figure 1)</ref>.</p><p>Besides, learning a reliable model for face parsing requires accurate pixel-level annotations. Nonetheless, there inevitably exist careless manual labeling errors on the training dataset as shown in the last row of <ref type="figure">Figure 1</ref>. Te et al. <ref type="bibr" target="#b35">[36]</ref> employ the traditional fully supervised learning scheme to train EAGRNet, failing to locate label noise because all pixels in the ground truth are processed equally. Notably, overlooking such incomplete annotations restricts the model generalization and prevents the performance from increasing to a higher level.</p><p>In this paper, we propose an end-to-end face parsing method, which is based on Decoupled Multi-task Learning with Cyclical Self-Regulation (DML-CSR). Specifically, given an input of facial image, the ResNet-101 <ref type="bibr" target="#b7">[8]</ref> pre-trained on ImageNet is taken as the backbone to extract features from different levels. Afterwards, our multi-task model consists of three tasks, namely face parsing, binary edge detection, and category edge detection. These tasks share low-level weights from the backbone but do not have high-level interactions. Therefore, our multi-task learning approach can detach additional edge detection tasks from face parsing at the inference stage. To tackle spatial inconsistency raised by the pooling operation, we develop a Dynamic Dual Graph Convolutional Network (DDGCN) in the face parsing branch to capture long-range contextual information. The proposed DDGCN contains no extra pooling operation and it can dynamically fuse the global context extracted from GCNs in both spatial and feature spaces. To solve the boundary confusion in both single-face and multiface scenarios, the proposed category-aware edge detection module exploits more semantic information than the binary edge detection module used in EARGNet <ref type="bibr" target="#b35">[36]</ref>.</p><p>To address the problem caused by noisy labels in training datasets, we introduce a cyclically learning scheduler inspired by self-training <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b49">49]</ref> to achieve advanced cyclical self-regulation. The proposed CSR contains a self-ensemble strategy that can aggregate a set of historical models to obtain a new reliable model and another self-distillation method that exploits the soft labels generated by the aggregated model to guide the successive model learning. Finally, the proposed CSR iteration alternates be-tween these two procedures, correcting the noisy labels during training and promoting the model generalization. The proposed CSR can significantly promote the reliability of the model and labels in a cyclical training scheduler without introducing extra computation costs.</p><p>To summarize, our main contributions are as follows:</p><p>? We propose a decoupled multi-task network including face parsing, binary edge detection, and category edge detection. The face parsing branch introduces a DDGCN without any extra pooling operation to solve the problem of spatial inconsistency, and an additional category edge detection branch is designed to handle the boundary confusion. ? We introduce a cyclical self-regulation mechanism during training. The iteration alternates between one self-ensemble procedure, boosting model generalization progressively, and another self-distillation processing, regulating noisy labels. ? Our method establishes new state-of-the-art performance on the Helen <ref type="bibr" target="#b34">[35]</ref> (93.8% overall F1 score), LaPa <ref type="bibr" target="#b20">[21]</ref> (92.4% mean F1) and CelebAMask-HQ <ref type="bibr" target="#b14">[15]</ref> (86.1% mean F1) datasets. Compared to EARGNet <ref type="bibr" target="#b35">[36]</ref>, our method utilizes fewer computation resources as the edge prediction modules can be decoupled from the whole network, decreasing the inference time from 89ms to 31ms but achieving much better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Face parsing. Most existing face parsing methods can be classified into global-based and local-based methods.</p><p>Global-based methods aim to predict a pixel-wise label directly from the whole RGB face image. Early works learn spatial correlation between facial parts using various handcrafted models, such as epitome model <ref type="bibr" target="#b10">[11]</ref> and exemplarbased method <ref type="bibr" target="#b34">[35]</ref>. Later, many works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b47">47]</ref> implant the CNN-based features into the Conditional Random Field (CRF) framework, and adopt a multi-objective learning method to model pixel-wise labels and neighborhood dependencies simultaneously. Lin et al. <ref type="bibr" target="#b16">[17]</ref> design a CNNbased framework with a RoI Tanh-Warping operator to use both central and peripheral information. Te et al. <ref type="bibr" target="#b35">[36]</ref> introduce an edge-aware graph module to effectively reason relationship between facial regions. These global-based approaches inherently integrate the prior into the face layout, but limit accuracy due to overlook on each individual part. Local-based methods aim to predict each facial part individually by training separated models for different facial regions. Luo et al. <ref type="bibr" target="#b23">[24]</ref> exploit a hierarchical approach to segment each detected facial component separately. Zhou et al. <ref type="bibr" target="#b48">[48]</ref> propose an interlinked CNN-based model to forecast pixel categories after face detection, taking a large expense of memory and computation consumption. Later, Liu <ref type="figure">Figure 2</ref>. Overview of our proposed DML-CSR method for face parsing. At the training stage, it includes three parallel sub-models of face paring, binary edge detection and category edge detection, jointly trained by a proposed cyclical self-regulation mechanism. At the testing stage, all edge models are decoupled from the whole model. et al. <ref type="bibr" target="#b18">[19]</ref> combines a shallow CNN and a spatially variant RNN in two successive stages to parse a face image at a very fast inference speed. These local-based approaches almost take the coarse-to-fine policy with consideration of both global consistency and local precision. However, it ignores the improvement of accuracy and efficiency from backbone sharing and joint optimization.</p><p>Multi-task learning is a common strategy which jointly trains various tasks through the shared feature mechanism or hidden layers of a "backbone" model <ref type="bibr" target="#b1">[2]</ref>. It has been widely applied for solving multiple pixel-level tasks. In the context of deep learning, multi-task learning can be categorized into hard or soft parameter sharing schemes. In hard parameter sharing based multi-task learning for image segmentation, the parameter set consists of shared and task-specific parameters. UberNet <ref type="bibr" target="#b13">[14]</ref> is the first hard parameter sharing model for image segmentation, where a large number of low-, mid-, and high-level image vision tasks are tackled concurrently. Later, most multi-task learning models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref> follow the hard parameter sharing schemes and simply share the same encoder layers. In these works, each task-specific decoding head tails at the end of the shared encoder, leading to sub-optimal task groupings.</p><p>In soft parameter sharing based multi-task learning for image segmentation, each task has its own group of parameters, and a feature sharing mechanism is used to handle the cross-task communication. Cross-stitch network <ref type="bibr" target="#b25">[26]</ref> is a typical multi-task architecture adopting the soft-parameter sharing schemes. It linearly combines the activations from every task-specific layer, regarding as soft feature fusion strategy. Afterwards, Ruder et al. <ref type="bibr" target="#b32">[33]</ref> extends this method to learn the selective sharing layers. Compared to the hard parameter sharing approaches, the problem of multi-task learning based on soft parameter sharing approaches is a lack of scalability, as the growth of tasks make the size of the multi-task network increase linearly <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section starts with the analysis of representative failure cases when applying EARGNet <ref type="bibr" target="#b35">[36]</ref> to face parsing. These issues motivate the proposal of a more accurate and robust training method, called Decoupled Multi-task Learning with Cyclical Self-Regulation (DML-CSR). The overall pipeline is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Limitations of EAGRNet</head><p>Even though EAGRNet <ref type="bibr" target="#b35">[36]</ref> achieves notable performance on face parsing, it has the following issues during training on public benchmark datasets (e.g. Helen <ref type="bibr" target="#b34">[35]</ref>, CelebAMask-HQ <ref type="bibr" target="#b14">[15]</ref> and LaPa <ref type="bibr" target="#b20">[21]</ref>). Spatial Inconsistency. As shown in the first-row of <ref type="figure">Figure  1</ref>, EAGRNet improperly predicts "neck" pixels within the cloth area, resulting in spatial inconsistency of cloth. As EAGRNet employs an adaptive average pooling within PSP module <ref type="bibr" target="#b45">[45]</ref> to capture global contextual information, the detailed spatial relationship and constraint between original pixels may be neglected. Therefore, a small part of area within a large region can be predicted as wrong classes. Since directly adopting the general object segmentation method to face parsing is sub-optimal, we explore to avoid the unnecessary pooling operation in our model design. Boundary Confusion. As intuitively illustrated in the second-row of <ref type="figure">Figure 1</ref>, EARGNet fails to distinguish boundaries between (1) "cloth" and "hair", and (2) the target face and the surrounding face under crowded scenario. Generally, component boundaries between different facial organs and instance boundaries between close faces can be confusing for face parsing models. As the edge network built in EARGNet simply integrates the binary edge prior into contextual features by the dot product and the pooling operation, it only recovers partial boundaries of regions. Impact from Label Noise. As the pixel-level annotation is difficult and expensive, most of the face parsing benchmarks (e.g. Helen <ref type="bibr" target="#b34">[35]</ref> and LaPa <ref type="bibr" target="#b20">[21]</ref>) are annotated in a semi-automatic approach. Therefore, label noises inevitably exist in these datasets. As given in the last row of <ref type="figure">Figure 1</ref>, annotators mark the "eyes" as "glasses". Such annotation errors can limit the model performance, especially for tail classes (e.g. "necklace"). Nevertheless, the EARGNet method is a fully supervised method and lacks a regulation mechanism to tackle label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoupled Multi-task Learning</head><p>Based on above analysis, we propose an end-to-end decoupled multi-task network to solve problems of spatial inconsistency and boundary confusion. Herein, we define three parallel tasks of face parsing, binary edge detection and category edge detection. To prevent using any pooling operation in context embedding, a customized GCN <ref type="bibr" target="#b12">[13]</ref> module is designed to gain global contextual relationships for the parsing branch. To alleviate the boundary confusion, a binary edge detection branch as well as a category-aware semantic edge detection branch are jointly trained to gain rich edge information. During training, feature representations are simultaneously optimized for these three tasks, but the auxiliary edge prediction branches are removed during testing, without introducing any extra computation cost.</p><p>An overview of our model architecture is depicted in <ref type="figure">Figure 2</ref>. Given an input facial image, the ResNet-101 <ref type="bibr" target="#b7">[8]</ref> pretrained on ImageNet is taken as backbone to extract features from different levels, marked as {C 1 , C 2 , C 3 , C 4 , C 5 }. Afterwards, remaining parts involve: (1) a face parsing branch, which consists of a context embedding and a parsing head <ref type="bibr" target="#b35">[36]</ref>, (2) a binary edge detection branch utilizing the same edge decoder as <ref type="bibr" target="#b31">[32]</ref>, and (3) a category edge detection branch, which features abundant information of component edges. Each task shares same feature representations of first four layers in the backbone model. For the edge detection branches, feature maps from C 2 , C 3 and C 4 are concate- <ref type="figure">Figure 3</ref>. Illustration of the proposed DDGCN for context embedding. DDGCN is composed of two branches, and each consists of a Graph Convolutional Network (GCN) to model contextual information in the spatial-dimensions and feature-dimensions for a convolutional feature map X. No pooling step is involved in DDGCN to avoid spatial inconstancy. nated as input. For the parsing branch, context embedding features from C 5 are concatenated with the feature maps from C 2 as the input. Since edge branches preserve boundary information in low-level feature maps, joint edge prediction can assist high-level semantic predictions. At the testing phase, these two edge branches are decoupled from the whole model, avoiding extra computation overhead. Context Embedding without Pooling. Context embedding is crucial for face parsing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b45">45]</ref>, but the pooling operation results in the problem of spatial inconsistency. To this end, we design a Dynamic Dual Graph Convolution Network (DDGCN), which exploits 1D convolution to build adjacent matrix of GCN over different 2D dimensions. As shown in <ref type="figure">Figure 3</ref>, the proposed DDGCN comprises one weighted GCN (labeled as H S ) with parameter ? in the spatial space and another weighted GCN (labeled as H F ) with parameter ? in the feature space.</p><formula xml:id="formula_0">Y = X ? (? ? HS) ? (? ? HF ),<label>(1)</label></formula><p>where ? denotes the operation of concatenation. The parameters ? and ? are learnable weights for both H S and H F , respectively. Different from DGCN <ref type="bibr" target="#b44">[44]</ref>, we remove the pooling operation during coordinate space projection and, we merge spatial and channel features into the input X via a dynamic concatenation instead of the addition operation.</p><p>To avoid buffer storage for gradient computation, all BN layers are replaced by Inplace-ABN <ref type="bibr" target="#b30">[31]</ref>. As the proposed DDGCN is only applied to the C 5 feature map, our context embedding is more efficient than EAGRNet, which employs low-level features for graph representation learning. Binary and Category Edge Assisted Face Parsing. As current training datasets for face parsing do not provide labels for the boundary detection, we first generate pseudo labels of binary and category-aware edges as illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. More specifically, binary edge pixels are identified from the pixel-wise label map by referring the neighboring four pixels. If there exists one neighboring pixel of zero value, the current pixel is regarded as an edge pixel.</p><p>By employing the same criteria, the category-aware edges are generated independently for each facial component.</p><p>To learn shared features for the layers {C 1 , C 2 , C 3 , C 4 } by simultaneously training the parsing and edge detection tasks, we design a loss function for each task and then sum them together with different weights. Different from the general semantic segmentation, face parsing features on tiny components. To retain the structure of small components, we also employ the Lov?sz-softmax <ref type="bibr" target="#b0">[1]</ref> loss, which utilizes the mean intersection-over-union score to measure difference between ground truth and predicted mask. Hence, the cross-entropy <ref type="bibr" target="#b5">[6]</ref> and Lov?sz-softmax <ref type="bibr" target="#b0">[1]</ref> losses are combined together to optimize the parsing module. Additionally, the weighted cross-entropy <ref type="bibr" target="#b5">[6]</ref> loss is employed to optimize both binary and category-aware edge detection. Consequently, the total multi-task loss is defined as</p><formula xml:id="formula_1">LMT = ?0 ? (L p ce + L p lov?sz ) parse + ?1 ? L b ce + ?2 ? L c ce edges ,<label>(2)</label></formula><p>where L b ce and L c ce represent the weighted cross-entropy losses <ref type="bibr" target="#b5">[6]</ref> corresponding to binary and category-aware semantic edges, respectively. The hyper-parameters ?0, ?1, and ?2 denote the different weights for each task.</p><p>Besides the above parallel optimization, we also develop a boundary assisted semantic loss which enlarges the parsing loss of boundary pixels according to the binary and category-aware boundary maps. As edge maps are highly related to segmentation maps, it is beneficial to inject two types of edge cues into the parsing module to improve the segmentation accuracy for the components with clear contours. To this end, we define a dual edge attention loss</p><formula xml:id="formula_2">L b attn = 1 N N i=1 1 bi * L p i ? Bi,<label>(3)</label></formula><formula xml:id="formula_3">L c attn = 1 N C N i=1 C j=1 wj * 1 cij * L p i ? Cij,<label>(4)</label></formula><p>where N is the total number of images in a batch, b i is the number of boundary pixels in a binary edge label map B i ? R H?W , c ij is the number of boundary pixels of a specific category j in a category-aware edge label map C ij ? R H?W , w j is a category-aware weight to emphasize a specific class j (e.g. the tail class of "necklace') which can increase the weights of tail classes, and L p i ? R H?W is the cross-entropy between a predicted parsing result and the ground-truth. Different from the binary boundary attention loss proposed in <ref type="bibr" target="#b20">[21]</ref>, we further introduce category-aware boundary-attention semantic loss, significantly improving segmentation results of underrepresented classes.</p><p>The overall loss of our decoupled multi-task learning can be summarized as  where ? 3 and ? 4 correspond to weights of attention losses for binary and category-aware edges, respectively.</p><formula xml:id="formula_4">LDML = ?0 ? (L p ce + L p lov?sz ) parse + ?1 ? L b ce + ?3 ? L b attn binary?edge + ?2 ? L c ce + ?4 ? L c attn category?edge ,<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cyclical Self-Regulation</head><p>To alleviate label noise, we introduce a Cyclical Selfregulation (CSR) training strategy to achieve online refinement labels. The proposed CSR depicted in <ref type="figure">Figure 2</ref> includes two parts, self-ensemble and self-distillation. Model Generalization via Self-Ensemble. As illustrated in the self-ensemble process of <ref type="figure">Figure 2</ref>, given a best model M best from previous epochs and a set of next successive models {M 1 , M 2 , . . . , M n }, a new model is obtained by aggregating weights of these models</p><formula xml:id="formula_5">M = k k + 1 M best + 1 (k + 1)N N n=1 Mn,<label>(6)</label></formula><p>where k is the current cycle number and 1 ? k ? K, and n is the number of models used in a cycle and 1 ? n ? N . Moreover, symbols M , M best and M n represent the weights of aggregated, best and current models, respectively. In addition, all training data is forwarded into new aggregated model to re-estimate the statistical parameters in all Inplace-ABN <ref type="bibr" target="#b30">[31]</ref> layers. Label Refinement via Self-Distillation. As the soft labels contain dark knowledge <ref type="bibr" target="#b8">[9]</ref> and less label noise, we explore self-distillation to improve the parsing performance. More specifically, as shown in the self-distillation process of <ref type="figure">Figure 2</ref>, the parsing results generated from the above aggregated model are exploited to supervise the multi-task learning. The total weighted loss is defined as</p><formula xml:id="formula_6">LCSR = ?0 ? (L p kl + L p lov?sz ) parse + ?1 ? L b kl + ?2 ? L c kl edges ,<label>(7)</label></formula><p>where L p kl , L b kl , L c kl represent the Kullback-Leibler divergence losses <ref type="bibr" target="#b5">[6]</ref> for face parsing, binary edge and categoryaware edge tasks, respectively. They compute the difference between soft labels of the aggregated model and prediction results of the current model. Hyper-parameters ? 0 , ? 1 , ? 2 are weights assigned to each task.</p><p>Finally, both self-ensemble and self-distillation processes mutually iterates in a cycle manner, promoting model generalization and correcting noisy labels progressively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Skin Nose U-lip I-mouth L-lip Eyes Brows Mouth Overall F1</head><p>Liu et al. <ref type="bibr" target="#b18">[19]</ref> 92. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We use Helen <ref type="bibr" target="#b34">[35]</ref>, CelebAMask-HQ <ref type="bibr" target="#b14">[15]</ref>, and LaPa <ref type="bibr" target="#b20">[21]</ref> for experiments. The Helen dataset contains 2,330 images with 11 labels: "background", "facial skin", "left/right brow", "left/right eye", "nose", "upper/lower lip", "inner mouth" and "hair". It is split into 2,000, 230 and 100 images for training, validation and testing. The CelebAMask-HQ dataset includes 24,183, 2,993, and 2,824 images for training, validation and testing. Apart from the 11 categories of the Helen dataset, the CelebAMask-HQ dataset adds extra 8 classes, including "left/right ear', "eyeglass", "earing", "necklace", "neck" and "cloth". The LaPa dataset features rich variations in expression, pose and occlusion, consisting of 11 categories as the Helen dataset. It is partitioned into 18,176 samples for training, 2,000 samples for validation, and 2,000 samples for testing. Implementation Details. The proposed method is implemented by Pytorch <ref type="bibr" target="#b29">[30]</ref>, adopting the ResNet101 <ref type="bibr" target="#b7">[8]</ref> as a backbone. The weights of the backbone are initialized with the pre-trained model on ImageNet <ref type="bibr" target="#b4">[5]</ref>. Batch normalizations in our network are all replaced by In-Place Activated Batch Norm <ref type="bibr" target="#b30">[31]</ref>. The input image size is 473 ? 473 at both training and testing stages. During training, the data is augmented using: random rotation selecting an angle within (-30?, 30?) and random scaling with a factor from 0.75 to 1.25. We set the batch size as 28 and the network is trained for 200 epochs in total. The first 150 epochs are trained as initialization, following K = 5 cycles and each containing N = 10 epochs of the self-training process.</p><p>During the decoupled multi-task learning, we follow the similar training strategies as EAGRNet <ref type="bibr" target="#b35">[36]</ref>, i.e. Stochastic Gradient Descent (SGD) optimizer with the base learning rate 0.001 and the weight of decay of 0.0005. For the total loss function, weights of parsing, binary edge and category edge losses are set as ? 0 = 1, ? 1 = 1, and ? 2 = 1. respectively. To recover boundaries of tail classes (e.g. necklace and earring), weights ? 3 = 4 and ? 4 = 1 are assigned to both binary and category edge attention losses, respectively. For the cyclical self-regulation, the cosine annealing learning rate scheduler <ref type="bibr" target="#b15">[16]</ref> with a learning rate of 10 ?5 is employed to optimize the model generalization. The weights of self-distillation losses for parsing, binary and categoryaware edges are set to ? 0 = 1, ? 1 = 1 and ? 2 = 0.1. Evaluation Metrics. To measure the performance of a face parsing model, two universally accepted evaluation metrics are employed, namely mean Intersection over Union (mIoU) and F1 score. To keep consistent comparison with the previous methods, the overall F1-score on the Helen dataset is calculated over the merged facial components: brows (left and right), eyes (left and right), nose, and mouth (upper lip, lower lip, and inner mouth). For the CelebAMask-HQ and LaPa datasets, the mean F1-score is computed over all categories excluding the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with State-of-the-art</head><p>In this paper, we thoroughly compare the performance of our proposed model with existing state-of-the-art methods (i.e. <ref type="bibr">Zhao</ref>    <ref type="table">Table 5</ref>. Comparisons of different contextual modules on the parsing branch. Here, "+" means that the context embedding is added into the baseline, and "-pooling" denotes that the pooling operation is removed from the context embedding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Analysis of Improvement. To illustrate the effect of individual modules and training strategy, the model after removing some components is trained from scratch under the same setting. The baseline method adopts the parsing module with a simple convolution unit, which includes a 3 ? 3 convolution and an Inplace-ABN <ref type="bibr" target="#b30">[31]</ref> to map features from the last layer of the backbone into new features of 256 dimensions. As shown in <ref type="table" target="#tab_2">Table 4</ref>, our proposed DML-CSR substantially improves performance on face parsing. Compared to our baseline, adopting the DDGCN without any pooling operation as context embedding achieves a significant performance improvement. Then, appending semantic edge modules to enhance shared features has a further advance on parsing performance. The best results are obtained by training the decoupled multi-task network in a self-regulation mechanism, resulting in around 3.2% and 4.0% improvements of mean IoU on the Helen and LaPa datasets, respectively. Besides, it outperforms the baseline by around 1.7% overall F1 score improvement on the Helen dataset, and by over 2.5% mean F1 improvement on the LaPa dataset. On the CelebAMask-HQ dataset, DML-CSR also outperforms the baseline by around 1.7% improvement in both mean IoU and mean F1.</p><p>Comparison of Various Contextual Modules. To prove the effectiveness of our proposed DDGCN for learning contextual representation, the above-mentioned simple convolution unit in the baseline is substituted by various context embedding modules. Ablation experiments in <ref type="table">Table 5</ref> show that the pooling operation in PSP <ref type="bibr" target="#b45">[45]</ref> and DGCNet <ref type="bibr" target="#b44">[44]</ref> is harmful for the performance and the proposed DDGCN surpasses other contextual modules by dropping the pooling step and adopting dynamic feature fusion strategies.</p><p>Comparison of Different Auxiliary Tasks. Visual examples of <ref type="figure" target="#fig_3">Figure 5</ref> show that auxiliary category edge modules can distinguish boundaries between facial components and different faces. To further explore the effect of category edge detection, several related experiments are executed. As we can see from the results listed in <ref type="table" target="#tab_3">Table 6</ref>, both the binary edge detection branch and the category-aware edge detection branch can obviously improve the performance of face parsing. However, the category-aware edge is more informative than binary edge, thus it is more beneficial for face parsing. Besides, our proposed dual edge attention loss on the equation (4) further improves overall performance of face parsing on three benchmark testing datasets. Analysis of Visual Results. To better understand the effect of the proposed methods step-by-step, we present visual examples in <ref type="figure">Figure 6</ref>. The second-column visual examples show that our baseline obviously address the issue of spatial inconsistency. However, examples in column (b) appear severe unclear boundaries between different facial components in the first three green boxes, and confusing contours of multi-faces in the last three green boxes. This is due to the fact that the baseline lacks a reasoning ability on global dependencies. The first three training datasets from degrading model generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present DML-CSR, a decoupled multi-task learning method with cyclical self-regulation for face parsing. Comprehensive experiments on Helen, CelebAMask-HQ, and LaPa verify the effectiveness of the proposed method. The results show that DML-CSR significantly outperforms other methods on all datasets. Training details will be released to encourage further research towards face parsing. Limitations. Our method achieves impressive results in face parsing. However, there is a slight performance degradation in low-resolution faces. This is because that we train our model on the high-resolution face dataset. Even so, we believe DML-CSR is a valuable method for training a reliable face parsing model on a large-scale dataset. Societal Impact. We develop a general model for face parsing in this paper, and the proposed model is not used for a specific application. Therefore, this work does not directly involve societal issues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a )</head><label>a</label><figDesc>Binary edge generation (b) Category-aware edge generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Binary edge label generation and category-aware edge label generation from the pixel-wise label map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>DML-CSR can easily distinguish different face instances under crowded scenarios due to the auxiliary category edge prediction. LaPa model is used here for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-row examples in column (c) show complete structure of individual component and almost clear boundaries between facial parts, illustrating the long-range inference ability of our proposed DDGCN. Nonetheless, the last three-row examples in the column (c) still exist different face instances, as the proposed DDGCN has a limited capability of localizing objects of similar contours. Compared to examples in columns (b)-(c), columns (d)-(e) present clear boundaries between different facial components in both single-face and multi-face scenes, due to the feature enhancement by semantic edges. Looking at the areas within green rectangles in columns (d)-(e), CSR can recover error pixels, preventing noisy labels in (a) Image (b) Baseline (c) +DDGCN (d) +DML (e) +CSR (f) GT Figure 6. DML-CSR can obtain complete facial components with clear boundaries in both single-face and multi-face scenarios. Visual examples in different columns are generated by the corresponding LaPa models. Here, "+" denotes that the current component is added into the model in the previous column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2</head><label>12</label><figDesc>Comparison with state-of-the-art methods on the Helen dataset in overall F1 score.</figDesc><table><row><cell></cell><cell></cell><cell>1</cell><cell>93.0</cell><cell>74.3</cell><cell>79.2</cell><cell>81.7</cell><cell>86.8</cell><cell>77.0</cell><cell>89.1</cell><cell>88.6</cell></row><row><cell cols="2">Guo et al. [7]</cell><cell>93.8</cell><cell>94.1</cell><cell>75.8</cell><cell>83.7</cell><cell>83.1</cell><cell>80.4</cell><cell>87.1</cell><cell>92.4</cell><cell>90.5</cell></row><row><cell cols="2">Lin et al. [17]</cell><cell>94.5</cell><cell>95.6</cell><cell>79.6</cell><cell>86.7</cell><cell>89.8</cell><cell>89.6</cell><cell>83.1</cell><cell>95.0</cell><cell>92.4</cell></row><row><cell cols="2">Wei et al. [46]</cell><cell>95.6</cell><cell>95.2</cell><cell>80.0</cell><cell>86.7</cell><cell>86.4</cell><cell>89.0</cell><cell>82.6</cell><cell>93.6</cell><cell>91.6</cell></row><row><cell cols="2">Liu et al. [21]</cell><cell>94.9</cell><cell>95.8</cell><cell>83.7</cell><cell>89.1</cell><cell>91.4</cell><cell>89.8</cell><cell>83.5</cell><cell>96.1</cell><cell>93.1</cell></row><row><cell cols="2">Te et al. [36]</cell><cell>94.6</cell><cell>96.1</cell><cell>83.6</cell><cell>89.8</cell><cell>91.0</cell><cell>90.2</cell><cell>84.9</cell><cell>95.5</cell><cell>93.2</cell></row><row><cell cols="3">DML-CSR (Ours) 96.6</cell><cell>95.5</cell><cell>87.6</cell><cell>91.2</cell><cell>91.2</cell><cell>90.9</cell><cell>88.5</cell><cell>95.9</cell><cell>93.8</cell></row><row><cell>Method</cell><cell cols="11">Skin Hair L-Eye R-Eye U-lip I-mouth L-lip Nose L-Brow R-Brow Mean F1</cell></row><row><cell>Zhao et al. [45]</cell><cell cols="2">93.5 94.1</cell><cell>86.3</cell><cell>86.0</cell><cell>83.6</cell><cell>86.9</cell><cell>84.7</cell><cell>94.8</cell><cell>86.8</cell><cell>86.9</cell><cell>88.4</cell></row><row><cell>Liu et al. [21]</cell><cell cols="2">97.2 96.3</cell><cell>88.1</cell><cell>88.0</cell><cell>84.4</cell><cell>87.6</cell><cell>85.7</cell><cell>95.5</cell><cell>87.7</cell><cell>87.6</cell><cell>89.8</cell></row><row><cell>Te et al. [36]</cell><cell cols="2">97.3 96.2</cell><cell>89.5</cell><cell>90.0</cell><cell>88.1</cell><cell>90.0</cell><cell>89.0</cell><cell>97.1</cell><cell>86.5</cell><cell>87.0</cell><cell>91.1</cell></row><row><cell cols="3">DML-CSR (Ours) 97.6 96.4</cell><cell>91.8</cell><cell>91.5</cell><cell>88.0</cell><cell>90.5</cell><cell>89.9</cell><cell>97.3</cell><cell>90.4</cell><cell>90.4</cell><cell>92.4</cell></row></table><note>. Comparison with state-of-the-art methods on the LaPa dataset in mean F1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>et al. [45], Liu et al. [21], Lee et al. [15], Luo et al. [23], Liu et al. [19], Guo et al. [7], Lin et al. [17], Wei et al. [46], and Te et al. [36]) on the Helen, LaPa and CelebAMask-HQ datasets. Statistical results in Table 1, Table 2, andTable 3demonstrate that the proposed DML-CSR significantly outperforms other methods, achieving 93.8%, 92.4%, and 86.1% F1 scores on Helen, LaPa and CelebAMask-HQ, respectively. On the Lapa dataset, DML-CSR exhibits obvious advantages on eyebrow parsing. On the CelebAMask-HQ dataset, DML-CSR achieves much better performance on tail classes, such as "earring" and "necklace". Compared to EAGRNet<ref type="bibr" target="#b35">[36]</ref>, DML-CSR re-</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="10">Face I-Mouth U-Lip L-Lip Nose Glasses L-Eye R-Eye L-Brow R-Brow Hair Hat Earring Necklace Neck L-Ear R-Ear Mean F1 Cloth</cell></row><row><cell cols="2">Zhao et al. [45]</cell><cell>94.8 89.8</cell><cell></cell><cell>90.3 87.1</cell><cell>75.8 88.8</cell><cell>79.9 90.4</cell><cell>80.1 58.2</cell><cell>77.3 65.7</cell><cell>78.0 19.4</cell><cell>75.6 82.7</cell><cell>73.1 64.2</cell><cell>76.2</cell></row><row><cell cols="2">Lee et al. [15]</cell><cell>95.5 63.4</cell><cell></cell><cell>85.6 88.9</cell><cell>92.9 90.1</cell><cell>84.3 86.6</cell><cell>85.2 91.3</cell><cell>81.4 63.2</cell><cell>81.2 26.1</cell><cell>84.9 92.8</cell><cell>83.1 68.3</cell><cell>80.3</cell></row><row><cell cols="2">Luo et al. [23]</cell><cell>96.0 93.8</cell><cell></cell><cell>93.7 88.6</cell><cell>90.6 90.3</cell><cell>86.2 93.9</cell><cell>86.5 85.9</cell><cell>83.2 67.8</cell><cell>83.1 30.1</cell><cell>86.5 88.8</cell><cell>84.1 83.5</cell><cell>84.0</cell></row><row><cell cols="2">Te et al. [36]</cell><cell>96.2 95.0</cell><cell></cell><cell>94.0 88.9</cell><cell>92.3 91.2</cell><cell>88.6 94.9</cell><cell>88.7 87.6</cell><cell>85.7 68.3</cell><cell>85.2 27.6</cell><cell>88.0 89.4</cell><cell>85.7 85.3</cell><cell>85.1</cell></row><row><cell cols="2">DML-CSR (Ours)</cell><cell>95.7 91.8</cell><cell></cell><cell>93.9 87.4</cell><cell>92.6 91.0</cell><cell>89.4 94.5</cell><cell>89.6 88.5</cell><cell>85.5 71.4</cell><cell>85.7 40.6</cell><cell>88.3 89.6</cell><cell>88.2 85.7</cell><cell>86.1</cell></row><row><cell></cell><cell cols="11">Table 3. Comparison with state-of-the-art methods on the CelebAMask-HQ dataset in mean F1.</cell></row><row><cell cols="4">Baseline DDGCN DML CSR</cell><cell cols="3">Helen Mean IoU Overall F1</cell><cell></cell><cell cols="2">CelebAMask-HQ Mean IoU Mean F1</cell><cell cols="2">Mean IoU</cell><cell>LaPa</cell><cell>Mean F1</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.36</cell><cell>92.11</cell><cell></cell><cell>76.14</cell><cell>84.34</cell><cell></cell><cell>83.16</cell><cell>89.84</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell cols="8">83.42 (+ 1.06) 92.56 (+ 0.45) 77.41 (+ 1.27) 85.33 (+ 0.99) 86.65 (+ 3.49) 92.10 (+ 2.26)</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell cols="8">85.48 (+ 3.12) 93.75 (+ 1.64) 77.69 (+ 1.55) 85.98 (+ 1.64) 87.00 (+ 3.84) 92.32 (+ 2.48)</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="8">85.58 (+ 3.22) 93.78 (+ 1.67) 77.81 (+ 1.67) 86.07 (+ 1.73) 87.13 (+ 3.97) 92.38 (+ 2.54)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of DML-CSR on the Helen, CelebAMask-HQ and LaPa datasets. Here, DDGCN is used for context embedding. DML denotes the multi-task learning for our decoupled model including face parsing, binary and category edge detection. CSR represents the cyclical self-regulation.</figDesc><table><row><cell>Method</cell><cell>Helen Overall F1</cell><cell>CelebAMask-HQ Mean F1</cell><cell>LaPa Mean F1</cell></row><row><cell>Baseline</cell><cell>92.11</cell><cell>84.34</cell><cell>89.84</cell></row><row><cell>+PSP [45]</cell><cell>92.20</cell><cell>84.76</cell><cell>90.80</cell></row><row><cell>+PSP-pooling</cell><cell>92.37</cell><cell>84.83</cell><cell>91.35</cell></row><row><cell>+DGCNet [44]</cell><cell>92.41</cell><cell>85.17</cell><cell>91.72</cell></row><row><cell>+DGCNet-pooling</cell><cell>92.45</cell><cell>85.20</cell><cell>91.99</cell></row><row><cell>+DDGCN</cell><cell>92.56</cell><cell>85.33</cell><cell>92.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Results of our proposed multi-task learning on the Helen, CelebAMask-HQ and LaPa datasets. Here, "+" denotes adding multi-task branches into the baseline where DDGCN is used as context embedding. Losses of face parsing, binary edge detection, and category edge detection are denoted as * p, * b and * c in the subscript. Binary edge attention and category edge attention losses are denoted as * ba and * ca in the subscript. duces the parameters from 66.72M to 59.67M, and decreases the FLOP count from 51.63G to 48.54G. Given an image of the same input size as EAGRNet<ref type="bibr" target="#b35">[36]</ref>, DML-CSR dramatically shortens the inference time from 89ms to 31ms per image. In a word, DML-CSR utilizes fewer computation resources to outperform the state-of-the-art method.</figDesc><table><row><cell></cell><cell>Helen</cell><cell>CelebAMask-HQ</cell><cell>LaPa</cell></row><row><cell></cell><cell>Overall F1</cell><cell>Mean F1</cell><cell>Mean F1</cell></row><row><cell>Baseline</cell><cell>92.11</cell><cell>84.34</cell><cell>89.84</cell></row><row><cell>+DML p+b</cell><cell>93.35</cell><cell>85.58</cell><cell>92.16</cell></row><row><cell>+DML p+b+ba</cell><cell>93.52</cell><cell>85.69</cell><cell>92.24</cell></row><row><cell>+DMLp+c</cell><cell>93.61</cell><cell>85.73</cell><cell>92.21</cell></row><row><cell>+DMLp+c+ca</cell><cell>93.71</cell><cell>85.87</cell><cell>92.28</cell></row><row><cell>+DML p+b+c</cell><cell>93.65</cell><cell>85.80</cell><cell>92.26</cell></row><row><cell>+DML all</cell><cell>93.75</cell><cell>85.98</cell><cell>92.32</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>Stefanos Zafeiriou acknowledges support from the EPSRC Fellowship DEFORM (EP/S010203/1), FACER2VM (EP/N007743/1) and a Google Faculty Fellowship. Ying Li acknowledges support from the National Natural Science Foundation of China (61871460).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The lov?sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Leveraging semi-supervised learning in video sequences for urban scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Residual encoder decoder network and adaptive prior for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngsung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Joon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changkyu</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Michel Valstar, and Georgios Tzimiropoulos. A cnn cascade for landmark guided semantic part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Labelfaces: Parsing facial features by multiclass labeling with an epitome prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warrell</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selfcorrection for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peike</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face parsing with roi tanh-warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Roi tanh-polar transformer network for face parsing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face parsing via recurrent propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-objective convolutional learning for face labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new dataset and boundary-attention semantic segmentation for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ehanet: An effective hierarchical aggregation network for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyu</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinglong</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical face parsing via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via a fully-convolutional localglobal context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Merget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fast scene understanding for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02550</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FSGAN: Subject agnostic face swapping and reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beauty emakeup: A deep makeup transfer system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefei</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Devil in the details: Towards accurate single and multiple human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Latent multi-task architecture learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Probability of error of some adaptive patternrecognition machines. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Scudder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exemplar-based face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Edgeaware graph representation learning and reasoning for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gusi</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multinet: Real-time joint semantic reasoning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Z?llner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-task learning for dense prediction tasks: A survey. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning adaptive receptive fields for deep image parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Zeki</forename><surname>Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dual graph convolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Accurate facial image parsing at real-time speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefei</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Face parsing via a fully-convolutional continuous crf neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangjian</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03736</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Interlinked convolutional neural networks for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISNN</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
