<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CMGAN: Conformer-Based Metric-GAN for Monaural Speech Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherif</forename><surname>Abdulatif</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhe</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Bin</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">CMGAN: Conformer-Based Metric-GAN for Monaural Speech Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speech enhancement</term>
					<term>deep learning</term>
					<term>attention models</term>
					<term>generative adversarial networks</term>
					<term>metric discriminator</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> Convolution-augmented transformers (Conformers)   <p>are recently proposed in various speech-domain applications, such as automatic speech recognition (ASR) and speech separation, as they can capture both local and global dependencies. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for speech enhancement (SE) in the time-frequency (TF) domain. The generator encodes the magnitude and complex spectrogram information using two-stage conformer blocks to model both time and frequency dependencies. The decoder then decouples the estimation into a magnitude mask decoder branch to filter out unwanted distortions and a complex refinement branch to further improve the magnitude estimation and implicitly enhance the phase information. Additionally, we include a metric discriminator to alleviate metric mismatch by optimizing the generator with respect to a corresponding evaluation score. Objective and subjective evaluations illustrate that CMGAN is able to show superior performance compared to state-of-the-art methods in three speech enhancement tasks (denoising, dereverberation and super-resolution). For instance, quantitative denoising analysis on Voice Bank+DEMAND dataset indicates that CMGAN outperforms various previous models with a margin, i.e., PESQ of 3.41 and SSNR of 11.10 dB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N real-life speech applications, the perceived speech quality and intelligibility are dependent on the performance of the underlying speech enhancement (SE) systems, e.g., speech denoising, dereverberation and acoustic echo cancellation. As such, SE frameworks are an indispensable component in modern automatic speech recognition (ASR), telecommunication systems and hearing aid devices <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. This is evident by the increasingly large amount of research continuously attempting to push the performance boundaries of current SE systems <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The majority of these approaches harness the recent advances in deep learning (DL) techniques as well as the increasingly more available speech datasets <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>. SE techniques can be roughly categorized into two prominent families of approaches. Chronologically, enhancing the speech time-frequency (TF) representation (spectrogram) constitutes the classical SE paradigm which encompasses the majority of model-based as well as more recent DL approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. More recently, a new set of approaches were introduced to enhance raw speech time-domain waveform directly without any transformational overheads <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b17">[18]</ref>. Each paradigm presents unique advantages and drawbacks.</p><p>The time-domain paradigm is based on generative models trained to directly estimate fragments of the clean waveform from the distorted counterparts without any TF-domain transformation or reconstruction requirements <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, the lack of direct frequency representation hinders these frameworks from capturing speech phonetics in the frequency domain. This limitation is usually reflected as artifacts in the The authors are with the Institute of Signal Processing and System Theory, University of Stuttgart, Germany (e-mail: sherif.abdulatif@ iss.uni-stuttgart.de; ruizhe.cao96@gmail.com; bin.yang@iss.uni-stuttgart.de). A shorter version is available in https://arxiv.org/abs/2203.15149 <ref type="bibr" target="#b0">[1]</ref>. reconstructed speech. Another drawback of this paradigm is the ample input space associated with the raw waveforms, which often necessitates the utilization of deep computationally complex frameworks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>In the TF-domain, most conventional model-based or DL techniques utilize the magnitude component while ignoring the phase. This is accounted to the unstructured phase component, which imposes challenges to the utilized architectures <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. To circumvent this challenge, several approaches follow the strategy of enhancing the complex spectrogram (real and imaginary parts), which implicitly enhances both magnitude and phase <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. However, the compensation effect between the magnitude and phase often leads to an inaccurate magnitude estimation <ref type="bibr" target="#b22">[23]</ref>. This problem will be discussed in details in Sec. II-A. Recent studies propose enhancing the magnitude followed by complex spectrogram refinement, which can alleviate the compensation problem effectively <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Furthermore, the commonly used objective function in SE is simply the L p ?norm distance between the estimated and the target spectrograms. Nevertheless, a lower distance does not always lead to higher speech quality. MetricGAN is proposed to overcome this issue by optimizing the generator with respect to the evaluation metric score that can be learned by a discriminator <ref type="bibr" target="#b10">[11]</ref>.</p><p>In addition, many approaches utilize transformers <ref type="bibr" target="#b24">[25]</ref> to capture the long-term dependency in the waveform or the spectrogram <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Recently, conformers have been introduced as an alternative to transformers in ASR and speech separation tasks due to their capacity of capturing both local context and global context <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Accordingly, they were also employed for time-domain SE <ref type="bibr" target="#b17">[18]</ref>. To the best of our knowledge, conformers are not yet explicitly investigated for TF-domain SE.</p><p>Inspired by the stated problems and previous works, we propose the first conformer-based MetricGAN (CMGAN) for various monaural speech enhancement tasks. The CMGAN consists of a generator and a metric discriminator. The generator is based on two-stage conformer blocks in the TF-domain, while the discriminator is responsible for estimating a blackbox non-differentiable metric. The concatenated magnitude, real and imaginary components are passed to the generator, which comprises an encoder with two-stage conformer blocks, a mask decoder and a complex decoder. The encoder aims to learn a compact feature representation of the input. The mask decoder estimates the mask for the input magnitude and the complex decoder refines the real and imaginary parts. In order to reduce the significant computational complexity of the conformer, we adopt the dual-path transformers <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref> into a two-stage conformer block, which can capture the dependencies along the time dimension and the frequency dimension sequentially. In a nutshell, the contributions of this work are summarized as follows:</p><p>? We investigate the performance of the two-stage conformer blocks and their capability of capturing time and frequency dependencies with a relatively low computational complexity.  <ref type="figure">Fig. 1</ref>: The TF-magnitude representation of distorted speech for different SE tasks, i.e., denoising, dereverberation and bandwidth extension (super-resolution). The variable ? represents the 60 dB reverberation time and s is the bandwidth upscaling ratio.</p><p>? We adopt a metric discriminator to our network, which helps to improve the corresponding evaluation metric without adversely affecting other metrics.</p><p>? The proposed model is tested on different SE tasks: speech denoising, dereverberation and bandwidth extension (superresolution) with relevant datasets and the model is shown to outperform state-of-the-art approaches.</p><p>? A comprehensive ablation study verifies the effectiveness of our design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM STATEMENT &amp; RELEVANT LITERATURE</head><p>In this paper, the proposed CMGAN will be evaluated on different SE tasks, namely speech denoising, dereverberation and super-resolution. Accordingly, for any acoustic environment the aforementioned SE tasks can be modeled as follows:</p><formula xml:id="formula_0">y(t) = x(t) * h(t) + n(t)<label>(1)</label></formula><p>where y(t) is the distorted speech, x(t) is the required clean speech, n(t) is a background noise and '*' is a convolution operation with a filter h(t). However, due to space constraints, this study will focus on evaluating each task alone and not the superimposed effects, as shown in <ref type="figure">Fig. 1</ref>. Hence, for denoising the additive background noise n(t) will only be considered ( <ref type="figure">Fig. 1b</ref>). For dereverberation ( <ref type="figure">Fig. 1c</ref>), the filter h(t) will represent a room impulse response (RIR) filter. Finally, h(t) will function as a low pass filter (LPF) in the super-resolution task to simulate the impact of low sampling frequency <ref type="figure">(Fig. 1d</ref>). The pertinent literature for each task will be presented in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Denoising</head><p>Speech denoising is considered as a source separation problem, where the objective is to suppress the background noise n(t) and predict the desired speechx(t) with maximum possible quality and intelligibility. Accordingly, the difficulty of this problem would highly depend on the nature of both the desired speech and the background noise. For instance, speech signals are highly non-stationary. As for the noise component, it can be divided into stationary scenarios (e.g., computer fan noise and air conditioners) and non-stationary scenarios (e.g., babble and street noise). Usually, the latter scenario is more challenging, as in these cases, the noise would occupy similar frequency bands as the desired speech <ref type="bibr" target="#b18">[19]</ref>.</p><p>In the speech denoising literature, due to the non-stationary nature of the problem, exploring the TF representations of the superimposed signal to reflect the time-varying frequency properties of the waveform is the typical approach <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. The only limitation arising from the TF-domain denoising is the unstructured phase representation. However, for a long time phase was considered insensitive to noise <ref type="bibr" target="#b31">[32]</ref>. As a result, research mostly focused on magnitude denoising while maintaining the noisy phase <ref type="bibr" target="#b5">[6]</ref>. Recently, many studies pointed out the importance of the phase on the denoised speech quality <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b32">[33]</ref>. To this end, TF speech denoising can be categorized into mapping-based and masking-based methods.</p><p>For mapping-based methods, a non-linear function is utilized to map the noisy speech to a corresponding denoised speech. These methods were first visited in time-domain speech denoising <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref>. For instance, SEGAN <ref type="bibr" target="#b13">[14]</ref> is introduced as an adversarial framework to map the noisy waveform to a corresponding denoised speech. Variants of SEGANs are also proposed to increase the capacity of the generator <ref type="bibr" target="#b36">[37]</ref>, or using an additional TF-domain loss to benefit from both domains <ref type="bibr" target="#b37">[38]</ref>. Building upon these trials, different mappingbased adversarial frameworks are also investigated on TFdomain speech denoising and they achieved more promising results <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>.</p><p>On the other hand, masking-based methods are mostly utilized in TF-domain with few trials on time-domain speech denoising <ref type="bibr" target="#b41">[42]</ref>. TF-domain masking-based methods operate under the assumption that two signals are considered to be Wdisjoint orthogonal if their short-time Fourier transformations (STFT) do not overlap <ref type="bibr" target="#b42">[43]</ref>. Accordingly, it is possible to demix the signals by determining the active source in each TF unit. Inspired by the auditory masking phenomenon and the exclusive allocation principle in auditory scene analysis <ref type="bibr" target="#b43">[44]</ref>, ideal binary masking (IBM) is the first masking-based method utilized in supervised speech denoising <ref type="bibr" target="#b44">[45]</ref>. In IBM, a mask is generated by assigning a value of 1 for a TF unit if the signal-to-noise ratio (SNR) in this unit exceeds a predefined threshold (required speech) and 0 otherwise (noise to suppress). In other words, IBM can be treated as a binary classification problem <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Although IBM has been shown to considerably improve speech intelligibility, it can degrade the speech quality by introducing musical noise distortions <ref type="bibr" target="#b47">[48]</ref>. Ideal ratio masking (IRM) is introduced as a remedy and it can be viewed as a soft version of the IBM, where each TF unit can take a value between 0 and 1 depending on the corresponding signal and noise powers <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Spectral magnitude mask (SMM) is considered as an unbounded variant of IRM <ref type="bibr" target="#b50">[51]</ref>.</p><p>The aforementioned masking-based methods would solely enhance the magnitude and keep the noisy phase unaltered. Subsequently, tackling the phase is divided into phase reconstruction and phase denoising approaches. For phase reconstruction, deep neural networks (DNNs) are trained to estimate the magnitude, which is then used for iterative phase reconstruction (IPR) <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b54">[55]</ref>. As for phase denoising, authors in <ref type="bibr" target="#b55">[56]</ref> are the first to introduce a phase-sensitive mask (PSM) as a variant of SMM and they claimed a considerable improvement in speech quality. Using IRM as a foundation, a complex ideal ratio masking (cIRM) approach is proposed that can operate on the real and imaginary parts, implicitly addressing both magnitude and phase denoising <ref type="bibr" target="#b20">[21]</ref>. Nevertheless, since the real and imaginary parts are not necessarily positive, the authors would compress the cIRM with a tanh activation to obtain values between ?1 and 1. The idea of cIRM is further extended by incorporating a deep complex-valued recurrent neural network (DCCRN) and new loss functions to estimate the relevant masks <ref type="bibr" target="#b56">[57]</ref>.</p><p>The main drawback behind these approaches is the magnitude and phase compensation effect discussed in <ref type="bibr" target="#b22">[23]</ref>. In this case, denoising the complex representations using only a complex loss (penalizing real and imaginary parts) would implicitly provide the trained model with a certain degree of freedom in estimating the magnitude and phase. Since the phase is unstructured and always challenging to estimate, this might result in an inaccurate magnitude estimation to compensate for the challenging phase. This problem can be mitigated by including both complex and magnitude losses or by complex refinement approaches, which basically decouple the problem into estimating a bounded mask for the magnitude followed by a complex refinement branch to further improve the magnitude and estimate the phase from the denoised complex representations <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b57">[58]</ref>- <ref type="bibr" target="#b59">[60]</ref>. However, since recent studies recommended mapping-based methods over the preceding masking-based approaches for complex spectrogram estimation <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b60">[61]</ref>, the complex refinement branch would follow a mapping-based approach. In this sense, the model can combine the fragmented benefits of both masking-based and mapping-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dereverberation</head><p>In an enclosed acoustic environment, the sound is perceived as a superposition of three distinct components: direct path, early reflections and late reverberations, which can be modeled by the convolutive RIR filter h(t) in Eq. 1 <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>. Thus, speech dereverberation would mainly focus on suppressing the unwanted reflections and maintaining the direct path representing the estimated desired speechx(t). Early reflections usually arrive shortly (50 ms) at the microphone as they come from a specific direction, thus they can be addressed as an attenuated copy of the direct path. In contrast, late reverberations arrive later as they represent delayed and attenuated superimposed signals from different directions. The difficulty of the dereverberation problem is accounted to different factors. For instance, room size and surface properties mainly contribute to the amount of reflections and degree of attenuation <ref type="bibr" target="#b63">[64]</ref>. Additionally, the distance between the microphone and the speaker would affect the reflection strength, i.e., the longer the distance, the stronger the reflections <ref type="bibr" target="#b64">[65]</ref>.</p><p>To the best of our knowledge, the dereverberation problem is usually addressed in TF-domain with limited trials on timedomain <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b65">[66]</ref>. This is due to the fact that time-domain models are prone to temporal distortions, which are severe in reverberant conditions. Similar to denoising, TF-domain masking-based methods are also extended to dereverberation. For instance, in <ref type="bibr" target="#b66">[67]</ref>, direct path and early reflections are considered as the desired speech and an IBM is utilized to suppress late reverberations. Unlike denoising, the SNR criteria for assigning 0 and 1 in each TF unit is modified in <ref type="bibr" target="#b67">[68]</ref> to address the speech presence probability. However, IBM is originally defined for additive noise under anechoic conditions. In reverberation, temporal smearing of speech is observed in the resultant TF representation, as shown in <ref type="figure">Fig. 1c</ref>. Hence, IBM with hard boundaries can cause a degradation in the resultant speech quality <ref type="bibr" target="#b68">[69]</ref> and soft IRM is usually the preferred method in this case <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b69">[70]</ref>- <ref type="bibr" target="#b71">[72]</ref>. Following the denoising path, IRM is extended with cIRM to include phase in the dereverberation process <ref type="bibr" target="#b72">[73]</ref>- <ref type="bibr" target="#b74">[75]</ref>.</p><p>Furthermore, mapping-based methods are also investigated in speech dereverberation. For instance, Han et al. <ref type="bibr" target="#b51">[52]</ref> is one of the first to investigate spectral mapping on dereverberation using a simple fully connected network. Later, authors in <ref type="bibr" target="#b75">[76]</ref> applied a fully convolutional U-Net (encoder-decoder) architecture with intermediate skip connections for this task. The SkipConvNet changed the U-Net architecture by replacing each skip connection with multiple convolutional modules to provide the decoder with intuitive feature maps <ref type="bibr" target="#b76">[77]</ref>. Additionally, a wide residual network is introduced in [78] to process different speech representations in the TF-domain, namely the magnitude of the STFT, Mel filterbank and cepstrum. Some approaches are able to provide significant performance gain by combining DNNs with conventional methods such as delayand-sum beamforming and late reverberation reduction by spectral subtraction <ref type="bibr" target="#b78">[79]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Super-resolution</head><p>The super-resolution problem is slightly different from prior SE use-cases. In denoising and dereverberation, the desired speech is available with superimposed unwanted noise or reflections and the task is to suppress these effects while preserving the speech. In contrast, super-resolution would reconstruct the missing samples from a low sampling frequency input signal. Accordingly, this problem can be formulated from two different perspectives based on the input domain. In the time-domain, the problem is closely related to super-resolution in natural images <ref type="bibr" target="#b79">[80]</ref>, where the task is to upsample an input signal of K ? 1 samples to an output signal of M ? 1 samples (K &lt; M ). In this case, a DNN can be trained for an interpolation task. On the other hand, for TF-domain, the task would rather resemble natural image inpainting <ref type="bibr" target="#b80">[81]</ref>, where a part of the image or spectrogram is missing and the DNN is trained to complete the image or reconstruct the missing highfrequency bands, as shown in <ref type="figure">Fig. 1a and 1d</ref>. Based on the previous description, it can be deduced that mapping-based is the only relevant approach in super-resolution.</p><p>In conventional audio processing, super-resolution has been investigated under the name of bandwidth extension <ref type="bibr" target="#b81">[82]</ref>. Recently, DL-based audio super-resolution studies demonstrated superior performance compared to traditional methods. In 2017, Kuleshov et al. <ref type="bibr" target="#b82">[83]</ref> proposed to use U-Net with skip connection architecture to reconstruct the waveform. TFiLM <ref type="bibr" target="#b84">[85]</ref> and AFiLM <ref type="bibr" target="#b85">[86]</ref> utilized recurrent models and attention blocks to capture the long-range time dependencies, respectively. However, the lack of frequency components limits further improvements in the performance. TFNet <ref type="bibr" target="#b83">[84]</ref> utilized both time and frequency domain by employing two branches, one branch models the reconstruction of spectral magnitude and the other branch models the waveform. However, the phase information is ignored in the frequency branch. Wang et al. <ref type="bibr" target="#b86">[87]</ref> proposed a time-domain modified autoencoder (AE) and a cross-domain loss function to optimize the hybrid framework. Recently, authors in <ref type="bibr" target="#b87">[88]</ref> proposed a neural vocoder based framework (NVSR) for the super-resolution task. While the above studies show promising results, many of them focus on the time-domain or hybrid time-domain and TF-domain magnitude representations. Nevertheless, the research on complex TF-domain super-resolution is not yet addressed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generator architecture</head><p>An overview of the generator architecture of CMGAN is shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>. For a distorted speech waveform y ? R L?1 , an STFT operation first converts the waveform into a complex spectrogram Y o ? R T ?F ?2 , where T and F denote the time and frequency dimensions, respectively. Then the compressed spectrogram Y is obtained by the power-law compression:</p><formula xml:id="formula_1">Y = |Y o | c e jYp = Y m e jYp = Y r + jY i (2)</formula><p>where Y m , Y p , Y r and Y i denote the magnitude, phase, real and imaginary components of the compressed spectrogram, respectively. c is the compression exponent which ranges from 0 to 1, here we follow Braun et al. <ref type="bibr" target="#b88">[89]</ref> to set c = 0.3. The power-law compression of the magnitude equalizes the importance of quieter sounds relative to loud ones, which is closer to human perception of sound <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>. The real and imaginary parts Y r and Y i are then concatenated with the magnitude Y m as an input to the generator. 1) Encoder: Given the input feature Y in ? R B?T ?F ?3 , where B denotes the batch size, the encoder consists of two convolution blocks with a dilated DenseNet <ref type="bibr" target="#b91">[92]</ref> in between. Each convolution block comprises a convolution layer, an instance normalization <ref type="bibr" target="#b92">[93]</ref> and a PReLU activation <ref type="bibr" target="#b93">[94]</ref>. The first convolution block is used to extend the three input features to an intermediate feature map with C channels. The dilated DenseNet contains four convolution blocks with dense residual connections, the dilation factors of each block are set to {1, 2, 4, 8}. The dense connections can aggregate all previous feature maps to extract different feature levels. As for the dilated convolutions, they serve to increase the receptive field effectively while preserving the kernels and layers count. The last convolution block is responsible for halving the frequency dimension to F = F/2 to reduce the complexity.</p><p>2) Two-stage conformer block: Conformers <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> achieved great success in speech recognition and separation as they combine the advantages of both transformers and convolutional neural networks (CNNs). Transformers can capture long-distance dependencies, while CNNs exploit local features effectively. Here we employ two conformer blocks sequentially to capture the time dependency in the first stage and the frequency dependency in the second stage. As shown in the <ref type="figure" target="#fig_1">Fig. 2b</ref>, given a feature map D ? R B?T ?F ?C , the input feature map D is first reshaped to D T ? R BF ?T ?C to capture the time dependency in the first conformer block. Then the output D T o is element-wise added with the input D T (residual connection) and reshaped to a new feature map D F ? R BT ?F ?C . The second conformer thus captures the frequency dependency. After the residual connection, the final output D o is reshaped back to the input size.</p><p>Similar to <ref type="bibr" target="#b26">[27]</ref>, each conformer block utilizes two halfstep feed-forward neural networks (FFNNs). Between the two FFNNs, a multi-head self-attention (MHSA) with four heads is employed, followed by a convolution module. The convolution module depicted in <ref type="figure" target="#fig_1">Fig. 2b</ref> starts with a layer normalization, a point-wise convolution layer and a gated linear unit (GLU) activation to diminish the vanishing gradient problem. The output of the GLU is then passed to a 1Ddepthwise convolution layer with a swish activation function, then another point-wise convolution layer. Finally, a dropout layer is used to regularize the network. Also, a residual path connects the input to the output.</p><p>3) Decoder: The decoder extracts the output from N twostage conformer blocks in a decoupled way, which includes two paths: the mask decoder and the complex decoder. The mask decoder aims to predict a mask that will be element-wise multiplied by the input magnitude Y m to predictX m . On the other hand, the complex decoder directly predicts the real and imaginary parts. Both mask and complex decoders consist of a dilated DenseNet, similar to the one in the encoder. The subpixel convolution layer is utilized in both paths to upsample the frequency dimension back to F <ref type="bibr" target="#b94">[95]</ref>. For the mask decoder, a convolution block is used to squeeze the channel number to 1, followed by another convolution layer with PReLU activation to predict the final mask. Note that the PReLU activation learns different slopes for each frequency band and initially the slopes are defined as a fixed positive value (0.2). Post-training evaluation indicates that all the slopes reflect different negative values, i.e., the output mask is always projected in the positive 1 st and 2 nd quadrants, as depicted in <ref type="figure" target="#fig_2">Fig. 3</ref>. For the complex decoder, the architecture is identical to the mask decoder, except no activation function is applied for the complex output.</p><p>Same as in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>, the masked magnitudeX m is first combined with the noisy phase Y p to obtain the magnitudeenhanced complex spectrogram. Then it is element-wise summed with the output of the complex decoder (X r ,X i ) to obtain the final complex spectrogram:</p><formula xml:id="formula_2">X r =X m cos(Y p ) +X rXi =X m sin(Y p ) +X i (3)</formula><p>The power-law compression is then inverted on the complex spectrogram (X r ,X i ) and an inverse short-time Fourier transform (ISTFT) is applied to get the time-domain signalx, as shown in <ref type="figure" target="#fig_3">Fig. 4a</ref>. To further improve the magnitude component and propagate magnitude loss on both decoder branches, we compute the magnitude loss onX m expressed by:</p><formula xml:id="formula_3">X m = X2 r +X 2 i (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metric discriminator</head><p>In SE, the objective functions are often not directly correlated to the evaluation metrics. Consequently, even if the objective loss is optimized, the evaluation score is still not satisfied. Furthermore, some evaluation metrics like perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b95">[96]</ref> and short-time objective intelligibility (STOI) <ref type="bibr" target="#b96">[97]</ref> cannot be used as loss functions because they are non-differentiable. Hence, the discriminator in CMGAN aims to mimic the metric score and use it as a part of the loss function. Here we follow the MetricGAN to use the PESQ score as a label <ref type="bibr" target="#b10">[11]</ref>. As shown in <ref type="figure" target="#fig_1">Fig. 2c</ref>, the discriminator consists of 4 convolution blocks. Each block starts with a convolution layer, followed by instance normalization and a PReLU activation. After the convolution blocks, a global average pooling is followed by two feedforward layers and a sigmoid activation. The discriminator is then trained to estimate the maximum normalized PESQ score (= 1) by taking both inputs as clean magnitudes. Additionally, the discriminator is trained to estimate the enhanced PESQ score by taking both clean and enhanced spectrum as an input together with their corresponding PESQ label, as shown in <ref type="figure" target="#fig_3">Fig. 4b</ref>. On the other hand, the generator is trained to render an enhanced speech resembling the clean speech, thus approaching a PESQ label of 1, as shown in <ref type="figure" target="#fig_3">Fig. 4c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss function</head><p>Inspired by Braun et al. <ref type="bibr" target="#b88">[89]</ref>, we use a linear combination of magnitude loss L Mag. and complex loss L RI in the TF-domain:</p><formula xml:id="formula_4">L TF = ? L Mag. + (1 ? ?) L RI L Mag. = E Xm,Xm X m ?X m 2 L RI = E Xr,Xr X r ?X r 2 + E Xi,Xi X i ?X i 2<label>(5)</label></formula><p>where ? is a chosen weight. Based on grid search, ? = 0.7 leads to the best performance. Similar to least-square GANs <ref type="bibr" target="#b97">[98]</ref>, the adversarial training is following a min-min optimization task over the discriminator loss L D and the corresponding generator loss L GAN expressed as follows:</p><formula xml:id="formula_5">L GAN = E Xm,Xm D(X m ,X m ) ? 1 2 L D = E Xm D(X m , X m ) ? 1 2 + E Xm,Xm D(X m ,X m ) ? Q PESQ 2<label>(6)</label></formula><p>where D refers to the discriminator and Q PESQ refers to the normalized PESQ score. Here we normalize the PESQ score to the range [0,1]. Moreover, an additional penalization in the resultant waveform L Time is proven to improve the restored speech quality <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_6">L Time = E x,x x ?x 1<label>(7)</label></formula><p>wherex is the enhanced waveform and x is the clean target waveform. The final generator loss is formulated as follows:</p><formula xml:id="formula_7">L G = ? 1 L TF + ? 2 L GAN + ? 3 L Time<label>(8)</label></formula><p>where ? 1 , ? 2 and ? 3 are the weights of the corresponding losses and they are chosen to reflect equal importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Datasets 1) Denoising: We investigate our proposed approach on the commonly used publicly available Voice Bank+DEMAND dataset <ref type="bibr" target="#b6">[7]</ref>. The clean tracks are selected from the Voice Bank corpus <ref type="bibr" target="#b98">[99]</ref> which includes 11,572 utterances from 28 speakers in the training set and 872 utterances from 2 unseen speakers in the test set. In the training set, the clean utterances are mixed with background noise (8 noise types from DEMAND database <ref type="bibr" target="#b99">[100]</ref> and 2 artificial noise types) at SNRs of 0 dB, 5 dB, 10 dB and 15 dB. In the test set, the clean utterances are mixed with 5 unseen noise types from the DEMAND database at SNRs of 2.5 dB, 7.5dB, 12.5 dB and 17.5 dB. The noise types are mostly challenging, e.g., public space noises (cafeteria, restaurant and office), domestic noises (kitchen and living room) and transportation/street noises (car, metro, bus, busy traffic, public square and subway station). All utterances are resampled to 16 kHz in our experiments. 2) Dereverberation: We choose the REVERB challenge dataset <ref type="bibr" target="#b7">[8]</ref>, the utterances are divided into simulated and real recordings. The simulated data is based on the wall street journal corpus (WSJCAM0) <ref type="bibr" target="#b100">[101]</ref> distorted by measured RIRs and a stationary ambient noise of SNR = 20 dB. The measured RIRs represent three different room sizes: smallroom 1, medium -room 2 and large -room 3, with a 60 dB reverberation time (? ) of 0.3, 0.6 and 0.7 seconds, respectively. For each room, the microphone is placed at a near condition (0.5 m) and a far condition (2 m). The real data is based on the multi-channel wall street journal audio-visual (MC-WSJ-AV) corpus <ref type="bibr" target="#b101">[102]</ref>, where the speakers are recording in a large room of ? = 0.7 seconds at a near (1 m) and a far (2.5 m) microphone conditions. The training set includes 7861 paired utterances from the simulated data. The test set contains both simulated paired utterances (2176) and real reverberant utterances (372). Different room recordings are used for the training and test sets. The datasets were originally captured in a single-channel, two-channel and eight-channel configuration with a 16 kHz sampling frequency. However, for the scope of this study, we only use the single-channel configuration.</p><p>3) Super-resolution: For comparative analysis, we utilize the English multi-speaker corpus (VCTK) <ref type="bibr" target="#b102">[103]</ref>. The VCTK dataset contains 44 hours recordings from 108 speakers with various English accents. For the super-resolution experiment, we follow the design choice of <ref type="bibr" target="#b82">[83]</ref>, where the low-resolution audio signal is generated from the 16 kHz original tracks by subsampling the signal with the desired upscaling ratio (s). The first task uses a single VCTK speaker (p225), the first 223 recordings are used for training and the last 8 recordings are used for testing. The second task takes the first 100 VCTK speakers as the training set and tests on the last 8 speakers. The upscaling ratios for both the single-speaker and the multispeaker tasks are set to {2, 4, 8}, representing a reconstruction from 8 kHz, 4 kHz, 2 kHz to 16 kHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental setup</head><p>The utterances in the training set are sliced into 2 seconds, while in the test set, no slicing is utilized and the length is kept variable. A Hamming window with 25 ms window length (400-point FFT) and hop size of 6.25 ms (75% overlap) is employed. Thus, the resultant spectrogram will have 200 frequency bins F , while the time dimension T depends on the variable track duration. The number of two-stage conformer blocks N , the batch size B and the channel number C in the generator are set to 4, 4 and 64, respectively. The channel numbers in the metric discriminator are set to {16, 32, 64, 128}. In the training stage, AdamW optimizer <ref type="bibr" target="#b105">[106]</ref> is used for both the generator and the discriminator to train for 50 epochs. The learning rate is set to 5?10 ?4 for the generator and 1?10 ?3 for the discriminator. A learning rate scheduler is applied with a decay factor of 0.5 every 12 epochs. In the generator loss L G , the weights are set to {? 1 = 1, ? 2 = 0.01, ? 3 = 1}. Audio samples and CMGAN implementations are available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Denoising</head><p>Objective scores: We choose a set of commonly used metrics to evaluate the denoised speech quality, i.e., PESQ with a score range from -0.5 to 4.5, segmental signal-to-noise ratio (SSNR) and composite mean opinion score (MOS) <ref type="bibr" target="#b106">[107]</ref> based metrics: MOS prediction of the signal distortion (CSIG), MOS prediction of the intrusiveness of background noise (CBAK) and MOS prediction of the overall effect (COVL), all of them are within a score range of 1 to 5. Additionally, we utilize STOI with a score range from 0 to 1 to judge speech intelligibility. Higher values indicate better performance for all given metrics.</p><p>Results analysis: Our proposed CMGAN is objectively compared with other state-of-the-art (SOTA) denoising baselines, as shown in <ref type="table" target="#tab_0">Table I</ref>. For the time-domain methods, we included the standard SEGAN <ref type="bibr" target="#b13">[14]</ref> and three recent methods: TSTNN <ref type="bibr" target="#b15">[16]</ref>, DEMUCS <ref type="bibr" target="#b16">[17]</ref> and SE-Conformer <ref type="bibr" target="#b17">[18]</ref>. For the TF-domain methods, we evaluate six recent SOTA methods, i.e., MetricGAN <ref type="bibr" target="#b10">[11]</ref>, PHASEN <ref type="bibr" target="#b11">[12]</ref>, PFPL <ref type="bibr" target="#b103">[104]</ref>, Metric-GAN+ <ref type="bibr" target="#b104">[105]</ref>, DB-AIAT <ref type="bibr" target="#b12">[13]</ref> and DPT-FSNet <ref type="bibr" target="#b25">[26]</ref>. It can be observed that most of the TF-domain methods outperform the time-domain counterparts over all utilized metrics. Moreover, our proposed TF conformer-based approach shows a major improvement over the time-domain SE-Conformer. Compared to frameworks involving metric discriminators (MetricGAN+), we have 0.26, 0.49, 0.78 and 0.48 improvements on the PESQ, CSIG, CBAK and COVL scores, respectively. Finally, our framework also outperforms recent improved transformerbased methods, such as DB-AIAT and DPT-FSNet in all of the evaluation scores with a relatively low model size of only 1.83 M parameters.</p><p>Ablation study: To verify our design choices, an ablation study is conducted, as shown in <ref type="table" target="#tab_0">Table II</ref>. We first investigate the influence of different inputs. Magnitude-only denotes that only magnitude is used as the input and the enhanced magnitude is then combined with the noisy phase for ISTFT  <ref type="bibr" target="#b6">[7]</ref>. "-" denotes the result is not provided in the original paper. Model size represents the number of trainable parameters in million.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Year operation. The network architecture remains the same, except the complex decoder is removed. Correspondingly, Complexonly denotes only the complex spectrogram is used as an input and the mask decoder is removed. Comparison shows that lacking the phase enhancement decreases the PESQ score by 0.18, while using a pure complex spectrogram reduces the SSNR score by 1.91 dB. This result indicates that although the complex spectrogram contains magnitude information, it is challenging for the utilized framework to enhance the magnitude implicitly. Moreover, explicitly addressing the magnitude in the loss functions would mitigate the compensation effect stated in Sec. II-A. Additionally, to validate the chosen mapping-based approach in the complex refinement branch, we keep the mask decoder unchanged and modify the complex decoder to involve a cIRM similar to <ref type="bibr" target="#b20">[21]</ref>. The comparison between CMGAN-cIRM and CMGAN shows a significant drop in both PESQ and SSNR scores. On the other hand, the result shows that the absence of time loss (w/o Time loss) further improves the PESQ score to 3.45, while the SSNR is slightly lower than the original CMGAN. This indicates the effectiveness of the time loss in balancing the performance for both PESQ and SSNR scores. We conducted two tests to demonstrate the discriminator choice: removing the discriminator (w/o Disc.) and replacing the metric discriminator with a patch discriminator, which is commonly used in image generation tasks <ref type="bibr" target="#b107">[108]</ref>. It can be realized that removing the discriminator negatively impacted all the given scores. Similarly, adding a patch discriminator only showed a marginal improvement, which reflects that the generator is fully capable of enhancing the tracks without the aid of a normal patch discriminator. However, a metric discriminator to directly improve the evaluation scores is proven to be beneficial. Furthermore, we investigate the influence of the two-stage conformer outline. Given an input feature map, the two-stage conformer will separately focus on the time and frequency dimensions. To this end, two different configurations can be proposed, either sequential or parallel. Accordingly, we compare our sequential CMGAN to a parallel connection counterpart without any further modifications (Parallel-Conformer). The results illustrate that the parallel approach is behind the proposed sequential, i.e., the PESQ and SSNR scores are reduced by 0.06 and 0.47 dB, respectively. Also, we flipped the order of the sequential conformer blocks (Freq. ? Time) and we can conclude that the scores are similar with a minor improvement in favor of the standard CMGAN (Time ? Freq.). Note that designing a single conformer to attend over both time and frequency is theoretically possible. However, in this case, the complexity will grow exponentially <ref type="bibr" target="#b108">[109]</ref>. To demonstrate the decoder decoupling requirement in complex refinement, we replace the original mask/complex decoders with a single-path decoder. The final output would represent three channels, the first channel is followed by a PReLU activation (magnitude) and no activation is given to the other two channels (complex). Comparing single-path decoder to mask/complex decoders shows a degradation in all metrics, especially the SSNR score (0.91 dB).</p><p>Preliminary literature mostly assumes the predicted magnitude mask to be between 0 and 1. Hence, sigmoid activation is usually the preferred activation to reflect this interval <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b104">[105]</ref>. Although this is true, a bounded sigmoid function would restrict the model to allocate values between 0  and 0.5 to all aggregated negative activations from the previous layer. On the other hand, an unbounded activation function such as PReLU could automatically learn this interval while mitigating the negative activations issue by learning a relevant slope to each frequency band as explained in Sec. III-A. To confirm this assumption, we construct a histogram of several magnitude masks from different noisy tracks. As shown in <ref type="figure">Fig. 5</ref>, the PReLU activations would always lie in the 0 to 1 interval. Moreover, the majority of low activations are assigned to frequency bands above 5 kHz (beyond human speech) <ref type="bibr" target="#b109">[110]</ref>.</p><p>We also extend our ablation study to involve different bounded and unbounded activations for the mask decoder, namely sigmoid, ReLU and the soft version of ReLU (softplus) <ref type="bibr" target="#b110">[111]</ref>. According to <ref type="table" target="#tab_0">Table II</ref>, both sigmoid and ReLU activations are comparable and they report lower scores than CMGAN with PReLU activation. Softplus achieves slightly higher PESQ, but at the expense of other metrics. Finally, we experiment with the number of TS-Conformer blocks. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, the performance of CMGAN without any conformer blocks is acceptable and even comparable with other SOTA methods, such as MetricGAN. However, only one conformer block effectively improves the PESQ by 0.4. The performance gradually increases with more blocks until no further improvement is observed after four blocks. Due to space constraints, the original CMGAN will be considered for upcoming tasks with few relevant ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dereverberation</head><p>Objective scores: For dereverberation, we utilize the recommended measures in the REVERB challenge paper <ref type="bibr" target="#b7">[8]</ref>: cepstrum distance (CD) <ref type="bibr" target="#b111">[112]</ref>, log-likelihood ratio (LLR) <ref type="bibr" target="#b112">[113]</ref>, frequency weighted segmental SNR (FWSegSNR) <ref type="bibr" target="#b113">[114]</ref> and speech-to-dereverberation modulation energy ratio (SRMR) <ref type="bibr" target="#b114">[115]</ref>. The paper also recommended PESQ as an optional measure, although most of the latest dereverberation literature did not take it into account. For outliers reduction, authors in <ref type="bibr" target="#b106">[107]</ref> suggested limiting the ranges of CD to [0,10] and LLR to <ref type="bibr">[0,</ref><ref type="bibr" target="#b1">2]</ref>. Lower values indicate better scores for CD and LLR, while higher values indicate better speech quality for FWSegSSNR, PESQ and SRMR. The CD, LLR, FWSegSNR and PESQ are chosen as they correlate to listening tests, albeit they are all intrusive scores, i.e., enhanced speech and clean reference are required. Accordingly, SRMR is employed as a non-intrusive score to operate on enhanced speech without a clean reference. Thus, it is quite important to measure the quality and intelligibility of enhanced unpaired real recordings.</p><p>Results: For quantitative analysis, the CMGAN is compared with recent dereverberation methods. As discussed in Sec. II-B, using time-domain approaches in dereverberation is limited and these methods did not use the REVERB challenge data. Thus, the chosen methods would all consider TF-domain analysis. For fair comparison, only papers recording individual room scores are considered. Based on this criteria, we compare against four recent methods: Xiao et al. <ref type="bibr" target="#b78">[79]</ref>, U-Net <ref type="bibr" target="#b75">[76]</ref>, wide residual network (WRN) <ref type="bibr" target="#b77">[78]</ref> and SkipConvNet <ref type="bibr" target="#b76">[77]</ref>. Unfortunately, none of these papers reported the PESQ scores, so it is excluded from the comparative analysis. However, PESQ is still used as the objective score to be maximized by the metric discriminator in CMGAN.</p><p>The results for both near and far microphone cases are shown in <ref type="table" target="#tab_0">Table III</ref> and IV, respectively. The first four columns represent the simulated data results for the three different room sizes (small -room 1, medium -room 2, largeroom 3 and average score). The last column represents the SRMR of the real recordings. As expected, larger rooms and further microphone placements result in lower scores, as these scenarios would introduce more distortions to the speech. In the simulated near microphone case, the proposed CMGAN shows superior performance compared to other methods in the majority of metrics, particularly FWSegSNR. For SRMR, Xiao et al. reports a higher SRMR score on simulated near data, but a significant drop is observed in near real recordings. SkipConvNet achieves better real SRMR scores in the near  Ablation study: To validate the PESQ choice for metric discriminator, we introduce a CMGAN variant operating on LLR as the objective metric discriminator score (CMGAN-LLR). LLR is chosen as it reflects a bounded metric and based on the LS-GAN formulation <ref type="bibr" target="#b97">[98]</ref>, the metric discriminator is more robust when the optimization space is bounded by a normalized score. Accordingly, we modify Eq. 6 to involve the normalized LLR scores Q LLR instead of Q P ESQ and the term 1 is changed to 0 in both L GAN and L D . Thus, the score is minimized to 0 instead of maximized to 1. It can be shown in <ref type="table" target="#tab_0">Table III</ref> and IV that the LLR score is marginally better than the original CMGAN trained with PESQ. However, a considerable improvement is observed in SRMR scores for both simulated and real recordings, especially in the near microphone case. Moreover, the CMGAN-LLR variant outperforms the SkipConvNet in real recordings for near and far microphone cases by 0.44 and 0.75, respectively. Comparing both CMGAN and CMGAN-LLR shows a balanced performance over most of the given metrics in favor of the standard proposed CMGAN, which indicates that the PESQ is a robust metric to optimize and is highly correlated with most of the given quality metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Super-resolution</head><p>Objective scores: Two metrics, log-spectral distance (LSD) and signal-to-noise ratio (SNR), are used to evaluate superresolution. Based on our literature review, the LSD definition is not the same for all papers. Mathematically, LSD measures the log distance between the magnitude spectrogram component of the enhanced speech with respect to the clean reference. Some papers would use the log to the base e, while others would evaluate the log to base 10. In both definitions, the STFT is evaluated with a Hanning window of 2048 samples and a hop size of 512. To ensure a fair comparison, the same STFT parameterization is used and the LSD results based on the two different definitions in the literature are presented. A lower LSD and a higher SNR represent better speech quality.</p><p>Results: Since masking-based methods are not relevant for the super-resolution task, as previously stated in Sec. II-C. Therefore, the CMGAN mask decoder part is modified by involving an element-wise addition instead of element-wise multiplication. This is reflected in Eq. 3 as follows:</p><formula xml:id="formula_8">X r = (M + Y m ) cos Y p +X r X i = (M + Y m ) sin Y p +X i<label>(9)</label></formula><p>where M represents the modified output of the mask decoder. Unlike the prior cases of denoising and dereverberation, the network is not learning mask activations between 0 and 1 to suppress the noise and preserve the speech, but rather activations that can complete the missing high-frequency bands while preserving the given low-frequency bands. As shown in <ref type="table" target="#tab_5">Table V</ref>, we compare our approach with five other methods: the U-Net architecture proposed by Kuleshov et al. <ref type="bibr" target="#b82">[83]</ref>, TFiLM <ref type="bibr" target="#b84">[85]</ref>, AFiLM <ref type="bibr" target="#b85">[86]</ref>, hybrid TFNet <ref type="bibr" target="#b83">[84]</ref>, hybrid AE <ref type="bibr" target="#b86">[87]</ref> and NVSR <ref type="bibr" target="#b87">[88]</ref>. All the scores are from the In the VCTK-Single experiment, our method achieved the best score in all three metrics on scale 2 when converting the audio signal from 8 kHz to 16 kHz, especially in SNR, a 2.3 dB improvement compared to the SOTA AE method. As for scale 4, the AE method shows a marginal improvement of 0.3 dB and 0.1 in SNR and LSD 10 , respectively. In the scale 8 task, our method exceeds other methods in terms of LSD e and LSD 10 . However, the SNR is lower than TFNet and similar to TFiLM and AFiLM approaches. We hypothesize that this is accounted for the limited training samples in the VCTK-Single dataset, which can lead to model overfitting. On the other hand, in the VCTK-Multi. evaluation, our method outperforms other approaches in all upscaling ratios on all metrics. Specifically, our method has an improvement of 2.3 dB, 1.0 dB, and 2.1 dB on SNR on scales 2/4/8. Note that CMGAN has a much better performance on scale 8 compared to the same scale in VCTK-Single evaluation, which verifies the overfitting assumption.</p><p>Ablation study: To demonstrate the effectiveness of complex TF-domain super-resolution. The CMGAN is modified to eliminate both complex decoder and metric discriminator, leaving only the magnitude loss (CMGAN-Mag.). A substantial improvement in both LSD e and LSD 10 is observed when the complex branch is removed and this is expected as the LSD is defined in magnitude component only. This LSD gain comes at the expense of a significant drop in the SNR scores, which considers the reconstructed time-domain signal. Thus, removing the complex branch would give a push in the LSD as the network would focus only on enhancing the magnitude component but with a degradation in the overall signal quality.</p><p>An illustration of the input, predicted and reference tracks from a scale 4 example is depicted in <ref type="figure" target="#fig_6">Fig. 7</ref>. Excitation of high-frequency bands are clear in the output mask M . Comparing <ref type="figure" target="#fig_6">Fig. 7c and 7d</ref> shows the potential of the CM-GAN in constructing missing high-frequency bands just from observing different speech phonetics in the training data. This performance is also reflected as an accurate interpolation of intermediate samples in the time-domain <ref type="figure" target="#fig_6">Fig. 7e, 7f</ref> and 7g. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SUBJECTIVE EVALUATION</head><p>Till now, the proposed architecture is quantitatively compared to different SOTA methods using objective metrics scores. Although these scores can serve as an indication of how well is the proposed method, they still cannot fully replace the subjective quality measure. Since subjective listening tests are costly and time consuming as it requires many participants and ideal listening conditions. Therefore, finding an objective measure that can highly correlate with the subjective quality score is still an open research topic <ref type="bibr" target="#b115">[116]</ref>. The most noticeable work in this area is introduced in <ref type="bibr" target="#b106">[107]</ref>, where the authors proposed a composite mean opinion score (MOS) based on traditional regression analysis methods <ref type="bibr" target="#b116">[117]</ref>. Note that these scores are used in Sec. V-A to evaluate speech denoising performance. The study involved 1792 speech samples rated according to ITU-T P.835 standards <ref type="bibr" target="#b117">[118]</ref> and well-established objective measures such as PESQ, segmental SNR, LLR and weighted spectral slope (WSS) <ref type="bibr" target="#b118">[119]</ref> are utilized as basis functions for construction of three different composite scores reflecting the signal distortion, background noise and overall quality. The proposed composite measure reported a correlation of 0.9 to 0.91 with the subjective ratings and the authors emphasized the importance of PESQ as it shows the highest correlation (0.89). However, this study is limited to only four background noise types under two SNR conditions (5 and 10 dB) and most importantly, the proposed scores are intrusive (requiring both paired clean and enhanced speech).</p><p>Recently, DNNs have been utilized for finding a subjective alternative score <ref type="bibr" target="#b119">[120]</ref>- <ref type="bibr" target="#b124">[125]</ref>. Unlike the previous composite measure, most of these methods will take the track as an input and the network is trained to mimic the subjective ratings. Thus, the scores will not depend on non-optimal objective scores, but rather on the whole track. Additionally, these scores are non-intrusive, hence evaluating enhanced tracks without the need for clean reference is possible. The standard score used as a subjective baseline for many recent studies is the DNSMOS proposed by Microsoft in <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b124">[125]</ref>. The DNSMOS is trained on 75 hours of rated speech. In accordance to ITU-T P.835, listeners assign a score between 1 and 5 (higher is better) for signal distortion, background noise, and overall quality. A significant correlation of 0.94 to 0.98 is reported over the three given quality assessment scores.</p><p>Following the literature, the DNSMOS will be evaluated as our subjective evaluation metric. Due to limited space and non-availability of open-source implementations, especially in dereverberation. The subjective evaluation will focus on the denoising aspect of the SE problem. Accordingly, four different denoising use-cases are included in this study to indicate the generalization capability of the network to unseen noise conditions, real noise samples and additional distortions not included in training. To this end, the frameworks will be all trained on a single use-case (Voice Bank+DEMAND), then the models will be evaluated on four different datasets: (a) Voice Bank+DEMAND test set <ref type="bibr" target="#b6">[7]</ref>: including 35 minutes (824 tracks) of noisy speech from two unseen speakers using noise types from DEMAND dataset <ref type="bibr" target="#b99">[100]</ref> which are not included in the training as explained in Sec. IV-A. (b) CHiME-3 <ref type="bibr" target="#b8">[9]</ref>: including 7.8 hours (4560 tracks) of real noisy speech recordings from 12 speakers at four different environments: bus, cafe, pedestrian area and street junction. In this data, no clean reference tracks are available. (c) DNS challenge <ref type="bibr" target="#b9">[10]</ref>: the original data includes 1934 English speaker reading speech samples from Librivox 2 and 181 hours of 150 different noise types from Audio Set <ref type="bibr" target="#b125">[126]</ref> and Freesound 3 . Based on this dataset, we construct 9 hours (3240 tracks) of noisy speech with SNRs from 0 to 10 dB. (d) DNS challenge+Reverb.: we use the same 9 hours, but we simulate reverberant conditions on the speech, then we add the same noise in the DNS challenge part. The RIRs are chosen from openSLR26/28 <ref type="bibr" target="#b126">[127]</ref>, including 248 real and 60k synthetic conditions. The RIRs are recorded in three different room sizes with a 60 dB reverberation time of 0.3-1.3 seconds.</p><p>All tracks are resampled to 16 kHz and the ratio of male-tofemale speakers is 50%. From <ref type="table" target="#tab_0">Table I</ref>, we choose a representative for each denoising paradigm. The methods were chosen based on the availability of open-source implementations and the reproducibility of the reported results in the corresponding papers. As a representative for metric discriminator, we used the MetricGAN+ <ref type="bibr" target="#b104">[105]</ref>. For time-domain methods, DEMUCS <ref type="bibr" target="#b16">[17]</ref> is selected. For TF-domain complex denoising, PHASEN <ref type="bibr" target="#b11">[12]</ref> is chosen as it attempts to correct magnitude and phase components. In addition to, PFPL <ref type="bibr" target="#b103">[104]</ref>   complex-valued network to enhance both real and imaginary parts. Most of the papers provided an official implementation with pretrained models. PHASEN is the only exception, as a non-official code is used and we trained the model to reproduce the results in the paper. For DEMUCS, the available model is pretrained on both Voice Bank+DEMAND and DNS challenge data. Thus, we retrain DEMUCS using the recommended configuration on Voice Bank+DEMAND data only to ensure a fair comparison between all presented models.</p><p>For space limitations, only the DNSMOS of the overall speech quality is reported, as shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. From the boxplots, CMGAN is outperforming all methods in the four use-cases. For instance, CMGAN shows an average improvement of 0.15 in comparison to the most competitive approach (PFPL) in the first three use-cases. Moreover, the interquartile range of the CMGAN is much narrower than all other methods, which indicates a low variance and thus a confident prediction, especially in the DNS challenge ( <ref type="figure" target="#fig_7">Fig. 8c</ref>). On the other hand, MetricGAN+ is showing the worst performance in all use-cases. We hypothesize that although the PESQ score is relatively high <ref type="bibr">(3.15)</ref>, the SSNR score that we calculate is below 1 dB, indicating that the metric discriminator in MetricGAN+ case, is only focusing on enhancing the PESQ at the expense of other metrics. Note that the SSNR score is not reported in the original paper. DEMUCS representing the time-domain paradigm is showing a robust performance over Voice Bank+DEMAND and real CHiME-3 use-cases. However, it is not generalizing to the DNS challenge dataset. This generalization issue is clearly mitigated in the TF-domain complex denoising methods (PHASEN, PFPL and CMGAN). From <ref type="figure" target="#fig_7">Fig. 8d</ref>, the overall DNSMOS of DNS challenge with additional reverberation dropped by 0.5 on average in comparison to DNS challenge <ref type="figure" target="#fig_7">(Fig. 8c</ref>). This is expected as generalizing to unseen effects such as reverberation is more challenging than unseen noise types. Despite of this drop, CMGAN is still showing superior performance over other competitive TF-domain approaches (PHASEN and PFPL). Additional visualization of the presented subjective evaluation models is presented in the Appendix section. Audio samples from all subjective evaluation methods are available online 4 for interested readers.</p><p>Despite the above results, this study is not without limitations. For instance, CMGAN is not yet tested for real-time speech enhancement, i.e., CMGAN can access the whole track.</p><p>In the future, CMGAN should be modified to only access few TF bins from the old samples and not the entire track, together with an extensive study on the exact amount of floating point operations in the real-time scenario. Due to space constraints, we focused on experimenting with each task separately. The superimposed effect (denoising and dereverberation) is only briefly addressed in the subjective evaluation part, so training and evaluating CMGAN for this use-case would be an important extension of our work. VII. CONCLUSIONS This paper introduces CMGAN as a unified framework operating on both magnitude and complex spectrogram components for various speech enhancement tasks, including denoising, dereverberation and super-resolution. Our approach combines recent conformers that can capture long-term dependencies as well as local features in both time and frequency dimensions, together with a metric discriminator that resolves metric mismatch by directly enhancing non-differentiable evaluation scores. Experimental results demonstrate that the proposed method achieves superior or competitive performance against SOTA methods in each task with relatively few parameters (1.83 M). Additionally, we conduct an ablation study to verify the fragmented benefits of each utilized component and loss in the proposed CMGAN framework. Finally, subjective evaluation illustrates that CMGAN outperforms other methods with a robust generalization to unseen noise types and distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the Institute of Natural Language Processing, University of Stuttgart for providing useful datasets to support this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>This section presents a visualization of the CMGAN in comparison to subjective evaluation methods. A wide-band nonstationary cafe noise from the DEMAND dataset (SNR = 0 dB) and a narrow-band high-frequency stationary doorbell noise from the Freesound dataset (SNR = 3 dB) are used to evaluate the methods. Both noises are added to sentences from the DNS challenge. Comparisons are made between time-domain, TF-magnitude, and TF-phase representations for comprehensive performance analysis. Since the phase is unstructured, we utilize the baseband phase difference (BPD) approach proposed in <ref type="bibr" target="#b127">[128]</ref> to enhance the phase visualization. From <ref type="figure">Fig. 9</ref>, MetricGAN+, DEMUCS and PHASEN show the worst performance by confusing speech with noise, particularly in the 1.5 to 2 seconds interval (similar speech and noise powers). The distortions and missing speech segments are annotated in the time and TF-magnitude representations by ( ) and ( ), respectively. Moreover, the denoised phase in methods employing only magnitude (MetricGAN+) and time-domain (DEMUCS) is very similar to the noisy input, in contrast to clear enhancement in complex TF-domain methods (PHASEN, PFPL and CMGAN). PFPL and CMGAN exhibit the best performance, with better phase reconstruction in CMGAN (1.5 to 2 seconds interval).</p><p>In general, stationary noises are less challenging than nonstationary counterparts. However, stationary noises are not represented in the training data. As depicted in <ref type="figure">Fig. 10</ref>, methods such as MetricGAN+ and PHASEN are showing a poor generalization performance, with doorbell distortions clearly visible at frequencies (3.5, 5, and 7 kHz). On the other hand, the performance is slightly better in DEMUCS and PFPL, whereas CMGAN perfectly attenuates all distortions. Note that high-frequency distortions are harder to spot in the timedomain than in TF-magnitude and TF-phase representations. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Low-res. track (s = 4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>(a) Encoder-decoders generator architecture (b) Two-stage conformer (TS-Conformer) (c) Metric discriminator An overview of the proposed CMGAN architecture III. METHODOLOGY</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>PReLU slopes of the resultant magnitude mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Non-adversarial generator losses (b) Discriminator loss (c) Adversarial generator loss An illustration of the propagated loss functions in the CMGAN architecture. For simplicity, X andX denote the three-channel magnitude and complex representations of the clean target and the estimated output spectrograms, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 1Fig. 5 :</head><label>55</label><figDesc>Fr eq ue nc y [k H z] A c ti v a ti o n [ A .U .] Probability Histogram of masking PReLU activations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Influence of TS-Conformer blocks on objective scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Example of scale 4 super-resolution (4 kHz ? 16 kHz). The upper row represents the TF-magnitude representations of the relevant spectrograms. The bottom row shows a 20 ms segment of the corresponding time-domain signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>DNSMOS of subjective evaluation methods tested on four different datasets. In the boxplots, the mean is represented by ( ), median (?) and the width of each box indicates the interquartile range (25 th and 75 th percentile). The whiskers show the maximum and minimum values excluding the outliers ( ). The mean value for each method is presented on the x-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Visualization of subjective approaches under a wide-band cafe noise (DEMAND dataset) at SNR = 0 dB. (a-g) represent the time-domain signal, while (h-n) are the TF-magnitude representations in dB and (o-u) are the reconstructed BPD of the given TF-phase representations. ( ) and ( ) reflect the distortions in time and TF-magnitude representations, respectively. Visualization of subjective approaches under a narrow-band doorbell noise (Freesound dataset) at SNR = 3 dB. (a-g) represent the time-domain signal, while (h-n) are the TF-magnitude representations in dB and (o-u) are the reconstructed BPD of the given TF-phase representations. ( ) and ( ) reflect the distortions in time and TF-magnitude representations, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Performance comparison on the Voice Bank+DEMAND dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Results of the denoising ablation study.</figDesc><table><row><cell>Method</cell><cell>PESQ CSIG CBAK COVL SSNR STOI</cell></row><row><cell cols="2">CMGAN Magnitude-only 3.23 4.60 3.76 4.00 9.82 0.95 3.41 4.63 3.94 4.12 11.10 0.96 Complex-only 3.35 4.56 3.79 4.05 9.19 0.96 CMGAN-cIRM 3.28 4.60 3.83 4.03 10.40 0.96 w/o Time loss 3.45 4.56 3.86 4.11 9.71 0.96 w/o Disc. 3.24 4.46 3.82 3.93 10.56 0.96 Patch Disc. 3.28 4.48 3.85 3.96 10.75 0.96 Parallel-Conf. 3.35 4.54 3.87 4.03 10.63 0.96 Freq. ? Time 3.39 4.56 3.91 4.07 10.84 0.96 Single Dec. 3.38 4.54 3.86 4.05 10.19 0.96</cell></row><row><cell></cell><cell>Magnitude mask activation function</cell></row><row><cell>Sigmoid ReLU</cell><cell>3.34 4.52 3.80 4.02 10.70 0.96 3.32</cell></row></table><note>4.54 3.80 4.04 10.69 0.96 Softplus 3.43 4.58 3.83 4.02 10.75 0.96</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Results of simulated and real data on near microphone case. .57 2.45 2.29 0.19 0.30 0.35 0.28 13.07 10.96 10.22 11.42 4.99 4.75 4.56 4.77 7.27 CMGAN 1.46 2.14 2.27 1.96 0.14 0.25 0.34 0.24 14.36 13.49 11.69 13.18 5.42 5.74 5.29 5.48 6.49 CMGAN-LLR 1.69 2.56 2.43 2.23 0.15 0.25 0.25 0.22 14.48 12.49 11.03 12.67 5.48 5.80 6.02 5.77 7.71</figDesc><table><row><cell>Room</cell><cell>1</cell><cell>2</cell><cell>CD ? 3</cell><cell>Avg.</cell><cell>1</cell><cell>LLR ? 2 3</cell><cell>Avg.</cell><cell>1</cell><cell>FWSegSNR ? 2 3</cell><cell>Avg.</cell><cell>1</cell><cell>SRMR ? 2 3</cell><cell>Avg.</cell><cell>SRMR-real ? -</cell></row><row><cell cols="10">Reverberant speech 1.99 4.63 4.38 3.67 0.35 0.49 0.65 0.50 8.12 3.35 2.27</cell><cell cols="4">4.58 4.50 3.74 3.57 3.94</cell><cell>3.17</cell></row><row><cell>Xiao et al. [79]</cell><cell cols="9">1.58 2.65 2.68 2.30 0.37 0.50 0.52 0.46 9.79 7.27 6.83</cell><cell cols="4">7.96 5.74 6.49 5.86 6.03</cell><cell>4.29</cell></row><row><cell>WRN [78]</cell><cell cols="9">2.02 4.61 4.15 3.59 0.36 0.46 0.60 0.47 8.28 3.57 2.54</cell><cell cols="4">4.80 4.04 3.46 3.27 3.59</cell><cell>-</cell></row><row><cell>U-Net [76]</cell><cell cols="13">1.75 2.58 2.53 2.28 0.20 0.41 0.45 0.35 13.32 10.87 10.40 11.53 4.51 5.09 4.94 4.85</cell><cell>5.47</cell></row><row><cell>SkipConvNet [77]</cell><cell>1.86 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Results of simulated and real data on far microphone case. but worse on the simulated data. U-Net and SkipConvNet report overall competitive scores, although CMGAN outperforms in average CD and FWSegSNR with 0.3 and 1.65 dB, respectively. For the far microphone, CMGAN is still able to show a gain in overall scores, especially FWSegSNR. Xiao et al. is still slightly better in SRMR for simulated data, but the gap is much closer than the near microphone case, only 0.05 on average. The same holds for SkipConvNet with slightly better real SRMR scores than the proposed CMGAN.</figDesc><table><row><cell>case</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Room</cell><cell>1</cell><cell>2</cell><cell>CD ? 3</cell><cell>Avg.</cell><cell>1</cell><cell>LLR ? 2 3</cell><cell>Avg.</cell><cell>1</cell><cell>FWSegSNR ? 2 3</cell><cell>Avg.</cell><cell>1</cell><cell>SRMR ? 2 3</cell><cell>Avg.</cell><cell>SRMR-real ? -</cell></row><row><cell cols="14">Reverberant speech 2.67 5.21 4.96 4.28 0.38 0.75 0.84 0.66 6.68 1.04 0.24 2.65 4.58 2.97 2.73 3.43</cell><cell>3.19</cell></row><row><cell>Xiao et al. [79]</cell><cell cols="13">1.92 3.17 2.99 2.69 0.41 0.61 0.58 0.53 9.12 6.31 5.97 7.13 5.67 5.80 5.03 5.50</cell><cell>4.42</cell></row><row><cell>WRN [78]</cell><cell cols="13">2.43 4.99 4.56 3.99 0.35 0.59 0.67 0.54 7.54 1.79 0.88 3.40 4.48 3.32 2.84 3.55</cell><cell>-</cell></row><row><cell>U-Net [76]</cell><cell cols="13">2.05 3.19 2.92 2.72 0.26 0.57 0.56 0.46 12.08 9.00 9.05 10.04 4.76 5.27 4.71 4.91</cell><cell>5.68</cell></row><row><cell>SkipConvNet [77]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>2.12 3.06 2.82 2.67 0.22 0.46 0.46 0.38 11.80 8.88 8.16 9.61 5.10 4.76 4.25 4.70 6.87 CMGAN 1.88 2.90 2.85 2.54 0.24 0.43 0.47 0.38 11.65 10.34 8.91 10.30 5.78 5.87 4.69 5.45 6.61 CMGAN-LLR 2.07 3.32 3.05 2.81 0.24 0.46 0.40 0.37 11.21 9.22 9.48 9.97 5.93 5.54 5.19 5.55 7.62</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Performance comparison for super-resolution, "-" denotes the result is not provided in the original paper. LSD e ? LSD 10 ? SNR ? LSD e ? LSD 10 ? SNR ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>VCTK-Single</cell><cell></cell><cell></cell><cell>VCTK-Multi.</cell><cell></cell></row><row><cell cols="2">Method s U-Net [83] 2</cell><cell>3.2</cell><cell>-</cell><cell>21.1</cell><cell>3.1</cell><cell>-</cell><cell>20.7</cell></row><row><cell>TFiLM [85]</cell><cell>2</cell><cell>2.5</cell><cell>-</cell><cell>19.5</cell><cell>1.8</cell><cell>-</cell><cell>19.8</cell></row><row><cell>AFILM [86]</cell><cell>2</cell><cell>2.3</cell><cell>-</cell><cell>19.3</cell><cell>1.7</cell><cell>-</cell><cell>20.0</cell></row><row><cell>AE [87]</cell><cell>2</cell><cell>-</cell><cell>0.9</cell><cell>22.4</cell><cell>-</cell><cell>0.9</cell><cell>22.1</cell></row><row><cell>NVSR [88]</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.8</cell><cell>-</cell></row><row><cell>CMGAN</cell><cell>2</cell><cell>1.7</cell><cell>0.7</cell><cell>24.7</cell><cell>1.6</cell><cell>0.7</cell><cell>24.4</cell></row><row><cell cols="2">CMGAN-Mag. 2</cell><cell>1.4</cell><cell>0.6</cell><cell>22.2</cell><cell>1.3</cell><cell>0.6</cell><cell>23.4</cell></row><row><cell>U-Net [83]</cell><cell>4</cell><cell>3.6</cell><cell>-</cell><cell>17.1</cell><cell>3.5</cell><cell>-</cell><cell>16.1</cell></row><row><cell>TFiLM [85]</cell><cell>4</cell><cell>3.5</cell><cell>-</cell><cell>16.8</cell><cell>2.7</cell><cell>-</cell><cell>15.0</cell></row><row><cell>AFILM [86]</cell><cell>4</cell><cell>3.1</cell><cell>-</cell><cell>17.2</cell><cell>2.3</cell><cell>-</cell><cell>17.2</cell></row><row><cell>TFNet [84]</cell><cell>4</cell><cell>-</cell><cell>1.3</cell><cell>18.5</cell><cell>-</cell><cell>1.3</cell><cell>17.5</cell></row><row><cell>AE [87]</cell><cell>4</cell><cell>-</cell><cell>0.9</cell><cell>18.9</cell><cell>-</cell><cell>1.0</cell><cell>18.1</cell></row><row><cell>NVSR [88]</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.9</cell><cell>-</cell></row><row><cell>CMGAN</cell><cell>4</cell><cell>2.3</cell><cell>1.0</cell><cell>18.6</cell><cell>2.2</cell><cell>1.0</cell><cell>19.1</cell></row><row><cell cols="2">CMGAN-Mag. 4</cell><cell>1.7</cell><cell>0.7</cell><cell>16.9</cell><cell>1.8</cell><cell>0.8</cell><cell>16.1</cell></row><row><cell>TFiLM [85]</cell><cell>8</cell><cell>4.3</cell><cell>-</cell><cell>12.9</cell><cell>2.9</cell><cell>-</cell><cell>12.0</cell></row><row><cell>AFILM [86]</cell><cell>8</cell><cell>3.7</cell><cell>-</cell><cell>12.9</cell><cell>2.7</cell><cell>-</cell><cell>12.0</cell></row><row><cell>TFNet [84]</cell><cell>8</cell><cell>-</cell><cell>1.9</cell><cell>15.0</cell><cell>-</cell><cell>1.9</cell><cell>12.0</cell></row><row><cell>NVSR [88]</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.1</cell><cell>-</cell></row><row><cell>CMGAN</cell><cell>8</cell><cell>2.6</cell><cell>1.1</cell><cell>12.9</cell><cell>2.7</cell><cell>1.2</cell><cell>14.1</cell></row><row><cell cols="2">CMGAN-Mag. 8</cell><cell>1.9</cell><cell>0.8</cell><cell>10.9</cell><cell>2.0</cell><cell>0.9</cell><cell>10.9</cell></row><row><cell cols="8">corresponding original papers. The value s = 2/4/8 implies upsampling scale from 8 kHz/4 kHz/2 kHz to 16 kHz speech.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ruizhecao96/CMGAN/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://librivox.org/ 3 https://freesound.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://sherifabdulatif.github.io/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CMGAN: Conformer-based metric GAN for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdulatif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech, 2022</title>
		<meeting>Interspeech, 2022</meeting>
		<imprint>
			<biblScope unit="page" from="936" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on latent variable analysis and signal separation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interactive speech and noise modeling for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14549" to="14557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The effect of hearing aid noise reduction on listening effort in hearing-impaired adults</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear and hearing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="600" to="610" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech Enhancement: Theory and Practice</title>
		<imprint>
			<publisher>CRC Press, Inc., USA</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Investigating RNN-based speech enhancement methods for noise-robust textto-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th ISCA Speech Synthesis Workshop</title>
		<imprint>
			<publisher>SSW</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A summary of the reverb challenge: State-of-theart and remaining challenges in reverberant speech processing research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The third &apos;CHiME&apos; speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ICASSP 2022 Deep noise suppression challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MetricGAN: Generative adversarial networks based black-box metric scores optimization for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2031" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PHASEN: A phase-andharmonics-aware speech enhancement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9458" to="9465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual-branch attention-in-attention transformer for singlechannel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="7847" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SEGAN: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improved speech enhancement with the Wave-U-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyde</surname></persName>
		</author>
		<idno>abs/1811.11307</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TSTNN: Two-stage transformer based neural network for speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="7098" to="7102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real time speech enhancement in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Defossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3291" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SE-Conformer: Time-Domain Speech Enhancement Using Conformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2736" to="2740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AeGAN: Time-frequency speech denoising via generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdulatif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Investigating cross-domain losses for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdulatif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="411" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Complex spectral mapping with a convolutional recurrent network for monaural speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6865" to="6869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the compensation between magnitude and phase in speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glance and gaze: A collaborative learning framework for single-channel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Acoustics</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">DPT-FSNet: Dual-path transformer based full-band and sub-band fusion network for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2104.13002</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech, 2020</title>
		<meeting>Interspeech, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Continuous speech separation with conformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="5749" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech, 2020</title>
		<meeting>Interspeech, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="2642" to="2646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning for audio signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Purwins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An overview of deep-Learning-based audio-visual speech enhancement and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michelsanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1368" to="1396" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The unimportance of phase in speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="679" to="681" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The importance of phase in speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>W?jcicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="465" to="494" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5069" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end waveform utterance enhancement for direct evaluation metrics optimization by fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1570" to="1584" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">TCNN: Temporal convolutional neural network for real-time speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6875" to="6879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving GANs for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1700" to="1704" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards generalized speech enhancement with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1791" to="1795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring speech enhancement with generative adversarial networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5024" to="5028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial networks for speech enhancement and noise-robust speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michelsanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2008" to="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarial feature-mapping for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3259" to="3263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the approximate W-disjoint orthogonality of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rickard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="529" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Auditory scene analysis: The perceptual organization of sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Blind separation of speech mixtures via time-frequency masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rickard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1830" to="1847" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards scaling up classification-based speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1381" to="1390" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An algorithm to improve speech recognition in noise for hearing-impaired listeners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3029" to="3038" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On the ideal ratio mask as the goal of computational auditory scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hummersone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Blind Source Separation: Advances in Theory, Algorithms and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="349" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Binary and ratio time-frequency masks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1486" to="1501" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ideal ratio mask estimation using deep neural networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7092" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning spectral mapping for speech dereverberation and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="982" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2708" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep learning based phase reconstruction for speaker separation: A trigonometric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Two-stage deep learning for noisy-reverberant speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">DCCRN: Deep complex convolution recurrent network for phase-aware speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2472" to="2476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">DBT-Net: Dual-branch federative magnitude and phase estimation with attention-in-attention transformer for monaural speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/2202.07931</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Two heads are better than one: A two-stage complex spectral mapping approach for monaural speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1829" to="1843" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A simultaneous denoising and dereverberation framework with target decoupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech, 2021</title>
		<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2801" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning complex spectral mapping with gated convolutional recurrent networks for monaural speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="380" to="390" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuttruff</surname></persName>
		</author>
		<title level="m">Room Acoustics</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>6th edition</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On the importance of early reflections for speech in rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3233" to="3244" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Diffusion in reverberation rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Sound and Vibration</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="28" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Double the trouble: handling noise and reverberation in far-field automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2185" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Real-time single-channel dereverberation and separation with time-domain audio separation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="342" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Intelligibility of reverberant noisy speech with ideal binary masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2153" to="2161" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Generalization of supervised learning for binary mask estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Workshop on Acoustic Signal Enhancement (IWAENC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="154" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A supervised learning approach to monaural segregation of reverberant speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="921" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">DNN-based enhancement of noisy and reverberant speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Merks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6525" to="6529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A deep ensemble learning method for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="967" to="977" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Ideal ratio mask estimation using deep neural networks for monaural speech segregation in noisy reverberant conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1203" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Speech dereverberation and denoising using complex ratio masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5590" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Time-Frequency masking in the complex domain for speech dereverberation and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1492" to="1501" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">SkipConvGAN: Monaural speech dereverberation using generative adversarial networks via complex timefrequency masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kothapally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1600" to="1613" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Speech dereverberation using fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Chazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="390" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">SkipConvNet: Skip convolutional neural network for speech dereverberation using optimally smoothed spectral mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kothapally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech, 2020</title>
		<meeting>Interspeech, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="3935" to="3939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Deep speech enhancement for reverberated and noisy signals using wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ribas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Llombart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vicente</surname></persName>
		</author>
		<idno>abs/1901.00660</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The NTU-ADSC systems for reverberation challenge 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Reverberation Challenge Workshop</title>
		<meeting>the Reverberation Challenge Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Bandwidth extension of audio signals by spectral band replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekstrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Benelux Workshop on Model Based Processing and Coding of Audio (MPCA)</title>
		<meeting>the IEEE Benelux Workshop on Model Based Processing and Coding of Audio (MPCA)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Audio super-resolution using neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Enam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop Track</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Time-frequency networks for audio super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="646" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Temporal FiLM: Capturing long-range sequence dependencies with feature-wise modulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birnbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Self-Attention for audio super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rakotonirina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st IEEE International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Towards robust speech super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2058" to="2066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Neural vocoder is all you need for speech superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2203.14941</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A consolidated view of loss functions for supervised deep learning-based speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th International Conference on Telecommunications and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="72" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Phase-sensitive joint learning algorithms for deep learning-based speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shabestary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1276" to="1280" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Exploring tradeoffs in models for low-latency speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Workshop on Acoustic Signal Enhancement (IWAENC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Densely connected neural network with dilated convolutions for real-time speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6629" to="6633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A shorttime objective intelligibility measure for time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4214" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">The voice bank corpus: Design, collection and data analysis of a large regional accent speech database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Oriental International Conference on Speech Database and Assessments COCOSDA</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The diverse environments multichannel acoustic noise database (DEMAND): A database of multichannel environmental noise recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Meetings on Acoustics</title>
		<meeting>Meetings on Acoustics</meeting>
		<imprint>
			<publisher>Acoustical Society of America</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">WSJCAMO: A British English speech corpus for large vocabulary continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">The multichannel Wall Street Journal audio visual corpus (MC-WSJ-AV): specification and initial experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vepa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Maganti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. The Centre for Speech Technology Research (CSTR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Improving perceptual quality by phone-fortified perceptual loss using wasserstein distance for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">MetricGAN+: An improved version of MetricGAN for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="201" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Evaluation of objective quality measures for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Image-to-Image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Speech enhancement based on masking properties of the auditory system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Virag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="796" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Improving deep neural networks using softplus units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Objective quality evaluation for low-bit-rate speech coding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitawaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagabuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Itoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="242" to="248" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">An effective quality evaluation protocol for speech enhancement algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pellom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Spoken Language Processing (ICSL)</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2819" to="2822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A study of complexity and quality of speech waveform coders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tribolet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Crochiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="page" from="586" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">A non-intrusive quality and intelligibility measure of reverberant and dereverberated speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1766" to="1774" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Mean opinion score (MOS) revisited: methods and applications, limitations and alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Streijl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Hands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="227" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Multivariate adaptive regression splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Subjective test methodology for evaluating speech communication systems that include noise suppression algorithm</title>
		<idno>ITU-T Recommendation P.835</idno>
	</analytic>
	<monogr>
		<title level="j">International Telecommunication Union</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Prediction of perceived phonetic distance from critical-band spectra: A first step</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page" from="1278" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Quality-Net: An end-to-end non-intrusive speech quality assessment model Based on BLSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1873" to="1877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Non-intrusive speech quality assessment using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Avila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="631" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Wawenets: A no-reference convolutional waveform-based approach to estimating narrowband and wideband speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Catellier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Voran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="331" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">SESQA: Semi-supervised learning for speech quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="381" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">DNSMOS: A nonintrusive perceptual objective speech quality metric to evaluate noise suppressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="6493" to="6497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">DNSMOS P.835: A nonintrusive perceptual objective speech quality metric to evaluate noise suppressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="886" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A study on data augmentation of reverberant speech for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5220" to="5224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">STFT phase reconstruction in voiced speech for an improved single-channel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1931" to="1940" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
