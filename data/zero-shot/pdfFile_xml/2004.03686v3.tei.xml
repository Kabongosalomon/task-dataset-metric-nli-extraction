<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Webpage : https://github.com/facebookresearch/eft</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Examples of 3D pseudo-ground truth annotations produced by our method, Exemplar Fine-Tuning (EFT), for images of people from COCO [37], MPII [5], PoseTrack [4], LSPet [24], and OCHuman [70] datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Differently from 2D image datasets such as COCO, largescale human datasets with 3D ground-truth annotations are very difficult to obtain in the wild. In this paper, we address this problem by augmenting existing 2D datasets with high-quality 3D pose fits. Remarkably, the resulting annotations are sufficient to train from scratch 3D pose regressor networks that outperform the current state-of-the-art on inthe-wild benchmarks such as 3DPW. Additionally, training on our augmented data is straightforward as it does not require to mix multiple and incompatible 2D and 3D datasets or to use complicated network architectures and training procedures. This simplified pipeline affords additional improvements, including injecting extreme crop augmentations to better reconstruct highly truncated people, and incorporating auxiliary inputs to improve 3D pose estimation accuracy. It also reduces the dependency on 3D datasets such as H36M that have restrictive licenses. We also use our method to introduce new benchmarks for the study of real-world challenges such as occlusions, truncations, and rare body poses. In order to obtain such high quality 3D pseudo-annotations, inspired by progress in internal learning, we introduce Exemplar Fine-Tuning (EFT). EFT combines the re-projection accuracy of fitting methods like SMPLify with a 3D pose prior implicitly captured by a pre-trained 3D pose regressor network. We show that EFT produces 3D annotations that result in better downstream performance and are qualitatively preferable in an extensive human-based assessment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While 3D human pose estimation has progressed significantly in the past few years, the availability of suitable training datasets is still problematic. Obtaining 3D annotations for realistic benchmarks collected under everyday scenarios is difficult. Thus, most of the available 3D annotations are for datasets such as H36M <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b40">[41]</ref>, captured mostly indoors or in laboratory conditions. In order to improve generalization, previous approaches propose relatively complicated training procedures such as adversarial learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> or SMPLify-in-the-loop <ref type="bibr" target="#b33">[34]</ref> that combine these 3D datasets with more realistic images with 2D keypoint annotations. For the same reason, using current 3D datasets for evaluation is also not necessarily representative of performance in the real world. Finally, many recent state-of-the-art approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33]</ref> rely on SMPL fits on the H36M dataset <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> which, due to licensing restrictions, make reproducibility problematic.</p><p>In this paper, we address these limitations by augmenting existing large-scale 2D datasets such as COCO <ref type="bibr" target="#b36">[37]</ref>, MPII <ref type="bibr" target="#b4">[5]</ref>, PoseTrack <ref type="bibr" target="#b3">[4]</ref>, LSPet <ref type="bibr" target="#b23">[24]</ref> with pseudo-ground truth (GT) 3D annotations, as shown in <ref type="figure">fig. 1</ref>. While still inferred from single images, the quality of thus produces annotations is surprisingly high. To show this, we demonstrate that the new data (e.g., COCO with our pseudo annotations) is sufficient to train state-of-the-art 3D pose regressor networks by itself, outperforming previous methods trained on the combination of datasets with 3D and 2D groundtruth <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b33">34]</ref> on challenging benchmarks such as 3DPW. Notably, since the new dataset directly contains 3D annota-tions, which are unified across different benchmarks, training becomes straightforward without requiring complicated training techniques. Our datasets and codes are publicly available in our webpage.</p><p>Naturally, the success of our strategy depends on the quality of the pseudo-annotations that we can generate. Prior attempts at generating pseudo annotations for human pose such as UP-3D <ref type="bibr" target="#b35">[36]</ref> adopted established techniques such as SMPLify that fit the parameters of a 3D human model to the location of 2D keypoints, manually or automatically annotated in images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47]</ref>. However, they regularize the solution by means of a "view-agnostic" 3D pose prior, which incurs limitations: fitting ignores the RGB images themselves, and balancing between the 3D prior, learned separately in laboratory conditions, and the data term (e.g., 2D keypoint error) is difficult. In practice, we show that the quality of the resulting fits is insufficient for our purposes.</p><p>To address these difficulties, we introduce Exemplar Fine-Tuning (EFT), an alternative pose fitting method inspired by recent progress in internal learning approaches <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b15">16]</ref>. The idea is to start from an image-based 3D pose regressor such as <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>. The pretrained regressor implicitly embodies an informal pose prior as a function ? = ? w (I), sending the image I to the parameters ? of the 3D body. This function can be interpreted as a conditional re-parameterization of pose, where the conditioning factor is the image I, and the new parameters are the weights w of the underlying neural network regressor. We can then fit 2D annotations similar to SMPLify, but using this as a new pose parameterization by fine-tuning the weights w rather than directly optimizing ?. While similar techniques have been used in <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b72">73]</ref> to improve test-time performance, they have not been rigorously assessed against classical optimization approaches such as SMPLify, nor they have been used to generate pseudo-labels. Our extensive quantitative and qualitative analysis demonstrate that EFT results in high quality fits simply by minimizing the 2D reprojection loss without the use of an explicit 3D pose prior.</p><p>Using our 3D pseudo-annotations simplifies training 3D pose regressors and makes it easier to incorporate additional improvements, of which we explore two. First, we introduce extreme crop augmentation to train regressors that work better with truncated human bodies (e.g., upper body only) <ref type="bibr" target="#b51">[52]</ref>. Second, we demonstrate that auxiliary inputs, such as color-coded segmentation maps <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> or DensePose IUV encodings <ref type="bibr" target="#b18">[19]</ref>, can further improve the 3D human pose estimation accuracy, outperforming previous state-of-the-art approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43]</ref> simply by training on the COCO data. Compared to the previous approaches <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b44">45]</ref>, these techniques are much easier to implement on top of our pseudo-GT dataset.</p><p>We also show that our pseudo-annotations are useful to benchmark models, not just to train them. For this, we in-troduce a new 3D benchmark representative of real-world scenarios not captured by existing indoor or outdoor datasets such as 3DPW <ref type="bibr" target="#b61">[62]</ref>. We include: (1) COCO validation images <ref type="bibr" target="#b36">[37]</ref> representative of various in-the-wild scenes, (2) LSPet <ref type="bibr" target="#b23">[24]</ref> images for gymnastic poses, and (3) OCHuman <ref type="bibr" target="#b69">[70]</ref> for severe occlusions. We use EFT to generate pseudo-GT annotations and assess them via a large human study conducted on Amazon Mechanical Turk (AMT). Furthermore, we propose a new protocol for the 3DPW dataset to assess performance when only part of the human body is visible (truncation). Overall, the scenarios in the new 3D benchmark complement 3DPW; furthermore, they contain a far large number of different subjects, and thus much greater diversity. Assessed against this difficult data, our networks still outperform competitors.</p><p>Summarizing, our contributions are: (1) providing large scale and high quality pseudo-GT 3D pose annotations that are sufficient to train state-of-the-art regressors without indoor 3D datasets; <ref type="bibr" target="#b1">(2)</ref> running an extensive analysis of the quality of the pseudo-annotations generated via EFT against alternative approaches; (3) demonstrating the benefits of integrating the pseudo-annotations with extreme crop augmentation and auxiliary input representations; (4) introducing new 3D human pose benchmarks to assess regressors in less studied real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep learning has significantly advanced 2D pose recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b56">57]</ref>, facilitating the more challenging task of 3D reconstruction <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12]</ref>, which is our focus. Single-view 3D Human Pose Estimation. Single-view 3D pose reconstruction methods differ in how they incorporate a 3D pose prior and in how they perform the prediction. Fitting-based methods assume a 3D body model such as SCAPE <ref type="bibr" target="#b5">[6]</ref>, SMPL <ref type="bibr" target="#b37">[38]</ref>, SMPL-X <ref type="bibr" target="#b46">[47]</ref>, and STAR <ref type="bibr" target="#b45">[46]</ref>, and use an optimization algorithm to fit it to the 2D observations. While early approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54]</ref> required manual input, starting with SMPLify <ref type="bibr" target="#b7">[8]</ref> the process has been fully automatized, then improved in <ref type="bibr" target="#b35">[36]</ref> to use silhouette annotations, and eventually extended to multiple people <ref type="bibr" target="#b67">[68]</ref>. A recent method replaces the gradient descent update rule by a learned deep network <ref type="bibr" target="#b55">[56]</ref>. Regression-based methods, on the other hand, predict 3D pose directly. The work of <ref type="bibr" target="#b50">[51]</ref> uses sparse linear regression that incorporates a tractable but somewhat weak pose prior. Later approaches use instead deep neural networks, and differ mainly in the nature of their inputs and outputs <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43]</ref>. Some works start from a pre-detected 2D skeleton <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b11">12]</ref> while others start from raw images <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b52">53]</ref> or other proxy representation (e.g., DensePose outputs) <ref type="bibr" target="#b66">[67]</ref>. Using a 2D skeleton relies on the quality of the underlying 2D keypoint detector and discards appearance details that could help fitting the 3D model to the image. Using raw images can potentially make use of this information, but training such models from current 3D indoor datasets might fail to generalize to unconstrained images. Hence several papers combine 3D indoor datasets with 2D in-the-wild ones <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b55">56]</ref>. Methods also differ in their output, with some predicting 3D keypoints directly <ref type="bibr" target="#b39">[40]</ref>, some predicting the parameters of a 3D human body model <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b64">65]</ref>, and others volumetric heatmaps for the body joints <ref type="bibr" target="#b48">[49]</ref> or meshes <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b11">12]</ref>. Finally, hybrid methods such as SPIN <ref type="bibr" target="#b46">[47]</ref> or MTC <ref type="bibr" target="#b64">[65]</ref> combine fitting and regression approaches. 3D Reconstruction Without Paired 3D Ground-truth. Fitting methods such as SMPLify <ref type="bibr" target="#b65">[66]</ref> only require 2D keypoint annotations and a human body model acquired independently on mocap data, but the output quality depends on the initialization, which is problematic for atypical poses. These methods can also use empirical pose priors by collecting a large number of samples in laboratory conditions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b46">47]</ref>, but those may lack realism. While most regression methods require paired 3D ground-truth, exceptions include <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b71">72]</ref> that instead use 2D datasets and motion capture sequences to induce an empirical 3D prior enforced by means of adversarial learning <ref type="bibr" target="#b26">[27]</ref> or by using a geometric constraint on bone length ratios <ref type="bibr" target="#b71">[72]</ref>. Human Pose Datasets. There are several in-the-wild datasets with sparse 2D pose annotations, including COCO <ref type="bibr" target="#b36">[37]</ref>, MPII <ref type="bibr" target="#b4">[5]</ref>, Leeds Sports Pose Dataset (LSP) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, PennAction <ref type="bibr" target="#b70">[71]</ref>, and Posetrack <ref type="bibr" target="#b3">[4]</ref>. DensePose-COCO <ref type="bibr" target="#b18">[19]</ref> offers dense surface point annotations instead. Annotating 3D human poses is much more challenging as both specialized hardware and manual work are usually required. Examples include the Human3.6M dataset <ref type="bibr" target="#b21">[22]</ref>, Human Eva <ref type="bibr" target="#b54">[55]</ref>, Panoptic Studio <ref type="bibr" target="#b24">[25]</ref>, and MPI-INF-3DHP <ref type="bibr" target="#b41">[42]</ref>, which are hardly realistic. Two exceptions are 3DPW <ref type="bibr" target="#b61">[62]</ref> and PedX <ref type="bibr" target="#b28">[29]</ref> that capture 3D ground-truth outdoors, but these are collected using specialized hardware (IMUs <ref type="bibr" target="#b61">[62]</ref> and LiDAR <ref type="bibr" target="#b28">[29]</ref>) which limits data diversity. Previous attempts at creating 3D pseudo-annotations include UP-3D <ref type="bibr" target="#b35">[36]</ref>, using SMPLIfy, ExPose <ref type="bibr" target="#b13">[14]</ref>, using SMPLIfy-X, and the work of Arnab et al. <ref type="bibr" target="#b6">[7]</ref>, using a multi-frame optimization. Their use of traditional fitting methods limits the quality and scope of the pseudo annotations, due to the challenge in modeling generic human pose priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Exemplar Fine Tuning</head><p>We are interested in 3D reconstruction of people depicted in images. Following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46]</ref>, the human is represented by a parametric model that spans variation in both body shapes and motions. For example, the SMPL <ref type="bibr" target="#b37">[38]</ref> model parameters ? = (?, ?) comprise the pose ? ? R 24?3 , which controls the rotations of 24 body joints, and the shape ? ? R 10 , which controls body shapes by means of 10 prin-cipal directions of variations. The 3D locations J ? R 24?3 of the body joints are obtained by first finding their configuration at rest using the shape parameters ?, and then by applying the joint rotations ? 1 resulting in the posing function J = M (?).</p><p>For reconstruction, fitting-based approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref> take 2D image cues such as joints, silhouettes, and part labels, and optimizes the model parameters ? to fit the 3D model to the 2D cues. For example, assume that the 2D locations? ? R 24?2 of the body joints are given. Furthermore, let ? : R 3 ? R 2 be the camera projection function, mapping 3D points to their 2D image locations. Then, the optimal fit is given by:</p><formula xml:id="formula_0">(? * , ? * ) = argmin ?,? L 2D (? (M (?)) ,?) + ?L pose (?) + ?L shape (?) ,<label>(1)</label></formula><p>where the data term L 2D is the re-projection error between reconstructed and observed 2D keypoints. Since the viewpoint of the image is unknown, the camera parameters ? must be optimized together with the body parameters ?. The pose prior term L pose prioritizes plausible solutions, compensating for the fact that 2D keypoints do not contain sufficient information to infer 3D pose uniquely, and ? is a scalar to adjust the weight of the prior term. SMPLify <ref type="bibr" target="#b7">[8]</ref> uses multiple prior terms including the pose prior expressed via a mixture of Gaussians or na?ve thresholding, using a separate 3D motion capture dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39]</ref> to learn the prior parameters beforehand, as well as an angle prior to penalize unnatural bending. L shape is a shape prior term and ? is its weight. However, the success in optimizing (1) depends strongly on the quality of the heuristic used for initialization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref> (e.g., a multi-stage approach by first aligning the torso and then optimizing the limbs together) and balancing the weights between the data term and multiple prior terms is crucial.</p><p>In contrast, regression-based approaches predict the model parameters ? directly from image-based cues I such as raw RGB values <ref type="bibr" target="#b26">[27]</ref> and sparse <ref type="bibr" target="#b39">[40]</ref> and dense <ref type="bibr" target="#b66">[67]</ref> keypoints. The mapping is implemented by a neural network ? = ? w (I). Learning uses a number of example images I i with ground truth annotations for 2D keypoints? i , 3D keypoints? i and SMPL model parameters? i , which may in whole or in part be available, depending on the dataset (e.g. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41]</ref> for 3D and <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b4">5]</ref> for 2D annotations). Learning then optimizes the following training objective:</p><formula xml:id="formula_1">w * = argmin w 1 N N i=1 L 2D (? (M (? w (I i ))) ,? i ) + ? i L 3D M (? w (I i )),? i + ? i L ? ? w (I i ),? i ,<label>(2)</label></formula><p>where ? i and ? i are loss-balancing coefficients which can be set to zero for samples that do not have 3D annotations. The camera projection ? is often predicted as additional output of the neural network ? <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>During training, the regressor ? w * (I) learns implicitly a prior on possible human poses, conditioned on the 2D input cues I. This is arguably stronger than the prior L prior used by fitting methods, which are learned separately, on different 3D data, and, being unconditional, tend to regress to a mean solution. On the other hand, fitting methods explicitly minimize the 2D re-projection error L 2D at test time, thus resulting in better 2D fits than regression methods that only minimize this term during training.</p><p>Exemplar Fine-Tuning (EFT) combines the advantages of fitting and regression methods. The idea is to interpret the network ? as a re-parameterization ?(w) = (?(w), ?(w)) = ? w (I) of the model as a function of the network parameters w. With this, we can rewrite eq. (1) as</p><formula xml:id="formula_2">? * = ?(w + ) where w + = argmin w L 2D (?(M (?(w))),?) + ? w ? w * 2 2 + ?L shape (?) ,<label>(3)</label></formula><p>The second term requires the network parameters w to be close to the pre-trained regressor parameters w * . The shape regularizer L shape = ?(w) 2 2 favors the default SMPL shape parameters (as in eq. (1)), but in practice its effect is very small (? = 0.001, see <ref type="table">table 5</ref>).</p><p>Compared to eq. (1), we dropped the explicit pose prior L pose , hypothesizing that a prior is implicitly captured by the pre-trained network, with the added advantage of accounting for the input image I. Concretely, consider using a large value of ? for the prior in eqs. (1) and <ref type="bibr" target="#b2">(3)</ref>. Traditional fitting methods would then fall back to predicting the mean pose implied by the prior L pose , ignoring the input. In contrast, in EFT a large value of ? falls back to the 'best guess' of the pre-trained regressor network for the pose of the observed input. Therefore, adjusting ? is much easier in EFT, and, in supp. material, we demonstrate that EFT is not sensitive to ?. In particular, fitting the pose prior in methods such as SMPLify may nullify the advantage of using a better regressor for initializing the pose. Instead, using better pose regressors in EFT generally result in better performance (see <ref type="table">table 5</ref> and supp. material). In our experiments, we employ EFT by setting ? = 0 to produce our pseudo annotations, using instead early stopping for regularization, starting from w = w * (sec. 6.6). In practice, we use less than 100 EFT iterations for all fits.</p><p>Note that, while eqs. (2) and (3) are optimized in a similar manner, the goals are very different: while eq. (2) is averaged over the training set and minimized to learn the parameters w * of the model, eq. (3) is optimized on a single example and only to find a better fit ? * = ?(w + ) of the model to it.</p><p>After an individual update ? * is obtained, w + is discarded. Implementation details. For the network ? w * , we use HMR <ref type="bibr" target="#b26">[27]</ref> pre-trained using SPIN <ref type="bibr" target="#b33">[34]</ref>. For robustness, we change the perspective projection model used in SPIN back to the weak-perspective projection of HMR <ref type="bibr" target="#b26">[27]</ref> and fine-tune the SPIN network to work correctly with this model (this step does not noticeably affect the performance of the model). The input RGB image I is a crop, 224 ? 224, around the 2D keypoint annotations. eq. (3) is optimized using Adam <ref type="bibr" target="#b29">[30]</ref> with the default PyTorch parameters and a learning rate 2 of 5 ? 10 ?6 , which is sufficient to cover both good and bad initializations. We switch off batch normalization and dropout. We iterate until the average 2D re-projection loss is less than 3 pixels, or up to a maximum of 50 iterations (100 for OCHuman <ref type="bibr" target="#b69">[70]</ref> as the initial regressed pose tend to contain larger errors). Although the input 2D keypoints are manually annotated, they still contain non-negligible errors. In particular, the heights of hips and the ankles are not consistently annotated, causing foreshortening (examples are shown in supp. material). To compensate, the hip locations are ignored while optimizing eq. (3) and we add a loss term to match the orientation of the lower leg, encouraging the reconstruction of the orientation of the vector connecting the knee to the ankle. We use ? = 0 in eq. (3) unless otherwise mentioned. Finally, we discard samples as unreliable by checking the maximum value of the SMPL shape parameters and of the 2D keypoint loss, rejecting if these are larger than 5 and 0.01 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The EFT Training and Validation Datasets</head><p>The main application of EFT is augmenting existing 2D human pose datasets with 3D pseudo-ground truth annotations for downstream model training and validation (benchmarking). We experiment with augmenting the COCO <ref type="bibr" target="#b36">[37]</ref>, MPII <ref type="bibr" target="#b4">[5]</ref>, LSPet <ref type="bibr" target="#b23">[24]</ref>, PoseTrack <ref type="bibr" target="#b3">[4]</ref>, and OCHuman <ref type="bibr" target="#b69">[70]</ref> datasets. Most of these datasets come with a Train and Val split, which we use. <ref type="bibr" target="#b2">3</ref> We discard samples that fail the EFT sanity check discussed in sec. 3. For COCO-Train and COCO-Val, we only retain samples with at least 6 keypoints annotations. We also consider a subset of COCO-Train, named "COCO-Part", used by <ref type="bibr" target="#b34">[35]</ref>, which more conservatively only uses instances with 12 keypoint annotations for which all limbs are present. For COCO-Val, LSPet and OCHuman we further carry out a manual filtering step, described below. We use the notation [?] EFT to denote an EFT-augmented version of a dataset and thus obtain the  Human Study and Validation. We conduct a human study and validation on Amazon Mechanical Turk (AMT) to assess the quality of the EFT fits. First, we use A/B testing to compare EFT and SMPLify fits. To this end, we show 500 randomly-chosen images from the MPII, COCO and LSPet datasets to human annotators in AMT and ask them whether they prefer the EFT or the SMPLify reconstruction. Each sample is evaluated by three different annotators, showing the input image and two views of the 3D reconstructions, from the same viewpoint as the image and from the side. Examples are shown in supp. material. Our method was preferred 61.8% of the times with a majority of at least 2 votes out of 3, and obtained 59.6% favorable votes by considering the 1500 votes independently. We found that in many cases the perceptual difference between SMPLify and EFT is subtle, but SMPLify suffers more from bad initialization due to occlusions and challenging body poses. We further conduct a full human assessment of COCO-Val, LSPet and OCHuman to validate the annotations before using them for benchmarking purposes. To this end, we show each sample to three annotators at random, asking whether the estimated 3D annotation accurately describes the pose of the target subject in the scene or not. Conservatively, we accept a sample only if all three annotators agree to accept it. The final number of samples and rejection rates are shown in table 1. The rejection rates (? 20%) are relatively high due to the strict selection criteria (the rate would be only ? 4% if were to accept samples with only two 'accept' votes). Truncated 3DPW Dataset. In order to assess the robustness of algorithms to people only partially visible due to view truncation (a very common case in applications as similarly addressed in <ref type="bibr" target="#b51">[52]</ref>), we also propose a new protocol for the 3DPW dataset using a pre-defined set of aggressive image crops (see <ref type="figure">fig. 2</ref>). As shown in <ref type="figure">fig. 2 (d)</ref>, crops are generated by computing bounding boxes in between the full bounding box (level 7) of a person and a much smaller bounding box containing only his/her face (level 0) whose 2D locations is computed by projecting the SMPL fits on images. For each  </p><formula xml:id="formula_3">[COCO-Train] EFT , [COCO-Part] EFT , [MPII] EFT , [LSP] EFT , [PoseTrack] EFT ,</formula><formula xml:id="formula_4">COCO-Train 79K 79K - - COCO-Part 28K 28K - - MPII 15K 14K - - PoseTrack-Train 29K 22K - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training Pose Regressors with EFT datasets</head><p>Our EFT datasets allow us to train 3D pose regressors from scratch using a simple and clean pipeline. Specifically, we train the HMR model <ref type="bibr" target="#b26">[27]</ref> (without the discriminator) using the same hyper parameters as used in SPIN <ref type="bibr" target="#b33">[34]</ref>. A key advantage in using the pseudo-GT annotations is that the training pipeline becomes straightforward, providing opportunities to apply additional techniques to improve performance. We explore two such techniques: applying extreme crop augmentation and inputting auxiliary inputs. Augmentation by extreme cropping. A shortcoming of previous 3D pose estimation methods is that it is assumed that most of the human body is visible in the input image <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>. However, humans captured in realworld videos are often truncated, so that only the upper body or even just the face is visible, dramatically increasing the ambiguity of pose reconstruction. In order to train models that are more robust to truncation, we propose to augment the training data with extreme cropping. While the same challenge has been recently addressed by <ref type="bibr" target="#b51">[52]</ref>, our approach is more straightforward because we already have full 3D annotations -we only need to randomly crop the training images. In practice, we generate random crops in the same way as described in sec. 4 for the Truncated 3DPW dataset. During the training, we trigger crop augmentation with 30% chance, randomly choosing a truncated bounding box among the pre-computed ones shown in <ref type="figure">fig. 2 (d)</ref>. Auxiliary Input for 3D Pose Regressor. Recent methods demonstrate that other types of input encoding such as Dense-Pose or body part segmentation can improve 3D pose regressors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b2">3]</ref>. Training with such auxiliary inputs is also straightforward in our pipeline. To show this, we train a pose regressor by concatenating the standard RGB input with an additional input encoding. We test color-coded segmentation maps <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> and DensePose map, by pre-computing these representations for all images in the datasets using Detectron2 <ref type="bibr" target="#b63">[64]</ref>, as shown in <ref type="figure">fig. 2</ref> (b,c). To accept 6 channel inputs (RGB concatenated with a color-coded auxiliary input), we modify the first layer of HMR (i.e. the first layer of ResNet50) by duplicating the initial weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>Our key result is showing that the quality of the EFTgenerated pseudo GT is sufficient to train from scratch stateof-the-art 3D pose regressors, outperforming previous methods (sec. 6.2 and 6.3) in the in-the-wild benchmark. We also use EFT to generate benchmark representative of difficult scenarios of practical importance and run a large comparison of state-of-the-art regressors using it (sec. 6.4). Finally, we use EFT as a post-processing step with off-the-shelf regressors (sec. 6.5), and provide additional analysis in (sec. 6.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>In addition to the EFT datasets of sec. 4, we also test the datasets with ground-truth 3D pose annotations, including H36M <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b40">[41]</ref>, collected in laboratory conditions. For these datasets, we use the existing SMPL fittings on H36M <ref type="bibr" target="#b26">[27]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>. We use the 3DPW test set <ref type="bibr" target="#b61">[62]</ref> as the major target benchmark that is captured outdoor and comes with 3D ground truth obtained by using IMUs and cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">EFT Datasets for Learning Models</head><p>In sec. 4 we have assessed the EFT-augmented datasets via a human study. We now assess them based on how well they work when used to train pose regressors from scratch 4 . To this end, we follow the procedure described in sec. 5 and train the HMR model using the EFT datasets in isolation as well as in combination with other 3D datasets such as H36M. In all cases, we use as single training loss the prediction error against 3D annotations (actual or pseudo), with a major simplification compared to approaches that mix, and thus need to balance, 2D and 3D supervision.</p><p>The results are summarized in  <ref type="table" target="#tab_3">Table 2</ref>: Quantitative evaluation on 3DPW (PA-MPJPE) and H36M (protocol-2 using frontal view, PA-MPJPE) in mm. Top: previous methods. Bottom: training using straight 3D supervision with actual and pseudo 3D annotations. We also compare generating pseudo-ground truth annotations using EFT, SPIN <ref type="bibr" target="#b33">[34]</ref>, and SMPLify <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>. We report in bold regression results that are better than the previously-reported state of the art (SPIN) that is based on the same network architecture.</p><p>[    <ref type="bibr" target="#b33">[34]</ref>, which are publicly available. We include the result of ExPose <ref type="bibr" target="#b12">[13]</ref> in table 2 that similarly built a pseudo-gt dataset using SMPLify-X with a manual filtering process by human annotators. As shown, the models trained on the EFT datasets perform better than the ones trained on the SMPLify-based ones, suggesting that the quality of the pseudo GT generated by EFT is better. This result shows the significance of the indoor-outdoor domain gap and highlights the importance of developing in-the-wild 3D datasets, as these are usually closer to applications, thus motivating our approach. As it might be expected, networks trained exclusively on indoor dataset with GT 3D annotations (H36M, MPI-INF-3DHP, and 3DPWtrain) show poor performance on 3DPW. Similarly, the networks exclusively trained on the the EFT-based datasets, which are 'in the wild', are not as good when tested on H36M. However, the error decreases markedly once the networks are also trained using the H36M data (H36M, [COCO-Part] EFT +H36M, and [COCO-Train] EFT + H36M +MPI-INF-3DHP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Learning Models with Auxiliary Inputs</head><p>Next, we test the performance of models trained with RGB and auxiliary inputs. In table 2 bottom, [COCO-Part] EFT w/ DensePose and [COCO-Train] EFT w/ DensePose show results obtained by using the concatenation of RGB and DensePose IUV map as input. These additional inputs improve the accuracy on in-the-wild scenes, previously shown by <ref type="bibr" target="#b52">[53]</ref>. In particular, the models trained with [COCO-Train] EFT only outperform the state-of-the-art video-based regressor VIBE (56.1 vs 56.5 mm). In the sup. mat. we show that, instead, the segmentation encodings do not noticeably improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">New 3D Human Pose Benchmarks</head><p>EFT Datasets. We evaluate the performance of various models on our new benchmark datasets with pseudo GT, OCHuman <ref type="bibr" target="#b69">[70]</ref> and LSPet <ref type="bibr" target="#b23">[24]</ref>. These datasets have challenging body poses, camera viewpoints, and occlusions. We found that most models trained on other datasets are struggling in these benchmarks, showing more than 100 mm errors, as shown in table 3. The models trained with pseudo annotations on similar data (training sets of LSPet and OCHuman) show better performance (less than 100 mm).</p><p>Testing with Truncated Input on 3DPW. We use the protocol defined in sec. 4 on 3DPW to assess performance on truncated body inputs. Among 8 different bounding box levels, we consider levels 1,2,4 that are shown in yellow in <ref type="figure">fig. 2</ref> (Right). We use PA-MPJPE as in the original benchmark test. The result is shown in table 4, where [?] ca E are the datasets where we apply crop augmentations. For comparison, we include a recent work of Rockwell et al. <ref type="bibr" target="#b51">[52]</ref> that is trained to handle similar partial view scenes.</p><p>While HMR, SPIN, VIBE, and our network trained with [COCO-Part] EFT without crop augmentation work poorly, we found that our models trained with [COCO-Train] EFT show better performance even without crop augmentation, even outperforming the work of Rockwell et al. <ref type="bibr" target="#b51">[52]</ref>. This is because [COCO-Train] EFT already includes many such samples with severe occlusions. Note that [COCO-Train] EFT includes all samples with 6 or more valid 2D keypoint annotations. Our models trained with crop augmentation show much better performance even for scenes with extreme truncation (crop level 1 and 2). Note that crop augmentation does not degrade the performance of our models in the original 3DPW benchmark, as shown in table 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">EFT as A Post-processing Method</head><p>EFT can also be be used as a drop-in post-processing step on top of any given pose regressor ? to improve its test-time performance. We compare EFT post-processing against performing the same refinement using a traditional fitting method such as SMPLify, starting from the same  <ref type="table">Table 5</ref>: Quantitative evaluation on 3DPW by using SM-PLify and EFT as post-processing, in PA-MPJPE errors (mm). The second column shows the errors after direct regressions, and the next columns show the post-processing results. In SMPLify, M, D, P stand for multi-stage approach, data-term, and prior term respectively. The numbers in blue indicate the cases in which post-processing improves over the original predictions, and the red color indicates loss in performance. 'CO+H3+M+W' stands for '[COCO-Train] EFT + H36M + [MI] EFT + 3DPW-T'.</p><p>regressor for initialization. In this experiment, we use the 'gold-standard setting' for each algorithm: for EFT, this is the same setup as in the other experiments, and for SMPLify, we use the best hyper-parameter setting determined in SPIN. See supp. mat. for details. In the sup. mat. we also report results when the settings are exactly the same, except for the fitting method, and arrive to analogous conclusions.</p><p>In <ref type="table">table 5</ref> we carry out a quantitative evaluation on the 3DPW dataset. For initialization, we use various regressors ? pre-trained on a number of different datasets, providing different initialization states. We then use EFT and SMPLify post-processing to fit the same set of 2D keypoint annotations, obtained automatically by means of the OpenPose detector <ref type="bibr" target="#b8">[9]</ref>. For SMPLify, we performed an ablation study by turning off multi-stage trick (M) and prior terms (P), compared to the standard setting (M+D+P) used in SPIN and the original SMPLify literature <ref type="bibr" target="#b7">[8]</ref>. Similarly, we tested EFT performance without shape regularizer (i.e., ? = 0). The key observation is that the performance of SMPLify postprocessing depends on the initialization quality, the use of their multi-stage trick, and balancing between data and prior terms. Specifically, if the initialization is already good, as in the last row of table 5, SMPLify degrades the performance, particularly when the pose prior is used. This illustrates the difficulty in balancing data and prior terms, which may be difficult to do in practice. In contrast, EFT does not suffer from such issues, improving the accuracy in all cases, regardless of the quality of the initialization. This is also true when the shape regularizer is removed entirely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Overfitting Analysis of EFT</head><p>As we do not use any explicit pose prior terms in our EFT optimization (? = 0 in eq. (3)), by overfitting to a single sample EFT could 'break' the implicit pose prior originally captured in the regressor, potentially causing the results drift-  To test this, we can check the generalization capabilities of the overfitted pose regressor from EFT procedure on each single sample, by evaluating the accuracy of the overfitted regressor on the entirety of the 3DPW test set. We repeat this test for 500 different samples by using 20 and 100 EFT iterations respectively and report the results in <ref type="figure" target="#fig_1">fig. 3</ref>. The performance of SPIN <ref type="bibr" target="#b33">[34]</ref> (blue line) and HMR <ref type="bibr" target="#b26">[27]</ref> (cyan line) are also shown for comparison. As can be noted, the overfitted regressors still perform very well overall, suggesting that the network retains its good properties despite EFT. In particular, the performance is at least as good as the HMR baseline <ref type="bibr" target="#b26">[27]</ref>, and occasionally the overall performance improves after overfitting a single sample. The effect is different for different samples -via inspection, we found that samples that are more likely to 'disrupt' the regressor contain significant occlusions or annotation errors. Note that the overfitted network is discarded after applying EFT on a single example -this analysis is only meant to illustrate the effect of fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">Qualitative Evaluation on Internet Videos</head><p>We demonstrate the performance of our 3D pose regressor models trained using our EFT datasets on various challenging real-wold Internet videos, containing cropping, blur, fast motion, multiple people, and other challenging effects. Example results are shown in the supp. videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>We provide, via EFT, large-scale and high-quality pseudo-GT 3D pose annotations that are sufficient to train state-ofthe art regressors. We expect out 'EFT datasets' to be of particular interest to the research community, stripping the training of 3D pose regressors from complicated preprocessing or balancing techniques. Our 3D annotations on the popular 2D datasets such as COCO can also provide more opportunities to relate 3D human pose estimation with other computer vision tasks such as object detection and human-object interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we provide additional experiments and other details. We describe implementations of the corresponding "gold-standard" settings of SMPLify <ref type="bibr" target="#b7">[8]</ref> and EFT in sec. A, and then further compare these two algorithms using an identical set of hyper parameters in sec. B. More extensive quantitative evaluation on public benchmarks is provided in sec. C. We also include other details that were not provided in main manuscript due to space constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details of Gold-standard</head><p>Setting (main paper sec. 6.5)</p><p>Below we provide more details on implementations of "gold-standard settings" for each algorithm benchmarked in Sec. 6.5 of the main manuscript.</p><p>SMPLify. For SMPLify, we use the setting from SPIN [34] 6 with minor modifications. For the data term, the 2D reprojection error is computed in pixel space of 224?224 input images, with Geman-McClure robust function (?=100). Prior terms include pose prior with GMM, angle prior to penalize unnatural bending, and shape prior, with respective weights of 4.78, 15.2, and 5.0. We use Adam optimizer with learning rate of 1?10 ?2 . The SMPL parameters ? = (?, ?) are first initialized with predictions of a pre-trained 3D pose regressor, same as in EFT. Then a multi-stage approach is employed by optimizing camera translation and body orientation first, and then all parts later.</p><p>We made a few modifications to the original code: (1) we use weak perspective projection; (2) we do not use openpose input (see sec. K for justification); and (3) we stop once the average 2d reprojection error is less than 3 pixels (max 50 iterations), which is the same setting as in EFT.</p><p>EFT. EFT uses the standard L2 loss for 2d keypoints data term (no robust loss function). Note that, as EFT follows the standard neural network training procedure, the reprojection loss is computed in a normalized image space (-1 to 1). As mentioned in the main manuscript, the hip locations are ignored when computing reprojection errors, and we add a loss term to match the orientation of lower legs (with weight of 0.005), encouraging the reconstruction of the orientation of the vector connecting the knee to the ankle to be similar to the one from 2D annotations. The key motivation underpinning the use of leg orientation loss is based on the observation that ankle annotations are particularly noisy, causing erroneous foreshortening during fitting process (see sec. H).</p><p>We use Adam optimizer with learning rate 5 ? 10 ?6 (instead of 5 ? 10 ?5 used for training the 3D pose regressor (also used in SPIN)), see more discussion below. We stop iterating once the average 2d error ? 3 pixels (max 50 iterations), which corresponds the same setting as SMPLify.</p><p>Note that, EFT computes the 2D reprojection error in a normalized space instead of the image pixel space as in the SMPLify. However, this does not make difference because Adam is re-scaling invariant. For example, we can compute the 2D reprojection error in either space after re-scaling 2D distance and adjusting the weights for the other terms accordingly, for which Adam provides the same updates.</p><p>Justification of Different Learning Rates. The learning rate of EFT 5 ? 10 ?6 should not be directly compared to the one in SMPLify 1 ? 10 ?2 . In SMPLify, the optimizer directly updates the 85-dimensional vector of SMPL parameters, while in EFT it optimizes the weights of the neural network (27 millions) that produces SMPL parameters via a series of non-linear computations (including Batch Normalization). In fact, no same learning rate can be used for both optimizations -we confirmed that with learning rate of 1 ? 10 ?2 EFT changes SMPL parameters too much in each iteration, while with learning of 5 ? 10 ?6 SMPLify almost does not change SMPL parameters. For balancing, we instead observed reprojection error changes in the first few iterations, and found that the current learning rate settings produce similar step size in updating SMPL parameters, which is also qualitatively confirmed by 3D visualizions.</p><p>Discussion. The performance of SMPLify is largely affected by hyper parameters such as weights between data term and prior terms, and also by the initialization states that motivates the use of a multi-stage technique. Given the excellent achievement of SPIN in the use of SMPLify on the same application with ours, we treat this setting as a "gold-starndard". The results shown in sec. 6.5 of our main manuscript should be understood as a comparison with their own best settings between EFT and SMPLify. In the following section, we perform an additional comparison in the same setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison SMPLify and EFT in the Same</head><p>Data Term Setting (main paper sec. 6.5)</p><p>In this section, we perform further comparison between EFT and SMPLify with the same set of hyper parameters. Concretely, the exact same data term is used for both methods, and an ablation study is performed by adding prior terms for each method with varying weights. Implementation Details. To employ the same data term for EFT and SMPLify, we make the following modifications.  <ref type="table">Table 6</ref>: Quantitative evaluation on 3DPW by using SMPLify and EFT as post-processing, in PA-MPJPE errors (mm). Each row is the result with a pretrained model trained by different dataset (shown in the first column). We use the same data term for both methods. For SMPLify we test by varying ? for the pose prior term (the default weight of SPIN is 0.0427).</p><p>Similarly, we also test EFT by varying ? for the neural network weight regularizer, where our original EFT uses ? = 0. ? = 100.0 means that we use ? = 100.0 with a fixed 100 iterations without early stop. The bold numbers represent the cases with the best accuracy among tests. The numbers in blue indicate cases in which post-processing improves over the original predictions (which is always the case for the EFT post-processing), and the red color indicates loss in performance. "CO+H36M+M+3P" stands for "[COCO-Train] EFT + H36M + [MI] EFT + 3DPW-T"</p><p>We remove Geman-McClure robust function in SMPLify and use standard L2 loss, computing the reprojection error in the normalized space (-1 to 1) as in EFT. For EFT, we remove the leg orientation data term. The exact same body keypoints, ignoring hips, are used in computing 2D reprojection loss for both methods. We keep the same learning rates as in previous experiments, 1 ? 10 ?2 for SMPLify and EFT 5 ? 10 ?6 .</p><p>After performing a comparison with data-term only (? = 0 in table 6), we further investigate by adding prior terms of each method (? &gt; 0). For SMPLify, we only consider the pose prior term, omitting the angle prior term and the shape prior term. The original weight for the pose prior term (4.78) of SMPLify is also adjusted to compensate the scaling change of the data term (i.e., ? = 0.0427 in table 6). With varying ?, we analyze the performance changes of SMPLify.</p><p>For EFT, we add the neural network weight regularizer term (the second term in eq. (3) of our main manuscript), by varying its weight ?.</p><p>Results. As in sec. 6.5 and table 5 of our main manuscript, we carry out a quantitative evaluation by using the 3DPW dataset. As in the previous experiment, we use various regressors ? to provide different initialization states, then use EFT and SMPLify for post-processing. The results are shown in <ref type="table">table 6</ref>, showing analogous conclusions to the result of our main paper (sec. 6.5 and table 5).</p><p>When no prior term is used (i.e., ? = 0), SMPLify produces mixed results, depending on the quality of initial states. For example, given a relatively accurate initialization (the last row), the post-processing makes the result slightly worse, and with intermediate models (the second and the third) it improves the final accuracy. With very poor initializations (the first low), the result becomes worse. Adding pose prior on SMPLify also shows mixed performances. It is advantageous to have the pose prior when initializations are poor. However, the use of prior degrades the accuracy if the initialization from regressor is accurate enough. Note that, the post-processing performance changes quite significantly with small amount of weight changes (e.g., 51.72mm to 57.64mm in the last row, and more changes in other models). This particular result indicates that the weight ? needs to be adjusted carefully to obtain the best performance in each scenario. To explore further, we investigate the performance with an extremely strong ? = 100 with fixed 100 iterations without early stopping (shown in 100.0 ), and, as expected, the results converge to a certain accuracy (around 86mm), presumably the mean pose defined by the pose prior term. Now, we see the results from EFT. When we use the data-term only ? = 0, the same setting we use throughout our paper, it improves accuracy in all cases. Furthermore, by performing experiments with varying ?, we confirmed that the results of EFT are not sensitive to the change of ?. Notably, adding prior does not degenerate the initial accuracy from regressor in all cases. This is still true even though an extremely large ? = 100 is used with fixed 100 iterations (similar setting with SMPLify as shown in 100.0 ), where the regularizer term almost overshadows the data term. In this case, as expected, the results fall back to the 'best guess' of the pre-trained regressor network. The use of prior term sometimes helps to reduce the accuracy further, but the advantage is marginal. It shows less than 1mm difference in most cases, providing a justification for our default setting with no pose prior term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>As demonstrated above, traditional fitting methods such as SMPLify suffer from the balancing issue between the data term and the "view-agnostic" 3D pose prior term. In contrast, EFT does not suffer from such issues, and we demonstrate that it can be applicable even without any prior terms. EFT effectively leverages better pose regressors, showing better performance in all cases.</p><p>Note that this experiment is intended to explore the major difference between EFT and SMPLify, rather than finding the best settings for each method. The best SMPLify setting on 3DPW is not necessarily the best in other cases, and, often, it is difficult to determine the optimal set of hyper parameters. For example, we found that the use of prior terms or multi-stage optimization sometimes degrade the accuracy, but they can be still advantageous on other cases with poor initialization, such as OCHuman <ref type="bibr" target="#b69">[70]</ref>. In general, the gold standard setting proposed in SMPLify <ref type="bibr" target="#b7">[8]</ref> and SPIN <ref type="bibr" target="#b33">[34]</ref> is recommended for practitioners. Importantly, as demonstrated, EFT can provide a better option, showing better performance without complicated parameter tuning.</p><p>C. Further Evaluation on Standard Benchmarks (main paper sec. 6.</p><p>2)</p><p>The full evaluation results on the standard benchmarks including MPI-INF-3DHP <ref type="bibr" target="#b40">[41]</ref> are shown in table 10. All results are measured in terms of reconstruction errors (PA-MPJPE) in mm after rigid alignment, following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>. We also report Per-Vertex Error in table 7. Here, we highlight several noticeable results.</p><p>Evaluation on MPI-INF-3DHP <ref type="bibr" target="#b40">[41]</ref>. The model trained with our EFT dataset is also competitive on the MPI-INF-3DHP dataset. The models trained by only, [COCO-Part] EFT [COCO-Train] EFT outperform HMR <ref type="bibr" target="#b26">[27]</ref>. Combining 3D datasets with our EFT dataset improves the performance further.</p><p>H36M Protocol-1. We also report quantitative results on H36m protocol 1. In this evaluation, we use all available camera views in H36M testing set. The overall errors are slightly higher than protocol-2 (frontal camera only), but the general tendency is similar to the results of H36M protocol-2.</p><p>A Baseline Model Trained with 2D Loss Only. As a na?ve baseline method, we train a 3D pose regressor by just using 2D annotations (that is, with only 2D keypoint loss) without including other 3D losses from real or pseudo-ground truth 3D datasets. The results are shown as [COCO-Part] 2D and [COCO-Train] 2D in table 10. As expected, the model cannot estimate accurate 3D pose, showing very poor performance. However, interestingly, we found the 2D projection of the estimated 3D keypoints to still be quite accurately aligned to the target individual's 2D joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PVE Error:</head><p>We report Per-Vertex Error in <ref type="table" target="#tab_11">Table 7</ref> between the ground truth mesh vertices and the vertices from predictions. While PVE error would be affected by shape parameters which is not the major focus of our work, its tendency is aligned to the performance quantified by MPJPE errors, showing lower PVE errors with the model with lower MPJPE errors.  In addition to the results shown in section 6.3, we explore the performance of the models trained with other auxiliary inputs. We analyze the results of the models trained with two different segmentation maps: Panoptic Segmentation <ref type="bibr" target="#b30">[31]</ref> that has both things and stuff classes and PointRend <ref type="bibr" target="#b31">[32]</ref> that has only thing classes but with more accurate contours. The results are shown in table 9, along with the original results by RGB only and the results with DensePose <ref type="bibr" target="#b18">[19]</ref>. As can be seen, the color-coded segmentation encodings do not noticeably improve performance, compared to the results with DensePose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Overfitting Analysis of EFT: Examples</head><p>(main paper sec. 6.6)</p><p>As shown in <ref type="figure" target="#fig_1">fig. 3</ref> of our main manuscript, overfitting the network to individual samples with EFT usually has a small effect on the overall regression performance, suggesting that the network retains its good properties despite fine-tuning on exemplars. The effect is different for different samples, and we show the examples with a strong effect in <ref type="figure" target="#fig_5">fig. 7</ref> that contain significant occlusions or annotation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. More Qualitative Comparisons Between</head><p>EFT vs. SMPLIfy (main paper sec. 4)</p><p>As addressed in our main paper, our EFT method was preferred 61.8% of the times with a majority of at least 2 votes out of 3 in Amazon Mechanical Turk (AMT) study. Here, we visualize the examples with 0 vote and 3 votes, as shown in <ref type="figure">fig. 9</ref> and 10. Note that in our AMT study, we show the meshes over white backgrounds to reduce the bias cased by 2D localization quality.</p><p>When annotators prefer SMPLify? There are 47 / 500 samples where SMPLify outputs are favored by all three annotators, and examples are shown in <ref type="figure">fig. 9</ref>. Surprisingly the difference is quite minimal with no obvious failure patterns.   When annotators prefer EFT? There were 132 / 500 samples where our EFT outputs were favored by all three annotators. Examples are shown in <ref type="figure">fig. 10</ref>. In most cases, EFT produces more convincing 3D poses leveraging the learned pose prior conditioned on the target raw image, while SMPLify tends to produce outputs similar to its prior poses (e.g., knees tend to be bent).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Computation Time</head><p>We compare computation time between our EFT and SMPLify processes. The computation times are computed for their gold-standard settings by using a single GeForce RTX 2080 GPU.  H. Noisy 2D keypoint issue in GT data (Implementation details in sec.</p><p>3)</p><p>The keypoints of hips and ankles are more difficult to correctly localize by annotators than other keypoints due to loose clothing and occlusions. This can be also observed by checking the standard deviation of annotated 2D keypoints in COCO dataset (called OKS sigma values) <ref type="bibr" target="#b6">7</ref> , where hips (1.07) and ankles (0.89) have higher values than shoulders (0.79) and wrists (0.62). We show examples from COCO dataset in <ref type="figure" target="#fig_3">fig. 4</ref>, where in the annotations the lower leg lengths are shorter than it should be. We empirically found that such noisy 2D localization causes artifacts in 3D, motivating us to use 2d orientations for the lower leg parts rather than GT locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Statistics of our EFT datasets</head><p>Since we now have high quality pseudo 3D annotations, we can check and compare the 3D pose variations among datasets. In <ref type="figure" target="#fig_4">fig. 5</ref>, we compare the variations of 3D pose parameters of the pseudo GT data produced from three inthe-wild datasets, COCO (red), MPII (green), and LSPet (blue). The top-left figure shows the standard deviation of pose parameters (we choose the X-axis in angle-axis representation) for each body joint, including necks, hips, shoulders, and elbows. We can see that the LSPet dataset has the most diverse pose variations compared to MPII and COCO. In the later figures, we visualize the histograms of the pose parameters of a particular joint (Left Hip), to see the pose variations and distributions of all samples. As shown, the 3D poses of LSPet have higher portion of samples that have large angle axis values (less than -1.0), showing that it has more gymnastic poses. In contrast, the majority of COCO dataset has the values around zeros, we is closer to the rest standing pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Sampling Ratios across Datasets During Training</head><p>Following the approach of SPIN <ref type="bibr" target="#b33">[34]</ref>, we use fixed sampling ratio across datasets while training the 3D pose regressor. The ratios used in our experiments are shown in table 8.</p><p>[X] EFT denotes our EFT dataset. If multiple EFT datasets are used (e.g., COCO and MPII), we sample from them uniformly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Analysis on the Failures of SMPLify Fittings in SPIN</head><p>We found erroneous cases in the publicly available SMPL fitting outputs from SPIN <ref type="bibr" target="#b33">[34]</ref> that are related to its SM-PLify <ref type="bibr" target="#b7">[8]</ref> implementation. In SPIN, both 2D keypoint annotations and OpenPose <ref type="bibr" target="#b8">[9]</ref> estimations are used for SMPLify, assuming that the OpenPose estimations have better localization accuracy if OpenPose is applied to the training data. However, we found many cases where the OpenPose outputs from different individuals are mistakenly associated to the target person, as shown in <ref type="figure">fig. 6</ref>. Due to this reason, we exclude the OpenPose estimations and only use the GT annotations for our SMPLify process. <ref type="figure">Figure 6</ref>: In SMPLify implementation of SPIN <ref type="bibr" target="#b33">[34]</ref>, sometimes the OpenPose estimations (blue 2D keypoints) from different individuals are incorrectly associated to the target person (with red GT 2d keypoints). Thus, we completely ignore OpenPose estimation during our SMPLify process. (1st column) input images with GT annotaiton (red) and OpenPose estimation (blue); (2nd column) Fitting output produced from SPIN <ref type="bibr" target="#b33">[34]</ref> (the publicly available data); (3rd column) our SMPLify output by using both GT and incorrectly associated OpenPose estimation; (4th column) our SMPLify output by using GT 2D keypoints without using OpenPose output.    DensePose Annotations. We can automatically render DensePose <ref type="bibr" target="#b18">[19]</ref> annotations from our pseudo GT data. Examples are shown in <ref type="figure" target="#fig_6">fig. 8</ref>. As a potential research direction, these automatically generated densepose annotations can be used for training.</p><p>Nearest Neighbor Search. Our pseudo-GT datasets allow us to further analyse diverse 3D human poses in the context of outdoor environments. For example, by using a simple nearest neighbor we can find similar 3D human poses in the COCO dataset. The examples are shown in <ref type="figure">fig. 11</ref>. <ref type="figure">Figure 9</ref>: The samples where SMPLify outputs are favored by all three annotators. The blue meshes are the results by EFT and the pink meshes are results by SMPLify. <ref type="figure">Figure 10</ref>: The samples where our EFT outputs are favored by all three annotators. The blue meshes are the results by EFT and the pink meshes are results by SMPLify. <ref type="figure">Figure 11</ref>: Nearest Neighbor 3D Pose Search in COCO. We can find similar 3D poses by using the one in the top-left as a query (shown in blue boxes) in COCO EFT Dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) RGB (b) DensePose IUV (c) Segmentation (d) Boxes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Test error (PA-MPJPE) on 3DPW with the overfitted regressors by EFT on 500 different samples, for 20 (left) and 100 (right) iterations.ing away from the true poses in the later EFT iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>EFT. A single EFT iteration takes about 0.04 sec, and a whole EFT process for a sample data (including reloading the pre-trained network model) takes about 0.82 sec with 20 iterations.SMPLify. Camera optimization takes about 0.01 sec per iteration, and, thus, it takes 0.5 sec per sample (with 50 iterations). Optimizing body pose takes about 0.02 sec/iteration, and it takes 1 sec with 50 iterations. Thus whole SMPLify process takes about 1.5 sec per sample with 50 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Noisy localization issue of 2D GTs, particularly on hips and ankle joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>We compare the 3D pose parameter variations among COCO (red), MPII (green), and LSPet (blue). (Topleft) standard deviations of X-axis in angle-axis representation per selected joints. (Others) the histograms of the X-axis angular values of LHip joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Example samples that cause significant changes in the 3DPW testing error in our EFT overfitting test (main paper sec. 6.6). The left two examples have annotations on occluded body parts, and the rightest example has incorrect annotations (left-right swap).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Automatically generated densepose annotationL. Potential Applications for EFT DatasetsOur EFT datasets provide high-quality pseudo-GT 3D pose data on in-the-wild images. This can potentially open</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and [OCHuman] EFT datasets, summarized in table 1. All datasets are publicly available in our webpage.</figDesc><table><row><cell>Dataset</cell><cell># Initial</cell><cell cols="2"># Final</cell><cell>Human Rej. Rate</cell></row><row><cell></cell><cell></cell><cell>Train</cell><cell>Val</cell><cell></cell></row><row><cell>COCO-Val [37]</cell><cell>13K</cell><cell></cell><cell>10K</cell><cell>20.5%</cell></row><row><cell>LSPet [24]</cell><cell>8.3K</cell><cell>3.3K</cell><cell>2.2K</cell><cell>18.6%</cell></row><row><cell>OCHuman [70]</cell><cell>10K</cell><cell>2.6K</cell><cell>1.7K</cell><cell>24.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of EFT validation (top) and training (bottom) datasets. The 'Human' and 'Rej. Rate' columns show whether manual filtering is used with the rejection rates.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table 2 .</head><label>2</label><figDesc>The table (bottom) evaluates the regressor trained from scratch using straight 3D supervision on standard 3D datasets (H36M, MPI-INF-3DHP, 3DPW), the EFT-lifted datasets [MPII] EFT , [LSP] EFT , [COCO-Part] EFT and [COCO-Train] EFT , as well as various combinations. Following<ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>, performance is measured in terms of reconstruction errors (PA-MPJPE) in mm after rigid alignment on two public benchmarks: 3DPW and H3.6M 5 . The models trained with indoor 3D pose datasets (H36M and MPI-INF-3DHP) perform poorly on the 3DPW dataset, which is collected outdoors. By comparison, training exclusively on the EFT datasets performs much better. Notably, the model trained only on</figDesc><table><row><cell>Previous method</cell><cell cols="2">3DPW ? H36M ?</cell></row><row><cell>HMR [27]</cell><cell>81.3</cell><cell>56.8</cell></row><row><cell>DSD [58]</cell><cell>75.0</cell><cell>44.3</cell></row><row><cell>SPIN [34]</cell><cell>59.2</cell><cell>41.1</cell></row><row><cell>I2L [43]</cell><cell>57.7</cell><cell>41.1</cell></row><row><cell>Pose2Mesh [12]</cell><cell>58.3</cell><cell>46.3</cell></row><row><cell>ExPose [13] (by SMPLify-X fits)</cell><cell>60.7</cell><cell>-</cell></row><row><cell>Song et al. [56] (w/ 2D keypoints)</cell><cell>55.9</cell><cell>56.4</cell></row><row><cell>VIBE (temporal) [33]</cell><cell>56.5</cell><cell>41.5</cell></row><row><cell cols="3">Direct 3D supervision, 3D pseudo-ground truth from EFT</cell></row><row><cell>H36M</cell><cell>146.2</cell><cell>53.3</cell></row><row><cell>MPI-INF-3DHP (MI)</cell><cell>127.8</cell><cell>110.6</cell></row><row><cell>3DPW-Train</cell><cell>91.0</cell><cell>131.4</cell></row><row><cell>[LSP] EFT</cell><cell>84.9</cell><cell>87.3</cell></row><row><cell>[MPII] EFT</cell><cell>69.1</cell><cell>78.9</cell></row><row><cell>[PoseTrack] EFT</cell><cell>72.6</cell><cell>88.6</cell></row><row><cell>[COCO-Part] EFT</cell><cell>59.0</cell><cell>66.9</cell></row><row><cell>[COCO-Train] EFT</cell><cell>57.5</cell><cell>64.2</cell></row><row><cell>[COCO-Train] EFT + [PoseTrack] EFT</cell><cell>56.5</cell><cell>63.5</cell></row><row><cell>[COCO-Part] EFT + H36M</cell><cell>57.8</cell><cell>45.0</cell></row><row><cell>[COCO-Train] EFT + H36M</cell><cell>55.5</cell><cell>46.2</cell></row><row><cell>[COCO-Train] EFT + H36M + MI</cell><cell>54.7</cell><cell>44.9</cell></row><row><cell cols="2">[COCO-Train] EFT + H36M + MI + 3DPW-T 51.6</cell><cell>44.0</cell></row><row><cell>[Co-Tr+PT+LSP] ca EFT + H36M + MI [Co-Tr+PT+LSP+OC] ca EFT + H36M + MI</cell><cell>53.6 54.6</cell><cell>45.6 45.4</cell></row><row><cell>[COCO-Part] SPIN [34]</cell><cell>70.9</cell><cell>78.2</cell></row><row><cell>[COCO-Part] SMPLify</cell><cell>72.7</cell><cell>81.0</cell></row><row><cell>[COCO-Part] EFT w/ Densepose</cell><cell>57.2</cell><cell>64.0</cell></row><row><cell>[COCO-Train] EFT w/ Densepose</cell><cell>56.1</cell><cell>64.7</cell></row></table><note>4 ResNet50 of HMR is initialized with ImageNet pretrained weights.5 See the supp. material for the result on the MPI-INF-3DHP benchmark.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>EFT is comparable to VIBE<ref type="bibr" target="#b32">[33]</ref> that is the previous best method using a video input (using temporal cues). Combining both EFT datasets and 3D datasets shows even better results with the result (54.7 mm) achieved by the [COCO-Train] EFT + H36M + MPI-INF-3DHP combination, and the best one (51.6 mm) by including the 3DPW training data.</figDesc><table><row><cell>Methods</cell><cell>OCHuman ?</cell><cell>LSP ?</cell></row><row><cell>SPIN [34]</cell><cell>104.1</cell><cell>-</cell></row><row><cell>H36M</cell><cell>165.4</cell><cell>201.8</cell></row><row><cell>MPI-INF-3DHP (MI)</cell><cell>158.1</cell><cell>190.2</cell></row><row><cell>3DPW (Train)</cell><cell>141.7</cell><cell>182.4</cell></row><row><cell>[LSP]E</cell><cell>129.7</cell><cell>105.7</cell></row><row><cell>[MPII]E</cell><cell>113.3</cell><cell>127.0</cell></row><row><cell>[PT]E</cell><cell>124.8</cell><cell>154.7</cell></row><row><cell>[CO-Pa]E</cell><cell>104.8</cell><cell>108.9</cell></row><row><cell>[CO-Tr]E</cell><cell>104.0</cell><cell>103.9</cell></row><row><cell>[CO-Pa]E + H36m</cell><cell>103.2</cell><cell>106.1</cell></row><row><cell>[CO-Tr]E + H36m + MI + [PT]E</cell><cell>103.1</cell><cell>99.9</cell></row><row><cell>[CO-Tr]E + H36m + MI + [PT]E + [LSP]E</cell><cell>103.9</cell><cell>79.4</cell></row><row><cell>[CO-Tr]E + H36m + MI + [PT]E + [LSP]E + [OCH]E</cell><cell>92.7</cell><cell>77.6</cell></row></table><note>COCO-Part] EFT is already comparable to the previous state- of-the art method, SPIN [35], on this benchmark. Com- bining multiple training datasets improves performance further; the model trained with [COCO-Train] EFT outper- forms SPIN achieving (57.5 mm) and the model trained with [COCO-Train] EFT + [PoseTrack]We also compare EFT to SMPLify [8] for generating the pseudo-annotations. In [COCO-Part] SMPLify we use the SM- PLify output to post-process the output of the fully trained</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Quantitative evaluation on OCHuman and LSP datasets with our pseudo annotations, in PA-MPJPE errors (mm).</figDesc><table><row><cell>Training DBs</cell><cell cols="2">crop lev.1 lev.2</cell><cell cols="2">lev.4 orig.</cell></row><row><cell>HMR [27]</cell><cell>153.6</cell><cell cols="2">122.2 82.0</cell><cell>81.3</cell></row><row><cell>Rockwell et al. [52]</cell><cell>133.2</cell><cell cols="2">106.1 78.0</cell><cell>71.9</cell></row><row><cell>SPIN [34]</cell><cell>184.3</cell><cell cols="2">198.8 79.4</cell><cell>59.2</cell></row><row><cell>VIBE [34]</cell><cell>150.5</cell><cell cols="2">158.9 75.1</cell><cell>56.5</cell></row><row><cell>[CO-P] E</cell><cell>168.7</cell><cell cols="2">145.0 69.7</cell><cell>59.0</cell></row><row><cell>[CO-T] E</cell><cell>113.5</cell><cell>97.4</cell><cell>66.3</cell><cell>57.5</cell></row><row><cell>[CO-T] E + [PT] E</cell><cell>118.0</cell><cell cols="2">100.1 68.3</cell><cell>56.5</cell></row><row><cell cols="2">[CO-P] ca E [CO-T] ca E [CO-T] ca E + [PT] ca E [CO+PT+LSP] ca E + H36m + MI [CO+PT+LSP+OC] ca E + H36m + MI 84.5 89.4 91.5 88.5 89.1</cell><cell>76.3 76.4 75.9 74.8 72.9</cell><cell>64.4 63.4 63.4 59.6 60.3</cell><cell>61.5 59.2 59.3 53.6 54.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Effect of crop-augmentation on 3DPW (PA- MPJPE in mm) with different level of truncated inputs. SPIN model, and in [COCO-Part] SPIN we use the fitting out- puts that SPIN generates during learning</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Per-Vertex-Error on 3DPW dataset.</figDesc><table><row><cell>D. Training Models with Auxiliary Inputs</cell></row><row><cell>(main paper sec. 6.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Training data sampling ratios across datasets.</figDesc><table><row><cell>Dataset Combinations</cell><cell>3DPW</cell></row><row><cell>[COCO-Part] EFT</cell><cell>59.0</cell></row><row><cell>[COCO-Part] EFT w/ DensePose</cell><cell>57.2</cell></row><row><cell>[COCO-Part] EFT w/ PanopticSeg</cell><cell>59.1</cell></row><row><cell>[COCO-Part] EFT w/ PointRend</cell><cell>58.9</cell></row><row><cell>[COCO-Train] EFT</cell><cell>57.5</cell></row><row><cell>[COCO-Train] EFT w/ DensePose</cell><cell>56.1</cell></row><row><cell>[COCO-Train] EFT w/ PanopticSeg</cell><cell>57.4</cell></row><row><cell>[COCO-Train] EFT w/ PointRend</cell><cell>57.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Quantitative evaluation on 3DPW (PA-MPJPE in mm) by learning models with auxiliary inputs: Dense-Pose<ref type="bibr" target="#b18">[19]</ref>, segmentation map by Panoptic Segmentation<ref type="bibr" target="#b30">[31]</ref>, and segmentation map by PointRend<ref type="bibr" target="#b31">[32]</ref> </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Previous method3DPW ? H36M (P1) ? H36M (P2) ? MPI-INF-3DHP ?</figDesc><table><row><cell>HMR [27]</cell><cell>81.3</cell><cell>58.1</cell><cell>56.8</cell><cell>89.8</cell></row><row><cell>DenseRaC [67]</cell><cell>-</cell><cell>-</cell><cell>48.0</cell><cell>-</cell></row><row><cell>DSD [58]</cell><cell>75.0</cell><cell>-</cell><cell>44.3</cell><cell>-</cell></row><row><cell>HoloPose [18]</cell><cell>-</cell><cell>-</cell><cell>50.6</cell><cell>-</cell></row><row><cell>HoloPose (w/ post-proc.) [18]</cell><cell>-</cell><cell>-</cell><cell>46.5</cell><cell>-</cell></row><row><cell>SPIN [34]</cell><cell>59.2</cell><cell>-</cell><cell>41.1</cell><cell>67.5</cell></row><row><cell>HMR (temporal) [28]</cell><cell>72.6</cell><cell>-</cell><cell>56.9</cell><cell>-</cell></row><row><cell>Sim2Real (temporal) [15]</cell><cell>74.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Arnab et al. (temporal) [7]</cell><cell>72.2</cell><cell>-</cell><cell>54.3</cell><cell>-</cell></row><row><cell>DSD (temporal) [58]</cell><cell>69.5</cell><cell>-</cell><cell>42.4</cell><cell>-</cell></row><row><cell>VIBE (temporal) [33]</cell><cell>56.5</cell><cell>44.2</cell><cell>41.4</cell><cell>63.4</cell></row><row><cell>Straight 3D supervision and pseudo-ground truth from EFT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>H36M</cell><cell>146.2</cell><cell>56.1</cell><cell>53.3</cell><cell>147.4</cell></row><row><cell>MPI-INF-3DHP (MI)</cell><cell>127.7</cell><cell>116.7</cell><cell>110.7</cell><cell>93.2</cell></row><row><cell>3DPW (Train)</cell><cell>91.0</cell><cell>139.8</cell><cell>131.4</cell><cell>125.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Quantitative evaluation on the 3DPW, H36M protocol-1 (by using all views), H36M protocol-2 (by using frontal views), and MPI-INF-3DHP datasets. Reconstruction errors are computed by PA-MPJPE and reported in mm after alignment by a rigid transformation. up several new research opportunities.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">SMPL also includes a mesh component that deforms along with the skeleton, but we ignore it here as the major loss constrains only the 3D location J of the joints.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This learning rate is chosen as a smaller value than the learning rate 5 ? 10 ?5 used for training 3D pose regressor.<ref type="bibr" target="#b2">3</ref> We make a random split for LSPet, and for OCHuman we use Val set as training and Test set as testing data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">See the public SMPLify code from SPIN for further details : https://github.com/nkolot/SPIN/blob/master/ smplify/smplify.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/cocodataset/cocoapi/ blob/8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9/ PythonAPI/pycocotools/cocoeval.py</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CMU graphics lab motion capture database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to reconstruct people in clothing from a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><forename type="middle">Lal</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ensafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">OpenPose: realtime multi-person 3D pose estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurips</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep manifold prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<idno>abs/2004.04242, 2020. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?ler</forename><surname>R?za Alp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natalia Neverova, and Iasonas Kokkinos</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning motion manifolds with convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joyce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2015 Technical Briefs</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pedx: Benchmark dataset for metric 3-d pose estimation of pedestrians in complex urban intersections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonhui</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manikandasriram</forename><surname>Srinivasan Ramanagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Rosaen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Goumas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson-Roberson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">STAR: A spare trained articulated human body regressor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Full-body awareness from partial observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Rockwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Delving deep into hybrid annotations for 3d human recovery in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeulIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Human body model fitting by learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3D human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu Fish</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense renderand-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes -the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An internal learning approach to video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">P</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pose2seg: Detection free human instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Paul L Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Three-d safari: Learning to estimate zebra pose, shape, and texture from images &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Berger-Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
