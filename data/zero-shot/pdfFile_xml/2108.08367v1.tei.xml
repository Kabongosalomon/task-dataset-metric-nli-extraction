<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Di</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
							<email>fabianmanhardt@google.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
							<email>xyji@tsinghua.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<email>nassir.navab@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
							<email>tombari@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Directly regressing all 6 degrees-of-freedom (6DoF) for the object pose (i.e. the 3D rotation and translation) in a cluttered environment from a single RGB image is a challenging problem. While end-to-end methods have recently demonstrated promising results at high efficiency, they are still inferior when compared with elaborate PnP/RANSACbased approaches in terms of pose accuracy. In this work, we address this shortcoming by means of a novel reasoning about self-occlusion, in order to establish a two-layer representation for 3D objects which considerably enhances the accuracy of end-to-end 6D pose estimation. Our framework, named SO-Pose, takes a single RGB image as input and respectively generates 2D-3D correspondences as well as self-occlusion information harnessing a shared encoder and two separate decoders. Both outputs are then fused to directly regress the 6DoF pose parameters. Incorporating cross-layer consistencies that align correspondences, selfocclusion and 6D pose, we can further improve accuracy and robustness, surpassing or rivaling all other state-ofthe-art approaches on various challenging datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the 6D pose has been widely adopted as an essential cue in high-level computer vision tasks, including robotic grasping and planning <ref type="bibr" target="#b0">[1]</ref>, augmented reality <ref type="bibr" target="#b39">[39]</ref>, and autonomous driving <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b29">29]</ref>. Driven by the recent success of deep learning, current methods are capable of estimating the 6D pose in a cluttered environment at impressive accuracy and high efficiency <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b17">18]</ref>. Almost all current top-performing frameworks adopt a two-stage strategy that resorts to first establishing 2D-3D correspondences and then computing the 6D pose with a RANSAC-based Perspective-n-Point (PnP) algorithm <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9]</ref>. Neverthe-* Codes will be released at https://github.com/shangbuhuan13/SO-Pose  <ref type="figure">Figure 1</ref>. The basic structures of end-to-end 6D pose estimation methods. Compared to single-layer methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">43]</ref> that depend on 2D-3D point matching as intermediate results, our method SO-Pose presents a novel two-layer representation that additionally incorporates self-occlusion information about the object. less, while achieving great results, these methods cannot be trained in an end-to-end manner and require extra computation for optimization of pose. Moreover, adopting surrogate training losses instead of directly predicting 6D poses also prevents further differentiable processing/learning (e.g. by means of self-supervised learning <ref type="bibr" target="#b42">[42]</ref>) and does not allow to incorporate other down-stream tasks. Despite two-stage approaches dominating the field, a few methods conducting end-to-end 6D pose estimation have been also recently proposed <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43]</ref>. They typically learn the 6D pose directly from dense correspondencebased intermediate geometric representations, as shown in <ref type="figure">Fig. 1(a)</ref>. Nevertheless, although end-to-end methods keep constantly improving, they are still far inferior to two-stage methods harnessing multi-view consistency check <ref type="bibr" target="#b17">[18]</ref>, symmetry analysis <ref type="bibr" target="#b8">[9]</ref>, or disentangled predictions <ref type="bibr" target="#b19">[20]</ref>.</p><p>What limits the accuracy of end-to-end methods? After in-depth investigation in challenging scenes, we observe that while the network is approaching the optimum, due to the inherent matching ambiguity of textureless object surface, mis-matching error caused by noise is inevitable, resulting often in one correspondence field corresponding to many 6D poses with similar fitting errors. This leads the training process to converge to a sub-optimum, hindering the overall 6D pose estimation performance. Since eliminating errors caused by noise is not trivial, an alternative solution to this problem is to replace the correspondence field with a more precise representation of the 3D object, thus reducing the influence of noise.</p><p>In this work, we attempt at closing the gap between end-to-end and two-stage approaches by leveraging selfocclusion information about the object. For an object in 3D space, we can logically only observe its visible parts due to the nature of the perspective projection. Yet, parts that are invisible due to (self-) occlusion are usually neglected during inference. Inspired by multi-layer models used in 3D reconstruction <ref type="bibr" target="#b34">[34]</ref>, we focus on self-occlusion information to establish a viewer-centered two-layer representation of the object pose. While the first layer preserves the correspondence field of visible points on the object and their projections, the second layer incorporates the self-occlusion information. In essence, instead of directly identifying whether and where each visible point occludes the object, we simplify the procedure by examining the self-occlusion between each pixel and the object coordinate planes. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, the ray passing through the camera center and each visible point intersects the object coordinate plane at most three different locations. The coordinates of these intersections are then utilized to form the second layer representation of the object, as shown in <ref type="figure">Fig. 1(b)</ref>. Finally, two cross-layer consistency losses are introduced to align self-occlusion, correspondence field and 6D pose simultaneously, reducing the influence of noise.</p><p>To summarize, our main contributions are as follows:</p><p>? We propose SO-Pose, a novel deep architecture that directly regresses the 6D pose from the two-layer representation of each 3D object. ? We propose to leverage self-occlusion and 2D-3D correspondences to establish a two-layer representation for each object in 3D space, which can be utilized to enforce two cross-layer consistencies. ? SO-Pose consistently surpasses all other end-to-end competitors on various challenging datasets. Moreover, SO-Pose also achieves comparable accuracy when compared with other state-of-the-art two-stage methods, whilst being much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The related works for monocular 6D pose estimation can be roughly partitioned into three different lines of works. In particular, while some methods directly regress the final 6D pose, others either learn a latent embedding for subsequent retrieval of the pose, or employ 2D-3D correspondences to solve for the 6D pose by means of the well-established RANSAC/PnP paradigm.</p><p>As for the first line of works, Kehl et al. <ref type="bibr" target="#b14">[15]</ref> extends SSD <ref type="bibr" target="#b22">[23]</ref> to estimate the 6D object pose, turning the regression into a classification problem. In their follow-up work <ref type="bibr" target="#b24">[24]</ref>, Manhardt et al. leverage multiple hypotheses to improve robustness towards ambiguities. In <ref type="bibr" target="#b26">[26]</ref>, the authors leverage ideas from projective contour alignment to estimate the pose. A few other works also make use of the point-matching loss to directly optimize for pose in 3D <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref>. Finally, <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b43">[43]</ref> both establish 2D-3D correspondences but attempt to learn the PnP paradigm in an end-to-end fashion.</p><p>The next branch employs latent embedding for pose estimation. These learned embeddings can be then leveraged for retrieval during inference. Specifically, inspired by <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b15">16]</ref>, Sundermeyer et al. <ref type="bibr" target="#b38">[38]</ref> utilize an Augmented AutoEncoder (AAE) to learn a low-dimensional pose embedding. After localizing the object in image space using a 2D object detector <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21]</ref>, the latent representation of the detection is calculate and compared against a pre-computed codebook to retrieve the pose. To further improve scalability to multiple objects, the authors of <ref type="bibr" target="#b37">[37]</ref> propose to employ a single-shared encoder together with separate decoders for each object.</p><p>Finally, the last branch is grounded on establishing 2D-3D correspondences, before solving for the pose using RANSAC/PnP. Thereby, some works propose to regress the 2D projections of the 3D bounding box corners <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b40">40]</ref>. To increase the robustness of these correspondences, Hu et al. predicts multiple hypotheses on the basis of segmented super-pixels. Nevertheless, most recent methods utilize 2D-3D correspondences with respect to the 3D model rather than the 3D bounding box. Peng et al. <ref type="bibr" target="#b28">[28]</ref> demonstrate that keypoints away from the object surface induce larger errors and, therefore, instead sample several keypoints on the object model based on farthest point sampling. Hybrid-Pose <ref type="bibr" target="#b36">[36]</ref> follows and develops <ref type="bibr" target="#b28">[28]</ref> by introducing hybrid representations. Noteworthy, the majority of works within this branch, however, establishes dense 2D-3D correspondences <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b8">9]</ref>. They are among the best-performing methods on several challenging benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Given an image I, we leverage a neural network to learn a mapping f (?) from I to the relative 3D rotation R and translation t, transforming the target object from the object frame to the camera frame,</p><formula xml:id="formula_0">R, t = f (I; ?),<label>(1)</label></formula><p>, with ? denoting the trainable parameters of the utilized network. In a cluttered environment, the available object information is oftentimes severely limited due to (self-) occlusion. Moreover, directly regressing the 3D rotation parameters under occlusion has proven to be challenging <ref type="bibr" target="#b10">[11]</ref>. Inspired by multi-layer models in 3D reconstruction <ref type="bibr" target="#b34">[34]</ref>, we propose to combine visible 2D-3D correspondences with invisible self-occlusion information to establish a twolayer representation for objects in 3D space, in an effort to capture more complete geometric features than singlelayer approaches relying only on correspondences <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">43]</ref>. Thereby, we enforce two cross-layer consistencies to align self-occlusion, correspondence field and 6D pose, to reduce the influence of noise and, thus, enhance pose estimation under various challenging external influences. The overall architecture of SO-Pose is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><formula xml:id="formula_1">Self-occlusion Correspondences (a) (b) (c) (d) (e) (f) (g) (i) (j) Losses Convolutional Layers Fully-connected Layers Output Forward Pass (h) o x y z Mask ? ? ? ? ? ? ?2 ? ?3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-occlusion for Robust Pose Estimation</head><p>The vast majority of CNN-based 6D pose estimation approaches focus only on the visible part of an object while discard the occluded part <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">43]</ref>. Nevertheless, in complex environments, the visible region of an object is often-times very limited or only exhibits little amount of textured information. Hence, single-layer representations can not encode the geometric features of the object completely and accurately, inducing ambiguities for 6D pose. Similar to multi-layer model in 3D reconstruction <ref type="bibr" target="#b34">[34]</ref>, we attempt to leverage self-occlusion information to obtain a richer representation of the 3D object. As shown in <ref type="figure" target="#fig_1">Fig. 2 (d)</ref> and (e), we combine self-occlusion with estimated 2D-3D correspondences to establish a novel two-layer representation for describing the pose of an object in 3D space. For better understanding, imagine a ray emitted from the camera center and passing through the object. This ray intersects the object surface at multiple different points, of which the first one is visible whereas all others are self-occluded. In contrast to <ref type="bibr" target="#b34">[34]</ref> which records the coordinates of self-occluded points, we instead note the coordinates of the intersections between each ray and the coordinate planes of the object. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, the ray OP intersects the object co-</p><formula xml:id="formula_2">ordinate frames o ? yz, o ? xz, o ? xy at points Q x , Q y , Q z .</formula><p>For an object obj, we combine P, Q = {Q x , Q y , Q z } to represent its two-layer model,</p><formula xml:id="formula_3">obj := {P, Q} P ? V,<label>(2)</label></formula><p>with V denoting the visible points of the current view w.r.t. the camera coordinate system. Notice that Q can be derived analytically from P , knowing the rotation R and translation t. Projecting P onto the 2D image plane, we obtain</p><formula xml:id="formula_4">? = 1 Z P KP,<label>(3)</label></formula><p>with K describing the camera intrinsic matrix and P = [X P , Y P , Z P ] T denoting the visible 3D point. Further, the object coordinate plane w.r.t. the camera coordinate system is defined as</p><formula xml:id="formula_5">(Rn * ) T X = (Rn * ) T t.<label>(4)</label></formula><p>where X represents a 3D point on the corresponding coordinate plane given</p><formula xml:id="formula_6">n * = ? ? ? ? ? n x = [1, 0, 0] T X ? o ? yz n y = [0, 1, 0] T X ? o ? xz n z = [0, 0, 1] T X ? o ? xy (5)</formula><p>From this we can derive Q x , lying on the plane o ? yz intersected by the ray OP , as follows,</p><formula xml:id="formula_7">Q x = (Rn x ) T t (Rn x ) T (K ?1 ?) K ?1 ?<label>(6)</label></formula><p>Substituting n x with n y or n z in Eq. 6, we can respectively derive Q y or Q z .</p><p>Since P and Q are represented w.r.t. the camera coordinate system, their corresponding coordinates w.r.t. the object coordinate system are calculated as P 0 = R T P ? R T t and Q 0 = R T Q ? R T t. Notice that we normalize P 0 and Q 0 based on the object diameter to stabilize optimization. Notably, as the ray passing through the camera center O and a visible point P may be parallel to one of the object coordinate planes, it can occur that the ray never intersects this plane. Therefore, to circumvent these cases and increase robustness, we only consider intersections inside the minimum bounding cuboid ? of the object, as visualized in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>According to the definition of self-occlusion in Eq. 6, our two-layer representation exhibits 3 advantages over singlelayer approaches. First, the self-occlusion coordinate Q is analytically derived with rotation and translation parameters for each visible point, independent of the object surface. Thus it eliminates errors arising from rendering. Moreover, since the self-occlusion coordinate Q 0 lies on a coordinate plane and has therefore only 2 degrees-of-freedom, we also only require to predict 2 values to represent Q 0 . Hence, this acts as a regularization term which can reduce the influence of noise. Finally, since P and Q lie on the same line, we can derive several cross-layer consistencies which align self-occlusion, 2D-3D correspondences and 6D pose, increasing significantly accuracy and robustness of SO-Pose, especially in challenging environments. Qz. We only consider the points inside the pre-defined region ? for training stability, thus Qz would be removed in this example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-layer Consistency</head><p>Leveraging the estimated two-layer representation for objects in 3D space, we enforce two cross-layer consistency loss terms to jointly align self-occlusion, correspondence field and 6D pose parameters. When substituting Eq. 3 into Eq. 6 and rearranging, we obtain that</p><formula xml:id="formula_8">(Rn) T tP = (Rn) T P Q.<label>(7)</label></formula><p>From this we enforce our first cross-layer consistency as</p><formula xml:id="formula_9">L cl?3D = 1 |Q 0 | P ?V,Q0?? (Rn) T t(RP 0 + t) ? (Rn) T (RP 0 + t)(RQ 0 + t) 1 ,<label>(8)</label></formula><p>with * 1 representing the L1 loss and |Q 0 | denoting the number of intersections within ?. Eq. 8 jointly aligns and refines 2D-3D correspondences P , self-occlusion Q and pose R, t in 3D space, based on the definition of Q in Eq. 6.</p><p>While the first cross-layer consistency is enforced in 3D space, we employ the second cross-layer loss on the 2D image plane. Since P and Q lie on the same ray, their projections describe the same point ? on the image plane. As a consequence we can derive our 2D consistency term as follows,</p><formula xml:id="formula_10">L cl?2D = 1 |Q 0 | P ?V,Q0?? ( e P Q 1 + e Q? 1 ) (9) with e P Q = 1 Z Q K(RQ 0 + t) ? 1 Z P K(RP 0 + t)<label>(10)</label></formula><p>and</p><formula xml:id="formula_11">e Q? = 1 Z Q K(RQ 0 + t) ? ?.<label>(11)</label></formula><p>where e P Q forces P and Q to project onto the same 2D point, while e Q? forces Q to project to ?, the corresponding ground truth projection of Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Overall Objective</head><p>SO-Pose takes a single RGB image as input and directly predicts 6D pose parameters of an object in 3D space. To establish a two-layer representation, our framework generates a correspondence field and three self-occlusion maps. In the following, both intermediate geometric features are concatenated and fed to the pose predictor to obtain the output pose in a fully differentiable fashion. Our overall objective function is composed of basic terms for pose, cross-layer consistency terms, and self-occlusion term,</p><formula xml:id="formula_12">L = L pose + L cl + L occ ,<label>(12)</label></formula><p>with</p><formula xml:id="formula_13">L cl = ? 1 L cl?2D + ? 2 L cl?3D<label>(13)</label></formula><p>and</p><formula xml:id="formula_14">L occ = ? 3 L Q .<label>(14)</label></formula><p>In particular, L pose is a combined loss term for correspondence field, translation parameters, visible mask, region classification and point matching as in <ref type="bibr" target="#b43">[43]</ref>. We kindly refer the reader to <ref type="bibr" target="#b43">[43]</ref> for more details on the pose terms.</p><p>As for self-occlusion, L Q is composed of two parts,</p><formula xml:id="formula_15">L Q = L Q1 + L Q2 .<label>(15)</label></formula><p>Thereby, we straightforwardly employ the L 1 loss according to</p><formula xml:id="formula_16">L Q1 = 1 |Q 0 | Q0?? Q 0 ?Q 0 1 ,<label>(16)</label></formula><p>withQ 0 denoting the ground truth self-occlusion coordinates. Similarly, also for L Q2 we directly employ the L 1 loss to ensure consistency after projection using</p><formula xml:id="formula_17">L Q2 = 1 |Q 0 | Q0?? 1 Z Q K(RQ 0 +t) ? ? 1 .<label>(17)</label></formula><p>Thereby,R andt represent the ground-truth rotation and translation. Eq. 17 enforces that all predicted self-occlusion coordinates Q x , Q y and Q z reside on the same ray with respect to P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>In this section we compare SO-Pose to current state-ofthe art methods in 6D pose estimation. We conduct extensive experiments on three challenging datasets to demonstrate the effectiveness and superiority of our approach. We also perform various ablation studies to verify that our twolayer model consistently surpasses the single-layer competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Structure</head><p>We feed SO-Pose with a zoomed-in RGB image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b43">43]</ref> of size 256?256 as input and directly output 6D pose. Similar to GDR-Net <ref type="bibr" target="#b43">[43]</ref>, we parameterize the 3D rotation using its allocentric 6D representation R 6d <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">43]</ref>, and the 3D translation as the projected 3D centroid and the object's distance <ref type="bibr" target="#b19">[20]</ref>. As backbone we leverage ResNet34 <ref type="bibr" target="#b5">[6]</ref> for all experiments on the LM dataset <ref type="bibr" target="#b6">[7]</ref>, while we employ ResNeSt50 <ref type="bibr" target="#b48">[48]</ref> for the more challenging datasets, i.e. LMO <ref type="bibr" target="#b2">[3]</ref> and YCB-V <ref type="bibr" target="#b45">[45]</ref>.</p><p>After feature extraction using the aforementioned backbone, we append two decoders for estimation of selfocclusion and 2D-3D correspondences. The first branch essentially outputs 6-channel self-occlusion maps with a resolution of 64?64. The second branch predicts three different groups of intermediate geometric feature maps of size 64?64. While the first group describes the visible object mask, the other two groups describe the 2D-3D correspondence field <ref type="bibr" target="#b4">[5]</ref> and surface region attention map as defined in <ref type="bibr" target="#b43">[43]</ref>. Finally, self-occlusion and point matching feature maps are fed into the pose regression network to predict 6D pose directly. We adopt the identical pose regression network as in <ref type="bibr" target="#b43">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Details</head><p>Implementation Details. Our network is trained end-toend using Ranger optimizer <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b46">46</ref>] on a single TITAN X GPU. We use a batch size of 24 and a base learning rate of 1e-4. We anneal the learning rate with a cosine schedule at 72% of the training phase. Unless specified otherwise, we set {? 1 , ? 2 , ? 3 } = {1/f, 10, 1}, where f denotes the focal length. Moreover, to increase stability, we first train the network without L cl?3D and L cl?2D and add them after 20% of the total training epochs. During training, color augmentation and mask erosion are randomly applied to avoid overfitting similar to <ref type="bibr" target="#b38">[38]</ref>. For 2D localization we utilize Faster-RCNN <ref type="bibr" target="#b33">[33]</ref> on LMO and FCOS <ref type="bibr" target="#b41">[41]</ref> on YCB-V. Notice that we do not take special care of symmetric objects <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b8">9]</ref> or post-refinement <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. Datasets. We test SO-Pose on three commonly-used datasets, i.e. LM <ref type="bibr" target="#b7">[8]</ref>, LMO <ref type="bibr" target="#b1">[2]</ref> and YCB-V <ref type="bibr" target="#b45">[45]</ref>. LM consists of individual sequences for 13 objects undergoing mild occlusion. We follow <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b19">20]</ref> and employ ?15% of the RGB images for training and the remaining part for testing. We additionally render 1K synthetic images for each object during training. LMO extends LM by annotating one sequence with other 8 visible objects, often imposing severe occlusion on the objects. Similarly, we render additional 10k synthetic images for each object. Finally, YCB-V is a very challenging dataset exhibiting strong occlusion, clutter and several symmetric objects. We adopt the provided real images of 21 objects and publicly available physicallybased rendered (pbr) data for training and testing as in <ref type="bibr" target="#b43">[43]</ref>.</p><p>We additionally evaluate our method following the BOP setup on LMO and YCB-V <ref type="bibr" target="#b10">[11]</ref>. Evaluation Metrics. We employ the most commonly used metrics for comparison with other state-of-the-art methods. Thereby, ADD(-S) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> measures the percentage of transformed model points whose deviation from ground truth lies below 10% of the object's diameter (0.1d). For symmetric objects, ADD(-S) measures the deviation to the closet model point <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Further, n ? n cm <ref type="bibr" target="#b35">[35]</ref> measures the percentage of predicted 6D poses whose rotation error is less than n ? and translation error is below n cm. On YCB-V dataset, we also compute the AUC (area under curve) of ADD-S and ADD(-S) similar to <ref type="bibr" target="#b45">[45]</ref>. For BOP settings on LMO and YCB-V, we additionally compute AR V SD , AR M SSD , AR M SP D as proposed by <ref type="bibr" target="#b10">[11]</ref>. We also provide the average AR score to compare the performance on various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State of the Art</head><p>This section compares SO-Pose with other state-of-theart methods on different datasets. Results on LM. As shown in Tab. 1, our method consistently outperforms all baseline methods for each metric, especially in terms of ADD(?S) 0.02d and 2 ? 2 cm. Compared to GDR-Net <ref type="bibr" target="#b43">[43]</ref>, under ADD(?S) 0.02d, we improves from 35.3 to 45.9, up to 30%. Since these two strict metrics are usually utilized to measure performance in robotic grasping or other high-level tasks, the significant improvement of SO-Pose in Tab. 1 demonstrates that our method has great potential in robotic applications. Results on LMO. We compare our method with stateof-the-art competitors in terms of ADD(?S) in Tab. 3. When trained with real+syn, our method achieves even comparable results to refinement-based method such as DeepIM <ref type="bibr" target="#b18">[19]</ref> and outperforms all other competitors. Further, using real+pbr data for training, our method achieves state-of-the-art performance on 5 out of 8 objects. Thereby, our average score surpasses all other methods by a large margin, with 62.3 against 24.9 ? 56.1. Results on YCB-V. As for YCB-V, we show our results in Tab. 2. Using ResNeSt50 <ref type="bibr" target="#b48">[48]</ref>, we outperform again all other methods under ADD(-S) and AUC of ADD-S, with 56.8 and 90.9 against second best results 53.9 and 89.8. Under AUC of ADD(-S), we are only a little inferior to CosyPose <ref type="bibr" target="#b17">[18]</ref>, with 83.9 compared to 84.5. Nevertheless, whilst achieving comparable results as Cosy-Pose, out method runs significantly faster as CosyPose is a refinement-driven method, while we only need a single forward pass to obtain the final 6D pose. Results under BOP metrics. In Tab. 4, we report our results under the BOP setup. To ensure a fair comparison with the related works, on LMO we only utilize the provided pbr data for training, whereas on YCB-V, both real and pbr data are utilized <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b43">43]</ref>. For all non-refinement  <ref type="table">Table 2</ref>. Results on YCB-V. We report the results of our method with different backbones. Ours <ref type="bibr" target="#b34">(34)</ref> uses ResNet34 <ref type="bibr" target="#b5">[6]</ref> while Ours(50) uses ResNeSt50 <ref type="bibr" target="#b48">[48]</ref>. Ref. stands for refinement. P.E. reflects the training strategy of pose estimator, 1 represents single model for all objects while M represents one model per object. In general, the latter strategy benefits accuracy yet limits practical use. methods, SO-Pose again achieves superior results reporting a mean AR of 0.664 compared to CDPN-v2 <ref type="bibr" target="#b19">[20]</ref> with 0.578 and EPOS <ref type="bibr" target="#b8">[9]</ref> with 0.621. Nevertheless, we are a little inferior to CosyPose, the overall best performing method that adopts sophisticated refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We conduct several ablations on each dataset. Thereby, except for ablated terms, we leave all other terms unchanged using the values from the experimental setup. Effectiveness of cross-layer consistency. In Tab. 1 we demonstrate the effectiveness of the proposed cross-layer consistency terms on LM. We gradually remove L cl?3D and L cl?2D to observe their impact on the 6D pose w.r.t ADD(-S), 2 ? 2 cm and 5 ? 5 cm. Thereby, we can observe that the accuracy decreases when removing either loss term, verifying the usefulness of our cross-layer consistencies. Benefits of employing self-occlusion. To show that selfocclusion consistently improves pose quality, we additionally adopted the two-stage method CDPN <ref type="bibr" target="#b19">[20]</ref> to also incorporate our two-layer representation. As CDPN is grounded on RANSAC/PnP to extract the 3D rotation from 2D-3D  <ref type="table">Table 3</ref>. Comparison with state-of-the-art methods on LMO. We list the Average Recall of ADD(-S). ( * ) denotes symmetric objects.  <ref type="table">Table 5</ref>. Evaluation of our two-layer model o top of another baseline method CDPN <ref type="bibr" target="#b19">[20]</ref>. We update the original CDPN to CDPN * . As for SO-Pose, we integrate our self-occlusion branch into the CDPN structure. Since CDPN predicts rotation with RANSAC/PnP, we rearrange Eq. 8 and Eq. 9 to derive a new loss term L cdpn , as defined in Eq. 18.</p><formula xml:id="formula_18">Method P.E. Ref. LMO YCB-V Mean AR V SD AR M SSD AR M SP D AR V SD AR M SSD AR M SP D AR</formula><p>correspondences, we slightly adjust our cross-layer consistency terms as follows,</p><formula xml:id="formula_19">L cdpn = L cl?3D (R ?R) + L cl?2D (R ?R)<label>(18)</label></formula><p>Essentially, the consistency term L cdpn is computed by replacing the predicted rotation R in L cl?3D and L cl?2D with the actual ground truth rotationR. Except for L cdpn , all original loss terms in CDPN are preserved. As shown in Tab. 5, after introducing our two-layer model into CDPN, the performance improves again significantly for all metrics. This clearly demonstrates the generalizability of our proposed two-layer model. Impact of different backbones. We report the results of our method with ResNet34 <ref type="bibr" target="#b5">[6]</ref> and ResNeSt50 <ref type="bibr" target="#b48">[48]</ref> as backbone in Tab. 2. Although the performance degrades slightly after changing ResNeSt50 to ResNet34, our method still outperforms most state-of-the-art methods, proving its efficacy regardless of the employed backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Runtime Analysis</head><p>Given a 640?480 image from YCB-V with multiple objects, our method takes about 30ms to handle a single object and 50ms to process all objects in the image on an Intel 3.30GHz CPU and a TITAN X (12G) GPU. This includes the additional 15ms for 2D localization using Yolov3 <ref type="bibr" target="#b32">[32]</ref>. As shown in <ref type="figure">Fig. 5</ref>, we demonstrate the Speed-AR score figure on YCB-V. Our method achieves second best results (AR: 0.715) in real time, which further verifies the great potential of our method in practical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative evaluation</head><p>We provide qualitative results for LMO in <ref type="figure" target="#fig_3">Fig. 4</ref> and YCB-V in <ref type="figure" target="#fig_4">Fig. 6</ref>. In particular, in <ref type="figure" target="#fig_3">Fig. 4</ref>, we show four exemplary results for pose estimation together with the corresponding error maps for rendered 2D-3D correspondences with the estimated pose. In <ref type="figure" target="#fig_3">Fig. 4 (a)</ref> ape, due to wrong detection, the predicted 6D pose diverges from the ground Correspondingly, we demonstrate the error of the predicted 2D-3D matching in the second row, from green to pink as illustrated in the middle line. We also visualize the ground truth bounding boxes (red). <ref type="figure">Figure 5</ref>. Comparison of running speed (Hz) and AR score on YCB-V dataset. We compare our method with CosyPose <ref type="bibr" target="#b17">[18]</ref>, GDR-Net <ref type="bibr" target="#b43">[43]</ref>, CDPN-v2 <ref type="bibr" target="#b19">[20]</ref> and EPOS <ref type="bibr" target="#b8">[9]</ref>. Along the direction of the arrow, method performs better, achieving higher accuracy in less inference time.</p><p>truth completely. In (d) driller, we demonstrate wrong prediction of 6D pose due to strong occlusion. Finally in <ref type="figure" target="#fig_4">Fig. 6</ref>, we show the normalized two-layer representation of an object from YCB-V as predicted by our model. For more qualitative results, please refer to the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a novel two-layer model that combines 2D-3D point correspondences and self-occlusion information to encapsulate explicitly the spatial cues of 3D object. Then based on the two-layer model, we establish SO-Pose, an end-to-end 6D pose regression framework that achieves significant improvements on various challeng- ing datasets over other single-layer competitors. The experimental evaluation also demonstrates that our two-layer model is applicable to a wide range of 6D pose estimation frameworks and can consistently benefit the performance.</p><p>In the future, we plan to focus on integrating the twolayer model into self-supervised 6D pose estimation and category-level unseen object analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Basic structure of baseline methods<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">43]</ref> Basic structure of our method SO-Pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Schematic overview of the proposed SO-Pose framework. Given an input image (a) and 3D model (c), we first use an offthe-shelf object detector<ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32]</ref> to crop the object of interest (b) from (a). Afterwards, (b) is fed into our encoder for high-level feature extraction. These features are then separately processed by two individual decoder networks to predict a two-layer representation. Thereby, while the first branch outputs a self-occlusion map (d), the latter branch estimates 2D-3D point correspondences (e) and object mask (f). In (h), we demonstrate the established 2D-3D correspondences between the visible surface of the object and the 3D model. A detailed illustration of self-occlusion is shown in (g). For a visible point on the object surface, it occludes coordinate planes o ? yz, o ? xz, o ? xy at Qx, Qy and Qz. Finally, we feed the self-occlusion map (d), together with the 2D-3D point matching field (e) to the pose estimator block that predicts the final 6D pose. Exemplary output for depth map and the rendered bounding box using the estimated pose are shown in (i) and (j), respectively. L * represent loss terms used in the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Demonstration of self-occlusion. The ray OP from camera center O towards the visible point P intersects the object coordinate planes o ? yz, o ? xz, o ? xy at 3 points Qx, Qy and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on LMO. We presents 4 examples of 6D pose estimation results. Given an input image, we first show the 2D detection results with confidence scores on top of the bounding boxes. Next to it we show the predicted 2D-3D matching (rendered with the predicted pose, color-coded) and object bounding box (estimated with the predicted pose, light blue) are shown on the top row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Demonstration of the predicted two-layer model. For object (a), we demonstrate its 2D-3D point matching in (b) and self-occlusion coordinates in (d), (e) and (f). (c), (g), (h), (i) are corresponding error maps of (b), (d), (e), (f). The color bar on the right side indicates the color-coded error for error maps. We normalize the error maps for better visualization, thus from the bottom to top of the color bar, the error ranges from 0 to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation Study on LM. We provide results of our method with different loss terms. Ours(S) stands for removing both the L cl?3D and L cl?2D terms.</figDesc><table><row><cell>Method</cell><cell cols="3">ADD(-S) 0.02d 0.05d 0.1d</cell><cell cols="2">2 ? 2cm 5 ? 5cm</cell></row><row><cell>CDPN [20]</cell><cell>-</cell><cell>-</cell><cell>89.9</cell><cell>-</cell><cell>94.3</cell></row><row><cell cols="2">GDR-Net [43] 35.3</cell><cell cols="2">76.3 93.7</cell><cell>62.1</cell><cell>95.6</cell></row><row><cell>Ours(S)</cell><cell>36.6</cell><cell cols="2">76.8 94.0</cell><cell>59.1</cell><cell>97.0</cell></row><row><cell>Ours (w/o L cl?3D )</cell><cell>41.6</cell><cell cols="2">81.7 95.7</cell><cell>67.4</cell><cell>97.1</cell></row><row><cell>Ours (w/o L cl?2D )</cell><cell>44.7</cell><cell cols="2">81.3 95.5</cell><cell>73.1</cell><cell>98.0</cell></row><row><cell>Ours</cell><cell>45.9</cell><cell cols="2">83.1 96.0</cell><cell>76.9</cell><cell>98.5</cell></row><row><cell>Method</cell><cell cols="2">P.E. Ref.</cell><cell cols="3">ADD AUC of (-S) ADD-S ADD(-S) AUC of</cell></row><row><cell>PoseCNN [45]</cell><cell>1</cell><cell></cell><cell>21.3</cell><cell>75.9</cell><cell>61.3</cell></row><row><cell>SegDriven [14]</cell><cell>1</cell><cell></cell><cell>39.0</cell><cell>-</cell><cell>-</cell></row><row><cell>PVNet [28]</cell><cell>M</cell><cell></cell><cell>-</cell><cell>-</cell><cell>73.4</cell></row><row><cell>S.Stage [12]</cell><cell>M</cell><cell></cell><cell>53.9</cell><cell>-</cell><cell>-</cell></row><row><cell>GDR-Net [43]</cell><cell>1</cell><cell></cell><cell>49.1</cell><cell>89.1</cell><cell>80.2</cell></row><row><cell>DeepIM [19]</cell><cell>1</cell><cell></cell><cell>-</cell><cell>88.1</cell><cell>81.9</cell></row><row><cell>CosyPose [18]</cell><cell>1</cell><cell></cell><cell>-</cell><cell>89.8</cell><cell>84.5</cell></row><row><cell>Ours(34)</cell><cell>1</cell><cell></cell><cell>54.6</cell><cell>89.7</cell><cell>82.3</cell></row><row><cell>Ours(50)</cell><cell>1</cell><cell></cell><cell>56.8</cell><cell>90.9</cell><cell>83.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison with state-of-the-art methods on LMO and YCB-V under BOP metrics. We provide results for ARV SD , ARMSSD and ARMSP D on LMO and YCB-V. Mean AR represents the overall performance on these two datasets as the average over all AR scores. Overall best results are in bold and the second best results are underlined.</figDesc><table><row><cell>CosyPose [18]</cell><cell>1</cell><cell></cell><cell>0.480</cell><cell>0.606</cell><cell>0.812</cell><cell>0.772</cell><cell>0.842</cell><cell>0.850</cell><cell>0.727</cell></row><row><cell>EPOS [9]</cell><cell>1</cell><cell></cell><cell>0.389</cell><cell>0.501</cell><cell>0.750</cell><cell>0.626</cell><cell>0.677</cell><cell>0.783</cell><cell>0.621</cell></row><row><cell>PVNet [28]</cell><cell>M</cell><cell></cell><cell>0.428</cell><cell>0.543</cell><cell>0.754</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CDPN-v2 [20]</cell><cell>M</cell><cell></cell><cell>0.445</cell><cell>0.612</cell><cell>0.815</cell><cell>0.396</cell><cell>0.570</cell><cell>0.631</cell><cell>0.578</cell></row><row><cell>GDR-Net [43]</cell><cell>1</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.584</cell><cell>0.674</cell><cell>0.726</cell><cell>-</cell></row><row><cell>Ours</cell><cell>1</cell><cell></cell><cell>0.442</cell><cell>0.581</cell><cell>0.817</cell><cell>0.652</cell><cell>0.731</cell><cell>0.763</cell><cell>0.664</cell></row><row><cell>Method</cell><cell></cell><cell>ADD(-S)</cell><cell>2 ? 2cm</cell><cell>5 ? 5cm</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CDPN *</cell><cell></cell><cell>92.82</cell><cell>68.56</cell><cell>97.03</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CDPN * -ours (w/o L cdpn )</cell><cell>93.09</cell><cell>69.37</cell><cell>97.06</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CDPN * -ours (w/ L cdpn )</cell><cell>94.77</cell><cell>71.33</cell><cell>97.07</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stereo-based 6d object localization for grasping with humanoid robot systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedram</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamim</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruediger</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="919" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning 6D object pose estimation using 3D object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6D pose estimation of objects and scenes from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3364" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end learnable geometric vision by backpropagating pnp optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8100" to="8109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D Pose Estimation and 3D Model Retrieval for Objects in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Epos: Estimating 6d pose of objects with symmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11703" to="11712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On evaluation of 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Hoda?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Obdr??lek</forename><surname>And?t?p?n</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCVW</publisher>
			<biblScope unit="page" from="606" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bop: Benchmark for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Glentbuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Ihrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenophon</forename><surname>Zabulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single-stage 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2930" to="2939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segmentation-driven 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmentation-driven 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3385" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cosypose: Consistent multi-view multi-object 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="574" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">DeepIM: Deep iterative matching for 6d pose estimation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7678" to="7687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Reed; Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Explaining the ambiguity of object detection and 6d pose from visual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">Martin</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6841" to="6850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ROI-10D: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2069" to="2078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d scene reconstruction with multi-layer depth and epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hybridpose: 6d object pose estimation under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-path learning for object pose estimation across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Durner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Kai O Arras, and Rudolph Triebel</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13916" to="13925" />
		</imprint>
	</monogr>
	<note>En Yen Puang, Zoltan-Csaba Marton, Narunas Vaskevicius</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time accurate 3d head tracking and pose estimation with consumer rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Joseph</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="158" to="183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Real-Time Seamless Single Shot 6D Object Pose Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self6d: Selfsupervised monocular 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhun</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Descriptors for Object Recognition and 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PoseCNN: A convolutional neural network for 6D object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gradient centralization: A new optimization technique for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="635" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dpod: Dense 6d pose object detector in rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Splitattention networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
