<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kinematic-Structure-Preserved Representation for Unsupervised 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundu</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Seth</surname></persName>
							<email>siddharthseth@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">M</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Chakraborty</surname></persName>
							<email>anirban@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Kinematic-Structure-Preserved Representation for Unsupervised 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimation of 3D human pose from monocular image has gained considerable attention, as a key step to several humancentric applications. However, generalizability of human pose estimation models developed using supervision on large-scale in-studio datasets remains questionable, as these models often perform unsatisfactorily on unseen in-the-wild environments. Though weakly-supervised models have been proposed to address this shortcoming, performance of such models relies on availability of paired supervision on some related tasks, such as 2D pose or multi-view image pairs. In contrast, we propose a novel kinematic-structure-preserved unsupervised 3D pose estimation framework 1 , which is not restrained by any paired or unpaired weak supervisions. Our pose estimation framework relies on a minimal set of prior knowledge that defines the underlying kinematic 3D structure, such as skeletal joint connectivity information with bone-length ratios in a fixed canonical scale. The proposed model employs three consecutive differentiable transformations named as forwardkinematics, camera-projection and spatial-map transformation. This design not only acts as a suitable bottleneck stimulating effective pose disentanglement, but also yields interpretable latent pose representations avoiding training of an explicit latent embedding to pose mapper. Furthermore, devoid of unstable adversarial setup, we re-utilize the decoder to formalize an energy-based loss, which enables us to learn from in-the-wild videos, beyond laboratory settings. Comprehensive experiments demonstrate our state-of-the-art unsupervised and weakly-supervised pose estimation performance on both Human3.6M and MPI-INF-3DHP datasets. Qualitative results on unseen environments further establish our superior generalization ability. * equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building general intelligent systems, capable of understanding the inherent 3D structure and pose of non-rigid humans from monocular RGB images, remains an illusive goal in the vision community. In recent years, researchers aim to solve this problem by leveraging the advances in two key aspects, i.e. a) improved architecture design <ref type="bibr" target="#b10">(Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b3">Chu et al. 2017</ref>) and b) increasing collection of diverse annotated samples to fuel the supervised learning paradigm <ref type="bibr" target="#b9">(Mehta et al. 2017b</ref>). However, obtaining 3D pose <ref type="table">Table 1</ref>: Characteristic comparison of our approach against prior unsupervised and weakly-supervised human 3D pose estimation works, in terms of access to direct (paired) or indirect (unpaired) supervision levels (MV: Multi-View). Note that, in the proposed framework the latent pose representation itself, is the 3D pose coordinates, thereby avoiding training of a separate latent to 3D pose mapper (last column).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Paired sup. Unpaired sup.  <ref type="bibr" target="#b3">(Chen et al. 2019b</ref>) <ref type="bibr" target="#b14">(Wandt et al. 2019</ref>) <ref type="bibr" target="#b2">(Chen et al. 2019a)</ref> Ours <ref type="bibr">(unsup.)</ref> ground-truth for non-rigid human-bodies is a highly inconvenient process. Available motion capture systems, such as body-worn sensors (IMUs) or multi-camera structure-frommotion (SFM), requires careful pre-calibration, and hence usually done in a pre-setup laboratory environment <ref type="bibr" target="#b6">(Ionescu et al. 2013;</ref><ref type="bibr" target="#b16">Zhang et al. 2017)</ref>. This often restricts diversity in the collected dataset, which in turn hampers generalization of the supervised models trained on such data. For instance, the widely used Human3.6M <ref type="bibr" target="#b6">(Ionescu et al. 2013)</ref> dataset captures 3D pose using 4 fixed cameras (i.e. only 4 backgrounds scenes), 11 actors (i.e. limited apparel variations), and 17 action categories (i.e. limited pose diversity). A model trained on this dataset delivers impressive results when tested on samples from the same dataset, but does not generalize to an unknown deployed environment, thereby yielding non-transferability issue.</p><p>To deal with this problem, researchers have started exploring innovative techniques to reduce dependency on annotated real samples. Aiming to enhance appearance diversity on known 3D pose samples (CMU-MoCap), synthetic datasets have been proposed, by compositing a diverse set of human template foregrounds with random backgrounds <ref type="bibr" target="#b13">(Varol et al. 2017)</ref>. However, models trained on such samples do not generalize to a new motion (e.g. a particular dance form), apparel, or environment much different from the training samples, as a result of large domain shift. Following a different direction, several recent works propose weakly-supervised approaches , where they consider access to a large-scale dataset with paired supervision on some related-tasks other than the task in focus (i.e. 3D pose estimation). Particularly, they access multiple cues for weak supervision, such as, a) paired 2D ground-truth, b) unpaired 3D ground-truth (3D pose without the corresponding image), c) multi-view image pair <ref type="bibr" target="#b10">(Rhodin et al. 2018)</ref>, d) camera parameters in a multi-view setup etc. (see <ref type="table">Table 1</ref> for a detailed analysis).</p><p>While accessing such weak paired-supervisions, the general approach is to formalize a self-supervised consistency loop, such as 2D?3D?2D <ref type="bibr" target="#b12">(Tung et al. 2017)</ref>, view-1?3D?view-2 (Kocabas, Karagoz, and Akbas 2019), etc. However, the limitations of domain-shift still persists as a result of using annotated data (e.g. 2D ground-truth or multiview camera extrinsic). To this end, without accessing such paired samples, <ref type="bibr" target="#b7">(Jakab et al. 2019)</ref> proposed to leverage unpaired samples to model the natural distribution of the expected representations (i.e. 2D or 3D pose) using adversarial learning. Obtaining such samples, however, requires access to a 2D or 3D pose dataset and hence the learning process is still biased towards the action categories presented in that dataset. One can not expect to have access to any of the above discussed paired or unpaired weak supervisory signals for an unknown deployed environment (e.g. frames of a dance-show where the actor is wearing a rare traditional costume). This motivates us to formalize a fully-unsupervised framework for monocular 3D pose estimation, where the pose representation can be adapted to the deployed environment by accessing only the RGB video frames devoid of dependency on any explicit supervisory signal.</p><p>Our contributions. We propose a novel unsupervised 3D pose estimation framework, relying on a carefully designed kinematic structure preservation pipeline. Here, we constrain the latent pose embedding, to form interpretable 3D pose representation, thus avoiding the need for an explicit latent to 3D pose mapper. Several recent approaches aim to learn a prior characterizing kinematically plausible 3D human poses using available MoCap datasets <ref type="bibr" target="#b8">(Kundu et al. 2019)</ref>. In contrast, we plan to utilize minimal kinematic prior information, by adhering to the restrictions to not use any external unpaired supervision. This involves, a) access to the knowledge of hierarchical limb connectivity, b) a vector of allowed bone length ratios, and c) a set of 20 synthetically rendered images with diverse background and pose (i.e. a minimal dataset with paired supervision to standardize the model towards the intended 2D or 3D pose conventions). The aforementioned prior information is very minimal in comparison to the pose-conditioned limits formalized by <ref type="bibr" target="#b0">(Akhter et al. 2015)</ref> in terms of both dataset size and parameters associated to define the constraints.</p><p>In the absence of multi-view or depth information, we infer 3D structure, directly from the video samples, for the unsupervised 3D pose estimation task. One can easily segment moving objects from a video, in absence of any background (BG) motion. However, this is only applicable to instudio static camera feeds. Aiming to work on in-the-wild YouTube videos , we formalize separate unsupervised learn-ing schemes for videos with both static and dynamic BG. In absence of background motion, we form pairs of video frames with a rough estimate of the corresponding BG image, following a training scheme to disentangle foregroundapparel and the associated 3D pose. However, in the presence of BG motion, we lack in forming such consistent pairs, and thus devise a novel energy-based loss on the disentangled pose and appearance representations. In summary,</p><p>? We formalize a novel collection of three differentiable transformations, which not only acts as a bottleneck stimulating effective pose disentanglement but also yields interpretable latent pose representations avoiding training of an explicit latent-to-pose mapper.</p><p>? The proposed energy-based loss, not only enables us to learn from in-the-wild videos, but also improves generalizability of the model as a result of training on diverse scenarios, without ignoring any individual image sample.</p><p>? We demonstrate state-of-the-art unsupervised and weakly-supervised 3D pose estimation performance on both Human3.6M and MPI-INF-3DHP datasets.</p><p>2 Related Works 3D human pose estimation. There is a plethora of fullysupervised 3D pose estimations works <ref type="bibr" target="#b5">(Fang et al. 2018;</ref><ref type="bibr" target="#b9">Mehta et al. 2017a;</ref><ref type="bibr" target="#b9">Mehta et al. 2017b)</ref>, where the performance is bench-marked on the same dataset, which is used for training. Such approaches do not generalize on minimal domain shifts beyond the laboratory environment. In absence of large-scale diverse outdoor datasets with 3D pose annotations, datasets with 2D pose annotations is used as a weak supervisory signal for transfer learning using various 2D to 3D lifting techniques <ref type="bibr" target="#b12">(Tung et al. 2017;</ref><ref type="bibr" target="#b10">Ramakrishna et al. 2012</ref>). However, these approaches still rely on availability of 2D pose annotations. Avoiding this, <ref type="bibr" target="#b8">(Kocabas et al. 2019;</ref><ref type="bibr" target="#b10">Rhodin et al. 2018)</ref> proposed to use multi-view correspondence acquired by synchronized cameras. But in such approaches <ref type="bibr" target="#b10">(Rhodin et al. 2018)</ref>, the latent pose representation remains uninterpretable and abstract, thereby requiring a substantially large amount of 3D supervision to explicitly train a latentto-pose mapping mapper. We avoid training of such explicit mapping, by casting the latent representation, itself as the 3D pose coordinates. This is realized as a result of formalizing the geometry-aware bottleneck. Geometry-aware representations. To capture intrinsic structure of objects, the general approach is to disentangle individual factors of variations, such as appearance, camera viewpoint and other pose related cues, by leveraging interinstance correspondence. In literature, we find unsupervised land-mark detection techniques <ref type="bibr" target="#b16">(Zhang et al. 2018)</ref>, that aim to utilize a relative transformation between a pair of instances of the same object, targeting the 2D pose estimation task. To obtain such pairs, these approaches rely on either of the following two directions, viz. a) frames from a video with an acceptable time-difference <ref type="bibr" target="#b6">(Jakab et al. 2018</ref>), or b) synthetically simulated 2D transformations <ref type="bibr" target="#b11">(Rocco, Arandjelovic, and Sivic 2017)</ref>. However, such techniques fail to capture the 3D structure of the object in the absence of multiview information. The problem becomes more challenging for deformable 3D skeletal structures as found in diverse human poses. Recently <ref type="bibr" target="#b6">(Jakab et al. 2018)</ref> proposed an unsupervised 2D landmark estimation method to disentangle pose from appearance using a conditional image generation framework. However, the predicted 2D landmarks do not match with the standard human pose key-points, hence are highly un-interpretable with some landmarks even lying on the background. Such outputs can not be used for a consequent task requiring a structurally consistent 2D pose input.</p><p>Defining structural constraints in 2D is highly ill-posed, considering images as projections of the actual 3D world. Acknowledging this, we plan to estimate 3D pose separately with camera parameters followed by a camera-projection to obtain the 2D landmarks. As a result of this inverse-graphics formalization, we have the liberty to impose structural constraints directly on the 3D skeletal representation, where the bone-length and other kinematic constraints can be imposed seamlessly using consistent rules as compared to the corresponding 2D representation. A careful realization of 3D structural constraints not only helps us to obtain interpretable 2D landmarks but also reduces the inherent uncertainty associated with the process of lifting a monocular 2D images to 3D pose <ref type="bibr" target="#b2">(Chen et al. 2019a)</ref>, in absence of any additional supervision such as multi-view or depth cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our aim is to learn a mapping function, that can map an RGB image of human to its 3D pose by accessing minimal kinematic prior information. Motivated by (Rhodin et al. 2018), we plan to cast it as an unsupervised disentanglement of three different factors i.e., a) foreground (FG) appearance, b) background (BG) appearance, and c) kinematic pose. <ref type="bibr">However, unlike (Rhodin et al. 2018</ref>) in absence of multi-view pairs, we have access to simple monocular video streams of human actions consisting of both static and dynamic BG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 1A</ref>, we employ two encoder networks each with a different architecture, E P and E A to extract the localkinematic parameters v k (see below) and FG-appearance, f a respectively from a given RGB image. Additionally, E P also outputs 6 camera parameters, denoted by c, to obtain coordinates of the camera-projected 2D landmarks, p 2D .</p><p>One of the major challenges in learning factorized representations (Denton and others 2017) is to realize purity among the representations. More concretely, the appearance representation should not embed any pose related information and vice-versa. To achieve this, we enforce a bottleneck on the pose representation by imposing kinematic-structure based constraints (in 3D) followed by an inverse-graphics formalization for 3D to 2D re-projection. This introduces three pre-defined transformations i.e., a) Forward kinematic transformation, T f k and b) Camera projection transformation T c , and c) Spatial-map transformation T m . a) Forward kinematic transformation, T f k Most of the prior 3D pose estimation approaches <ref type="bibr" target="#b2">(Chen et al. 2019a;</ref><ref type="bibr" target="#b10">Rhodin et al. 2018)</ref> aim to either directly regress joint locations in 3D or depth associated with the available 2D landmarks. Such approaches do not guarantee validity of the kinematic structure, thus requiring additional loss terms in the optimization pipeline to explicitly impose kinematic constraints such as bone-length and limb-connectivity information <ref type="bibr" target="#b5">(Habibie et al. 2019)</ref>. In contrast, we formalize a view-invariant local-kinematic representation of the 3D skeleton based on the knowledge of skeleton joint connectivity. We define a canonical rule (see <ref type="figure" target="#fig_1">Fig. 1B</ref>), by fixing the neck and pelvis joint (along z-axis, with pelvis at the origin) and restricting the trunk to hip-line (line segment connecting the two hip joints) angle, to rotate only about x-axis on the YZ-plane(i.e. 1-DOF) in the canonical coordinate system C (i.e. Cartesian system defined at the pelvis as origin). Our network regresses one pelvis to hip-line angle and 13 unit-vectors (all 3-DOF), which are defined at their respective parent-relative local coordinate systems, L P a(j) , where P a(j) denotes the parent joint of j in the skeletal kinematic tree. Thus, v k ? R 40 (i.e. 1+13*3). These predictions are then passed on to the forward-kinematic transformation to obtain the 3D joint coordinates</p><formula xml:id="formula_0">p 3D in C, i.e. T f k : v k ? p 3D where p 3D ? R 3J</formula><p>, with J being the total number of skeleton joints. First, positions of the 3 root joints, p (j) 3D for j as left-hip, right-hip and neck, are obtained using the above defined canonical rule after applying the estimate of the trunk to hip-line angle, v</p><p>k . Let len (j) store the length of the line-segment (in a fixed canonical unit) connecting a joint j with P a(j). Then, p (j) 3D for rest of the joints is realized using the following recursive equation, p <ref type="figure" target="#fig_1">Fig. 1B (dotted box)</ref> for a more clear picture. b) Camera-projection transformation, T c As p 3D is designed to be view-invariant, we rely on estimates of the camera extrinsics c (3 angles, each predicted as 2 parameters, the sin and cos component), which is used to rotate and translate the camera in the canonical coordinate system C, to obtain 2D landmarks of the skeleton (i.e. using the rotation and translation matrices, R c and T c respectively). Note that, these 2D landmarks are expected to register with the corresponding joint locations in the input image. Thus, the 2D landmarks are obtained as, p</p><formula xml:id="formula_2">(j) 3D = p (P a(j)) 3D + len (j) v (j) k . See</formula><formula xml:id="formula_3">(j) 2D = P (R c * p (j) 3D + T c ),</formula><p>where P denotes a fixed perspective camera transformation. c) Spatial-map transformation, T m After obtaining coordinates of the 2D landmarks p 2D ? R 2J , we aim to effectively aggregate it with the spatial appearance-embedding f a . Thus, we devise a transformation procedure T m , to transform the vectorized 2D coordinates into spatial-maps denoted by f 2D ? R H?W ?Ch , which are of consistent resolution to f a , i.e. T m : p 2D ? f 2D . To effectively encode both joint locations and their connectivity information, we propose to generate two sets of spatial maps namely, a) heatmap, f hm and b) affinity-map, f am (i.e., f 2D : (f hm , f am )). Note that, the transformations to obtain these spatial maps must be fully differentiable to allow the disentaglement of pose using the cross-pose image-reconstruction loss, computed at the decoder output (discussed in Sec. 3.3a). Keeping  this in mind, we implement a novel computational pipeline by formalizing translated and rotated Gaussians to represent both joint positions (i.e. f hm ) and skeleton-limb connectivity (i.e. f am ). We use a constant variance ? along both spatial directions to realize the heat-maps for each joint j, as f <ref type="figure" target="#fig_3">Fig. 2A</ref>).</p><formula xml:id="formula_4">(j) hm (u) = exp(?0.5||u ? p (j) 2d || 2 /? 2 ), where u : [u x , u y ] denotes the spatial-index in a H ? W lattice (see</formula><p>We formalize the following steps to obtain the affinity maps based on the connectivity of joints in the skeletal kinematic tree (see <ref type="figure" target="#fig_3">Fig. 2A</ref>). For each limb (line-segment), l with endpoints p l(j1)</p><formula xml:id="formula_5">2D and p l(j2) 2D , we first compute loca- tion of its mid-point, ? (l) : [? (l) x , ? (l)</formula><p>y ] and slope ? (l) . Following this, we perform an affine transformation to obtain,</p><formula xml:id="formula_6">u = R ? (l) * (u ? ? (l) ), where R ? (l) is the 2D rotation ma- trix. Let, ? (l)</formula><p>x and ? (l) y denote variance of a Gaussian along both spatial directions representing the limb l. We fix ? (l) y from prior knowledge of the limb width. Whereas, ? (l)</p><p>x is computed as ? * len(l) in the 2D euclidean space (see <ref type="bibr">Supplementary)</ref>. Finally, the affinity map is obtained as,</p><formula xml:id="formula_7">f (l) am (u) = exp(?0.5||u x /? (l)</formula><p>x || 2 ? 0.5||u y /? (l) y || 2 ) T f k , T c and T m (collectively denoted as T k ) are designed using perfectly differentiable operations, thus allowing back-propagation of gradients from the loss functions defined at the decoder output. As shown in <ref type="figure" target="#fig_1">Fig. 1A</ref>, the decoder takes in a tuple of spatial-pose-map representation and appearance (f 2D and f a respectively, concatenated along the channel dimension) to reconstruct an RGB image. To effectively disentangle BG information in f a , we fuse the background image B t towards the end of decoder architecture, inline with (Rhodin et al. 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Access to minimal prior knowledge</head><p>One of the key objectives of this work is to solve the unsupervised pose estimation problem with minimal access to prior knowledge whose acquisition often requires manual annotation or a data collection setup, such as CMU-MoCap . Adhering to this, we restrict the proposed framework from accessing any paired or unpaired data samples as shown in <ref type="table">Table 1</ref>. Here, we list the specific prior information that has been considered in the proposed framework,</p><p>? Kinematic skeletal structure (i.e. the joint connectivity information) with bone-length ratios in a fixed canonical scale. Note that, we do not consider access to the kinematic angle limits for the limb joints, as such angles are highly pose dependent particularly for diverse human skeleton structures <ref type="bibr" target="#b0">(Akhter and Black 2015)</ref>.</p><p>? A set of 20 synthetically rendered SMPL models with diverse 3D poses and FG appearance <ref type="bibr" target="#b13">(Varol et al. 2017)</ref>. We have direct paired supervision loss (denoted by L prior ) on these samples to standardize the model towards the intended 2D or 3D pose conventions (see Supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised training procedure</head><p>In contrast to <ref type="bibr" target="#b6">(Jakab et al. 2018)</ref>, we aim to disentangle foreground (FG) and background (BG) appearances, along with the disentanglement of pose. In a generalized setup, we also aim to learn from in-the-wild YouTube videos in contrast to in-studio datasets, avoiding dataset-bias.</p><p>Separating paired and unpaired samples. For an efficient disentanglement, we aim to form image tuples of the form (I s , I t , B t ). Here, I s and I t are video frames, which have identical FG-appearance with a nonidentical kinematicpose (pairs formed between frames beyond a certain timedifference). As each video-clip captures action of an individual in a certain apparel, FG-appearance remains identical among frames from the same video. Here, B t denotes an estimate of BG image without the human subject corresponding to the image I t , which is obtained as the median of pixel intensities across a time-window including the frame I t . However, such an estimate of B t is possible only for scenarios with no camera movement beyond a certain time window to capture enough background evidence (i.e. static background with a moving human subject). Given an in-the-wild dataset of videos, we classify temporal clips of a certain duration (&gt;5 seconds) into two groups based on the amount of BG motion in that clip. This is obtained by measuring the pixel-wise L2 loss among the frames in a clip, considering human action covers only 10-20% of pixels in the full video frame (see Supplementary). Following this, we realize two disjoint datasets denoted by D p = {(I   <ref type="figure" target="#fig_1">Fig. 1A</ref>, given a source and target image (i.e. I s and I t ), we aim to transfer the pose of I t (i.e. f 2D ) to the FGappearance extracted from I s (i.e. f a ) and background from B t to reconstruct? t . Here, the FG and BG appearance information can not leak through pose representation because of the low dimensional bottleneck i.e. p 2D ? R 2J . Moreover, consecutive predefined matrix and spatial-transformation operations further restrict the framework from leaking appearance information through the pose branch even as lowmagnitude signals. Note that, the BG of I s may not register with the BG of I t , when the person moves in the 3D world (even in a fixed camera scenario) as these images are outputs of an off-the shelf person-detector. As a result of this BG disparity and explicit presence of the clean spatially-registered background B t , D I catches the BG information directly from B t , thereby forcing f a to solely model FG-appearance from the apparel-consistent source, I s . Besides this, we also expect to maintain perceptual consistency between I t and? t through the encoder networks, keeping in mind the later energy-based formalization (next section). Thus, all the network parameters are optimized for the paired samples using the following loss function,</p><formula xml:id="formula_8">L P = |I t ?? t | + ? 1 |p 2D ?p 2D | + ? 2 |f a ?f a |. Here, p 2D = T k ? E P (? t ) andf a = E A (? t ).</formula><p>b) Training objective for unpaired samples, D unp Although, we find a good amount of YouTube videos where human motion (e.g. dance videos) is captured on a tripod mounted static camera, such videos are mostly limited to indoor environments. However, a diverse set of human actions are captured in outdoor settings (e.g. sports related activities), which usually involves camera motion or dynamic BG. Aiming to learn a general pose representation, instead of ignoring the frames from video-clips with dynamic BG, we plan to formalize a novel direction to adapt the parameters of E P and E A even for such diverse scenarios. We hypothesize that the decoder D I expects the pose and FG-appearance representation in a particular form, satisfying the corresponding input distributions, P (f 2D ) and P (f a ). Here, a reliable estimate of P (f 2D ) and P (f a ) can be achieved solely on samples from D p in presence of paired supervision, avoiding mode-collapse. More concretely, the parameters of D I should not be optimized on samples from D unp (as shown in <ref type="figure" target="#fig_3">Fig. 2B</ref> with a lock sign). Following this, one can treat D I analogous to a critic, which outputs a reliable prediction (an image of human with pose from I t , FGappearance from I s and BG from B t ) only when its inputs f 2D and f a satisfy the expected distributions-P (f 2D ) and P (f a ) respectively. We plan to leverage this analogy to effectively use the frozen D I network as an energy function to realize simultaneous adaptation of E P and E A for the unpaired samples from D unp .</p><p>We denote B r to represent a random background image. As shown in <ref type="figure" target="#fig_3">Fig. 2B</ref>, here? t = D I (f 2D , f a , B r ), in absence of access to a paired image to enforce a direct pixel-wise loss. Thus, the parameters of E P and E A are optimized for the unpaired samples using the following loss function,</p><formula xml:id="formula_9">L UNP = |p 2D ?p 2D | + ? 2 |f a ?f a |, wher? p 2D = T ?1 ? T k ? E P ? T (? t ) andf a = E A (? t ).</formula><p>Here, T and T ?1 represents a differentiable spatial transformation (such as image flip or in-plane rotation) and its inverse, respectively. We employ this to maintain a consistent representation across spatial-transformations. Note that, for the flip-operation of p 2D , we also exchange the indices of the joints associated with the left side to right and vice-versa.</p><p>We train on three different loss functions, viz. L prior , L P , and L UNP at separate iterations, each with different optimizer. Here, L prior denotes the supervised loss directly on p 3D and p 2D for the synthetically rendered images on randomly selected backgrounds, as discussed before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe experimental details followed by a thorough analysis of the framework for bench-marking on two widely used datasets, Human3.6M and MPI-INF-3DHP.</p><p>We use Resnet-50 (till res4f ) with ImageNet-pretrained parameters as the base pose encoder E P , whereas the appearance encoder is designed separately using 10 Convolutions. E P later divides into two parallel branches of fullyconnected layers dedicated for v k and c respectively. We use J = 17 for all our experiments as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. The <ref type="table">Table 2</ref>: Results on Human3.6M following the standard protocol-II setup. Here, Sup. (2nd column) denotes the amount of supervision accessed by the respective approaches. Accordingly, the table is divided into 4 row-groups, a) row 1-5 use full 3D pose sup., b) row 6-10 use full 2D pose as weak sup. c) row 11-12: unsupervised approaches, and d) row 13: Ours(semi-sup.). We outperform prior approaches in both weakly supervised and unsupervised setting (highlighted as boldface).  channel-wise aggregation of f am (16-channels) and f hm (17-channels) is passed through two convolutional layers to obtain f 2D (128-maps), which is then concatenated with f a (512-maps) to form the input for D I (each with 14?14 spatial dimension). Our experiments use different AdaGrad optimizers (learning rate: 0.001) for each individual loss components in alternate training iterations, thereby avoiding any hyper-parameter tuning. We perform several augmentations (color jittering, mirroring, and in-plane rotation) of the 20 synthetic samples, which are used to provide a direct supervised loss at the intermediate pose representations.</p><p>Datasets. The base-model is trained on a mixture of two datasets, i.e. Human3.6M and an in-house collection of YouTube videos (also refereed as YTube). In contrast to the in-studio H3.6M dataset, YTube contains human subjects in diverse apparel and BG scenes performing varied forms of motion (usually dance forms such as western, modern, contemporary etc.). Note that all samples from H3.6M contribute to the paired dataset D p , whereas ?40% samples in YTube contributed to D p and rest to D unp based on the associated BG motion criteria. However, as we do not have ground-truth 3D pose for the samples from YTube (in-the-wild dataset), we use MPI-INF-3DHP (also refereed as 3DHP) to quantitatively benchmark generalization of the proposed pose estimation framework. a) Evaluation on Human3.6M. We evaluate our framework on protocol-II, after performing scaling and rigid alignment of the poses inline with the prior arts <ref type="bibr" target="#b2">(Chen et al. 2019a;</ref><ref type="bibr" target="#b10">Rhodin et al. 2018)</ref>. We train three different variants of the proposed framework i.e. a) Ours(unsup.), b) Ours(semi-sup.), and c) Ours(weakly-sup.) as reported in <ref type="table">Table 2</ref>. After training the base-model on the mixed YTube+H3.6M dataset, we finetune it on the static H3.6M dataset by employing L prior and L p (without using any multi-view or pose supervision) and denote this model as Ours <ref type="bibr">(unsup.)</ref>. This model is further trained with full supervision on the 2D pose landmarks simultaneously with L prior and L p to obtain Ours(weakly-sup.). Finally, we also train Ours(unsup.) with supervision on 5% 3D of the entire trainset simultaneously with L prior and L p (to avoid over-fitting) and denote it as Ours(semi-sup.). As shown in <ref type="table">Table 2</ref>, Ours(unsup.) clearly outperforms the priorart (Rhodin et al. 2018) with a significant margin (89.4 vs. 98.2) even without leveraging multi-view supervision. Moreover, Ours(weakly-sup.) demonstrates state-of-the-art performance against prior weakly supervised approaches. b) Evaluation on MPI-INF-3DHP. We aim to realize a higher level of generalization in consequence of leveraging rich kinematic prior information. The proposed framework outputs 3D pose, which is bounded by the kinematic plausibility constraints even for unseen apparel, BG and action categories. This characteristic is clearly observed while evaluating performance of our framework on unseen 3DHP dataset. We take Ours(weakly-sup.) model trained on YTube+H3.6M dataset to obtain 3D pose predictions on unseen 3DHP testset (9th row in <ref type="table" target="#tab_2">Table 3</ref>). We clearly outperform the prior work <ref type="bibr" target="#b2">(Chen et al. 2019a</ref>) by a significant margin in a fully-unseen setting (8th and 9th row with -3DHP in <ref type="table" target="#tab_2">Table 3</ref>). Furthermore, our weakly supervised model (with 100% 2D pose supervision) achieves state-of-the-art performance against prior approaches at equal supervision level. c) Ablation study. In the proposed framework, our major contribution is attributed to the design of differentiable transformations and an innovative way to facilitate the us-  Figure 4: Qualitative results on 4 different datasets. Note that, results on LSP is obtained in an unseen setting (i.e. not even unpaired unsup.</p><p>training). The pink box highlights some failure cases, specifically in presence of self-occlusion as a result of joint-position ambiguity. age of unpaired samples even in presence of BG motion. Though effectiveness of camera-projection has been studied in certain prior works <ref type="bibr" target="#b2">(Chen et al. 2019a</ref>), use of forwardkinematic transformation T f k and affinity map in the spatialmap transformation T m is employed for the first time in such a learning framework. Therefore, we evaluate importance of both T f k and T m by separately bypassing these modules through neural network transformations. Results in <ref type="table" target="#tab_4">Table 4</ref> clearly highlight effectiveness of these carefully designed transformations for the unsupervised 3D pose estimation task.</p><p>d) Qualitative results. <ref type="figure" target="#fig_4">Fig. 3</ref> depicts qualitative results derived from Ours(unsup.) on in-studio H3.6M and in-thewild YTube dataset. It highlights effectiveness of unsuper-vised disentanglement through separation or cross-transfer of apparel, pose, camera-view and BG, for novel image synthesis. Though, our focus is to disentangle 3D pose information, separation of apparel and pose transfer is achieved as a byproduct of the proposed learning framework. In <ref type="figure">Fig. 4</ref> we show results on the 3D pose estimation task obtained from Ours(weakly-sup.) model. Though we train our model on H3.6M, 3DHP and YTube datasets, results on LSP dataset (Johnson and Everingham 2010) is obtained without training on the corresponding train-set, i.e. in a fully-unseen setting. Reliable pose estimation on such diverse unseen images highlights generalization of the learned representations thereby overcoming the problem of dataset-bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present an unsupervised 3D human pose estimation framework, which relies on a minimal set of prior knowledge regarding the underlying kinematic 3D structure. The proposed local-kinematic model indirectly endorses a kinematic plausibility bound on the predicted poses, thereby limiting the model from delivering implausible pose outcomes. Furthermore, our framework is capable of leveraging knowledge from video frames even in presence of background motion, thus yielding superior generalization to unseen environments. In future, we would like to extend such frameworks for predicting 3D mesh, by characterizing the prior knowledge on human shape, alongside pose and appearance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>A. Illustration of the proposed framework indicating output notation of individual modules. B. An overview of the three differentiable transformations, with step-wise progression of forward kinematics using local-kinematic parameters, v k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t )} N i=1 and D unp = {(I Training pipeline for unpaired samples from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>A. Illustration of the steps to obtain the spatial heat-map and affinity-map from the projected 2D coordinates. B. An overview of the proposed data-flow pipeline enabling energy-based loss formalization targeting unpaired samples from D unp . as sets of tuples with extractable BG pair (paired) and unextractable BG pair (unpaired), respectively. a) Training objective for paired samples, D p As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>A.FGFigure 3 :</head><label>3</label><figDesc>On H36M, in-studio dataset (samples from w/ paired BG sup.) Qualitative results, showing disentanglement of Pose (ID'd as P1 and P2), FG (ID'd as A1 and A2) and BG (ID'd as B1, B2, and B3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Training pipeline for paired samples (in blue) from</figDesc><table><row><cell>Option for</cell><cell></cell><cell></cell><cell></cell></row><row><cell>similar bg</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A.</cell><cell>1 DOF B. Differentiable transformations 3 DOF 3 DOF</cell><cell>Affinity map,</cell><cell cols="2">Heat map</cell></row><row><cell></cell><cell></cell><cell>camera</cell><cell>Spatial-map</cell><cell>transformer</cell></row><row><cell></cell><cell>Progression of Forward Kinematics</cell><cell>Camera</cell><cell></cell></row><row><cell></cell><cell></cell><cell>projection</cell><cell></cell></row><row><cell></cell><cell>B</cell><cell>Affinity map</cell><cell cols="2">Heat map</cell></row><row><cell></cell><cell></cell><cell>camera</cell><cell>Spatial-map</cell><cell>transformer</cell></row><row><cell></cell><cell>Progression of Forward Kinematics</cell><cell>Camera</cell><cell></cell></row><row><cell></cell><cell></cell><cell>projection</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Disc. Eat Greet Phone Photo Pose Purch. Sit SitD Smoke Wait Walk WalkD WalkT Avg.(?)<ref type="bibr" target="#b0">(Akhter et al. 2015)</ref> Full-3D 199.2 177.6 161.8 197.8 176.2 186.5 195.4 167.3 160.7 173.7 177.8 181.9 198.6 176.2 192.7 181.1   </figDesc><table><row><cell cols="18">Protocol-II Direct. (Zhou et al. 2016) Sup. Full-3D 99.7 95.8 87.9 116.8 108.3 107.3 93.5 95.3 109.1 137.5 106.0 102.2 110.4 106.5 115.2 106.7</cell></row><row><cell>(Bogo et al. 2016)</cell><cell>Full-3D</cell><cell cols="14">62.0 60.2 67.8 76.5 92.1 77.0 73.0 75.3 100.3 137.3 83.4 77.3 79.7 86.8</cell><cell>87.7</cell><cell>82.3</cell></row><row><cell>(Moreno et al. 2017)</cell><cell>Full-3D</cell><cell cols="14">66.1 61.7 84.5 73.7 65.2 67.2 60.9 67.3 103.5 74.6 92.6 69.6 78.0 71.5</cell><cell>73.2</cell><cell>74.0</cell></row><row><cell cols="2">(Martinez et al. 2017) Full-3D</cell><cell cols="14">44.8 52.0 44.4 50.5 61.7 59.4 45.1 41.9 66.3 77.6 54.0 58.8 35.9 49.0</cell><cell>40.7</cell><cell>52.1</cell></row><row><cell>(Wu et al. 2016)</cell><cell>Full-2D</cell><cell cols="13">78.6 90.8 92.5 89.4 108.9 112.4 77.1 106.7 127.4 139.0 103.4 91.4 79.1</cell><cell>-</cell><cell>-</cell><cell>98.4</cell></row><row><cell>(Tung et al. 2017)</cell><cell>Full-2D</cell><cell cols="13">77.6 91.4 89.9 88.0 107.3 110.1 75.9 107.5 124.2 137.8 102.2 90.3 78.6</cell><cell>-</cell><cell>-</cell><cell>97.2</cell></row><row><cell>(Chen et al. 2019a)</cell><cell>Full-2D</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.0</cell></row><row><cell>(Wandt et al. 2019)</cell><cell>Full-2D</cell><cell cols="14">53.0 58.3 59.6 66.5 72.8 71.0 56.7 69.6 78.3 95.2 66.6 58.5 63.2 57.5</cell><cell>49.9</cell><cell>65.1</cell></row><row><cell>Ours (weakly-sup.)</cell><cell>Full-2D</cell><cell cols="14">56.0 53.2 56.3 63.6 74.1 77.5 53.4 67.9 75.8 90.8 64.2 56.9 61.4 56.3</cell><cell>49.7</cell><cell>63.8</cell></row><row><cell cols="2">(Rhodin et al. 2018) Multi-view</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>98.2</cell></row><row><cell>Ours (unsup.)</cell><cell>No sup.</cell><cell cols="14">80.2 81.3 86.0 86.7 94.1 83.4 87.5 84.2 101.2 110.9 86.0 87.8 86.9 94.3</cell><cell>90.9</cell><cell>89.4</cell></row><row><cell>Ours (semi-sup.)</cell><cell>5%-3D</cell><cell cols="14">46.6 54.5 50.1 46.4 81.3 42.4 41.1 56.4 86.7 82.9 49.0 47.7 64.1 48.2</cell><cell>44.3</cell><cell>56.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="6">: Results for the MPI-INF-3DHP dataset. Here,</cell></row><row><cell cols="6">Trainset (2nd column) denotes access to 3DHP trainset im-</cell></row><row><cell cols="6">ages before evaluation. Sup. (3rd column) denotes supervi-</cell></row><row><cell cols="6">sion level on 3DHP image-pose pairs. 4 row-groups, a) row</cell></row><row><cell cols="6">1-2: Fully supervised, b) row 3-7: Weakly supervised, c) row</cell></row><row><cell cols="5">8-10: Unsupervised, d) row 11: Semi-supervised.</cell><cell></cell></row><row><cell>No. Method</cell><cell cols="5">Trainset Sup. PCK (?) AUC (?) MPJPE (?)</cell></row><row><cell>1. (Mehta et al. 2017c)</cell><cell cols="3">+3DHP Full-3D 76.6</cell><cell>40.4</cell><cell>124.7</cell></row><row><cell>2. (Rogez et al. 2017)</cell><cell cols="3">+3DHP Full-3D 59.6</cell><cell>27.6</cell><cell>158.4</cell></row><row><cell>3. (Zhou et al. 2017)</cell><cell cols="3">+3DHP Full-2D 69.2</cell><cell>32.5</cell><cell>137.1</cell></row><row><cell cols="4">4. (Kanazawa et al. 2018) +3DHP Full-2D 77.1</cell><cell>40.7</cell><cell>113.2</cell></row><row><cell>5. (Yang et al. 2018)</cell><cell cols="3">+3DHP Full-2D 69.0</cell><cell>32.0</cell><cell>-</cell></row><row><cell>6. (Chen et al. 2019a)</cell><cell cols="3">+3DHP Full-2D 71.7</cell><cell>36.3</cell><cell>-</cell></row><row><cell>7. Ours (weakly-sup.)</cell><cell cols="3">+3DHP Full-2D 80.2</cell><cell>44.8</cell><cell>97.1</cell></row><row><cell>8. (Chen et al. 2019a)</cell><cell>-3DHP</cell><cell>-</cell><cell>64.3</cell><cell>31.6</cell><cell>-</cell></row><row><cell>9. Ours (unsup.)</cell><cell>-3DHP</cell><cell>-</cell><cell>76.5</cell><cell>39.8</cell><cell>115.3</cell></row><row><cell>10. Ours (unsup.)</cell><cell cols="3">+3DHP No sup. 79.2</cell><cell>43.4</cell><cell>99.2</cell></row><row><cell>11. Ours (semi-sup.)</cell><cell cols="3">+3DHP 5%-3D 81.9</cell><cell>52.6</cell><cell>89.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). Images in first column (of each panel) define the IDs which are later used for novel image synthesis. Devoid of a direct pixel-wise loss, energy-based losses for samples from Dunp, help to clearly separate the FG person even in absence of a BG estimate (right panel).</figDesc><table><row><cell></cell><cell cols="3">A. Results of H36M dataset (in-studio)</cell><cell></cell><cell></cell><cell>C. Results of LSP dataset (unseen, in-the-wild samples)</cell></row><row><cell>Pred.</cell><cell>GT</cell><cell>Pred.</cell><cell>GT</cell><cell>Pred.</cell><cell>GT</cell><cell>Pred.</cell></row><row><cell></cell><cell cols="2">B. Results of 3DHP dataset</cell><cell></cell><cell></cell><cell></cell><cell>D. Results of YTube dataset (in-the-wild)</cell></row><row><cell>Pred.</cell><cell>GT</cell><cell>Pred.</cell><cell>GT</cell><cell>Pred.</cell><cell>GT</cell><cell>Pred.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on ablations of the proposed framework. It clearly highlights importance of T f k , T m , and use of D unp in the unsupervised training pipeline. Notice the improvement in 3DPCK on the unseen 3DHP testset as a result of incorporating D unp in the unsupervised training pipeline.</figDesc><table><row><cell>Method</cell><cell>Training set</cell><cell>MPJPE on</cell><cell>3DPCK on</cell></row><row><cell>(unsup.)</cell><cell>YTube+H3.6M</cell><cell>H36M</cell><cell>MPI-3DHP</cell></row><row><cell>Ours w/o T f k</cell><cell>Dp</cell><cell>134.8</cell><cell>47.9</cell></row><row><cell>Ours w/o Tm</cell><cell>Dp</cell><cell>101.8</cell><cell>61.7</cell></row><row><cell>Ours(unsup.)</cell><cell>Dp</cell><cell>91.1</cell><cell>66.3</cell></row><row><cell>Ours(unsup.)</cell><cell>Dp ? Dunp</cell><cell>89.4</cell><cell>71.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by a Wipro PhD Fellowship (Jogendra) and in part by DST, Govt. of India (DST/INT/UK/P-179/2017).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Poseconditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. 2, 4</title>
		<meeting>CVPR. 2, 4</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bogo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. 2 [Chen et al. 2019a]</title>
		<meeting>CVPR. 2 [Chen et al. 2019a]</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Proc. CVPR. 1, 3</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. 1</title>
		<meeting>CVPR. 1</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS. 3</title>
		<meeting>NeurIPS. 3</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI. 2 [Habibie</title>
		<meeting>AAAI. 2 [Habibie</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Proc. CVPR. 3</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Unsupervised learning of object landmarks through conditional image generation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jakab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02055.2</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning landmarks from unaligned data using image translation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. 6 [Kocabas, Karagoz, and Akbas</title>
		<meeting>CVPR. 6 [Kocabas, Karagoz, and Akbas</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Proc. ICCV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deng ; Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV. 1 [Ramakrishna, Kanade, and Sheikh</title>
		<meeting>ECCV. 1 [Ramakrishna, Kanade, and Sheikh</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Proc. ECCV. . 1, 2, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. 2 [Rogez, Weinzaepfel</title>
		<meeting>CVPR. 2 [Rogez, Weinzaepfel</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. 1, 6</title>
		<meeting>CVPR. 1, 6</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proc. ECCV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Martial arts, dancing and sports dataset: A challenging stereo and multi-view dataset for 3d human pose estimation. Image and Vision Computing. 1 [Zhang et al</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Sparse representation for 3d shape estimation: A convex relaxation approach</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weaklysupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
