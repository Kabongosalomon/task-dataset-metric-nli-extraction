<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DMT: Dynamic Mutual Training for Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="202211-05-12">May 12, 2022 11 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SenseTime Research</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuequan</forename><surname>Lu</surname></persName>
							<email>xuequan.lu@deakin.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">Deakin University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SenseTime Research</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DMT: Dynamic Mutual Training for Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="202211-05-12">May 12, 2022 11 May 2022</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Pattern Recognition</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>dynamic mutual training</term>
					<term>inter-model disagreement</term>
					<term>noisy pseudo label</term>
					<term>semi-supervised learning 1 Equal contribution 2 Corresponding authors Emails: tanxin2017@sjtu</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent semi-supervised learning methods use pseudo supervision as core idea, especially self-training methods that generate pseudo labels. However, pseudo labels are unreliable. Self-training methods usually rely on single model prediction confidence to filter low-confidence pseudo labels, thus remaining highconfidence errors and wasting many low-confidence correct labels. In this paper, we point out it is difficult for a model to counter its own errors. Instead, leveraging inter-model disagreement between different models is a key to locate pseudo label errors. With this new viewpoint, we propose mutual training between two different models by a dynamically re-weighted loss function, called Dynamic Mutual Training (DMT). We quantify inter-model disagreement by comparing predictions from two different models to dynamically re-weight loss in training, where a larger disagreement indicates a possible error and corresponds to a lower loss value. Extensive experiments show that DMT achieves state-of-theart performance in both image classification and semantic segmentation. Our codes are released at https://github.com/voldemortX/DST-CBC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, with the rise of deep learning, substantial improvements have been shown in various computer vision tasks, e.g. image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and semantic segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. However, deep learning methods require a large amount of annotated data to learn generalized representations. Although a large-scale dataset is easily gathered from cameras or web pages, the labor cost for labeling such a dataset has become unbearable in many real-world applications. For example, it takes 1.5 hours for a human annotator to label a high-resolution image of urban street scenes with pixel-wise annotations <ref type="bibr" target="#b4">[5]</ref>. In this work, we focus on semi-supervised learning to alleviate the label costs, by taking semantic segmentation and image classification as examples.</p><p>Semi-supervised learning labels only a small part of the dataset (labeled subset), and exploits the remaining part as unlabeled data (unlabeled subset).</p><p>To learn without labels, a natural idea is "bootstrapping" (pulling oneself up by one's own bootstraps) <ref type="bibr" target="#b5">[6]</ref>, i.e. using self (pseudo) supervisions. Two lines of approaches have achieved good performance on both semi-supervised image classification and semantic segmentation: entropy minimization (i.e. self-training) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and consistency regularization <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Recently, hybrid methods that combine those two directions, MixMatch with sharpening <ref type="bibr" target="#b10">[11]</ref>, s4GAN + MLMT with separate network branches <ref type="bibr" target="#b11">[12]</ref>, show state-of-the-art performance on image classification and semantic segmentation, respectively. Our method is based on offline self-training with data augmentation as consistency regularization, a hybrid method applicable to both tasks (Section 3.4).</p><p>Nevertheless, bootstrapping methods face a common issue, that is, pseudo supervisions tend to have classification errors. To address this, previous selftraining methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref> select pseudo labels by confidence, e.g. the predicted probability from a model trained on the labeled subset. However, methods based on the common assumption that higher confidence corresponds to cleaner labels still have drawbacks. We conduct a pseudo labeling experiment ( <ref type="figure" target="#fig_5">Fig. 1)</ref> to illustrate this issue. As shown in <ref type="figure" target="#fig_5">Fig. 1</ref>, using confidence to select pseudo (UURUUDWH <ref type="figure" target="#fig_5">Figure 1</ref>: Pseudo label error statistics. We report pseudo label error rates on 1,000 random images from CIFAR-10, using a 28-layer WideResNet model trained with 4,000 labeled samples only. The overall error rate is 12.90%, error rates for top-20%, top-40%, top-60%, top-80% images according to prediction confidence are also plotted. It can be observed that highconfidence errors do exist and lots of data will be discarded to achieve a low error rate, e.g.</p><p>&lt; 3% means discarding 60% data. labels suffers from two limitations. First, low-confidence correct pseudo labels are often ignored, i.e. in order to achieve a low label error rate for pseudo supervision, a large portion of low-confidence correct pseudo labels have to be discarded. Second, high confidence errors do exist. We can observe that even pseudo labels with top-20% confidence still have some errors. Moreover, pseudo supervision error from the model itself (termed as self-error ) can be extremely harmful in semi-supervised learning (Section 3.5).</p><p>To address these limitations, we propose a novel method from perspective of the inter-model disagreement. In particular, no matter what pseudo label selection metrics are employed, it is difficult for one model to find its own errors. Instead, two different models with disagreements on classification decisions, could potentially identify each other's errors. For instance, in image classification, model A could provide a pseudo label on unlabeled image x for model B to learn, and we can quantify the disagreement between A and B by their prediction statistics (i.e. assigning lower loss/gradient to this image if their disagreement is larger). Since the possibility of different models confidently making the same mistakes is low, most incorrect pseudo labels will have limited impact on learning.  Specifically, we propose Dynamic Mutual Training (DMT) with a noiserobust loss <ref type="figure" target="#fig_0">(Fig. 2)</ref>. First, we instantiate two different models. Then, one model provides unreliable pseudo labels for the other on unlabeled data. In mutual training, We define three disagreement cases and corresponding loss reweighting strategies, based on the relation of prediction confidence between the two models. In this way, loss is dynamically weighed to be lower when the disagreement is larger. Furthermore, inspired by curriculum learning, or easy to hard <ref type="bibr" target="#b13">[14]</ref>, we apply DMT iteratively, each time considering more unlabeled data, to gradually increase performance.</p><p>Our main contributions are summarized as follows.</p><p>? We analyze the pseudo label noise problem for semi-supervised learning and propose a new method from the new viewpoint of inter-model disagreement, i.e. instead of single model confidence, disagreement between models may indicate possible pseudo label errors.</p><p>? To quantify the inter-model disagreement, we propose a general and efficient bootstrapping approach, called Dynamic Mutual Training (DMT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DMT exploits the relation between different model predictions by a noise-</head><p>robust loss function where a larger inter-model disagreement corresponds to a lower loss weighting. The performance of DMT is further enhanced by casting it into an iterative framework.</p><p>? We demonstrate the effectiveness of our approach in different tasks and datasets, i.e. semi-supervised image classification on CIFAR-10 and semisupervised semantic segmentation on PASCAL VOC 2012 and Cityscapes.</p><p>Through extensive comparisons and ablations, the proposed method shows state-of-the-art performance on both tasks. In the harder semantic segmentation task, our method even surpasses manual annotation under a certain setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semi-Supervised Learning</head><p>Here we focus on methods that are most relevant to DMT. (Mainstream semi-supervised learning approaches including entropy minimization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8]</ref>, consistency regularization <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref> and disagreement-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> will be formally described in the following Section 3.) Among typical deep learning methods that use more than one models to explicitly/implicitly exploit the inter-model disagreement, the most similar to DMT are Dual Student <ref type="bibr" target="#b18">[19]</ref> and Deep Co-training <ref type="bibr" target="#b16">[17]</ref>. In Dual Student, two models are trained in parallel online to select stable examples for each other to learn. However, the stable examples are decided by one model alone and the other model cannot dispute that supervision. In Deep Co-training, two models are also trained in parallel online, and the learning objective is to minimize their disagreement on the unlabeled subset. Since disagreement is minimized, the models can rapidly converge to the same set of weights online, thus explicit weight distance constrains have to be employed to avoid collapse. Largely different from them, the dynamic loss in DMT is determined by both models and does not require special constrains to avoid collapse. We illustrate the major differences between DMT and these two methods in Note that co-training methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> use different models and learn by maximizing their agreement on unlabeled data. In contrast, we view disagreement <ref type="bibr" target="#b19">[20]</ref> as a principle in this work, i.e. the inter-model disagreement provides the possibility of learning, i.e. the combined correct predictions are more than any of the two models. The exact method formulation of using it can vary.</p><p>In this work, we employ the inter-model disagreement to specifically combat pseudo label noise by loss re-weighting, rather than penalizing disagreement in the learning objective.</p><p>Other than methodological differences to previous methods, DMT is also generally applicable to both image classification and semantic segmentation tasks. While most semi-supervised learning methods are ad-hoc and only work well in a limited range of tasks (e.g. only work on either image classification <ref type="bibr" target="#b8">[9]</ref> or pixel-wise task <ref type="bibr" target="#b21">[22]</ref>). To the best of our knowledge, no previous methods have been shown to reach state-of-the-art performance on both image classification and semantic segmentation without additional efforts. For instance, consistency-based methods such as Mean Teacher <ref type="bibr" target="#b8">[9]</ref> works well in image classification but performs only comparable to Baseline on PASCAL VOC 2012 semantic segmentation (Tab. 4). High-dimensional perturbations have to be imposed to work in semantic segmentation, e.g. CutMix <ref type="bibr" target="#b9">[10]</ref> and CCT <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning with Noisy Labels</head><p>Learning with noisy labels is a well-studied topic. Most researches on this topic tackle random noise that can be modeled by a noise transition matrix, with each matrix entry as the label random flip probability from one class to another <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. We focus on general methods that have no explicit noise source modeling. Decoupling <ref type="bibr" target="#b24">[25]</ref> trains two models simultaneously online, and only performs gradient descent when two models disagree, to decouple "when" and "how" to update model parameters, i.e. not allowing the noisy labels to control when to learn. Co-teaching <ref type="bibr" target="#b25">[26]</ref> also trains two models online, while each model selects low loss examples for the other to train, and a similar policy has been exploited in semi-supervised learning by Dual Student <ref type="bibr" target="#b18">[19]</ref>. Recently, Co-teaching+ <ref type="bibr" target="#b20">[21]</ref> combines the idea of Decoupling and Co-teaching. However, these methods still mostly show good performance regarding only random noise.</p><p>Contrary to how disagreement is leveraged in Decoupling/Co-teaching+ <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21]</ref>, in semi-supervised learning we deal with pseudo label noise made by deep nets similar to the model in training, where the disagreement between models signifies possible errors instead of an parameter update opportunity. Besides, Co-teaching <ref type="bibr" target="#b25">[26]</ref> uses two models to select examples for each other, and the training targets are still determined by one model only. While in DMT the two models collaborate explicitly to re-weigh loss. Major differences between Co-teaching+ and DMT are illustrated in <ref type="figure">Fig. 3</ref>.</p><p>There is one approach that deals particularly with pseudo labels <ref type="bibr" target="#b26">[27]</ref>. Specifically, given a feature embedding graph with every data point (image) as a node, a node's pseudo label can thus be rectified by its neighbors. It is feasible for relatively small datasets such as CIFAR-10, but rather unrealistic for segmentation datasets, where one image has millions of data points (pixels). Besides, the graph-based method is applied at the pseudo-labeling phase, making it complementary to DMT, which is adopted at training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Semi-supervised learning methods are often based on certain prior knowledge, thus models can "bootstrap" themselves with extra unlabeled data for better generalization. There are mainly three types of prior knowledge.</p><p>1. Entropy minimization. Predictive entropy is minimized for a model to make decisions on unlabeled data, which is apparently better than not making any decisions at all <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Consistency regularization. Prediction should remain consistent when</head><p>unlabeled data is perturbed by data augmentation <ref type="bibr" target="#b8">[9]</ref>.</p><p>3. Disagreement-based. Multiple classifiers should reach an agreement on unlabeled data predictions <ref type="bibr" target="#b16">[17]</ref>.</p><p>First, we focus on entropy minimization <ref type="bibr" target="#b27">[28]</ref> and summarize self-training methods as two types, online and offline. Then, we briefly describe the other two types of approaches: consistency regularization and disagreement-based methods. After that, we explain how consistency regularization can be integrated. Finally, we introduce the pseudo label noise problem, and how our method incorporating the inter-model disagreement can address it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Entropy Minimization and Self-training</head><p>Entropy H is defined as:</p><formula xml:id="formula_0">H = ? C c=1 p c log p c ,<label>(1)</label></formula><p>where p c is the predicted probability for class c, and C is the total number of classes. Since C c=1 p c = 1, H approaches the minimum value of 0 when one class is 1 and other classes are 0. Thus, entropy minimization encourages the model to make a certain decision.</p><p>Self-training takes the most probable class as a pseudo label and train models on unlabeled data, which is a common approach to achieve the minimum entropy. Note that here we do not consider soft pseudo labels (labels as probability vectors instead of a hard label or one-hot vector), since they do not directly General data augmentation Explicit consistency <ref type="bibr">Figure 4</ref>: Comparison between general data augmentation and explicit consistency regularization. It can be observed that they have similar principles and similar outcomes. General data augmentation can be seen as a simple form of "anchoring". Since the pseudo label on the unperturbed image has higher accuracy, it should be used as a semantic anchor for other perturbations. A similar concept of augmentation anchoring is also mentioned in <ref type="bibr" target="#b28">[29]</ref>.</p><p>correspond to entropy minimization and we do not observe decent performance of using soft pseudo labels.</p><p>Denote c ? arg max c F (c|x), pseudo label l is defined as:</p><formula xml:id="formula_1">l = ? ? ? ? ? c , F (c |x) &gt; T ignored, otherwise .<label>(2)</label></formula><p>where x is an image and F (?) is a classification model that predicts a probability distribution. T is the threshold for selection (e.g. fixed value such as 0.5 or ranked value such as the 20-th percentile). We summarize self-training for semisupervised learning as two types by when pseudo labels are generated.</p><p>Online self-training generates pseudo labels after each forward pass in a network. Pseudo labels are directly selected based on some selection metric online and provide supervision for the immediate backward pass <ref type="bibr" target="#b6">[7]</ref>.</p><p>Offline self-training firstly generates all pseudo labels, with a model trained only on the labeled subset (often followed by some form of selection process).</p><p>Then the model is fine-tuned/re-trained on all labels (pseudo and human-labeled).</p><p>This procedure can be applied iteratively by relabeling unlabeled data with the most recently trained model <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Consistency Regularization</head><p>Consistency regularization perturbs unlabeled data randomly, so as to derive the consistency loss L con , usually in the form of mean squared error <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_2">L con = ||F (x + ?) ? F (x + ? )|| 2 2 ,<label>(3)</label></formula><p>where F is the model, x is the input image. ?, ? are different perturbations, often achieved by random data augmentation. The recently popular variant Mean Teacher <ref type="bibr" target="#b8">[9]</ref> uses a teacher model T whose weights are defined as the exponential moving average of F . L con then becomes:</p><formula xml:id="formula_3">L con = ||F (x + ?) ? T (x + ? )|| 2 2 ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Disagreement-based Methods</head><p>Disagreement-based semi-supervised learning is formulated as Co-training <ref type="bibr" target="#b16">[17]</ref>, where prediction agreement is enforced between models. The loss for unlabeled data is Jensen-Shannon divergence (a common similarity metric):</p><formula xml:id="formula_4">L cot = H( 1 2 (p 1 + p 2 )) ? 1 2 (H(p 1 ) + H(p 2 )) ,<label>(5)</label></formula><p>where p 1 and p 2 are predictions from two models. H is entropy defined in Eq.</p><p>1. To prevent the two models from collapsing to the same results, a diversity loss should be added, inspired by exploiting adversarial samples <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Self-training and Consistency Regularization</head><p>We find that the consistency regularization plays a similar role to data augmentation in general. As shown in <ref type="figure">Fig. 4</ref>, with the same set of augmentation transforms, general data augmentation in offline self-training is similar to an "anchored" version of explicit consistency-based methods such as Mean Teacher <ref type="bibr" target="#b8">[9]</ref>. Anchoring may bring better performance overall if the anchor is positive, and degradation if the anchor is negative. In summary, consistency regularization and data augmentation have similar principles, and we observe good performance with data augmentation in offline self-training, which is the same as fully-supervised training after pseudo labeling.</p><p>However, for online self-training, it can be non-trivial to impose consistency regularization. Multiple different perturbations on input require multiple forward passes and induce higher computational cost, as demonstrated in many consistency-based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9]</ref>. Online self-training is also more sensitive to pseudo label noise, which will be elaborated in Section 3.5. Thus, we investigate the offline self-training case in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Noisy Pseudo Label</head><p>Pseudo labels are noisy, since they are not generated by human annotators.</p><p>Pseudo label noise is unique, coming from the model itself, different from random noise and noise from crowd-sourcing or search engines, which are widely explored and modeled according to the types and levels of noise <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Note that in other semi-supervised learning methods without explicit pseudo labels, self-supervision noise also exists. For instance, in consistency-based Mean</p><p>Teacher <ref type="bibr" target="#b8">[9]</ref> where the Mean Squared Error (MSE) loss is used to enforce consistency of student model output to teacher model output, the pseudo-supervision (probability distribution) provided by the teacher is noisy. In this work, we discuss noisy pseudo labels (one-hot vectors) in self-training, which is more straightforward.</p><p>To address pseudo label noise, self-training methods adopt prediction confidence as noise indicator, i.e. selecting high confidence pseudo labels. The simplest policy is confidence thresholding, where only pseudo labels with confidence higher than a fixed threshold are selected <ref type="bibr" target="#b6">[7]</ref>. Zou et al. further use different thresholds for each class <ref type="bibr" target="#b12">[13]</ref>. Hung et al. consider the confidence of a specialized two-class discriminator to discriminate between real and fake (wrong) labels <ref type="bibr" target="#b7">[8]</ref>.</p><p>Although noise rate is roughly lower when confidence is higher, it is difficult to attain sufficiently clean pseudo labels without discarding a large portion of unlabeled data ( <ref type="figure" target="#fig_5">Fig. 1</ref>), proved by observations on semantic segmentation tasks <ref type="bibr" target="#b7">[8]</ref> where only 27% ? 36% pixels can be pseudo-labeled without performance degradation on PASCAL VOC 2012 and <ref type="bibr" target="#b30">[31]</ref> where they only use 30% ? 50% pseudo labels on Cityscapes. This goes against the purpose of semi-supervised learning, which is using rather than discarding unlabeled data. By contrast, we define different weights to samples instead of discarding them.</p><p>Other than wasting unlabeled data, selected pseudo labels still have some noise. This is particularly troublesome for online self-training. However, there is always the same dilemma: a model can hardly correct itself. We introduce a new viewpoint from the inter-model disagreement, since it is possible to use one model to correct another. We assume that a same mistake made by different models simultaneously is not very likely, and the chances of different models all predicting the wrong class with high confidence are even less likely. In our proposed method, we quantify the disagreement between different models' predictions, allowing models to supervise each other with a disagreement re-weighted loss function. The proposed loss emphasizes on learning high-confidence agreements, effectively circumventing this dilemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In  is then trained by them. There are three possible cases in mutual training and three corresponding loss re-weighting strategies based on the two models' disagreement degree, defined in Eq. 6. All histograms above are for illustration purposes, rather than real outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dynamic Mutual Training</head><p>We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">The Dynamic Loss</head><p>We propose the dynamic loss, where the quantified inter-model disagreement serves as the dynamic loss weight. Taking image classification for example, we assume F A trains F B , let X , U denote labeled and unlabeled (pseudo labeled) samples in a batch of size N and let u be an unlabeled image in U, we define its</p><formula xml:id="formula_5">pseudo label as y A ? arg max y F A (y|u), with confidence c A ? F A (y A |u). And the prediction in training is y B ? arg max y F B (y|u), with confidence c B ? F B (y B |u). Let p B ? F B (y A |u) be the predicted probability of class y A by F B .</formula><p>The dynamic loss weight ? u is defined as:</p><formula xml:id="formula_6">? u = ? ? ? ? ? ? ? ? ? ? ? p ?1 B , y A = y B p ?2 B , y A = y B , c A ? c B 0, y A = y B , c A &lt; c B .<label>(6)</label></formula><p>The dynamic loss on unlabeled samples L U is then defined as:</p><formula xml:id="formula_7">L U = 1 N u,y A ?U ? u CE y A , F B (u) ,<label>(7)</label></formula><p>CE(?) is the cross-entropy loss:</p><formula xml:id="formula_8">CE(y, p) = ? C c=1 y c log p c ,<label>(8)</label></formula><p>where C is the total number of classes, y, p are one-hot label vector and probability vector, respectively.</p><p>Intuitively, for the pseudo labeled data, there are three different cases in training:</p><p>1. Agreement. F B agrees with the pseudo label.</p><p>2. Negative disagreement. F B disagrees with the pseudo label but the confidence on F B 's decision is lower than the pseudo label's.</p><p>3. Positive disagreement. F B disagrees with the pseudo label and has higher confidence.</p><p>In cases 1 and 2, we use the current model's predicted probability p B on the pseudo labeled class as weight, perceived as the quantified disagreement, i.e. a higher p B means F B has a higher agreement with F A . In case 3, we set the dynamic weight to 0 because the pseudo label is probably incorrect.</p><p>Dynamic weights are further re-scaled by hyper-parameters ? 1 , ? 2 , and a higher ? magnifies confidence differences and suppresses gradients overall. For instance, assuming dynamic weights 0.9 and 0.5, the loss scale ratio is 0.9/0.5 = 1.8; with ? = 2, the new ratio becomes larger: 0.9 2 /0.5 2 = 3.24. It can be interpreted that a relatively larger ? 1 represents a more emphasized entropy minimization, a larger ? 2 represents a more emphasized mutual learning. Large ? values are often better for high-noise scenarios, or to maintain larger intermodel disagreement.</p><p>Note that training uses the labeled subset along with the pseudo-labeled data, and the loss for labeled data L X remains unchanged, i.e. the typical cross-entropy loss:</p><formula xml:id="formula_9">L X = 1 N x,gt?X CE gt, F B (x) ,<label>(9)</label></formula><p>where x and gt denote image and ground truth pairs. The combined loss L is defined as:</p><formula xml:id="formula_10">L = L X + L U .<label>(10)</label></formula><p>The above example is given on classification. With regard to semantic segmentation, ? H?W u is a pixel-wise map (H for height and W for width), the re-weighting strategy remains the same and applies on each pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Initialize Disagreement</head><p>A key problem for leveraging the inter-model disagreement is how to initialize sufficiently different models. For simpler tasks such as CIFAR-10 image classification, it is simple to just randomly initialize different models for sufficient disagreement. However, for tasks that require pre-trained weights to work well, e.g. semantic segmentation, sufficiently different off-the-shelf pre-trained weights are hard to obtain, and the extra amount of time needed for a new pre-training is too costly compared to the task at hand. Thus, we mainly use pre-trained weights from different datasets (ImageNet and COCO) in semantic segmentation. For some extreme cases where labeled data is scarce and one set of pre-trained weights is clearly superior (about 100-200 labeled images), we use the better pre-trained weights, and train two models from different sub-subsets of the labeled subset by the following method.</p><p>Difference maximized sampling. Let R be the randomly shuffled labeled subset with size L. We aim to sample two equal-sized sub-subsets S A , S B from R, each with size ?L (0.5 &lt; ? &lt; 1). The goal (maximized difference) is to have the smallest intersection for them. We can simply derive S A , S B as:</p><formula xml:id="formula_11">? ? ? ? ? S A = R 0:?L S B = R (1??)L:L<label>(11)</label></formula><p>where R s:e represents a selection by index from s to e, including s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Iterative Framework</head><p>In this section we cast DMT into an iterative framework for better performance. Curriculum learning <ref type="bibr" target="#b13">[14]</ref>, or easy to hard, has been explored in semisupervised image classification <ref type="bibr" target="#b14">[15]</ref> and unsupervised domain adaptive semantic segmentation (a similar setup to semi-supervised learning) <ref type="bibr" target="#b12">[13]</ref> for better bootstrapping performance. Specifically, the same bootstrapping algorithm is repeated for multiple iterations, each iteration explores a harder setting, e.g. more pseudo labels with lower confidence. Inspired by their successes, we also perform DMT iteratively to achieve better performance. Output: Final best model F . Randomly initialize F i with a previously unused random seed <ref type="bibr" target="#b6">7</ref> Train F i on both S l and latest S p with the dynamic loss</p><formula xml:id="formula_12">8 F = F 5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Image Classification</head><p>For this task, we first train on the labeled subset, then conduct DMT iteratively for multiple times; each time we select more top-confident pseudo labels from the unlabeled subset and re-train a randomly initialized model for sufficient disagreement, same as concurrent work Curriculum Labeling <ref type="bibr" target="#b14">[15]</ref>. Pseudo code is shown in Alg. 1. However, the model re-trained from scratch provides little meaningful information at early training stage, thus we use a sigmoid-like function for ? values inspired by <ref type="bibr" target="#b8">[9]</ref>. Concretely, with the total training steps t max , at step t, ? = ? max e 5(1? t tmax ) 2 . Algorithm 2: Pseudo code for iterative DMT process in semantic segmentation. Input: Unlabeled subset S u , labeled sub S l .</p><p>Output: Final best model F . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Semantic Segmentation</head><p>Motivated by the fact that some classes are much easier to learn than others in semantic segmentation, CBST <ref type="bibr" target="#b12">[13]</ref> proposed an iterative self-training scheme by using more top-confident pseudo labels from each class at each iteration.</p><p>Furthermore, unlike image classification, fine-tuning performs reasonably well in semantic segmentation, and converges faster than re-training. Inspired by CBST, we conduct two separate fine-tunings between two differently initialized models, as shown in Alg. 2. In this setup, two models train each other equally and the inter-model disagreement is more straightforwardly utilized. To better illustrate how DMT is used iteratively for this harder task, we provide some examples of pseudo labels and dynamic weights during training in <ref type="figure" target="#fig_7">Fig. 6</ref>.</p><p>However, there is some difference between our iterative framework and CBST, since CBST does not select top-confident pseudo labels by direct ranking like us.</p><p>Instead, it first uses class-wise thresholds (defined by ranking) to re-normalize softmax predictions by dividing the thresholds class-wise, then pixels with confidence over 1 are selected. In most cases, this is the same as direct ranking, while in extreme cases, the predicted class would change, e.g. for a binary classification task, softmax result [0.6, 0.4] that originally predicts class 0, re-normalized by ranked thresholds [0.61, 0.39], will be changed to prediction class 1. This kind of mistake happen only when using nearly all pseudo labels, which does not affect the original CBST experiments since they use less than half pseudo labels.</p><p>While we use up to all data, re-normalization brings a slight degradation to final performance. Therefore, we use direct ranking instead of re-normalization.</p><p>We set 5 iterations to our method for training. Since fine-tuning converges very fast, the total number of training steps for each one of the two models remains similar to a fully-supervised training on the entire dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first specify dataset configurations (Section 5.1) and implementation details (Section 5.2). Then we compare the proposed DMT with state-of-the-art methods on image classification and semantic segmentation (Section 5.3). Finally, we analyze DMT by conducting a set of ablation studies (Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>For image classification, we employ the commonly used CIFAR-10 <ref type="bibr" target="#b31">[32]</ref> dataset.</p><p>For semantic segmentation, we evaluate our method on the popular PASCAL VOC 2012 <ref type="bibr" target="#b32">[33]</ref> and Cityscapes <ref type="bibr" target="#b4">[5]</ref> datasets.</p><p>? CIFAR-10. validation samples (val ) featuring common objects. We use the SBD <ref type="bibr" target="#b34">[35]</ref> augmented version with 10,582 training samples following common practice.</p><p>? Cityscapes. The Cityscapes <ref type="bibr" target="#b4">[5]</ref> dataset has 19 classes, 2,975 training samples, and 500 validation samples (val ) for urban driving scenes. Following <ref type="bibr" target="#b11">[12]</ref>, we down-sample the images by half to 512 ? 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Since DMT in an iterative framework has better performance and overall fast convergence as described in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Network Architectures</head><p>? Image classification. We follow MixMatch <ref type="bibr" target="#b28">[29]</ref> and use a shallow residual network WideResNet-28-2 (WRN-28-2) <ref type="bibr" target="#b0">[1]</ref> as the backbone.</p><p>? Semantic segmentation. We follow <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8]</ref> and use DeepLab-v2 ResNet-101 <ref type="bibr" target="#b2">[3]</ref> as the backbone, without multi-scale fusion or CRF post-processing.</p><p>Our implementation has slightly better performance than previous works, which is better aligned with the original DeepLab-v2 paper (74.75% averaged mean IoU on PASCAL VOC 2012 val set, higher than 73.6% reported in <ref type="bibr" target="#b7">[8]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Training</head><p>? Image classification. Each DMT iteration has 750 epochs with a learning rate of 0.1, a weight decay of 5 ? 10 ?4 , a momentum of 0.9, the cosine annealing technique and a batch size of 512, which is the same as Curriculum Labeling <ref type="bibr" target="#b14">[15]</ref>; we do not use SWA <ref type="bibr" target="#b36">[37]</ref> for fair comparisons with other methods. Data augmentations are RandAugment <ref type="bibr" target="#b37">[38]</ref> with Cutout.</p><p>We randomly select one augmentation operation with random intensity at each step to avoid hyper-parameter tuning (number of operations n and intensity m are hyper-parameters in RandAugment). We also use mixup <ref type="bibr" target="#b38">[39]</ref> by interpolating dynamic weights along with input images.</p><p>? Semantic segmentation. Each DMT iteration has less training steps due to fine-tuning. We use SGD with a momentum of 0.9, the poly learning rate schedule and a batch size of 8. Data augmentations include random To avoid too much hyper-parameter tuning, we set ? 1 = ? 2 . Pseudo-labeled data are used along with labeled data in DMT, thus we have a certain ratio (labeled : unlabeled, e.g. 1 : 7) to combine them in a batch. More details are listed in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Testing</head><p>? Image classification. We report the 5-times averaged test set performance with an exponential moving averaged (EMA) network on CIFAR-10 following MixMatch <ref type="bibr" target="#b10">[11]</ref>.</p><p>? Semantic segmentation. We report the three-times averaged val set mean intersection-over-union (mean IoU) in semantic segmentation tasks following common practice, provided the test set labels for these datasets are not publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons</head><p>To show the effectiveness and generality of DMT, we compare it with stateof-the-art methods on both image classification and semantic segmentation benchmarks as detailed in Section 5.1: CIFAR-10 <ref type="bibr" target="#b31">[32]</ref>, PASCAL VOC 2012 <ref type="bibr" target="#b32">[33]</ref> and Cityscapes <ref type="bibr" target="#b4">[5]</ref>. Standard practice for evaluating semi-supervised learning on these datasets is to treat most of a dataset as the unlabeled subset and use a small portion as the labeled subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">CIFAR-10</head><p>For CIFAR-10, we compare our method with consistency-based Mean Teacher (MT) <ref type="bibr" target="#b8">[9]</ref>, self-training method Curriculum Labeling (CL) <ref type="bibr" target="#b14">[15]</ref>, methods explicitly/implicitly using multiple models, Deep Co-Training between two models (DCT) <ref type="bibr" target="#b16">[17]</ref> and Dual Student (DS) <ref type="bibr" target="#b18">[19]</ref>, strong hybrid method MixMatch <ref type="bibr" target="#b10">[11]</ref>, and the combination of graph-based pseudo label propagation techniques in Density Aware Graph-based framework (DAG) <ref type="bibr" target="#b26">[27]</ref>. Methods are evaluated on the commonly adopted 1,000 labels and 4,000 labels splits. Supervised performance with mixup and strong data augmentation on the labeled subset is reported as Baseline. Baseline, CL (our re-implementation, without SWA) and DMT are implemented in the same codebase, while for other methods with WRN-28-2 we take the reported numbers from MixMatch. The remaining numbers are taken from the original papers.</p><p>As shown in Tab. 2, DMT steadily improves CL with the dynamic loss, larger performance gain is shown in harder setting (smaller labeled subset).</p><p>Note that CL is already a very high-performance method (only 2.33% less than</p><p>Oracle performance using 4,000 labels), it is rather difficult to gain further improvements in a strictly controlled comparison such as ours. While our method still achieves a slight improvement. We also compare DMT with more methods in Tab. 3, where DMT shows better performance than state-of-the-art methods. We are aware that recently ReMixMatch <ref type="bibr" target="#b28">[29]</ref> has obtained an accuracy of 94.86%, which is certainly benefited from using multiple forward passes and multiple losses, e.g. rotation loss, thus takes much more computing to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">PASCAL VOC 2012</head><p>PASCAL VOC 2012 is the most commonly used benchmark for semi-supervised semantic segmentation. We compare our method with consistency-based Mean  Teacher directly adapted to semantic segmentation (MT-Seg), Mean Teacher with strong CutMix augmentation <ref type="bibr" target="#b9">[10]</ref>, feature-level consistency-based method Cross-Consistency Training (CCT) <ref type="bibr" target="#b15">[16]</ref>, adapted Dual Student for semantic segmentation with an auxiliary flaw detector (GCT) <ref type="bibr" target="#b21">[22]</ref>, GAN-based method <ref type="bibr" target="#b7">[8]</ref> that pre-trains a discriminator to select pseudo labels, and hybrid method s4GAN + MLMT <ref type="bibr" target="#b11">[12]</ref> that adds consistency regularization upon <ref type="bibr" target="#b7">[8]</ref>   . In addition, DMT shows more stable performance across different labeled ratios <ref type="figure" target="#fig_10">(Fig. 7)</ref>.</p><p>Comparing with human supervision. We design an interesting experi-  Qualitative results. We provide qualitative comparisons in segmentation results among Baseline, DMT and ground truth. As shown in <ref type="figure" target="#fig_11">Fig. 8</ref>, there is confusion between similar classes in Baseline predictions (column 3), such as horse and cow (row 1, 2), dog and bird (row 3), train and motorbike (row 4).</p><p>After dynamic mutual training (column 4), class confusion is mostly resolved.</p><p>Also finer details are recovered (e.g. distant people in row 5 and chairs in row 6). Moreover, Baseline entirely fails to detect the dining table in row 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Cityscapes</head><p>Cityscapes features complex street scenes which are less ventured by semi-  As shown in Tab. 6, Hung et al. <ref type="bibr" target="#b7">[8]</ref> is merely comparable to our fully-tuned Baselines. Although stronger methods s4GAN + MLMT <ref type="bibr" target="#b11">[12]</ref> and CutMix <ref type="bibr" target="#b9">[10]</ref> still obtain good results, DMT outperforms them by 2 ? 3%. Refer to Appendix B for proper baseline training and Appendix A for qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablations</head><p>To further validate our method choice and provide additional insights for online and offline self-training, we conduct the following ablations.</p><p>? Online ST. Instead of 5 iterations of DMT, we perform online selftraining with fixed confidence threshold 0.9 for 20 epochs.  ? CBST. The CBST algorithm <ref type="bibr" target="#b12">[13]</ref> is modified to direct ranking, i.e. iterative class-balanced self-training without dynamic loss, similar to a classbalanced version of CL <ref type="bibr" target="#b14">[15]</ref>.</p><p>? DST. Same as DMT, except using only one model to provide pseudo labels for itself, i.e. DMT with only one model F A fine-tuning itself.</p><p>? DMT-Naive. We directly re-weight the loss by confidence without distinguishing the three cases in Eq. 6.</p><p>? DMT-Flip. In the third condition of Eq. 6, since the pseudo label is likely incorrect, instead of setting loss to 0, we flip the pseudo label to the current model's prediction and weight the loss by (1 ? c A ) ?2 , acting as an estimate of disagreement between models, given the pseudo label is flipped.</p><p>To show clear differences between setups, the ablations are carried out on PASCAL VOC 2012 in Tab. 7. This dataset is sufficiently complex and experiments run faster due to fine-tuning. ,WHUDWLRQ 0HDQ,R8 &amp;%67 '07 Online ST has very limited performance due to continuously fitting on selferrors, where it even performs worse than Baseline when labels are extremely scarce (1/50). We also observe that its performance remains similar without data augmentation on the unlabeled subset, echos our analysis that consistency regularization cannot be integrated for online self-training without multiple forward passes (Section 3.4).</p><p>CBST is an offline method, thus it is less sensitive to self-error than Online ST and consistently improves over Baseline by a clear margin. However, it only conducts self-training without considering pseudo label noise. In our experiments, performance increase stops at iteration three for CBST while DMT benefits from all 5 iterations. Because pseudo label noise prevents further improvements on CBST when using more unlabeled data.</p><p>DST requires half the computing budget compared to DMT and performs well. As fine-tuning goes on in each iteration, a relatively large learning rate and data augmentations drive the model to deviate from its previous-self 4 rapidly.</p><p>Thus, sufficient inter-model disagreement is provided for dynamic weighting to take effects. While in DMT, larger inter-model disagreement by starting from different model initializations naturally enables better final results, especially on the more challenging 1/50 split.</p><p>DMT-Naive is a simpler formulation to integrate inter-model disagreement, the performance is even comparable to DMT on the 1/20 split. But its performance degrades significantly when label noise is severe (1/50 split). Although this naive policy outperforms CBST on this task, its performance is similar or worse than CL on CIFAR-10, indicates poor generalization ability.</p><p>DMT-Flip is more complex than DMT. But its performance is similar to DMT, differences are at the level of random variations. We suspect by flipping labels, there is a similar drawback as online self-training: fitting newly made self-errors (Section 3.5). Thus the results are no better than ignoring those labels (usually fewer than 5% in training). Moreover, considering simplifying the policy also brings notable degradation (DMT-Naive), the three-cases setup in DMT (Eq. 6) offers a reasonable trade-off between performance and complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Discussions</head><p>In  confusion among similar classes from Baseline (column 3), such as bus and car (row 1), wall, fence and building (row 2), road and sidewalk (row 3). In contrast, DMT does a better job (column 4). DMT also detects the existence of small objects better, e.g. motorcycle in row 4, pole in row 5. While Baseline claims existence of non-existent class sky in row 6. In row 7, where the realworld fence is more complex, Baseline produces chaotic results which become consistent after dynamic mutual training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Proper Baseline Training</head><p>A concern of non-fully training baselines in semi-supervised learning has been raised in <ref type="bibr" target="#b33">[34]</ref>. We find that other than unifying data augmentation schemes and tricks (e.g. using the same strong augmentations and mixup in CIFAR-10 baselines), one important factor is the number of epochs. For example, we have a labeled subset that is 1/8 the entire dataset, 8 times more epochs (i.e.</p><p>keep the number of steps unchanged) are too many, and the same number of epochs are too few. Thus, we make a compromise between them and train for 1 labeled ratio N epochs, where N is the number of epochs used in Oracle training, which is 300, 30, 60 for CIFAR-10, PASCAL VOC 2012 and Cityscapes, respectively. As a result, the reported baseline performance in this paper is noticeably higher than previous works, sometimes even comparable to some previous state-of-the-art methods on Cityscapes (Tab. 5).</p><p>In our fully-supervised baselines, the learning rate is set to 0.2 (CIFAR-10), 2 ? 10 ?3 (PASCAL VOC 2012), and 4 ? 10 ?3 (Cityscapes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Realistic Validation</head><p>Oliver et al. <ref type="bibr" target="#b33">[34]</ref> has raised a concern of using unrealistically large validation sets in semi-supervised image classification. For instance, most prior arts split a validation set of 5,000 images for hyper-parameter tuning on CIFAR-10, even larger than their labeled subset. While we use only 200 images (we call this validation set valtiny and will release it along with our source codes). Intuitively, 200 images can hardly tell the difference between two models with 100%/200 = 0.5% accuracy gap. Different from semantic segmentation, since the mean IoU is a fine-grained metric that can work with a small validation set.</p><p>Fine-grained testing. To test with fewer images, we propose fine-grained testing for image classification, where we count the probability of being correct.</p><p>Concretely, if a model makes the right decision on an image, we count it as the probability that model assigns for the correct class instead of 1. In this way, we assess not only whether a prediction is correct, but also how correct it is.</p><p>We observe it helpful when normal testing can not tell the difference, especially when comparing similar setups (a few different hyper-parameter values). But fine-grained testing does not work well when comparing mixup methods and non-mixup methods, since mixup is better class-calibrated <ref type="bibr" target="#b40">[41]</ref>.</p><p>Hyper-parameter choice on ?. ? are viewed as hyper-parameters, similar to other common hyper-parameters, such as the number of epochs, learning rate, weight decay, etc. We choose the optimal gamma values using a small-ranged grid-search method, which is a typical technique for hyper-parameter tuning.</p><p>We choose a single gamma value for each dataset, which has stable performance across different labeled ratios. The trial grid for our grid-search method was: <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9)</ref> on PASCAL VOC, which validated the best gamma value 5, then 4 and 6 is tried but no obvious improvement is observed. It is further tuned for the other datasets: <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6)</ref> for CIFAR and Cityscapes until no notable improvement can be observed. To be realistic <ref type="bibr" target="#b34">[35]</ref>, we used a small validation set for each dataset, as well as empirical observations (e.g. loss curve, intuitions from other tasks)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Complexity</head><p>Firstly, our method does not contain any modification to the model architecture. Thus in test phase, it has the same computational complexity as the supervised baseline model. The only difference is model training, and details are shown in following Table D.8. In segmentation, 5 iterations of DMT (including pseudo label generation) plus baseline training, take only roughly 2.5x the time of training a fullysupervised model (Oracle), because each iteration is fine-tuning and thus very fast. Considering two models are produced at the end, the training cost is similar to Oracle training.</p><p>In classification, the models are re-trained 5 times, and each time it takes longer to converge than the fully-supervised baseline (750 epochs, compared to 300 epochs for Oracle). The overall training time seems much longer. We use the same training epochs as CL <ref type="bibr" target="#b14">[15]</ref>. As noted in CL <ref type="bibr" target="#b14">[15]</ref>, this is still much faster than other semi-supervised learning methods on CIFAR-10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Extra Illustrations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset B Subset A Complete set</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our training framework. There are two different models trained on the labeled subset, and one model provides pseudo supervisions for the other. A noise-robust loss (dynamic loss) is introduced for learning on the unlabeled data, leveraging the inter-model disagreement based on two models' predictions and confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 Figure 3 :</head><label>33</label><figDesc>Comparison of inter-model disagreement usages among Co-Teaching+ (CT+) [21], Deep Co-Training (DCT) [17], Dual Student (DS) [19] and DMT. Left: CT+ let models decide training examples for each other, but it only updates when models disagree ( =). Middle-Left: DCT maximizes the agreement between two models (A, B). Middle-Right: DS let one model teach another when it is more certain on an example (&gt;). Right: DMT updates one model with fixed (grey) pseudo labels from another, depending on how much they disagree (?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Because modern networks are trained with random and heavy data augmentation and the model in training changes after each parameter update, its predictions and errors also change. The change of errors can be extremely confusing when a model keeps minimizing entropy (fitting) on new errors, resulting in self-error accumulation throughout training. For offline self-training, although self-errors remain an obstacle, it is not as severe as that in online self-training, since the pseudo label errors are fixed throughout training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Dynamic Mutual Training (DMT). There are two different models (F A , F B ). F A is used to generate pseudo labels (y A ) and record corresponding confidence (c A ) before mutual training (procedure marked by green lines), and F B (with prediction y B and confidence c B )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>propose Dynamic Mutual Training (DMT), to quantify the inter-model disagreement and enable noise-robust training, illustrated in Fig. 5. First, we train two different models F A and F B on the labeled subset from two different initializations/sub-samplings. Then, one model, e.g. F A , is fixed and generates pseudo labels and confidence on the unlabeled subset. And the other, F B , finetunes on all data (labeled and pseudo labeled) with our dynamically weighted cross-entropy loss. In the same way, F B can train F A .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 :</head><label>1</label><figDesc>Pseudo code for iterative DMT process in image classification. Input: Unlabeled subset S u , labeled subset S l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 if start from different pre-trained weights then 2 3 Train F 0 A on S l 4 Train F 0 B on S l 5 else 6 7 R ? Randomly shuffled S l 8 S 11 9 1 A 14 Fine-tune F i B from F i? 1 B 1 B 16 Fine-tune F i A from F i? 1 A</head><label>2346781111411161</label><figDesc>Initialize F 0A and F 0 B with different pre-trained weightsInitialize F 0 A and F 0 B with the same pre-trained weights A , S B = DifferenceMaximizedSampling(R) // see Eq. Train F 0 A on S A 10 Train F 0 B on S B 11 ? = {20%, 40%, 60%, 80%, 100%} 12 foreach iteration i ? {1,2, 3, 4, 5} do 13 Pseudo labeled set S p ? Predict and save top ? i pixels from each classes on S u with F i?on both S l and latest S p with the dynamic loss 15 Pseudo labeled set S p ? Predict and save top ? i pixels from each classes on S u with F i?on both S l and latest S p with the dynamic loss 17 F = The best model from F 5 A , F 5 B , in term of mean IoU</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of iterative DMT training on PASCAL VOC 2012. We show pseudo labels from one model and dynamic weights generated when training another. At each iteration, more pixels are pseudo labeled. For some apparently incorrect regions we can observe rather low dynamic weights. Models are trained with 1/20 manually labeled data in the training set.White regions are ignored in labels and corresponding dynamic weights are shown as 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Section 4.2, all DMT experiments are conducted with the default 5 iterations. Our method is implemented on PyTorch with mixed-precision training [36]. All experiments were conducted on a single RTX-2080 Ti GPU. The fully-supervised learning result on the entire dataset is termed as Oracle, i.e. the performance upper-bound for semi-supervised learning. However, this upper-bound only holds when human annotations have good quality across the entire dataset. We show how this supposed upper-bound can be surpassed by our semi-supervised learning in Section 5.3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>by an extra classification branch. Methods are evaluated on 4 challenging splits: 1/106 (100 labels), 1/50, 1/20 and 1/8. We do not use more than 1/8 data which is becoming easier and pose less challenge to state-of-the-art methods. Supervised performance on the labeled subset is reported as Baseline. MT-Seg, CCT and GCT performance are the re-evaluated results in the GCT codebase 3 ; others are taken from the original papers. All methods use the same network architecture as ours except for CCT in which a slightly superior architecture PSPNet-ResNet-101 [4] is used for evaluation. As shown in Tab. 4, DMT outperforms other methods with a clear margin. However, some methods use GAN and extra network branch, or have implementation flaws, resulting in different Oracle and Baseline performances. Thus, we further show performance gaps to Oracle in brackets for fair comparisons,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>ment to show how DMT is even superior to human annotators. Specifically, the original PASCAL VOC 2012 dataset only labels 1,464 training images, called the train set. While the commonly used 10,582 training set trainaug contains 9,118 images from SBD [35]. The SBD dataset uses the same set of images as PASCAL VOC and annotates object outlines by Amazon Mechanical Turk (AMT), which can be filled as segmentation masks. However, unprofessional annotators from AMT tend to draw coarse outlines. Thus, trainaug has worse label quality than train. Therefore, we use train as the labeled subset and the 9,118 images from SBD as the unlabeled subset (by removing the SBD labels), and experiment DMT with the same hyper-parameters in the 1/8 split experiment. Surprisingly, as shown in Tab. 5, DMT exceeds the performance of Oracle (fully-supervised training on trainaug). This suggests that DMT renders human supervision at this quality (AMT) unnecessary for semantic segmentation, except for somewhat faster training since DMT needs two models (roughly twice the training budget of one model). Comparison of semi-supervised semantic segmentation performance on PASCAL VOC 2012 with multiple data splits. The proposed DMT has more stable performance across different labeled ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results on PASCAL VOC 2012. Models are trained with 1/20 pixel-level labeled data in the training set. White regions are ignored in ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Comparison of DMT and CBST (both COCO initialized) on one PASCAL VOC 2012 1/20 split. Iteration 0 represents training on the labeled subset only. CBST performance starts to degrade at iteration three due to too much pseudo label noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure A. 10 :</head><label>10</label><figDesc>Qualitative results on Cityscapes. Models are trained with 1/8 pixel-level labeled data in the training set. Black regions are ignored in ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure E. 11 :</head><label>11</label><figDesc>Difference maximized sampling. The complete set is randomly shuffled at first, and subset A and B are drawn with an equal size but with fewest overlapped samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Different initializations Pixel-wise Probabilities Agree Negative disagree Positive disagree Case 1 Case 2 Case 3 Re-weight loss by confidence Re-weight loss by 0 (No gradient)</head><label></label><figDesc>this section, we first present Dynamic Mutual Training (DMT) in Section 4.1 with the dynamic loss (Section 4.1.1) and techniques for how to initialize models with disagreement in different tasks (Section 4.1.2). Then we cast DMT to an iterative learning framework for better performance (Section 4.2).</figDesc><table><row><cell>Model A</cell></row><row><cell>(Fixed)</cell></row><row><cell>Unlabeled data</cell></row><row><cell>Model B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Hyper-parameter settings.</figDesc><table><row><cell></cell><cell>dataset</cell><cell>labeled</cell><cell cols="2">?1 ?2</cell><cell>learning</cell><cell>training</cell><cell>epochs</cell><cell>batch</cell><cell>batch</cell><cell>augmentations</cell></row><row><cell></cell><cell></cell><cell>ratio</cell><cell></cell><cell></cell><cell>rate</cell><cell></cell><cell></cell><cell>size</cell><cell>ratio</cell></row><row><cell cols="2">1 PASCAL VOC</cell><cell>1/8</cell><cell>5</cell><cell cols="3">5 1 ? 10 ?3 fine-tuning</cell><cell>5</cell><cell>8</cell><cell>7:1</cell></row><row><cell cols="2">2 PASCAL VOC</cell><cell>1/20</cell><cell>5</cell><cell cols="3">5 1 ? 10 ?3 fine-tuning</cell><cell>4</cell><cell>8</cell><cell>7:1</cell></row><row><cell cols="2">3 PASCAL VOC</cell><cell>1/50</cell><cell>5</cell><cell cols="3">5 1 ? 10 ?3 fine-tuning</cell><cell>4</cell><cell>8</cell><cell>7:1</cell><cell>random scale</cell></row><row><cell cols="2">4 PASCAL VOC</cell><cell>1/106</cell><cell>5</cell><cell cols="3">5 1 ? 10 ?3 fine-tuning</cell><cell>4</cell><cell>8</cell><cell>7:1</cell><cell>random crop</cell></row><row><cell>5</cell><cell>Cityscapes</cell><cell>1/8</cell><cell>3</cell><cell cols="3">3 4 ? 10 ?3 fine-tuning</cell><cell>10</cell><cell>8</cell><cell>3:1</cell><cell>random horizontal flip</cell></row><row><cell>6</cell><cell>Cityscapes</cell><cell>1/30</cell><cell>3</cell><cell cols="3">3 4 ? 10 ?3 fine-tuning</cell><cell>8</cell><cell>8</cell><cell>7:1</cell></row><row><cell>7</cell><cell>CIFAR-10</cell><cell cols="2">4k labels 4</cell><cell cols="3">4 1 ? 10 ?1 re-training</cell><cell>750</cell><cell>512</cell><cell>7:1</cell><cell>random augmentation</cell></row><row><cell>8</cell><cell>CIFAR-10</cell><cell cols="2">1k labels 4</cell><cell cols="3">4 1 ? 10 ?1 re-training</cell><cell>750</cell><cell>512</cell><cell>31:1</cell><cell>with random intensity</cell></row><row><cell></cell><cell cols="9">scaling, random cropping and random flipping. We train and pseudo label</cell></row><row><cell></cell><cell cols="9">at a spatial resolution of 321 ? 321 (PASCAL VOC 2012) and 256 ? 512</cell></row><row><cell></cell><cell cols="2">(Cityscapes).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results (%) between DMT and CL on CIFAR-10 test set.</figDesc><table><row><cell></cell><cell cols="2">Baseline CL [15]</cell><cell>DMT</cell></row><row><cell>4000 labels</cell><cell>86.08</cell><cell>94.02</cell><cell>94.21 (+0.19)</cell></row><row><cell>1000 labels</cell><cell>75.14</cell><cell>90.61</cell><cell>91.51 (+0.90)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results (%) for DMT and other methods on CIFAR-10 test set using 4,000 labels.Oracle performance is 96.35%.</figDesc><table><row><cell>method</cell><cell>Baseline</cell><cell>MT [9]</cell><cell cols="4">DCT [17] DS [19] MixMatch [11] DAG [27]</cell><cell>CL [15]</cell><cell>Ours (DMT)</cell></row><row><cell cols="3">network WRN-28-2 WRN-28-2</cell><cell>CNN-13</cell><cell>CNN-13</cell><cell>WRN-28-2</cell><cell>CNN-13</cell><cell>WRN-28-2</cell><cell>WRN-28-2</cell></row><row><cell>accuracy</cell><cell>86.08</cell><cell>89.64</cell><cell>90.97</cell><cell>91.11</cell><cell>93.76</cell><cell>93.87</cell><cell>94.02</cell><cell>94.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Mean IoU (%) results for DMT and other methods on PASCAL VOC 2012 val set. Performance gap to Oracle is shown in brackets. ? Updated numbers from s4GAN + MLMT.</figDesc><table><row><cell>method</cell><cell>network</cell><cell>1/106</cell><cell>1/50</cell><cell>1/20</cell><cell>1/8</cell><cell>Oracle</cell></row><row><cell>Baseline</cell><cell>DeepLab-v2</cell><cell>46.66 (-28.09)</cell><cell>55.62 (-19.13)</cell><cell>62.29 (-12.46)</cell><cell>67.37 (-7.38)</cell><cell>74.75</cell></row><row><cell>MT-Seg [9]</cell><cell>DeepLab-v2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>67.65 (-5.94)</cell><cell>73.59</cell></row><row><cell>Hung et al. [8]</cell><cell>DeepLab-v2</cell><cell>-</cell><cell>57.2  ? (-17.7)</cell><cell>64.7  ? (-10.2)</cell><cell>69.5 (-5.4)</cell><cell>74.9</cell></row><row><cell cols="2">s4GAN + MLMT [12] DeepLab-v2</cell><cell>-</cell><cell>63.3 (-12.3)</cell><cell>67.2 (-8.4)</cell><cell>71.4 (-4.2)</cell><cell>75.6</cell></row><row><cell>CutMix [10]*</cell><cell>DeepLab-v2</cell><cell>53.79 (-18.75)</cell><cell>64.81 (-7.73)</cell><cell>66.48 (-6.06)</cell><cell>67.60 (-4.94)</cell><cell>72.54</cell></row><row><cell>CCT [16]</cell><cell>PSPNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.45 (-4.80)</cell><cell>75.25</cell></row><row><cell>GCT [22]</cell><cell>DeepLab-v2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.57 (-3.49)</cell><cell>74.06</cell></row><row><cell>Ours (DMT)</cell><cell cols="6">DeepLab-v2 63.04 (-11.71) 67.15 (-7.60) 69.92 (-4.83) 72.70 (-2.05) 74.75</cell></row></table><note>* ImageNet pre-training.where DMT's performance is also the closest to Oracle. Our proposed DMT is the only method showing a clear improvement over Baseline on the challenging 100 labels split other than CutMix (1/106 in Tab. 4)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Mean IoU (%) comparisons between Oracle and DMT on PASCAL VOC 2012 1464/9118 split. val mean IoU (%) reported.</figDesc><table><row><cell></cell><cell cols="2">number of images number of labels</cell><cell>mean IoU</cell></row><row><cell>Baseline</cell><cell>1464</cell><cell>1464</cell><cell>72.10 ? 0.53</cell></row><row><cell>DMT</cell><cell>10582</cell><cell>1464</cell><cell>74.85 ? 0.29</cell></row><row><cell>Oracle</cell><cell>10582</cell><cell>10582</cell><cell>74.75 ? 0.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Mean IoU (%) results for DMT and other methods on Cityscapes val set. Performance gap to Oracle is shown in brackets. * ImageNet pre-training.</figDesc><table><row><cell>method</cell><cell>1/30</cell><cell>1/8</cell><cell>Oracle</cell></row><row><cell>Baseline</cell><cell>49.54 (-18.62)</cell><cell>59.65 (-8.51)</cell><cell>68.16</cell></row><row><cell>Hung et al. [8]</cell><cell>-</cell><cell>58.8 (-8.9)</cell><cell>67.7</cell></row><row><cell>s4GAN + MLMT [12]*</cell><cell>-</cell><cell>59.3 (-6.5)</cell><cell>65.8</cell></row><row><cell>CutMix[10]*</cell><cell>51.20 (-16.48)</cell><cell>60.34 (-7.34)</cell><cell>67.68</cell></row><row><cell>Ours (DMT)</cell><cell cols="3">54.80 (-13.36) 63.03 (-5.13) 68.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Ablations on PASCAL VOC 2012 (one random 1/20 split and one random 1/50 split). val mean IoU (%) is reported.</figDesc><table><row><cell cols="7">ablations Baseline Online ST CBST DST DMT-Naive DMT DMT-Flip</cell></row><row><cell>1/20</cell><cell>61.90</cell><cell>63.12</cell><cell>65.09 69.43</cell><cell>70.00</cell><cell>70.16</cell><cell>70.17</cell></row><row><cell>1/50</cell><cell>56.29</cell><cell>53.52</cell><cell>62.29 66.50</cell><cell>64.95</cell><cell>68.37</cell><cell>68.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>this paper, we have proposed Dynamic Mutual Training (DMT) to counter the pseudo supervision noise by a re-weighted loss function based on the intermodel disagreement. Furthermore, we have adapted DMT to an iterative framework for better performance in both image classification and semantic segmentation. DMT is flexible and easy to implement. We have evaluated the proposed method on different datasets including CIFAR-10, PASCAL VOC 2012 and Cityscapes. The experiments (comparisons and ablations) clearly demonstrate the effectiveness of the proposed DMT, and show its state-of-the-art outcomes in classification and segmentation.We find that DMT is more promising in semantic segmentation than image classification, probably because dynamic weighting exploits pixels with highquality pseudo labels in an image and provides better pseudo supervision on each image overall. In addition, image classification on CIFAR-10 requires retraining for each iteration and the two models do not have equal classifica-tion ability throughout training, thus making it difficult to exploit inter-model disagreement. Besides, it is hard to estimate confidence when recent image classification models require heavy data augmentation in training, confidence distribution is quite different compared to the generated pseudo labels. Thus, a better confidence estimation process could bring further gains, e.g. multiple forward pass statistics (at the cost of computing). Also advances in the learning with noisy labels community may be potentially useful, which is worthy of future investigations.Our work shares the common limitation of most offline self-training methods:the initial model learned on the labeled subset may be insufficient if the labels are too few, e.g. 100 labels on Cityscapes. Better pre-trained weights, e.g. COCO pre-trained weights for PASCAL VOC 2012, can potentially alleviate this issue, given a small gap to Oracle on PASCAL VOC 2012 (11.71%, Tab. 4). If off-theshelf pre-trained weights are unavailable, self-supervised learning<ref type="bibr" target="#b39">[40]</ref> is a good way to initiate learning. We intend to investigate how self-supervised learning can help semi-supervised learning in the future, especially for structured tasks like semantic segmentation. Cityscapes are shown inFig. A.10. Specifically, there is</figDesc><table><row><cell>7. Acknowledgements This work was partially supported by National Key Research and Develop-ment Program of China (No. 2019YFC1521104), Art major project of National Social Science Fund (I8ZD22). The author Qianyu Zhou is supported by Wu Wenjun Honorary Doctoral Scholarship, AI Institute, Shanghai Jiao Tong Uni-versity. Also, the authors would like to thank Shikun Liu (Imperial College London) for insightful discussions and identifying an incorrect data augmenta-tion policy in the earlier semantic segmentation code. Appendix A. Extra Qualitative Results Qualitative comparisons among Baseline (supervised only), our method (DMT) Sidewalk Fence Wall Building Pole Traffic light Traffic sign Vegetation Person Sky Terrain Rider Car Truck Bus Bicycle Motorcycle Train and ground truth on Road Input Ground truth Baseline DMT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table D .</head><label>D</label><figDesc>8: Training time of DMT, measured by a single 2080 Ti GPU.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Training time (hour) Relative time to Oracle training</cell></row><row><cell>PASCAL VOC 2012</cell><cell>8</cell><cell>2.5x</cell></row><row><cell>Cityscapes</cell><cell>6</cell><cell>2.5x</cell></row><row><cell>CIFAR-10</cell><cell>10</cell><cell>8x</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/ZHKKKe/PixelSSL/tree/master/task/sseg</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Previous-self denotes the model state that produced the pseudo labels before the current mutual training iteration.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>Wide residual networks</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Van Den Hengel, Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning workshop</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial learning for semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weightaveraged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5050" to="5060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and low-level consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cascante-Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep co-training for semi-supervised image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep co-training for semisupervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107269</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual student: Breaking the limits of the teacher in semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6728" to="6736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by disagreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="415" to="439" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Guided collaborative training for pixel-wise semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning from noisy examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Angluin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="343" to="370" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Density-aware graph for deep semi-supervised visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13400" to="13409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Confidence regularized selftraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>Mixed precision training</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition workshop</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>mixup: Beyond empirical risk minimization</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13888" to="13899" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
