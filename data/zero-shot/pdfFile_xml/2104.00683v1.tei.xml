<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimPoE: Simulated Character Control for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SimPoE: Simulated Character Control for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Our SimPoE framework learns a kinematics-aware video-conditioned policy that controls a character in a physics simulator (Top)   and estimates accurate and physically-plausible human motion (Bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Accurate estimation of 3D human motion from monocular video requires modeling both kinematics (body motion without physical forces) and dynamics (motion with physical forces). To demonstrate this, we present SimPoE, a Simulation-based approach for 3D human Pose Estimation, which integrates image-based kinematic inference and physics-based dynamics modeling. SimPoE learns a policy that takes as input the current-frame pose estimate and the next image frame to control a physically-simulated character to output the next-frame pose estimate. The policy contains a learnable kinematic pose refinement unit that uses 2D keypoints to iteratively refine its kinematic pose estimate of the next frame. Based on this refined kinematic pose, the policy learns to compute dynamics-based control (e.g., joint torques) of the character to advance the current-frame pose estimate to the pose estimate of the next frame. This design couples the kinematic pose refinement unit with the dynamics-based control generation unit, which are learned jointly with reinforcement learning to achieve accurate and physically-plausible pose estimation. Furthermore, we propose a meta-control mechanism that dynamically adjusts the character's dynamics parameters based on the character state to attain more accurate pose estimates. Experiments on large-scale motion datasets demonstrate that our approach establishes the new state of the art in pose accuracy while ensuring physical plausibility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We aim to show that accurate 3D human pose estimation from monocular video requires modeling both kinematics and dynamics. Human dynamics, i.e., body motion modeling with physical forces, has gained relatively little attention in 3D human pose estimation compared to its counterpart, kinematics, which models motion without physical forces. There are two main reasons for the disparity between these two equally important approaches. First, kinematics is a more direct approach that focuses on the geometric relationships of 3D poses and 2D images; it sidesteps the challenging problem of modeling the physical forces underlying human motion, which requires significant domain knowledge about physics and control. Second, compared to kinematic measurements such as 3D joint positions, physical forces present unique challenges in their measurement and annotation, which renders standard supervised learning paradigms unsuitable. Thus, almost all state-of-the-art methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref> for 3D human pose estimation from monocular video are based only on kinematics. Although these kinematic methods can estimate human motion with high pose accuracy, they often fail to produce physically-plausible motion. Without modeling the physics of human dynamics, kinematic methods have no notion of force, mass or contact; they also do not have the ability to impose physical constraints such as joint torque limits or friction. As a result, kinematic methods often generate physically-implausible motions with pronounced artifacts: body parts (e.g., feet) penetrate the ground; the estimated poses are jittery and vibrate excessively; the feet slide back and forth when they should be in static contact with the ground. All these physical artifacts significantly limit the application of kinematic pose estimation methods. For instance, jittery motions can be misleading for medical monitoring and sports training; physical artifacts also prevent applications in computer animation and virtual/augmented reality since people are exceptionally good at discerning even the slightest clue of physical inaccuracy <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>To improve the physical plausibility of estimated human motion from video, recent work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref> has started to adopt the use of dynamics in their formulation. These methods first estimate kinematic motion and then use physicsbased trajectory optimization to optimize the forces to induce the kinematic motion. Although they can generate physically-grounded motion, there are several drawbacks of trajectory optimization-based approaches. First, trajectory optimization entails solving a highly-complex optimization problem at test time. This can be computationally intensive and requires the batch processing of a temporal window or even the entire motion sequence, causing high latency in pose predictions and making it unsuitable for interactive real-time applications. Second, trajectory optimization requires simple and differentiable physics models to make optimization tractable, which can lead to high approximation errors compared to advanced and non-differentiable physics simulators (e.g., MuJoCo <ref type="bibr" target="#b53">[54]</ref>, Bullet <ref type="bibr" target="#b7">[8]</ref>). Finally and most importantly, the application of physics in trajectory optimization-based methods is implemented as a post-processing step that projects a given kinematic motion to a physically-plausible one. Since it is optimizationbased, there is no learning mechanism in place that tries to match the optimized motion to the ground truth. As such, the resulting motion from trajectory optimization can be physically-plausible but still far from the ground-truth, especially when the input kinematic motion is inaccurate.</p><p>To address these limitations, we present a new approach, SimPoE (Simulated Character Control for Human Pose Estimation), that tightly integrates image-based kinematic inference and physics-based dynamics modeling into a joint learning framework. Unlike trajectory optimization, Sim-PoE is a causal temporal model with an integrated physics simulator. Specifically, SimPoE learns a policy that takes the current pose and the next image frame as input, and produces controls for a proxy character inside the simulator that outputs the pose estimate for the next frame. To perform kinematic inference, the policy contains a learnable kinematic pose refinement unit that uses image evidence (2D keypoints) to iteratively refine a kinematic pose estimate. Concretely, the refinement unit takes as input the gradient of keypoint reprojection loss, which encodes rich informa-tion about the geometry of pose and keypoints, and outputs the kinematic pose update. Based on this refined kinematic pose, the policy then computes a character control action, e.g., target joint angles for the character's proportionalderivative (PD) controllers, to advance the character state and obtain the next-frame pose estimate. This policy design couples the kinematic pose refinement unit with the dynamics-based control generation unit, which are learned jointly with reinforcement learning (RL) to ensure both accurate and physically-plausible pose estimation. At each time step, a reward is assigned based on the similarity between the estimated motion and the ground truth. To further improve pose estimation accuracy, SimPoE also includes a new control mechanism called meta-PD control. PD controllers are widely used in prior work <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b64">65]</ref> to convert the action produced by the policy into the joint torques that control the character. However, the PD controller parameters typically have fixed values that require manual tuning, which can produce sub-optimal results. Instead, in meta-PD control, SimPoE's policy is also trained to dynamically adjust the PD controller parameters across simulation steps based on the state of the character to achieve a finer level of control over the character's motion.</p><p>We validate our approach, SimPoE, on two large-scale datasets, Human3.6M <ref type="bibr" target="#b14">[15]</ref> and an in-house human motion dataset that also contains detailed finger motion. We compare SimPoE against state-of-the-art monocular 3D human pose estimation methods including both kinematic and physics-based approaches. On both datasets, SimPoE outperforms previous art in both pose-based and physics-based metrics, with significant pose accuracy improvement over prior physics-based methods. We further conduct extensive ablation studies to investigate the contribution of our proposed components including the kinematic refinement unit, meta-PD control, as well as other design choices.</p><p>The main contributions of this paper are as follows: (1) We present a joint learning framework that tightly integrates image-based kinematic inference and physics-based dynamics modeling to achieve accurate and physicallyplausible 3D human pose estimation from monocular video.</p><p>(2) Our approach is causal, runs in real-time without batch trajectory optimization, and addresses several drawbacks of prior physics-based methods. (3) Our proposed meta-PD control mechanism eliminates manual dynamics parameter tuning and enables finer character control to improve pose accuracy. (4) Our approach outperforms previous art in both pose accuracy and physical plausibility. <ref type="bibr" target="#b4">(5)</ref> We perform extensive ablations to validate the proposed components to establish good practices for RL-based human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Kinematic 3D Human Pose Estimation. Numerous prior works estimate 3D human joint locations from monoc-ular video using either two-stage <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b39">40]</ref> or end-toend <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29]</ref> frameworks. On the other hand, parametric human body models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38]</ref> are widely used as the human pose representation since they additionally provide skeletal joint angles and a 3D body mesh. Optimization-based methods have been used to fit the SMPL body model <ref type="bibr" target="#b26">[27]</ref> to 2D keypoints extracted from an image <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>. Alternatively, regression-based approaches use deep neural networks to directly regress the parameters of the SMPL model from an image <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10]</ref>, using weak supervision from 2D keypoints <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b16">17]</ref> or body part segmentation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39]</ref>. Song et al. <ref type="bibr" target="#b50">[51]</ref> propose neural gradient descent to fit the SMPL model using 2D keypoints. Regression-based <ref type="bibr" target="#b16">[17]</ref> and optimization-based <ref type="bibr" target="#b4">[5]</ref> methods have also been combined to produce pseudo ground truth from weakly-labeled images <ref type="bibr" target="#b21">[22]</ref> to facilitate learning. Recent work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref> starts to exploit the temporal structure of human motion to estimate smooth motion. <ref type="bibr">Kanazawa et al. [18]</ref> model human kinematics by predicting past and future poses. Transformers <ref type="bibr" target="#b55">[56]</ref> have also been used to improve the temporal modeling of human motion <ref type="bibr" target="#b51">[52]</ref>. All the aforementioned methods disregard human dynamics, i.e., the physical forces that generate human motion. As a result, these methods often produce physically-implausible motions with pronounced physical artifacts such as jitter, foot sliding, and ground penetration.</p><p>Physics-Based Human Pose Estimation. A number of works have addressed human dynamics for 3D human pose estimation. Most prior works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref> use trajectory optimization to optimize the physical forces to induce the human motion in a video. As discussed in Sec. 1, trajectory optimization is a batch procedure which has high latency and is typically computationally expensive, making it unsuitable for real-time applications. Furthermore, these methods cannot utilize advanced physics simulators with non-differentiable dynamics. Most importantly, there is no learning mechanism in trajectory optimizationbased methods that tries to match the optimized motion to the ground truth. Our approach addresses these drawbacks with a framework that integrates kinematic inference with RL-based character control, which runs in real-time, is compatible with advanced physics simulators, and has learning mechanisms that aim to match the output motion to the ground truth. Although prior work <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b15">16]</ref> has used RL to produce simple human locomotions from videos, these methods only learn policies that coarsely mimic limited types of motion instead of precisely tracking the motion presented in the video. In contrast, our approach can achieve accurate pose estimation by integrating images-based kinematic inference and RL-based character control with the proposed policy design and meta-PD control.</p><p>Reinforcement Learning for Character Control. Deep RL has become the preferred approach for learning char-acter control policies with manually-designed rewards <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>. GAIL <ref type="bibr" target="#b11">[12]</ref> based methods are proposed to learn character control without reward engineering <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b58">59]</ref>. To produce long-term behaviors, prior work has used hierarchical RL to control characters to achieve high-level tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34]</ref>. Recent work also uses deep RL to learn user-controllable policies from motion capture data for character animation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b61">62]</ref>. Prior work in this domain learns control policies that reproduce training motions, but the policies do not transfer to unseen test motions, nor do they estimate motion from video as our method does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The overview of our SimPoE (Simulated Character Control for Human Pose Estimation) framework is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The input to SimPoE is a video I 1:T = (I 1 , . . . , I T ) of a person with T frames. For each frame I t , we first use an off-the-shelf kinematic pose estimator to estimate an initial kinematic pose q t , which consists of the joint angles and root translation of the person; we also extract 2D keypoints q</p><p>x t and their confidence c t from I t using a given pose detector (e.g., OpenPose [7])). As the estimated kinematic motion q 1:T = ( q 1 , . . . , q T ) is obtained without modeling human dynamics, it often contains physicallyimplausible poses with artifacts like jitter, foot sliding, and ground penetration. This motivates the main stage of our method, simulated character control, where we model human dynamics with a proxy character inside a physics simulator. The character's initial pose q 1 is set to q 1 . At each time step t shown in <ref type="figure" target="#fig_0">Fig. 2 (b)</ref>, SimPoE learns a policy that takes as input the current character pose q t , velocitiesq t , as well as the next frame's kinematic pose q t+1 and keypoints (q x t+1 , c t+1 ) to produce an action that controls the character in the simulator to output the next pose q t+1 . By repeating this causal process, we obtain the physically-grounded estimated motion q 1:T = (q 1 , . . . , q T ) of SimPoE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Automated Character Creation</head><p>The character we use as a proxy to simulate human motion is created from skinned human mesh models, e.g., the SMPL model <ref type="bibr" target="#b26">[27]</ref>, which can be recovered via SMPL-based pose estimation methods such as VIBE <ref type="bibr" target="#b20">[21]</ref>. These skinned mesh models provide a skeleton of B bones, a mesh of V vertices, and a skinning weight matrix W ? R V ?B where each element W ij specifies the influence of the j-th bone's transformation on the i-th vertex's position. We can obtain a rigid vertex-to-bone association A ? R V by assigning each vertex i to the bone with the largest skinning weight for it: A i = arg max j W ij . With the vertex-to-bone association A, we can then create the geometry of each bone by computing the 3D convex hull of all the vertices assigned to the bone. Assuming constant density, the mass of each bone is determined by the volume of its geometry. Our character , the policy network F ? use the current pose q t , velocitiesq t , and the next frame's estimated kinematic pose q t+1 and keypoints (q xt+1, ct+1) to generate an action at, which controls the character in the physics simulator (450Hz) via PD controllers to produce the next pose q t+1 .</p><p>(c) The policy network F ? outputs the mean action at</p><formula xml:id="formula_0">(ut, ? t , ? p t , ? d t ).</formula><p>The kinematic refinement unit iteratively refines a kinematic pose estimate by learning pose updates. The refined pose q (n) t+1 is used by the control generation unit to produce the mean action at.</p><p>creation process is fully automatic, is compatible with popular body mesh models (e.g., SMPL), and ensures proper body geometry and mass assignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Simulated Character Control</head><p>The task of controlling a character agent in physics simulation to generate desired human motions can be formulated as a Markov decision process (MDP), which is defined by a tuple M = (S, A, T , R, ?) of states, actions, transition dynamics, a reward function, and a discount factor. The character agent interacts with the physics simulator according to a policy ?(a t |s t ), which models the conditional distribution of choosing an action a t ? A given the current state s t ? S of the agent. Starting from some initial state s 1 , the character agent iteratively samples an action a t from the policy ? and the simulation environment with transition dynamics T (s t+1 |s t , a t ) generates the next state s t+1 and gives the agent a reward r t . The reward is assigned based on how well the character's motion aligns with the ground-truth human motion. The goal of our character control learning process is to learn an optimal policy ? * that maximizes the expected return J(?) = E ? [ t ? t r t ] which translates to imitating the ground-truth motion as closely as possible. We apply a standard reinforcement learning algorithm (PPO <ref type="bibr" target="#b48">[49]</ref>) to solve for the optimal policy. In the following, we provide a detailed description of the states, actions and rewards of our control learning process. We then use a dedicated Sec. 3.3 to elaborate on our policy design.</p><p>States. The character state s t (q t ,q t , q t+1 , q x t+1 , c t+1 ) consists of the character's current pose q t , joint velocities (time derivative of the pose)q t , as well as the estimated kinematic pose q t+1 , 2D keypoints q</p><p>x t+1 and keypoint confidence c t+1 of the next frame. The state includes informa-tion of both the current frame (q t ,q t ) and next frame ( q t+1 , q x t+1 ,c t+1 ), so that the agent learns to take the right action a t to transition from the current pose q t to a desired next pose q t+1 , i.e., pose close to the ground truth. Actions. The policy ?(a t |s t ) runs at 30Hz, the input video's frame rate, while our physics simulator runs at 450Hz to ensure stable simulation. This means one policy step corresponds to 15 simulation steps. One common design of the policy's action a t is to directly output the torques ? t to be applied at each joint (except the root), which are used repeatedly by the simulator during the 15 simulation steps. However, finer control can be achieved by adjusting the torques at each step based on the state of the character. Thus, we follow prior work <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b64">65]</ref> and use proportionalderivative (PD) controllers at each non-root joint to produce torques. With this design, the action a t includes the target joint angles u t of the PD controllers. At the j-th of the 15 simulation (PD controller) steps, the joint torques ? t are computed as</p><formula xml:id="formula_1">? t = k p ? (u t ? q nr</formula><p>bustness of character control. Thus, we also add the residual forces and torques ? t of the root into the action a t . Overall, the action is defined as a t (u t , ? t , ? p t , ? d t ). Rewards. In order to learn the policy, we need to define a reward function that encourages the motion q 1:T generated by the policy to match the ground-truth motion q 1:T . Note that we use ? to denote ground-truth quantities. The reward r t at each time step is defined as the multiplication of four sub-rewards:</p><formula xml:id="formula_2">r t = r p t ? r v t ? r j t ? r k t .<label>(2)</label></formula><p>The pose reward r p t measures the difference between the local joint orientations o j t and the ground truth o j t :</p><formula xml:id="formula_3">r p t = exp ? ? ?? p ? ? J j=1 o j t o j t 2 ? ? ? ? ,<label>(3)</label></formula><p>where J is the total number of joints, denotes the relative rotation between two rotations, and ? computes the rotation angle. The velocity reward r v t measures the mismatch between joint velocitiesq t and the ground truth q t :</p><formula xml:id="formula_4">r v t = exp ?? v q t ? q t 2 .<label>(4)</label></formula><p>The joint position reward r j t encourages the 3D world joint positions X j t to match the ground truth X j t :</p><formula xml:id="formula_5">r j t = exp ? ? ?? j ? ? J j=1 X j t ? X j t 2 ? ? ? ? .<label>(5)</label></formula><p>Finally, the keypoint reward r k t pushes the 2D image projection x j t of the joints to match the ground truth x j t :</p><formula xml:id="formula_6">r k t = exp ? ? ?? k ? ? J j=1 x j t ? x j t 2 ? ? ? ? .<label>(6)</label></formula><p>Note that the orientations o j t , 3D joint positions X j t and 2D image projections x j t are functions of the pose q t . The joint velocitiesq t are computed via finite difference. There are also weighting factors ? p , ? v , ? j , ? k inside each reward. These sub-rewards complement each other by matching different features of the generated motion to the ground-truth: joint angles, velocities, as well as 3D and 2D joint positions. Our reward design is multiplicative, which eases policy learning as noticed by prior work <ref type="bibr" target="#b61">[62]</ref>. The multiplication of the sub-rewards ensures that none of them can be overlooked in order to achieve a high reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Kinematics-Aware Policy</head><p>As the action a t is continuous, we adopt a parametrized Gaussian policy ? ? (a t |s t ) = N (a t , ?) where the mean a t (u t , ? t , ? p t , ? d t ) is output by a neural network F ? with parameters ?, and ? is a fixed diagonal covariance matrix whose elements are treated as hyperparameters. The noise inside the Gaussian policy governed by ? allows the agent to explore different actions around the mean action a t and use these explorations to improve the policy during training. At test time, the noise is removed and the character agent always takes the mean action a t to improve performance. Now let us focus on the design of the policy network F ? that maps the state s t to the mean action a t . Based on the design of s t , the mapping can be written as</p><formula xml:id="formula_7">a t = F ? q t ,q t , q t+1 , q x t+1 , c t+1 .<label>(7)</label></formula><p>Recall that q t+1 is the kinematic pose, q x t+1 and c t+1 are the detected 2D keypoints and their confidence, and that they are all information about the next frame. The overall architecture of our policy network F ? is illustrated in <ref type="figure" target="#fig_0">Fig. 2 (c)</ref>. The components (u t , ? t , ? p t , ? d t ) of the mean action a t are computed as follows:</p><formula xml:id="formula_8">q (n) t+1 = R ? q t+1 , q x t+1 , c t+1 ,<label>(8)</label></formula><formula xml:id="formula_9">(?u t , ? t , ? p t , ? d t ) = G ? q (n) t+1 , q t ,q t ,<label>(9)</label></formula><formula xml:id="formula_10">u t = q (n) t+1 + ?u t .<label>(10)</label></formula><p>In Eq. <ref type="formula" target="#formula_8">(8)</ref>, R ? is a kinematic refinement unit that iteratively refines the kinematic pose q t+1 using the 2D keypoints q x t+1 and confidence c t+1 , and q (n) t+1 is the refined pose after n iterations of refinement. Eq. (9) and (10) describe a control generation unit G ? that maps the refined pose q (n) t+1 , current pose q t and velocitiesq t to the components of the mean action a t . Specifically, the control generation unit G ? includes a hand-crafted feature extraction layer, a normalization layer (based on running estimates of mean and variance) and another MLP V ? , as illustrated in <ref type="figure" target="#fig_0">Fig. 2 (c)</ref>. As described in Eq. (10), an important design of G ? is a residual connection that produces the mean PD controller target angles u t using the refined kinematic pose q (n) t+1 , where we ignore the root angles and positions in q (n) t+1 for ease of notation. This design builds in proper inductive bias since q (n) t+1 provides a good guess for the desired next pose q t+1 and thus a good base value for u t . It is important to note that the PD controller target angles u t do not translate to the same next pose q t+1 of the character, i.e., q t+1 = u t . The reason is that the character is subject to gravity and contact forces, and under these external forces the joint angles q t+1 will not be u t when the PD controllers reach their equilibrium. As an analogy, since PD controllers act like springs, a spring will reach a different equilibrium position when you apply external forces to it. Despite this, the next pose q t+1 generally will not be far away from u t and learning the residual ?u t to q (n) t+1 is easier than learning from scratch as we will demonstrate in the experiments. This design also synergizes the kinematics of the character with its dynamics as the kinematic pose q (n) t+1 is now tightly coupled with the input of the character's PD controllers that control the character in the physics simulator.</p><p>Kinematic Refinement Unit. The kinematic refinement unit R ? is formed by an MLP U ? that maps a feature vector z (specific form will be described later) to a pose update:</p><formula xml:id="formula_11">? q (i) t+1 = U ? (z) ,<label>(11)</label></formula><formula xml:id="formula_12">q (i+1) t+1 = q (i) t+1 + ? q (i) t+1 ,<label>(12)</label></formula><p>where i denotes the i-th refinement iteration and q (0) t+1 = q t+1 . To fully leverage the 2D keypoints and kinematic pose at hand, we design the feature z to be the gradient of the keypoint reprojection loss with respect to current 3D joint positions, inspired by recent work <ref type="bibr" target="#b50">[51]</ref> on kinematic body fitting. The purpose of using the gradient is not to minimize the reprojection loss, but to use it as an informative kinematic feature to learn a pose update that eventually results in stable and accurate control of the character; there is no explicit minimization of the reprojection loss in our formulation. Specifically, we first obtain the 3D joint positions X t+1 = FK( q (i) t+1 ) through forward kinematics and then compute the reprojection loss as</p><formula xml:id="formula_13">L( X t+1 ) = J j=1 ? X j t+1 ? q x j t+1 2 ? c j t+1 ,<label>(13)</label></formula><p>where X j t+1 denotes the j-th joint position in X t+1 , ?(?) denotes the perspective camera projection, and (q x j t+1 , c j t+1 ) are the j-th detected keypoint and its confidence. The gradient feature z ?L/? X t+1 is informative about the kinematic pose q (i) t+1 as it tells us how each joint should move to match the 2D keypoints q x j t+1 . It also accounts for keypoint uncertainty by weighting the loss with the keypoint confidence c j t+1 . Note that z is converted to the character's root coordinate to be invariant of the character's orientation. The refinement unit integrates kinematics and dynamics as it utilizes a kinematics-based feature z to learn the update of a kinematic pose, which is used to produce dynamics-based control of the character. The joint learning of the kinematic refinement unit R ? and the control generation unit G ? ensures accurate and physically-plausible pose estimation.</p><p>Feature Extraction Layer. After refinement, the control generation unit G ? needs to extract informative features from its input to output an action that advances the character from the current pose q t to the next pose q t+1 . To this end, the feature extraction layer uses information from both the current frame and next frame. Specifically, the extracted feature includes q t ,q t , the current 3D joint positions X t , the pose difference vector between q t and the refined kinematic pose q (n) t+1 , and the difference vector between X t and the next-frame joint position X t+1 computed from q </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Meta-PD control</head><p>PD controllers are essential in our approach as they relate the kinematics and dynamics of the character by converting target joint angles in pose space to joint torques. However, an undesirable aspect of PD controllers is the need to specify the parameters k p and k d for computing the joint torques ? t as described in Eq. (1). It is undesirable because (i) manual parameter tuning requires significant domain knowledge and (ii) even carefully designed parameters can be suboptimal. The difficulty, here, lies in balancing the ratio between k p and k d . Large ratios can lead to unstable and jittery motion while small values can result in motion that is too smooth and lags behind ground truth.</p><p>Motivated by this problem, we propose meta-PD control, a method that allows the policy to dynamically adjust k p and k d based on the state of the character. Specifically, given some initial values k p and k d , the policy outputs ? p and ? d as additional elements of the action a t that act to scale k p and k d . Moreover, we take this idea one step further and let the policy output two sequences of scales</p><formula xml:id="formula_14">? p t = (? p t1 , . . . , ? p tm ) and ? d t = (? d t1 , . . . , ? d tm )</formula><p>where m = 15 corresponds to the number of PD controller (simulation) steps during a policy step. The PD controller parameters k p and k d at the j-th step of the 15 PD controller steps are then computed as follows:</p><formula xml:id="formula_15">k p = ? p tj k p , k d = ? d tj k d .<label>(14)</label></formula><p>Instead of using fixed k p and k d , meta-PD control allows the policy to plan the scaling of k p and k d through the 15 PD controller steps to have more granular control over the torques produced by the PD controllers, which in turn enables a finer level of character control. With meta-PD control, the action a t is now defined as a t (u t , ? t , ? p t , ? d t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We perform experiments on two large-scale human motion datasets. The first dataset is Human3.6M <ref type="bibr" target="#b14">[15]</ref>, which includes 7 annotated subjects captured at 50Hz and a total of 1.5 million training images. Following prior work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>, we train our model on 5 subjects (S1, S5, S6, S7, S8) and test on the other 2 subjects (S9, S11). We subsample the dataset to 25Hz for both training and testing. The second dataset we use is an in-house human motion dataset that also contains detailed finger motion. It consists of 3 subjects captured at 30Hz performing various actions from free body motions to natural conversations. There are around 335k training frames and 87k test frames. Our inhouse dataset has complex skeletons with twice more joints than the SMPL model, including fingers. The body shape variation among subjects is also greater than that of SMPL, which further evaluates the robustness of our approach.  Metrics. We use both pose-based and physics-based metrics for evaluation. To assess pose accuracy, we report mean per joint position error (MPJPE) and Procrustes-aligned mean per joint position error (PA-MPJPE). We also use three physics-based metrics that measure jitter, foot sliding, and ground penetration, respectively. For jitter, we compute the difference in acceleration (Accel) between the predicted 3D joint and the ground-truth. For foot sliding (FS), we find body mesh vertices that contact the ground in two adjacent frames and compute their average displacement within the frames. For ground penetration (GP), we compute the average distance to the ground for mesh vertices below the ground. The units for these metrics are millimeters (mm) except for Accel (mm/frame 2 ). MPJPE, PA-MPJPE and Accel are computed in the root-centered coordinate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details.</head><p>Character Models. We use MuJoCo <ref type="bibr" target="#b53">[54]</ref> as the physics simulator. For the character creation process in Sec. 3.1, we use VIBE <ref type="bibr" target="#b20">[21]</ref> to recover an SMPL model for each subject in Human3.6M. Each MuJoCo character created from the SMPL model has 25 bones and 76 degrees of freedom (DoFs). For our in-house motion dataset, we use nonrigid ICP <ref type="bibr" target="#b0">[1]</ref> and linear blend skinning <ref type="bibr" target="#b18">[19]</ref> to reconstruct a skinned human mesh model for each subject. Each of these models has fingers and includes 63 bones and 114 DoFs.</p><p>Initialization. For Human3.6M, we use VIBE to provide the initial kinematic motion q 1:T . For our in-house motion dataset, since our skinned human models have more complex skeletons and meshes than the SMPL model, we develop our own kinematic pose estimator, which is detailed  <ref type="table">Table 1</ref>. Results of pose-based (MPJPE, PA-MPJPE) and physicsbased (Accel, FS, GP) metrics on Human3.6M and our in-house motion dataset. Symbol "-" means results are not available and "*" means self-implementation (better results than the original paper).</p><formula xml:id="formula_16">Human3.6M Method Physics MPJPE ? PA-MPJPE ? Accel ? FS ? GP</formula><p>in Appendix A. To recover the global root position of the person, we assume the camera intrinsic parameters are calibrated and optimize the root position by minimizing the reprojection loss of 2D keypoints, similar to the kinematic initialization in <ref type="bibr" target="#b49">[50]</ref>.</p><p>Other Details. The kinematic refinement unit in the policy network refines the kinematic pose n = 5 times. To facilitate learning, we first pretrain the refinement unit with supervised learning using an MSE loss on the refined kinematic pose. The normalization layer in the policy computes the running average of the mean and variance of the input feature during training, and uses it to produce a normalized feature. Our learned policy runs at <ref type="bibr" target="#b37">38</ref>   <ref type="table">Table 2</ref>. Ablation studies on Human3.6M and our in-house motion dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to state-of-the-art methods</head><p>We compare SimPoE against state-of-the-art monocular 3D human pose estimation methods, including both kinematics-based (VIBE <ref type="bibr" target="#b20">[21]</ref>, NeurGD <ref type="bibr" target="#b50">[51]</ref>) and physicsbased (PhysCap <ref type="bibr" target="#b49">[50]</ref>, EgoPose <ref type="bibr" target="#b64">[65]</ref>) approaches. The results of VIBE and EgoPose are obtained using their publicly released code and models. As PhysCap and NeurGD have not released their code, we directly use the reported results on Human3.6M from the PhysCap paper and implement our own version of NeurGD. <ref type="table">Table 1</ref> summarizes the quantitative results on Human3.6M and the in-house motion dataset. On Human3.6M, we can observe that our method, SimPoE, outperforms previous methods in pose accuracy as indicated by the smaller MPJPE and PA-MPJPE. In particular, SimPoE shows large pose accuracy improvements over state-of-the-art physics-based approaches (Ego-Pose <ref type="bibr" target="#b64">[65]</ref> and PhysCap <ref type="bibr" target="#b49">[50]</ref>), reducing the MPJPE almost by half. For physics-based metrics (Accel, FS and GP), SimPoE also outperforms prior methods by large margins. It means that SimPoE significantly reduces the physical artifacts -jitter (Accel), foot sliding (FS), and ground penetration (GP), which particularly deteriorate the results of kinematic methods (VIBE <ref type="bibr" target="#b20">[21]</ref> and NeurGD <ref type="bibr" target="#b50">[51]</ref>). On the in-house motion dataset, SimPoE again outperforms previous methods in terms of both pose-based and physics-based metrics. In the table, KinPose denotes our own kinematic pose estimator used by SimPoE. We note that the large acceleration error (Accel) of EgoPose is due to the frequent falling of the character, which is a common problem in physics-based methods since the character can lose balance when performing agile motions. The learned policy of Sim-PoE is robust enough to stably control the character without falling, which prevents irregular accelerations.</p><p>We also provide qualitative comparisons in <ref type="figure" target="#fig_2">Fig. 3</ref>, where we show the estimated poses in the camera view and the same poses rendered from an alternative view. The alternative view shows that SimPoE can estimate foot contact with the ground more accurately and without penetration. As the quality and physical plausibility of the estimated motions are best seen in videos, please refer to the supplementary video for additional qualitative results and comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>To further validate our proposed approach, we conduct extensive ablation studies to investigate the contribution of each proposed component to the performance. <ref type="table">Table 2</ref> summarizes the results where we train different variants of Sim-PoE by removing a single component each time. First, we can observe that both meta-PD control and the kinematic refinement unit contribute to better pose accuracy as indicated by the corresponding ablations (w/o Meta-PD and w/o Refine). Second, the ablation (w/o ResAngle) shows that it is important to have the residual connection in the policy network for producing the mean PD controller target angles u t . Next, the residual forces ? t we use in action a t are also indispensable as demonstrated by the drop in performance of the variant (w/o ResForce). Without the residual forces, the policy is not robust and the character often falls down as indicated by the large acceleration error (Accel). Finally, it is evident from the ablation (w/o FeatLayer) that our feature extraction layer in the policy is also instrumental, because it extracts informative features of both the current frame and next frame to learn control that advances the character to the next pose. We also perform ablations to investigate how the number of refinement iterations in the policy affects pose accuracy. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, the performance gain saturates around 5 refinement iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Future Work</head><p>In this work, we demonstrate that modeling both kinematics and dynamics improves the accuracy and physical plausibility of 3D human pose estimation from monocular video. Our approach, SimPoE, unifies kinematics and dynamics by integrating image-based kinematic inference and physics-based character control into a joint reinforcement learning framework. It runs in real-time, is compatible with advanced physics simulators, and addresses several drawbacks of prior physics-based approaches.</p><p>However, due to its physics-based formulation, SimPoE depends on 3D scene modeling to enforce contact constraints during motion estimation. This hinders direct evaluation on in-the-wild datasets, such as 3DPW <ref type="bibr" target="#b56">[57]</ref>, which includes motions such as climbing stairs or even trees. Future work may include integration of video-based 3D scene reconstruction to address this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. In-House Motion Dataset and Kinematic Pose Estimator</head><p>In-House Dataset. Our in-house motion dataset uses a more complex skeleton model (with twice as many joints, including fingers) than SMPL <ref type="bibr" target="#b26">[27]</ref>. To recover the human skinning mesh model of each subject, we use an offline process that uses multiview 3D reconstruction to produce a person-specific skinning template with 32 bone scaling parameters based on nonrigid ICP deformation <ref type="bibr" target="#b0">[1]</ref> and linear blend skinning <ref type="bibr" target="#b18">[19]</ref>. For motion, we solve for 94 local joint angles (degrees of freedom) and the global 3D position of 159 joints, including finger joints, for every frame.</p><p>Kinematic Pose Estimator. Since existing kinematic pose estimators, such as VIBE <ref type="bibr" target="#b20">[21]</ref>, cannot be directly applied to the in-house dataset due to the dataset's more complex skeletons and skinning models, we design a simple kinematic tracker ("KinPose" in the main paper) that also uses monocular inputs to produce kinematic pose estimates. The model does not have any temporal component, outputting both 2D keypoint heatmaps and joint angles frame-by-frame. These two outputs are required by our approach (SimPoE) and NeurGD <ref type="bibr" target="#b51">[52]</ref>. Below, we detail the network architecture and training procedure of this model, which are typical for such models as the performance of SimPoE is not sensitive to these design choices.</p><p>Network Architecture. We use a 3-stage cascaded network <ref type="bibr" target="#b59">[60]</ref> with a backbone based on ResNet-50 <ref type="bibr" target="#b10">[11]</ref>. The output of the network at each stage is a tensor of m + n channels with a spatial size that is 8x smaller than the input image, where m = 77 is the number of heatmap channels for 2D keypoints (a subset of the 159 joints), and n = 94 + 6 is the number of local joint angles and global pose dimensions. Each of the 94 angle channels is an "angle map" corresponding to a joint. The final output angle (scalar) is calculated by summing the element-wise product between an angle map and its corresponding keypoint heatmaps, where the correspondence is defined based on the skeleton. This design is a type of attention mechanism, which encourages the model to predict the angle of a joint based only on relevant image regions.</p><p>Training. At each stage of the network, we apply L 2 losses on heatmaps, 2D keypoints, 3D joint positions, and joint angles to train the model, similar to VIBE <ref type="bibr" target="#b20">[21]</ref>.  <ref type="table">Table 3</ref>. Hyperparameters for experiments on Human3.6M <ref type="bibr" target="#b14">[15]</ref> and our in-house motion dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Implementation Details</head><p>For both Human3.6M and our in-house motion dataset, our method uses the same hyperparameter settings unless stated otherwise. <ref type="table">Table 3</ref> summarizes the hyperparameter setting.</p><p>Policy Network. The learnable parts in the policy network are the two MLPs, i.e., U ? inside the kinematic refinement unit and V ? inside the control generation unit. We use ReLU activations for both U ? and V ? . The MLP U ? consists of hidden layers with size (256, 512, 256). The MLP V ? contains hidden layers with size (2048, 1024).</p><p>Policy Training. For Human3.6M, we train a single policy using data from all the training subjects and directly transfer the policy to test subjects, so it is a cross-subject experiment. For our in-house motion dataset, due to the large variation of body proportion and shape, we train a model for each subject using subject-specific data and test on separate data. All baselines are trained using the same data as our method. For learning the policy, each RL episode is constructed by randomly sampling a video segment of 200 frames from all training data. For the initial pose q 1 of the character, we initialize it to the refined kinematic pose q (n) 1 . For the initial velocityq 1 , we set it to the kinematic velocity q (n) 1 computed using finite differences.</p><p>The episode is terminated when the end frame is reached or the character's root height is 0.5 below the root height of the kinematic pose (i.e., to detect if the character has lost balance). We train the policy ? ? for 2000 epochs. For each epoch, we keep collecting data by sampling RL episodes until the total number of time steps reaches 50000. The reward weighting factors (? p , ? v , ? j , ? k ) are set to (30, 0.2, 100, 0.02) for Human3.6M and (60, 0.2, 300, 0.02) for the in-house dataset. For Human3.6M, we only have access to ground-truth 3D joint positions but not ground-truth joint angles, so we use the refined kinematic pose as pseudo-ground truth (for regularization) when computing rewards that need ground-truth joint angles. The elements of the policy's diagonal covariance matrix ? are set to 0.1 for Human3.6M and 0.05 for our in-house motion dataset. The residual forces ? t output by the policy is scaled by 500 before being input to the physics simulator. We use the proximal policy optimization (PPO <ref type="bibr" target="#b48">[49]</ref>) to learn the policy ? ? . The discount factor for the Markov decision process (MDP) is 0.95. We use the generalized advantage estimator GAE(?) <ref type="bibr" target="#b47">[48]</ref> to compute the advantage estimate for policy gradient and the GAE coefficient ? is 0.95. At each epoch, the policy is updated 10 times using Adam <ref type="bibr" target="#b19">[20]</ref> with a step size 5 ? 10 ?5 . The clipping coefficient in PPO is set to 0.2. Since PPO is an actor-critic based method, it also learns a value function that mirrors the design of the policy but outputs a single value estimate. The value function is updated using Adam with a step size 3 ? 10 ?4 whenever the policy is updated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our SimPoE framework. (a) SimPoE is a physics-based causal temporal model. (b) At each frame (30Hz)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>features are converted to the character's root coordinate to be orientation-invariant and encourage robustness against variations in absolute pose encountered at test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of estimated poses in the camera view and an alternative view. SimPoE estimates more accurate poses and foot contact. Pose mismatch and ground penetration are highlighted with boxes. Please see the supplementary video for more comparisons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Effect of refinement unit. details such as training procedures and hyperparameter settings can be found in Appedix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>PA-MPJPE ? Accel ? FS ? GP ? MPJPE ? PA-MPJPE ? Accel ? FS ? GP ?</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Human3.6M</cell><cell></cell><cell></cell><cell cols="3">In-House Motion Dataset</cell></row><row><cell cols="2">Method MPJPE ? w/o Meta-PD 59.9</cell><cell>44.7</cell><cell>5.9</cell><cell>2.2 1.4</cell><cell>39.8</cell><cell>31.7</cell><cell>7.1</cell><cell>0.4 0.1</cell></row><row><cell>w/o Refine</cell><cell>61.2</cell><cell>43.5</cell><cell>8.0</cell><cell>3.4 2.0</cell><cell>47.9</cell><cell>38.9</cell><cell>9.6</cell><cell>0.6 0.1</cell></row><row><cell>w/o ResAngle</cell><cell>68.7</cell><cell>51.0</cell><cell>6.4</cell><cell>4.1 2.1</cell><cell>193.4</cell><cell>147.6</cell><cell>6.5</cell><cell>0.9 0.3</cell></row><row><cell>w/o ResForce</cell><cell>115.2</cell><cell>65.1</cell><cell>23.5</cell><cell>6.1 3.2</cell><cell>48.4</cell><cell>31.3</cell><cell>12.5</cell><cell>0.9 0.3</cell></row><row><cell>w/o FeatLayer</cell><cell>81.4</cell><cell>47.6</cell><cell>9.3</cell><cell>5.0 1.8</cell><cell>36.9</cell><cell>27.5</cell><cell>9.5</cell><cell>0.6 0.1</cell></row><row><cell>SimPoE (Ours)</cell><cell>56.7</cell><cell>41.6</cell><cell>6.7</cell><cell>3.4 1.6</cell><cell>26.6</cell><cell>21.2</cell><cell>8.4</cell><cell>0.5 0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FPS on a standard</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PC with an Intel Core i9 Processor. More implementation</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ) ? k d ?q nr t ,(1)where k p and k d are the parameters of the PD controllers, q nr t andq nr t denote the joint angles and velocities of nonroot joints at the start of the simulation step, and ? denotes element-wise multiplication. The PD controllers act like damped springs that drive joints to target angles u t , where k p and k d are the stiffness and damping of the springs. In Sec. 3.4, we will introduce a new control mechanism, meta-PD control, that allows k p and k d to be dynamically adjusted by the policy to achieve an even finer level of character control. With Meta-PD control, the action a t includes elements ? p t and ? d t for adjusting k p and k d respectively. As observed in prior work<ref type="bibr" target="#b65">[66]</ref>, allowing the policy to apply external residual forces to the root greatly improves the ro-</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimal step nonrigid icp algorithms for surface registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Drecon: data-driven responsive control of physics-based characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bergamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Clavet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Richard</forename><surname>Forbes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating contact dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2389" to="2396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bullet physics engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
		</author>
		<ptr target="http://bulletphysics.org" />
	</analytic>
	<monogr>
		<title level="j">Open Source Soft</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">84</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10884" to="10894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Push it real: Perceiving causality in virtual interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Hoyet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol O&amp;apos;</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optical non-line-of-sight physics-based 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariko</forename><surname>Isogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Toole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7013" to="7022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skinning with dual quaternions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Kavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji????ra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol O&amp;apos;</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 symposium on Interactive 3D graphics and games</title>
		<meeting>the 2007 symposium on Interactive 3D graphics and games</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating 3d motion and forces of person-object interactions from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongmian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Sedlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Mansard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8640" to="8649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to schedule control fragments for physics-based characters using deep qlearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning basketball dribbling skills using trajectory optimization and deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d human motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saran</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruva</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09656</idno>
		<title level="m">Hierarchical visuomotor control of humanoids</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Galashov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11711</idno>
		<title level="m">Yee Whye Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid control</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning human behaviors from motion capture by adversarial imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Lemmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02201</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Catch &amp; carry: reusable neural controllers for vision-guided whole-body tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saran</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="39" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning predict-and-simulate policies from unorganized human motion data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soohwan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoseok</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mcp: Learning composable hierarchical control with multiplicative compositional policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xue Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3681" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sfv: Reinforcement learning of physical skills from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning locomotion skills using deeprl: Does the choice of action space matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation</title>
		<meeting>the ACM SIGGRAPH/Eurographics Symposium on Computer Animation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Perceptual metrics for character animation: sensitivity to errors in ballistic motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">S</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2003 Papers</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="537" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Contact and human dynamics from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02438</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Physcap: Physically plausible monocular 3d motion capture in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soshi</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Human body model fitting by learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3, 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Indirect deep structured learning for 3d human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Kai Vince</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5236" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Video-based 3d motion capture through biped control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Vondrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odest</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions On Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Robust imitation of diverse behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">S</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5320" to="5329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Videomocap: Modeling physically realistic human motion from monocular video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2010 papers</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A scalable approach to control diverse behaviors for physically simulated characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungdam</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10965" to="10974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">3d ego-pose estimation via imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="735" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ego-pose estimation and forecasting as real-time pd control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10082" to="10092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Residual force control for agile human behavior imitation and extended motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Joint 3d human motion capture and physical analysis from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petrissa</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
