<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bounding Box Tightness Prior for Weakly Supervised Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Wang</surname></persName>
							<email>wangjuan313@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Delta Micro Technology, Inc</orgName>
								<address>
									<postCode>92653</postCode>
									<settlement>Laguna Hills</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen SiBright Co. Ltd</orgName>
								<address>
									<postCode>518052</postCode>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bounding Box Tightness Prior for Weakly Supervised Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Weakly supervised image segmentation ? Bounding box tight- ness prior ? Multiple instance learning ? Smooth maximum approximation ? Deep neural networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a weakly supervised image segmentation method that adopts tight bounding box annotations. It proposes generalized multiple instance learning (MIL) and smooth maximum approximation to integrate the bounding box tightness prior into the deep neural network in an end-to-end manner. In generalized MIL, positive bags are defined by parallel crossing lines with a set of different angles, and negative bags are defined as individual pixels outside of any bounding boxes. Two variants of smooth maximum approximation, i.e., ?-softmax function and ?-quasimax function, are exploited to conquer the numeral instability introduced by maximum function of bag prediction. The proposed approach was evaluated on two pubic medical datasets using Dice coefficient. The results demonstrate that it outperforms the state-of-the-art methods. The codes are available at https: //github.com/wangjuan313/wsis-boundingbox.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, image segmentation has been made great progress with the development of deep neural networks in a fully-supervised manner <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3]</ref>. However, collecting large-scale training set with precise pixel-wise annotation is considerably labor-intensive and expensive. To tackle this issue, there have been great interests in the development of weakly supervised image segmentation. All kinds of supervisions have been considered, including image-level annotations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>, scribbles <ref type="bibr" target="#b9">[10]</ref>, bounding boxes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5]</ref>, and points <ref type="bibr" target="#b1">[2]</ref>. This work focuses on image segmentation by employing supervision of bounding boxes.</p><p>In the literature, some efforts have been made to develop the weakly supervised image segmentation methods adopting the bounding box annotations. For example, Rajchl et al. <ref type="bibr" target="#b14">[15]</ref> developed an iterative optimization method for image segmentation, in which a neural network classifier was trained from bounding box annotations. Khoreva et al. <ref type="bibr" target="#b5">[6]</ref> employed GrabCut <ref type="bibr" target="#b17">[18]</ref> and MCG proposals arXiv:2110.00934v1 [cs.CV] 3 Oct 2021 <ref type="bibr" target="#b13">[14]</ref> to obtain pseudo label for image segmentation. Hsu et al. <ref type="bibr" target="#b3">[4]</ref> exploited multiple instance learning (MIL) strategy and mask R-CNN for image segmentation. Kervadec et al. <ref type="bibr" target="#b4">[5]</ref> leveraged the tightness prior to a deep learning setting via imposing a set of constraints on the network outputs for image segmentation.</p><p>In this work, we present a generalized MIL formulation and smooth maximum approximation to integrate the bounding box tightness prior into the network in an end-to-end manner. Specially, we employ parallel crossing lines with a set of different angles to obtain positive bags and use individual pixels outside of any bounding boxes as negative bags. We consider two variants of smooth maximum approximation to conquer the numeral instability introduced by maximum function of bag prediction. The experiments on two public medical datasets demonstrate that the proposed approach outperforms the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Problem description Suppose I denotes an input image, and Y ? {1, 2, ? ? ? , C} is its corresponding pixel-level category label, in which C is the number of categories of the objects. The image segmentation problem is to obtain the prediction of Y , denoted as P , for the input image I.</p><p>In the fully supervised image segmentation setting, for an image I, its pixelwise category label Y is available during training. Instead, in this study we are only provided its bounding box label B. Suppose there are M bounding boxes, then its bounding box label is B = {b m , y m }, m = 1, 2, ? ? ? , M , where the location label b m is a 4-dimensional vector representing the top left and bottom right points of the bounding box, and y m ? {1, 2, ? ? ? , C} is its category label.</p><p>Deep neural network This study considers deep neural networks which output the pixel-wise prediction of the input image, such as Unet <ref type="bibr" target="#b15">[16]</ref>, FCN <ref type="bibr" target="#b11">[12]</ref>, etc. Due to the possible overlaps of objects of different categories in images, especially in medical images, the image segmentation problem is formulated as a multi-label classification problem in this study. That is, for a location k in the input image, it outputs a vector p k with C elements, one element for a category; each element is converted to the range of [0, 1] by the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MIL baseline</head><p>MIL definition and bounding box tightness prior Multiple instance learning (MIL) is a type of supervised learning. Different from the traditional supervised learning which receives a set of training samples which are individually labeled, MIL receives a set of labeled bags, each containing many training samples. In MIL, a bag is labeled as negative if all of its samples are negative, a bag is positive if it has at least one sample which is positive.</p><p>Tightness prior of bounding box indicates that the location label of bounding box is the smallest rectangle enclosing the whole object, thus the object must touch the four sides of its bounding box, and does not overlap with the region outside its bounding box. The crossing line of a bounding box is defined as a line with its two endpoints located on the opposite sides of the box. In an image I under consideration, for an object with category c, any crossing line in the bounding box has at least one pixel belonging to the object in the box; any pixels outside of any bounding boxes of category c do not belong to category c. Hence pixels on a cross line compose a positive bag for category c, while pixels outside of any bounding boxes of category c are used for negative bags.</p><p>MIL baseline For category c in an image, the baseline approach simply considers all of the horizontal and vertical crossing lines inside the boxes as positive bags, and all of the horizontal and vertical crossing lines that do not overlap any bounding boxes of category c as negative bags. This definition is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(a) and has been widely employed in the literature <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. To optimize the network parameters, MIL loss with two terms are considered <ref type="bibr" target="#b3">[4]</ref>. For category c, suppose its positive and negative bags are denoted by B + c and B ? c , respectively, then loss L c can be defined as follows:</p><formula xml:id="formula_0">L c = ? c (P ; B + c , B ? c ) + ?? c (P )<label>(1)</label></formula><p>where ? c is the unary loss term, ? c is the pairwise loss term, and ? is a constant value controlling the trade off between the unary loss and the pairwise loss. The unary loss ? c enforces the tightness constraint of bounding boxes on the network prediction P by considering both positive bags B + c and negative bags B ? c . A positive bag contains at least one pixel inside the object, hence the pixel with highest prediction tends to be inside the object, thus belonging to category c. In contrast, no pixels in negative bags belong to any objects, hence even the pixel with highest prediction does not belong to category c. Based on these observations, the unary loss ? c can be expressed as:</p><formula xml:id="formula_1">? c = ? 1 |B + c | + |B ? c | ? ? b?B + c log P c (b) + b?B ? c log(1 ? P c (b)) ? ? (2)</formula><p>where P c (b) = max k?b (p kc ) is the prediction of the bag b being positive for category c, p kc is the network output of the pixel location k for category c, and |B| is the cardinality of B.</p><p>The unary loss is binary cross entropy loss for bag prediction P c (b), it achieves minimum when P c (b) = 1 for positive bags and P c (b) = 0 for negative bags. More importantly, during training the unary loss adaptively selects a positive sample per positive bag and a negative sample per negative bag based on the network prediction for optimization, thus yielding an adaptive sampling effect.</p><p>However, using the unary loss alone is prone to segment merely the discriminative parts of an object. To address this issue, the pairwise loss as follows is introduced to pose the piece-wise smoothness on the network prediction.</p><formula xml:id="formula_2">? c = 1 |?| (k,k )?? (p kc ? p k c ) 2 (3)</formula><p>where ? is the set containing all neighboring pixel pairs, p kc is the network output of the pixel location k for category c. Finally, considering all C categories, the MIL loss L is: Generalized negative bags The similar issue also exists for the negative bag definition in the MIL baseline. To tackle this issue, for a category c, we propose to define each individual pixel outside of any bounding boxes of category c as a negative bag. This definition greatly increases the number of negative bags, and forces the network to see every pixel outside of bounding boxes during training.</p><formula xml:id="formula_3">L = C c=1 L c<label>(4)</label></formula><p>Improved unary loss The generalized MIL definitions above will inevitably lead to imbalance between positive and negative bags. To deal with this issue, we borrow the concept of focal loss <ref type="bibr" target="#b16">[17]</ref> and use the improved unary loss as follows:</p><formula xml:id="formula_4">? c = ? 1 N + ? ? b?B + c ? (1 ? P c (b)) ? log P c (b) + b?B ? c (1 ? ?)P c (b) ? log(1 ? P c (b)) ? ? (5) where N + = max(1, |B + c |), ? ? [0, 1]</formula><p>is the weighting factor, and ? ? 0 is the focusing parameter. The improved unary loss is focal loss for bag prediction, it achieves minimum when P c (b) = 1 for positive bags and P c (b) = 0 for negative bags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Smooth maximum approximation</head><p>In the unary loss, the maximum prediction of pixels in a bag is used as bag prediction P c (b). However, the derivative ?P c /?p kc is discontinuous, leading to numerical instability. To solve this problem, we replace the maximum function by its smooth maximum approximation <ref type="bibr" target="#b7">[8]</ref>. Let the maximum function be f (x) = max n i=1 x i , its two variants of smooth maximum approximation as follows are considered.</p><p>(1) ?-softmax function:</p><formula xml:id="formula_5">S ? (x) = n i=1 x i e ?xi n i=1 e ?xi<label>(6)</label></formula><p>where ? &gt; 0 is a constant. The higher the ? value is, the closer the approximation S ? (x) to f (x). For ? ? 0, a soft approximation of the mean function is obtained.</p><p>(2) ?-quasimax function:</p><formula xml:id="formula_6">Q ? (x) = 1 ? log n i=1 e ?xi ? log n ?<label>(7)</label></formula><p>where ? &gt; 0 is a constant. The higher the ? value is, the closer the approximation Q ? (x) to f (x). One can easily prove that Q ? (x) ? f (x) always holds.</p><p>In real application, each bag usually has more than one pixel belonging to object segment. However, ?f /?x i has value 0 for all but the maximum x i , thus the maximum function considers only the maximum x i during optimization. In contrast, the smooth maximum approximation has ?S ? /?x i &gt; 0 and ?Q ? /?x i &gt; 0 for all x i , thus it considers every x i during optimization. More importantly, in the smooth maximum approximation, large x i has much greater derivative than small x i , thus eliminating the possible adverse effect of negative samples in the optimization. In the end, besides the advantage of conquering numerical instability, the smooth maximum approximation is also beneficial for performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>This study made use of two public medical datasets for performance evaluation. The first one is the prostate MR image segmentation 2012 (PROMISE12) dataset <ref type="bibr" target="#b10">[11]</ref> for prostate segmentation and the second one is the anatomical tracings of lesions after stroke (ATLAS) dataset <ref type="bibr" target="#b8">[9]</ref> for brain lesion segmentation.</p><p>Prostate segmentation: The PROMISE12 dataset was first developed for prostate segmentation in MICCAI 2012 grand challenge <ref type="bibr" target="#b10">[11]</ref>. It consists of the transversal T2-weighted MR images from 50 patients, including both benign and prostate cancer cases. These images were acquired at different centers with multiple MRI vendors and different scanning protocols. Same as the study in <ref type="bibr" target="#b4">[5]</ref>, the dataset was divided into two non-overlapping subsets, one with 40 patients for training and the other with 10 patients for validation.</p><p>Brain lesion segmentation: The ATLAS dataset is a well-known open-source dataset for brain lesion segmentation. It consists of 229 T1-weighted MR images from 220 patients. These images were acquired from different cohorts and different scanners. The annotations were done by a group of 11 experts. Same as the study in <ref type="bibr" target="#b4">[5]</ref>, the dataset was divided into two non-overlapping subsets, one with 203 images from 195 patients for training and the other with 26 images from 25 patients for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation details</head><p>All experiments were implemented using PyTorch in this study. Image segmentation was conducted on the 2D slices of MR images. The parameters in the MIL loss (1) were set as ? = 10 based on experience, and those in the improved unary loss (5) were set as ? = 0.25 and ? = 2 according to the focal loss <ref type="bibr" target="#b16">[17]</ref>. As indicated below, most experimental setups were set to be same as study in <ref type="bibr" target="#b4">[5]</ref> for fairness of comparison.</p><p>For the PROMISE12 dataset, a residual version of UNet <ref type="bibr" target="#b15">[16]</ref> was used for segmentation <ref type="bibr" target="#b4">[5]</ref>. The models were trained with Adam optimizer <ref type="bibr" target="#b6">[7]</ref> with the following parameter values: batch size = 16, initial learning rate = 10 ?4 , ? 1 = 0.9, and ? 2 = 0.99. To enlarge the set of images for training, an off-line data augmentation procedure <ref type="bibr" target="#b4">[5]</ref> was applied to the images in the training set as follows: 1) mirroring, 2) flipping, and 3) rotation.</p><p>For the ATLAS dataset, ENet <ref type="bibr" target="#b12">[13]</ref> was employed as a backbone architecture for segmentation <ref type="bibr" target="#b4">[5]</ref>. The models were trained with Adam optimizer with the following parameter values: batch size = 80, initial learning rate = 5 ? 10 ?4 , ? 1 = 0.9, and ? 2 = 0.99. No augmentation was performed during training <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance evaluation</head><p>To measure the performance of the proposed approach, the Dice coefficient was employed, which has been applied as a standard performance metric in medical image segmentation. The Dice coefficient was calculated based on the 3D MR images by stacking the 2D predictions of the networks together.</p><p>To demonstrate the overall performance of the proposed method, we considered the baseline method in the experiments for comparison. Moreover, we further perform comparisons with state-of-the-art methods with bounding-box annotations, including deep cut <ref type="bibr" target="#b14">[15]</ref> and global constraint <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation study</head><p>Generalized MIL To demonstrate the effectiveness of the proposed generalized MIL formulation, in <ref type="table" target="#tab_0">Table 1</ref> we show the performance of the generalized MIL for the two datasets. For comparison, we also show the results of the baseline method. It can be observed that the generalized MIL approach consistently outperforms the baseline method at different angle settings. In particular, the PROMISE12 dataset got best Dice coefficient of 0.878 at ? best = (?40 ? , 40 ? , 20 ? ) for the generalized MIL, compared with 0.859 for the baseline method. The AT-LAS dataset achieved best Dice coefficient of 0.474 at ? best = (?60 ? , 60 ? , 30 ? ) for the generalized MIL, much higher than 0.408 for the baseline method. Smooth maximum approximation To demonstrate the benefits of the smooth maximum approximation, in <ref type="table" target="#tab_1">Table 2</ref> we show the performance of the MIL baseline method when the smooth maximum approximation was applied. As can be seen, for the PROMISE12 dataset, the better performance is obtained for ?softmax function with ? = 4 and 6 and for ?-quasimax function with ? = 6 and 8. For the ATLAS dataset, the improved performance can also be observed for ?-softmax function with ? = 6 and 8 and for ?-quasimax function with ? = 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main experimental results</head><p>In <ref type="table" target="#tab_2">Table 3</ref> the Dice coefficients of the proposed approach are given, in which ? = 4, 6, and 8 are considered in smooth maximum approximation functions and those with highest dice coefficient are reported. For comparison, the full supervision results are also shown in <ref type="table" target="#tab_2">Table 3</ref>. As can be seen, the PROMISE12 dataset gets Dice coefficient 0.878 for ?-softmax function and 0.880 for ?-quasimax function, which are same as or higher than the results in the ablation study. More importantly, these values are close to 0.894 obtained by the full supervision. The similar trends are observed for the ATLAS dataset. Furthermore, we also show the Dice coefficients of two state-of-the-art methods in <ref type="table" target="#tab_2">Table 3</ref>. The PROMISE12 dataset gets Dice coefficient 0.827 for deep cut and 0.835 for global constraint, both of which are much lower than those of the proposed approach. Similarly, the proposed approach also achieves higher Dice coefficients compared with these two methods for the ATLAS dataset.</p><p>Finally, to visually demonstrate the performance of the proposed approach, qualitative segmentation results are depicted in <ref type="figure">Fig. 2</ref>. It can be seen that the proposed method achieves good segmentation results for both prostate segmentation task and brain lesion segmentation task. <ref type="figure">Fig. 2</ref>. Ground-truth segmentation (top row) and predicted segmentation results for the proposed approach with setting ? best + ?-softmax (middle row) and setting ? best + ?-quasimax (bottom row) on the validation set for PROMISE12 (first two columns) and ATLAS (last two columns) datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper described a weakly supervised image segmentation method with tight bounding box supervision. It proposed generalized MIL and smooth maximum approximation to integrate the supervision into the deep neural network. The experiments demonstrate the proposed approach outperforms the state-of-the-art methods. However, there is still performance gap between the weakly supervised approach and the fully supervised method. In the future, it would be interesting to study whether using multi-scale outputs and adding auxiliary object detection task improve the segmentation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Demonstration of positive and negative bags, in which the red rectangle is the bounding box of the object. (a) MIL baseline. Examples of positive bags are denoted by green dashed lines, and examples of negative bags are marked by the blue dashed lines. (b) Generalized MIL. Examples of positive bags with different angles are shown (? = 25 ? for the yellow dashed lines and ? = 0 ? for the green dashed lines). Individual pixels outside of the red box are negative bags, which are omitted here for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2. 3</head><label>3</label><figDesc>Generalized MIL Generalized positive bags For an object of height H pixels and width W pixels, the positive bag definition in the MIL baseline yields only H + W positive bags, a much smaller number when compared with the size of the object. Hence it limits the selected positive samples during training, resulting in a bottleneck for image segmentation.To eliminate this issue, this study proposes to generalize positive bag definition by considering all parallel crossing lines with a set of different angles. An parallel crossing line is parameterized by an angle ? ? (?90 ? , 90 ? ) with respect to the edges of the box where its two endpoints located. For an angle ?, two sets of parallel crossing lines can be obtained, one crosses up and bottom edges of the box, and the other crosses left and right edges of the box. As examples, inFig. 1(b), we show positive bags of two different angles, in which those marked by yellow dashed colors have ? = 25 ? , and those marked by green dashed lines are with ? = 0 ? . Note the positive bag definition in MIL baseline is a special case of the generalized positive bags with ? = 0 ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dice coefficients of the proposed generalized MIL for image segmentation, where ? = (?1, ?2, ?) denotes evenly spaced angle values within interval (?1, ?2) with step ?. The standard deviation of Dice coefficients among different MR images are reported in the bracket. For comparison, results of the baseline method are also given.</figDesc><table><row><cell>Method</cell><cell>PROMISE12</cell><cell>ATLAS</cell></row><row><cell>MIL baseline</cell><cell>0.859 (0.038)</cell><cell>0.408 (0.249)</cell></row><row><cell>? = (?40 ? , 40 ? , 10 ? )</cell><cell>0.868 (0.031)</cell><cell>0.463 (0.278)</cell></row><row><cell cols="2">? = (?40 ? , 40 ? , 20 ? ) 0.878 (0.027)</cell><cell>0.466 (0.248)</cell></row><row><cell>? = (?60 ? , 60 ? , 30 ? )</cell><cell>0.868 (0.047)</cell><cell>0.474 (0.262)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Dice coefficients of the MIL baseline method when smooth maximum approximation was applied.</figDesc><table><row><cell>Method</cell><cell>PROMISE12</cell><cell>ATLAS</cell></row><row><cell></cell><cell cols="2">? = 4 0.861 (0.031) 0.401(0.246)</cell></row><row><cell>?-softmax</cell><cell cols="2">? = 6 0.861 (0.036) 0.424(0.255)</cell></row><row><cell></cell><cell cols="2">? = 8 0.859 (0.030) 0.414(0.264)</cell></row><row><cell></cell><cell cols="2">? = 4 0.856 (0.026) 0.405(0.246)</cell></row><row><cell>?-quasimax</cell><cell cols="2">? = 6 0.873 (0.018) 0.371(0.240)</cell></row><row><cell></cell><cell cols="2">? = 8 0.869 (0.024) 0.414(0.256)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of Dice coefficients for different methods.</figDesc><table><row><cell>Method</cell><cell>PROMISE12</cell><cell>ATLAS</cell></row><row><cell>Full supervision</cell><cell>0.894 (0.021)</cell><cell>0.512 (0.292)</cell></row><row><cell>? best + ?-softmax</cell><cell cols="2">0.878 (0.031) 0.494 (0.236)</cell></row><row><cell cols="3">? best + ?-quasimax 0.880 (0.024) 0.488 (0.240)</cell></row><row><cell>Deep cut [15]</cell><cell>0.827 (0.085)</cell><cell>0.375 (0.246)</cell></row><row><cell>Global constraint [5]</cell><cell>0.835 (0.032)</cell><cell>0.474 (0.245)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation using the bounding box tightness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6586" to="6597" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bounding boxes for weakly supervised segmentation: Global constraints get close to full supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging with Deep Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Applications of lp-norms and their smooth approximations for gradient based learning vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Z?hlke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Villmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mittweida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ESANN</publisher>
			<biblScope unit="page" from="271" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A large, open source dataset of stroke anatomical brain images and manual lesion segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Anglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sondag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Khoshab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation of prostate segmentation algorithms for mri: the promise12 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hoeks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kerkstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="373" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="140" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepcut: Object segmentation from bounding box annotations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Passerat-Palmbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Damodaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="674" to="683" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GrabCut&quot; interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12275" to="12284" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
