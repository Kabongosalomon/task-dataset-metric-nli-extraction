<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015">AUGUST 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="2015">AUGUST 2015</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D Human Pose Tracking</term>
					<term>Volumetric</term>
					<term>Multiple Camera Views !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present VoxelTrack for multi-person 3D pose estimation and tracking from a few cameras which are separated by wide baselines. It employs a multi-branch network to jointly estimate 3D poses and re-identification (Re-ID) features for all people in the environment. In contrast to previous efforts which require to establish cross-view correspondence based on noisy 2D pose estimates, it directly estimates and tracks 3D poses from a 3D voxel-based representation constructed from multi-view images. We first discretize the 3D space by regular voxels and compute a feature vector for each voxel by averaging the body joint heatmaps that are inversely projected from all views. We estimate 3D poses from the voxel representation by predicting whether each voxel contains a particular body joint. Similarly, a Re-ID feature is computed for each voxel which is used to track the estimated 3D poses over time. The main advantage of the approach is that it avoids making any hard decisions based on individual images. The approach can robustly estimate and track 3D poses even when people are severely occluded in some cameras. It outperforms the state-of-the-art methods by a large margin on three public datasets including Shelf, Campus and CMU Panoptic. Index Terms-3D Human Pose Tracking, Volumetric, Multiple Camera Views ! ? Chunyu Wang and Wenjun Zeng are with Microsoft Research Asia.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T his work addresses the problem of multi-person 3D pose estimation and tracking from a few cameras separated by wide baselines. The problem draws attention from multiple areas such as human pose estimation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, person re-identification <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> and multi-object tracking <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The problem can benefit many applications such as smart retail <ref type="bibr" target="#b22">[23]</ref> and sport video analysis <ref type="bibr" target="#b23">[24]</ref>.</p><p>Mainstream methods such as <ref type="bibr" target="#b24">[25]</ref> usually address the problem by three separate models. First, they estimate 2D poses in each camera view by CNN <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Second, they associate 2D poses of the same person in different views based on epipolar geometry or image features, and recover the corresponding 3D pose for each person by geometric methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Third, they link the estimated 3D poses over time by linear bipartite matching based on keypoint locations and image features. The three tasks have been independently addressed by researchers from different areas which may unfortunately lead to degraded performance (1) 2D pose estimation is noisy especially when occlusion occurs;</p><p>(2) 3D estimation accuracy depends on the 2D estimation and association results in all views; (3) unreliable appearance features caused by occlusion harms 3D pose tracking accuracy. In the following, we discuss the challenges in detail and present an overview of how our end-to-end approach successfully addresses them. The bottom-left area shows the 3D poses estimated by our approach. The numbers represent the identities of the estimates. The points with different colors represent the 3D trajectories of the root joints of different persons over time. We project the estimated 3D poses to images for visualization. The red pyramids represent the locations and orientations of the five cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Challenges in 2D Human Pose Estimation</head><p>Introducing of CNN and large scale datasets <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> has boosted 2D pose estimation accuracy <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> on benchmark datasets. However, even top-ranking methods suffer when we apply them to crowded scenes with severe occlusion and background clutter. <ref type="figure" target="#fig_0">Figure 1</ref> shows some images of this type from the Panoptic dataset <ref type="bibr" target="#b38">[39]</ref>. On one hand, top-down pose estimators <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b39">[40]</ref>, which first detect all people in the image and then perform single person pose estimation for each detection, often fail to detect people that are mostly occluded. On the other hand, bottom-up arXiv:2108.02452v1 [cs.CV] 5 Aug 2021 methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b40">[41]</ref>, which first detect all joints in an image and then group them into different instances, have limited capability to detect joints of small scale people. In summary, 2D pose estimates are very noisy in real-world applications which will inevitably cause negative and irreversible impact to the 3D pose estimation step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Challenges in 3D Human Pose Estimation</head><p>To estimate 3D poses of multiple people, we first need to associate the 2D poses of the same person in different views. This can be achieved by matching 2D poses based on epipolar geometry. Some methods <ref type="bibr" target="#b24">[25]</ref> also use image features to improve robustness. But still, it is a challenging task because 2D pose estimates may have large errors and appearance features may be corrupted when occlusion occurs. For example in <ref type="figure" target="#fig_0">Figure 1</ref>, we can see that some people are only partially visible in some camera views. So features of the people in those camera views are very different from those in normal views. After we obtain the corresponding joint locations of each person, we estimate their 3D locations by triangulation <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> or pictorial structure models <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b41">[42]</ref>. The final 3D pose estimation accuracy largely depends on the cross view association step which in turn depends on the accuracy of the 2D pose estimation step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Challenges in 3D Human Pose Tracking</head><p>The third step is to link the estimated 3D poses over time and obtain a number of tracks. Although the task itself is rarely studied, there are many works on 2D pose tracking in videos <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> which can be easily extended to track 3D poses. The fundamental problem in the task is to measure the similarity between every pair of poses between neighboring frames and then solve the classic assignment problem by bipartite matching <ref type="bibr" target="#b46">[47]</ref>. Two sources of information have been used to compute pose similarity. The first is based on motion cues which uses Kalman Filter or optical flow to predict future positions of the tracklets and prevents them from being linked to the poses (in the current frame) which are far from the predictions. This effectively promotes smoothness in tracking results. The second class of information is appearance features computed from images. However, when occlusion occurs, appearance features may be corrupted and unreliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Overview of Our Approach</head><p>We present an approach termed VoxelTrack for robustly tracking 3D poses of multiple people in challenging environment. This builds on, and gives a more detailed description of our preliminary work VoxelPose <ref type="bibr" target="#b0">[1]</ref> which estimates 3D poses from multiple cameras. Our new contributions include 1) extending the work to be able to track 3D poses over time and 2) exploiting sparseness of the 3D representation to improve the inference speed so that it can be applied to very large 3D spaces such as football court. An overview of our approach is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>(I) 2D Feature Extraction Given synchronized images as input, it first estimates 2D pose heatmaps <ref type="bibr" target="#b26">[27]</ref> and Re-ID feature maps <ref type="bibr" target="#b47">[48]</ref> by a CNN-based image encoder. Recall that a pose heatmap encodes per-pixel likelihood of a joint. Similarly, a Re-ID feature at each pixel encodes identity embedding of the person centered at the pixel. Note that we do not recover 2D poses from the heatmaps because they are usually very noisy. Instead, we keep the ambiguity in the representation and postpone decision making to the later 3D stage in which multi-view information is available to resolve the ambiguity.</p><p>(II) Holistic 3D Representation To avoid making incorrect decisions in each view, our approach directly operates in the 3D space by fusing information from all views. Specifically, we divide the 3D motion capture space by regular voxels and compute a feature vector for each voxel by inversely projecting the 2D heatmap vector at the corresponding location in each view using camera parameters. The resulting 3D heatmap volume, which carries positional information of body joints, will be fed to 3D network to estimate the likelihood of each voxel having a particular body joint. The holistic 3D representation elegantly avoids cross view association of 2D poses. In parallel, we also compute a 3D Re-ID feature volume for tracking 3D poses over time.</p><p>(III) 3D Pose Estimation The resulting 3D heatmap volume is sparse. So we apply a lightweight network with sparse 3D CNNs <ref type="bibr" target="#b48">[49]</ref> to the volume to estimate a 3D heatmap which encodes per-voxel likelihood of all joints. We first obtain a number of person instances by finding peak heatmap values of the root joint. Then for each instance, we crop a smaller fixed-size region from the volume centered at the root joint and use an independent lightweight network to estimate 3D heatmaps of all joints that belong to the instance. Finally, we obtain the 3D pose of the instance by computing expectation over the heatmaps.</p><p>(IV) 3D Pose Linking Considering that occlusion occurs frequently in real-world applications, we introduce an occlusionaware multi-view feature fusion strategy when we compute the Re-ID feature volume. The idea is that, for each 3D pose, we estimate whether it is occluded by other people in each camera view which determines whether the features in this view will be used for fusion. We link the 3D poses over time by bipartite matching based on the fused features and the 3D locations. The occlusion-aware matching strategy eliminates the harm of unreliable appearance features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How VoxelTrack Addresses the Challenges?</head><p>The most prominent advantage of VoxelTrack is that it does not require to do 2D pose estimation in every camera view nor pose association in different views as in previous works which is error-prone. Instead, all hard decisions are postponed and made in the 3D space after fusing the inversely projected 2D image features from all views. As a result, the "end-toend" inference style effectively avoids error accumulation. In addition, the representation is robust to occlusion because it fuses the features in all camera views (a joint occluded in one view may be visible in other views). Then the heatmaps are warped to construct a 3D feature grid which is fed to a 3D pose network to estimate 3D poses. We estimate person-person occlusion relationships in each view by using the 3D poses and camera parameters. Finally, we perform 3D pose tracking with the 3D poses, occlusion-masks and Re-ID features as input.</p><p>We evaluate our approach on three public datasets including the CMU Panoptic <ref type="bibr" target="#b38">[39]</ref>, Shelf <ref type="bibr" target="#b49">[50]</ref> and Campus <ref type="bibr" target="#b49">[50]</ref> datasets. VoxelTrack outperforms the existing methods by a large margin which validates the advantages of performing tracking in 3D space. More importantly, the estimation and tracking results are very stable even when severe occlusion occurs in some camera views. We also evaluate different factors in our approach such as occlusion-masks, similarity metrics and network structures. In addition, we find that the 3D network can be accurately trained on automatically generated synthetic heatmaps. This is possible mainly because the heatmap based 3D feature volume representation is a high level abstraction that is disentangled from appearance/lighting, etc. This favorable property dramatically enhances the applicability of the approach. The whole system runs at 15 FPS with 5 camera views as input on a single 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly review the existing work which are related to 3D pose tracking including 2D pose estimation, 3D pose estimation, box-level human tracking and key-point-level human tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">2D Human Pose Estimation</head><p>Estimating 2D poses in images has been a long-standing goal in computer vision <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. Before deep learning, this had often been approached by modeling human as a graph and estimating the locations of the graph nodes in images according to image features and structural priors. Development of CNNs <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> has led to remarkable accuracy improvement on benchmark datasets. In particular, introducing of the heatmap representation <ref type="bibr" target="#b37">[38]</ref>, which encodes per-pixel likelihood of body joints for each image, has dramatically improved the robustness of pose estimation. In fact, this probabilistic representation has become the de facto standard for pose estimation and is the main factor behind the success of those approaches. Our approach also uses this representation. But different from <ref type="bibr" target="#b37">[38]</ref>, we do not make hard decisions on heatmaps to recover joint locations because they are unreliable when occlusion occurs.</p><p>When an image has multiple people, an additional challenge is to group the detected joints into different instances. Existing multi-person pose estimation methods can be classified into two classes based on how they do grouping: top-down and bottom-up approaches. Top-down approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> operate in two steps: detecting all people in the image by a number of boxes and then performing single person pose estimation for each box. They crop the image according to the boxes and normalize the image patches to have the same scale which notably improves the estimation accuracy. However, those approaches suffer a lot when a large part of a person is occluded because detection in such cases barely gets satisfying results. In addition, it suffers from the scalability issue because the computation time increases linearly as the number of people in images.</p><p>In contrast, bottom-up approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b40">[41]</ref> first detect all joints in the image, and then group them into instances according to spatial and appearance affinities among them. However, since scales of different instances in a single image may vary significantly, pose estimation results are generally worse than those of the top-down methods. But joint detection is more robust to occlusion. The 2D pose estimation module in VoxelTrack is a bottom-up approach. However, we do not group joints into instances. Instead, we keep the heatmap representation and warps heatmaps of all views to a common 3D space in order to detect 3D person instances. So joint association is implicitly accomplished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">3D Human Pose Estimation</head><p>There are two challenges in multi-person 3D pose estimation. First, it needs to associate joints of the same person as discussed previously. Second, it needs to associate the 2D poses of the same person in different views based on appearance features <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> or geometric features <ref type="bibr" target="#b22">[23]</ref> which is unstable when people are occluded. Some methods adopt model-based methods by maximizing the consistency between the model projections and image observations. For example, the pictorial structure model is extended to deal with multiple person 3D pose estimation in <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b56">[57]</ref>. However, the interactions across people introduce loops to the graph which notably complicates optimization. These challenges limit the 3D pose estimation accuracy of those methods.</p><p>Dong et al. <ref type="bibr" target="#b24">[25]</ref> propose a multi-way matching algorithm to find cycle-consistent correspondences of detected 2D poses across multiple views using both appearance and geometric cues, which is able to prune false detections and deal with partial overlaps between views. Then they recover the 3D pose for each person using triangulation-based methods. Chen et al. <ref type="bibr" target="#b22">[23]</ref> exploit temporal consistency in videos to match the detected 2D poses with the estimated 3D poses directly in 3D space and update the 3D poses iteratively via the crossview multi-human tracking. This novel formulation improves both accuracy and efficiency. However, both approaches are venerable to inaccurate 2D pose estimates in each view which is often the case in practice.</p><p>Our work is better than the pictorial structure models <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b56">[57]</ref> because it does not suffer from local optimum and does not need the number of people in each frame to be known as input. It differs from the model-free methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> in that it elegantly avoids the two association problems. The approach is readily applicable to large spaces such as basketball court. The computation time is hardly affected by the number of people in the environment because we use the coarse-to-fine approach to divide space into voxels and the lightweight sparse 3D CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Human Pose Tracking</head><p>Human pose tracking is related to box-level object tracking which aims to estimate trajectories of objects in videos. Most methods follow the tracking-by-detection paradigm, which first detect objects in each frame and then link them over time. The key is to compute similarity between detections and tracklets. One class of methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref> use location and motion cues. For example, Kalman Filter <ref type="bibr" target="#b60">[61]</ref> or optical flow are used to predict future tracklet positions and then they compute the distance between the predicted and detected object positions as the similarity. Another class of methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b61">[62]</ref> use image features to compute similarity. The first class are fast and effective for short-range linking while the second are better at handling long-range linking which is critical to track objects that re-appear after being occluded for a while. To track objects in multiple cameras, some works <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref> propose to first detect 2D boxes in each view and then link them across both time and view points according to appearance similarity. Some other works use multi-view geometry and location cues to track ground plane detections <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, 3D points <ref type="bibr" target="#b71">[72]</ref> or 3D centroid-with-extent detections <ref type="bibr" target="#b72">[73]</ref>.</p><p>Compared to box-level tracking, pose tracking has access to finer-grained joint locations. Some offline trackers such as <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b73">[74]</ref> formulate pose tracking as a graph partitioning problem in which the joints of the same person in different frames are expected to be connected while the joints of different persons are disconnected. Some online trackers <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref> solve the problem by bipartite matching which first estimate 2D poses in the current frame and then link them to the closest tracklets, respectively. In <ref type="bibr" target="#b4">[5]</ref>, optical flow is used as the motion model to reduce missing detections and the similarity is computed by the human joint distances. In a recent work <ref type="bibr" target="#b75">[76]</ref>, a Graph Convolution Network Re-ID model is used to extract Re-ID features based on all joints instead of bounding boxes.</p><p>To our best, few works have systematically studied 3D pose tracking in multiple cameras which is the focus of this work. Different from previous multi-view box-level tracking methods, we have more precise 3D keypoint coordinates which allows us to reliably reason about occlusion. We combine keypoint distances and occlusion-aware appearance feature distances to compute similarity and achieve very stable tracking results in challenging scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VOXELTRACK: 3D POSE ESTIMATION</head><p>In this section, we present the first part of VoxelTrack which estimates 3D poses of all people in the environment. This includes estimating 2D pose heatmaps, Re-ID features and 3D poses from multiview images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Backbone Network</head><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we use DLA-34 <ref type="bibr" target="#b76">[77]</ref> as backbone network to extract intermediate features for images of all views independently. The DLA network <ref type="bibr" target="#b77">[78]</ref> was first proposed for image classification. We use a recent variant <ref type="bibr" target="#b76">[77]</ref> for dense prediction tasks which uses iterative deep aggregation to increase feature map resolution. It takes an image I v ? R 3?H?W from view v as input and outputs a feature map</p><formula xml:id="formula_0">F v ? R C? H 4 ? W 4 .</formula><p>The feature map will be fed to two networks to estimate 2D pose heatmaps and Re-ID features, respectively as will be discussed subsequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">2D Pose Heatmap Estimation</head><p>We use a simple network to estimate 2D pose heatmaps</p><formula xml:id="formula_1">H v ? R J? H 4 ? W 4</formula><p>from backbone features F v where J is the number of body joint types. The network consists of two convolutional layers as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. A heatmap encodes per-pixel likelihood of a body joint which is a common surrogate representation for human pose used in many work <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. We train the 2D pose heatmaps by minimizing:</p><formula xml:id="formula_2">L 2D = H * v ? H v 2 ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">H * v ? R J? H 4 ? W 4</formula><p>is the ground truth 2D pose heatmaps computed following <ref type="bibr" target="#b4">[5]</ref>. Different from the previous works, we do not make any hard decisions on 2D heatmaps but use them as input to our 3D pose network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Re-ID Features</head><p>We use a simple network to estimate Re-ID feature maps</p><formula xml:id="formula_4">G v ? R d? H 4 ? W 4</formula><p>from backbone features F v where d is the dimension of Re-ID features. It consists of two convolutional layers as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Inspired by FairMOT <ref type="bibr" target="#b10">[11]</ref>, for each person in an image, we sample a d-dimensional feature from the feature maps at the pelvis joint as its Re-ID features. We use a Fully Connected (FC) network and a softmax operation to map the sampled features to a onehot vector P = {p(t), t ? [1, T ]} representing the person's identity. Denote the one-hot representation of the GT class label as L i (p). We train the Re-ID network as a classification task using cross entropy loss as follows:</p><formula xml:id="formula_5">L ID = ? N i=1 T t=1 L i (t)log(p(t)),<label>(2)</label></formula><p>where N is the number of people in the image and T is the total number of unique people in the training dataset. During training, we use ground-truth pelvis joint locations in images to extract Re-ID features. During testing, we project estimated 3D pelvis locations to images to sample Re-ID features. During testing, we use the sampled Re-ID features before the classification layer to represent each person as will be described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">3D Joint Estimation</head><p>We discretize the 3D motion space by X ? Y ? Z discrete voxels {D x,y,z }. Each voxel is a candidate location for body joints. In order to reduce quantization error, we usually set the size of a voxel to be as small as possible (62.5mm in this paper). We compute a feature vector for each voxel by calculating average 2D heatmap values sampled at its projected locations in all camera views. Denote the 2D heatmap of view v as</p><formula xml:id="formula_6">H v ? R J? H 4 ? W 4</formula><p>where J is the number of body joints. For each voxel D x,y,z , we denote its projected location in view v as P x,y,z v . The heatmap feature at P x,y,z v is denoted as H x,y,z v ? R J . We compute the feature vector of the voxel as:</p><formula xml:id="formula_7">V x,y,z = 1 V V v=1 H x,y,z v</formula><p>where V is the number of camera views. We can see that V x,y,z actually encodes the likelihood that the body joints are at D x,y,z . Note that the feature volume V is usually very noisy because some voxels which do not correspond to body joints may also get non-zero features due to lack of depth information.</p><p>We present Joint Estimation Network (JEN) to estimate 3D joint heatmaps U ? R J,X,Y,Z from V. The network structure is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Each confidence score U j,x,y,z represents the likelihood that there is a joint of type j at voxel D x,y,z . The likelihood of all joints at all voxels form 3D joint heatmaps U ? R J,X,Y,Z . During training, we compute ground-truth joint heatmaps U j,x,y,z * for every voxel according to its distance to ground-truth joint locations. Specifically, for each pair of ground-truth joint location and voxel, we compute a Gaussian score according to their distance. The score decreases exponentially when distance increases. Note that there could be multiple scores for one voxel if there are multiple people in the environment and we simply keep the largest one. We train JEN by minimizing:</p><formula xml:id="formula_8">L JEN = J j=1 X x=1 Y y=1 Z z=1 U j,x,y,z * ? U j,x,y,z 2<label>(3)</label></formula><p>Inspired by the voxel-to-voxel prediction network in <ref type="bibr" target="#b78">[79]</ref>, we adopt 3D convolutions as the basic building block for estimating 3D joint heatmaps. Since input feature volumes V are usually sparse and have clear semantic meanings, we propose a simpler structure than <ref type="bibr" target="#b78">[79]</ref> as shown in <ref type="figure" target="#fig_4">Figure 4</ref>. In some scenarios such as football court, the motion capture space can be very large which will inevitably result in high dimensional feature volumes. This will notably decrease the inference speed. We solve the problem by using sparse 3D convolutions <ref type="bibr" target="#b48">[49]</ref> because in general the feature volume only has a small number of non-zero values. This significantly improves the inference speed in our experiments, especially for large voxel sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">3D Joint Grouping</head><p>Suppose we have already estimated 3D joint locations (represented by 3D joint heatmaps) as in the above section. The  remaining task is to assemble the estimated joints into poses of different instances. To that end, we present an Ambiguity Resolution Network (ARN) to fulfill the task which is shown in the bottom part of <ref type="figure" target="#fig_4">Figure 4</ref>. We first obtain a number of 3D pelvis joint locations by finding peak responses in the 3D joint heatmaps. Each location represents a candidate instance of person. We perform Non-Maximum Suppression (NMS) based on the heatmap scores to extract local peaks. Then for each person, we pool features around the pelvis joint from the 3D joint heatmaps with a fixed size X ? Y ? Z which is sufficiently large to enclose a person in arbitrary poses. We set X = Y = Z = 32, which corresponds to 2000mm in real world. We feed the pooled features to ARN to estimate a 3D pose heatmap A k ? R X ,Y ,Z for each joint k of this person. The joint responses of other persons are learned to be suppressed by ARN. Finally, we use a soft argmax operation <ref type="bibr" target="#b28">[29]</ref> to generate 3D location J k of the joint k from the pose heatmaps. It can be obtained by computing the center of mass of A k according to the following formula:</p><formula xml:id="formula_9">J k = X x=1 Y y=1 Z z=1 (x, y, z) ? A k (x, y, z)<label>(4)</label></formula><p>Note that we do not obtain the location J k by finding the maximum of A k because the quantization error of 62.5mm is still large. Computing the expectation as in the above equation effectively reduces the error. We train ARN by comparing the estimated 3D poses to the ground-truth 3D poses J * with L 1 loss as follows:</p><formula xml:id="formula_10">L ARN = J k=1 J k * ? J k 1<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VOXELTRACK: 3D POSE TRACKING</head><p>We now present the second part of VoxelTrack which links the estimated 3D poses over time. This is a standalone module which does not require training. The core is to compute a similarity matrix between 3D poses of the current and subsequent frame. With the similarity matrix, it accomplishes linking by solving a standard linear bipartite matching problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Occlusion Relationship Reasoning</head><p>Since Re-ID features in our work are extracted from images, they suffer from occlusion. Considering that most occlusion in the benchmark datasets belongs to person-person occlusion, we estimate to what extent a 3D pose is occluded by other people in the environment. The idea can also be used to handle human-object occlusion.</p><p>In general, if a person is severely occluded by other people in one view, we decrease the contribution of the corresponding Re-ID feature. To achieve the target, for each estimated 3D pose, we estimate its approximate depth relative to each camera using the camera parameters. <ref type="figure">Figure 5</ref> shows the way to compute how much a person is occluded. Specifically, we use the average depth of all joints to represent the depth of a person. For every camera, we put a 2D bounding box parallel to the camera plane at the average depth tightly enclosing all body joints. For each location in the box, we can easily determine whether it is occluded by the boxes of other poses. We first compute a minimum depth map for each pixel of the image. Then, we can compute the occluded area by comparing each person's depth map to the minimum depth map. The percentage of locations that are not occluded is used as a reliability score for its Re-ID feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera 3D Poses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person Depth Map Minimum Depth Map</head><p>Person1: no occlusion Person2: partly occlusion Person3: partly occlusion <ref type="figure">Fig. 5</ref>. Some steps to compute the occlusion relationship based on depth. The person depth map is computed using camera parameters and 3D poses. For the minimum depth map, the color becomes deeper as the depth becomes larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Similarity Metrics</head><p>We compute similarity between two 3D poses according to their appearance and spatial features. We project the 3D pelvis joint locations of the 3D poses to all cameras and sample corresponding Re-ID features. Denote the Re-ID features of the i th and j th poses in all camera views as</p><formula xml:id="formula_11">{G 1 i , G 2 i , ? ? ? , G V i } and {G 1 j , G 2 j , ? ? ? , G V j },</formula><p>respectively. We compute the weighted average of all camera views as the final Re-ID feature</p><formula xml:id="formula_12">G i = V v=1 ? v i G v i</formula><p>where ? v i represents the reliability score computed according to occlusion relationship. In particular, if more than 70% of the box is occluded by other people, ? v i is set to be 0. We compute the cosine distance between the fused Re-ID features as the appearance cues. We also compute Euclidean distance between two poses to promote smoothness of tracking as the location cues. For each tracklet, we normalize the Euclidean distance between it and all the detections. We compute the average of the appearance and location similarity as the final metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tracking Framework</head><p>We adopt a simple framework for online multiple object tracking. In the first frame, we initialize the estimated 3D poses as tracklets. We use the Hungarian algorithm to assign the 3D poses in the current frame to the existing tracklets. We prevent a 3D pose from being matched to a tracklet which has very large distance. If the spatial distance between the tracklet and the 3D pose is too large, we reject the assignment. If a 3D pose is not matched to any tracklets, we start a new track one. When a tracklet is not matched to any 3D poses for more than 30 frames, we set the tracklet to inactive state and it will not be used in the future. We use the appearance features of the newly matched 3D pose to update the appearance features of the tracklet by linear blending following <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DATASETS AND METRICS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Campus Dataset <ref type="bibr" target="#b56">[57]</ref> It captures three people interacting with each other in an outdoor environment by three cameras. We follow <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b56">[57]</ref> to split the dataset into training and testing subsets. To avoid over-fitting to this small training data, we train the 2D part on the COCO Keypoint dataset <ref type="bibr" target="#b31">[32]</ref> and train the 3D part using the camera parameters of the Campus dataset to generate the sythetic 3D poses and 2D heatmap pairs. Shelf Dataset <ref type="bibr" target="#b56">[57]</ref> It captures four people disassembling a shelf by five cameras. This dataset has more occlusion than the Campus dataset. Similar to what we do for Campus, we do not use the images or poses to train on the Shelf dataset. <ref type="bibr" target="#b38">[39]</ref> This is a recently introduced large scale multi-camera dataset for 3D pose estimation and tracking. It captures people doing daily activities by dozens of cameras among which five HD cameras <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23)</ref> are used in our experiments. We also report results when we use even fewer cameras. Following <ref type="bibr" target="#b79">[80]</ref>, the training set consists of the following sequences: ''160422 ultimatum1'',''16022 4 haggling1'',''160226 haggling1'',''161202 haggling1'',' '160906 ian1'',''160906 ian2'',''160906 ian3'',''160906 b and1'',''160906 band2'',''160906 band3''. The testing set consists of :''160906 pizza1'',''160422 haggling1'',''16090 6 ian5'',and''160906 band4''.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CMU Panoptic Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Metrics</head><p>3D Pose Estimation Metric Following <ref type="bibr" target="#b24">[25]</ref>, we use the Percentage of Correct Parts (PCP3D) metric to evaluate the estimated 3D poses. Specifically, for each ground-truth 3D pose, it finds the closest pose estimate and computes percentage of correct parts. We can see that this metric does not penalize false positive pose estimates. To overcome the limitation, we also extend the Average Precision (AP K ) metric <ref type="bibr" target="#b80">[81]</ref> in object detection to evaluate multi-person 3D pose estimation quality which is more comprehensive than PCP3D. In particular, if Mean Per Joint Position Error (MPJPE) of an estimate is smaller than K millimeters, we think the pose is accurately estimated. AP K computes the average precision value for recall value over 0 and 1.</p><p>3D Pose Tracking Metric We modify the standard bounding box Multi-Object Tracking (MOT) metric CLEAR <ref type="bibr" target="#b81">[82]</ref> and the 2D pose tracking metrics <ref type="bibr" target="#b42">[43]</ref> for 3D pose tracking. In particular, we compute a MOTA score for each body joint independently in a similar way as 2D pose tracking. In particular, the matching threshold of the predicted joint and the ground truth joint is half of a head size (150mm). The MOTA score jointly considers the pose estimation and the pose linking accuracy. We count the identity switches (ID Switch) for each joint. We also compute IDF1 scores <ref type="bibr" target="#b82">[83]</ref> to make an overall evaluation of the identification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation Details</head><p>The training and testing images are resized to 800 ? 608. The resulting 2D heatmaps and Re-ID feature maps have the resolution of 200?152. We use DLA-34 <ref type="bibr" target="#b76">[77]</ref> as our backbone which is pre-trained on the ImageNet classification dataset. The number of body joints J is set to be 15 in accordance with the COCO dataset. The dimension of Re-ID features is set to be 64 following FairMOT <ref type="bibr" target="#b47">[48]</ref>.</p><p>The motion capture space is set to be 10m?10m?4m for the three datasets. We divide the space into 160?160?64 bins. So each bin is approximately of size 62.5mm?62.5mm?62.5mm. Note that since we compute expectation of joint locations over 3D heatmaps, the actual error is much smaller than 62.5mm. Recall that we apply sparse 3D convolution to feature volume to estimate 3D heatmaps. To promote sparsity, we set features in the volume to zero if their original values is smaller than 0.15. For ARN, we pool features from the space of size 2000mm ? 2000mm ? 2000mm (about 32 ? 32 ? 32 voxels) around estimated pelvis joints.</p><p>We train VoxelTrack in three separate stages. We use Adam optimizer in all stages. In the first stage, we train the 2D model for estimating heatmaps and Re-ID feature maps for 20 epochs with a start learning rate of 1e ?4 . The learning rate decreases to 1e ?5 after the 15 th epoch. Next, we fix the 2D model and train the 3D joint estimation network for 10 epochs. The learning rate is set to be 1e ?4 . Finally, we train ARN to estimate 3D poses of all instances for 10 epochs with learning rate set to be 1e ?4 . Note that we can also jointly train the three models if we have access to a large number of paired (image, 3D pose) training data. There are several reasons why we choose separate training: (1) when we apply our model to a new environment, it may be impossible to obtain a large number of (image, 3D pose) pairs for training the model. In this case, we can train the 2D model on public datasets and train the 3D model by generating a large number of 3D pose and 2D heatmap pairs; (2) separate training allows us to use larger batch size which helps stabilize training.</p><p>For training on the Campus and Shelf dataset <ref type="bibr" target="#b56">[57]</ref>, we do not use the images or poses and only use the camera parameters to avoid over-fitting to these small datasets. We use <ref type="bibr" target="#b26">[27]</ref> as our 2D backbone network and train on the COCO Keypoint dataset <ref type="bibr" target="#b31">[32]</ref>. It is worth noting that COCO does not have identity annotations and we cannot directly train the Re-ID branch on it. We propose a weakly supervised learning approach to train the Re-ID part on the COCO dataset. We assign each 2D pose a unique identity and thus regard each object instance in the dataset as a separate class. We apply different transformations to the whole image including flipping, rotation, scaling and translation to help create different appearances of the same instance.</p><p>For training the 3D part on the Campus and Shelf dataset, we generate many synthetic heatmaps using the camera parameters. we place a number of 3D poses (sampled from the motion capture datasets such as Panoptic <ref type="bibr" target="#b38">[39]</ref>) at random locations in the space and project them to all views to get the respective 2D locations. Then we generate 2D heatmaps from the locations to train the 3D part. This has significant practical values as we can easily apply our model to new environments such as a retail store with the camera parameters available. <ref type="table" target="#tab_1">Table 1</ref> shows the 3D pose estimation results of the state-of-the-art methods on the Campus and the Shelf datasets in the top and bottom sections, respectively. We can see that our approach improves PCP3D from 96.6% of <ref type="bibr" target="#b22">[23]</ref> to 96.7% on the Campus dataset and 96.9% of <ref type="bibr" target="#b24">[25]</ref> to 97.1% on the Shelf dataset, which is a decent improvement considering the already very high accuracy. As discussed in Section 5.2, the PCP3D metric does not penalize false positive estimates. However, it is also meaningless to report AP scores because the GT pose annotations in this dataset are incomplete. So we propose to visualize and publish all of our estimated poses of the Shelf dataset 1 and the Campus datset 2 . We find that our approach usually gets accurate estimates as long as joints are visible in at least two views. The previous works <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b83">[84]</ref> did not report numerical results on the large scale Panoptic dataset. We encourage future works to do so as in <ref type="table" target="#tab_3">Table 2 and Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison to the State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Pose Estimation</head><p>3D Pose Tracking <ref type="table" target="#tab_1">Table 1</ref> shows the 3D pose tracking results of our method on the Campus and the Shelf datasets. The MOTA metric is computed by FP, FN and ID Switch, which jointly consider the person detection, pose estimation and pose tracking performance. The ID Switch and IDF1 metrics can better reveal the tracking performance. We achieve 0 ID Switch and high IDF1 score (94.6 on Campus and 97.2 on Shelf) on both of the datasets with severe occlusion in each single view, which indicates that our multi-view 3D tracking method can achieve accurate tracking results. We achieve higher MOTA on the Shelf dataset than on the Campus dataset (94.4 vs 89.3) mainly because the pose estimation results on the Shelf dataset is better.</p><p>Qualitative Study We show some 3D pose tracking results on the Shelf dataset in <ref type="figure" target="#fig_5">Figure 6</ref>. We can see that there is severe occlusion in the images of all camera views. However, by fusing heatmaps from multiple cameras, our approach obtains more robust features which allows us to successfully estimate the 3D poses without bells and whistles. It is noteworthy that we do not need to associate 2D poses in different views based on noisy 2D poses by combining a number of sophisticated techniques. This significantly improves the robustness of the approach. We can see that people with identity 2, 3 and 4 are walking around the shelf with lots of occlusion. The identities of the four people keep the same across the frames, which indicates that our approach has stable tracking performance in severe occlusion cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Factors that Impact Estimation Accuracy</head><p>We conduct ablation studies to evaluate a variety of factors of our approach. The results on the Panoptic dataset <ref type="bibr" target="#b38">[39]</ref>    are shown in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref>. We evaluate both the 3D pose estimation accuracy, the 3D tracking accuracy and the computation time. <ref type="table" target="#tab_3">Table 2</ref> shows the 3D factors and <ref type="table" target="#tab_4">Table 3</ref> shows the 2D factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Voxel Size</head><p>We evaluate three different voxel sizes: 160 ? 160 ? 64, 120 ? 120 ? 48 and 80 ? 80 ? 32. The motion capture space is set to be 10m ? 10m ? 4m. By comparing the first three lines in <ref type="table" target="#tab_3">Table 2</ref>, we can see that increasing the voxel size from 80 ? 80 ? 32 to 120 ? 120 ? 48 significantly improves the 3D pose estimation accuracy as the AP 25 metric improves from 39.74 to 71.00 and the MPJPE metric decreases from 26.16mm to <ref type="bibr">19.83mm</ref>. When further increasing the size from 120 ? 120 ? 48 to 160 ? 160 ? 64, the improvement is not that large, which is also reasonable because the accuracy is more difficult to be increased as the grids become more fine-grained. By comparing the MOTA, IDF1 and ID Switch metrics, we can see that the voxel size does not influence the tracking accuracy much. The computation time of JEN increases as the voxel size increases. To strike a good balance between accuracy and speed, we use 160 ? 160 ? 64 for the rest of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse Convolution</head><p>We use the sparse convolution to replace the standard convolution as the 3D feature volume only has a small number of non-zero values and the sparse convolution only computes for the non-zero values. By comparing the sparse convolution to the standard convolution with the same voxel size in <ref type="table" target="#tab_3">Table 2</ref>, we can see that the computation time of JEN significantly decreases when using sparse convolution, especially under large voxel sizes such as 160 ? 160 ? 64 and 120 ? 120 ? 48. For the small voxel size such as 80 ? 80 ? 32, the sparse convolution is a little slower than the standard convolution. This is because the sparse convolution needs to find the index of the non-zero values and it takes considerable time. Thus, we do not apply sparse convolution to ARN because the size of the input 3D heatmap to ARN is much smaller. We can also see that the pose estimation accuracy of the sparse convolution is a little higher than the standard convolution under all voxel sizes. This is because we set the values of the feature volume to zero if their original values is smaller than 0.15 when applying the sparse convolution. The sparsity of the feature volume may reduce some ambiguity and thus increase the pose estimation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Cameras</head><p>As shown in the first three lines of <ref type="table" target="#tab_4">Table  3</ref>, reducing the number of cameras generally increases the 3D pose estimation error because the information in the feature volume becomes less complete. The tracking accuracy is less affected by the number of cameras as the ID Switch is always 0. We can see that using 3 cameras can already accurately track the 3D poses. The computation time also decreases as the number of cameras decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone Networks</head><p>We evaluate three different backbone networks including DLA-34 <ref type="bibr" target="#b76">[77]</ref>, MobileNet-V2 <ref type="bibr" target="#b85">[86]</ref> and Higher-HRNet-W32 <ref type="bibr" target="#b26">[27]</ref>. For the MobileNet-V2, we add several de-convolution layers after the backbone network following <ref type="bibr" target="#b4">[5]</ref>. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Higher-HRNet-W32 achieves the highest AP and the lowest MPJPE.</p><p>MobileNet-V2 achieves the highest running speed. DLA-34 achieves a good balance between accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Sizes</head><p>We also evaluate three different image sizes including 960 ? 512, 800 ? 448, 640 ? 384. As shown in <ref type="table" target="#tab_4">Table 3</ref>, we use DLA-34 as the backbone and evaluate different image sizes. Reducing the image sizes generally increases the 3D pose estimation error because large size images provide more detailed information. The tracking performance is hardly affected by image sizes. We can see that getting accurate 2D heatmaps is critical to the 3D accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Factors that Impact Tracking Accuracy</head><p>There are three main components in our tracking procedure: 1) 3D poses, 2) Re-ID features, 3) occlusion-mask. We evaluate the impact of each of these components. The tracking results are shown in <ref type="table" target="#tab_2">Table 4</ref>. The MOTA and IDF1 metrics are the average of all keypoints. The ID Switch metric is the switches of all the keypoints and it is a multiple of the number of joints (i.e. 15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Poses</head><p>We only use the normalized Euclidean distances of the 3D poses to link the detections to the tracklets. The result is shown in the first line of <ref type="table" target="#tab_2">Table 4</ref>. We can only get 93.82 IDF1 score and 90 ID switches when only using the 3D poses. We find that the ID switches often occur when FP appears. In general, the 3D poses are reliable because there is almost no occlusion in the 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-ID Features</head><p>We only use the cosine distance of Re-ID features to perform linking. For each 3D person, we fuse the Re-ID features in each view by just adding them without reasoning about the occlusion. The result is shown in the second line of    views where the person is not occluded. Our approach can keep the identities of the six people the same and has stable tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Whole System Running Time</head><p>We divide our whole system into the 2D part, the 3D part and the tracking part and compute the running time of each of them. The running time of the 3D part is shown in <ref type="table" target="#tab_3">Table  2</ref> which is the sum of the "JEN Time" and the "ARN Time". The running time of the 2D part and the tracking part is shown in <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table" target="#tab_2">Table 4</ref>, respectively. From <ref type="table" target="#tab_3">Table 2</ref> we can see that the sparse convolution can reduce a large amount of running time of JEN. The voxel size also notably affects the running time. It is worth noting that the running time of ARN is hardly affected by the number of people (i.e. 1 ms for 1 person). From <ref type="table" target="#tab_4">Table 3</ref> we can see that the number of views, the backbone network, and the image size together determine the running time of the 2D part. From <ref type="table" target="#tab_2">Table 4</ref> we can see that the running time of the tracking part can almost be ignored (i.e. 2 ms), which indicates the simplicity of our tracking algorithm. The light version of our system using MobileNet-V2 <ref type="bibr" target="#b85">[86]</ref> as the 2D backbone and 120 ? 120 ? 48 JEN with the sparse convolution can run at 15 FPS with 5 camera views as input, which dramatically enhances the practical values of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We present a novel approach for multi-person 3D pose estimation and tracking. It employs a multi-branch network to jointly estimate 3D poses and Re-ID features for all people in the environment. Different from the previous methods, it only makes hard decisions in the 3D space which allows to avoid the challenging association problems in the 2D space. In particular, noisy and incomplete information of all camera views are warped to a common 3D space to form a comprehensive feature volume which is used for 3D estimation. We also introduce an occlusion-aware matching strategy during tracking. The experimental results on the benchmark datasets validate that the approach is robust to occlusion. In addition, the 3D part of the approach can be directly trained on synthetic data which has practical values. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The top and right area show the images captured by five synchronized cameras.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of VoxelTrack for 3D pose tracking. Given multi-view images as input, it first estimates pixel-wise pose heatmaps and Re-ID features for each view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The network structure for estimating 2D pose heatmaps and Re-ID features. I v , F v , H v and G v represent the image, backbone feature map, 2D heatmap and Re-ID feature map of camera v, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The network structure for estimating 3D joint heatmaps and 3D poses. We first use a Joint Estimation Network (JEN) to get the 3D heatmaps of all the joints and then use a Ambiguity Resolution Network (ARN) to get the 3D poses. H v , D, V, U represent the 2D heatmap, empty discrete voxels, feature vectors of the voxels and the 3D heatmap, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization results on the Shelf dataset. The top is the 2D images captured by 5 cameras and the bottom is the 3D pose tracking results. Different numbers and colors represent different person identities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization results on the "160906 pizza1" sequence of the Panoptic dataset. The top is the 2D images captured by 5 cameras and the bottom is the 3D pose tracking results. Different numbers and colors represent different person identities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Comparison to the state-of-the-art methods on the Campus and the Shelf datasets. The metric is PCP3D and AP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The IDF1 score (94.38 vs 93.82) and ID Switch (15 vs 90) are better than only using 3D poses. There still exists some ID Switches because some views with heavily occluded people provide some unreliable Re-ID features which cause some ambiguity.Occlusion Mask We use the occlusion mask mentioned in Section 4.1 computed by each person's depth to fuse the Re-ID features of all the views. If the person in the view is heavily occluded, we do not use the Re-ID features of the person in this view. The result is shown in the third line ofTable 4. We can see that using the occlusion mask to fuse Re-ID features achieves the highest IDF1 score 98.67 and does not have ID Switch which agrees with our expectation. When we further use the Re-ID features with occlusion mask and the 3D poses together, the tracking results keep the same, which indicates that our multi-view fused Re-ID features have powerful discriminative ability.Qualitative Study We show the 3D pose tracking results of the Panoptic dataset inFigure 7. We can see that there are severe occlusions in the images of all camera views. The person with identity 6 has the most obvious movement. He comes to the table and then leaves. He is occluded in most of the camera views and the Re-ID features of the person are not reliable in most views. Thus, we need to use the occlusion mask to choose the Re-ID features in the specific</figDesc><table><row><cell>Voxel Size</cell><cell>JEN Type</cell><cell>AP 25</cell><cell>AP 50</cell><cell>AP 100</cell><cell>MPJPE</cell><cell>MOTA</cell><cell>IDF1</cell><cell>ID Switch</cell><cell>JEN Time</cell><cell>ARN Time</cell></row><row><cell>160 ? 160 ? 64</cell><cell>SP Conv</cell><cell>79.34</cell><cell>96.83</cell><cell>99.58</cell><cell>18.49 mm</cell><cell>98.45</cell><cell>98.67</cell><cell>0</cell><cell>48.46 ms</cell><cell>2.72 ? n ms</cell></row><row><cell>120 ? 120 ? 48</cell><cell>SP Conv</cell><cell>71.00</cell><cell>97.04</cell><cell>99.45</cell><cell>19.83 mm</cell><cell>98.27</cell><cell>98.52</cell><cell>0</cell><cell>30.93 ms</cell><cell>1.35 ? n ms</cell></row><row><cell>80 ? 80 ? 32</cell><cell>SP Conv</cell><cell>39.74</cell><cell>94.27</cell><cell>99.10</cell><cell>26.16 mm</cell><cell>97.62</cell><cell>95.13</cell><cell>0</cell><cell>22.01 ms</cell><cell>1.21 ? n ms</cell></row><row><cell>160 ? 160 ? 64</cell><cell>Conv</cell><cell>74.09</cell><cell>96.87</cell><cell>99.55</cell><cell>19.05 mm</cell><cell>98.32</cell><cell>98.39</cell><cell>0</cell><cell>132.64 ms</cell><cell>2.71 ? n ms</cell></row><row><cell>120 ? 120 ? 48</cell><cell>Conv</cell><cell>68.89</cell><cell>97.06</cell><cell>99.51</cell><cell>20.28 mm</cell><cell>98.16</cell><cell>98.21</cell><cell>0</cell><cell>57.30 ms</cell><cell>1.36 ? n ms</cell></row><row><cell>80 ? 80 ? 32</cell><cell>Conv</cell><cell>38.66</cell><cell>94.40</cell><cell>99.17</cell><cell>25.93 mm</cell><cell>98.27</cell><cell>98.56</cell><cell>0</cell><cell>18.75 ms</cell><cell>1.21 ? n ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Ablation study of voxel size and sparse convolution on the Panoptic dataset.</figDesc><table><row><cell>Views</cell><cell>Backbone</cell><cell>Image Size</cell><cell>AP 25</cell><cell>AP 50</cell><cell>AP 100</cell><cell>MPJPE</cell><cell>MOTA</cell><cell>IDF1</cell><cell>ID Switch</cell><cell>2D Time</cell></row><row><cell>5</cell><cell>DLA-34</cell><cell>960 ? 512</cell><cell>79.34</cell><cell>96.83</cell><cell>99.58</cell><cell>18.49 mm</cell><cell>98.45</cell><cell>98.67</cell><cell>0</cell><cell>85.71 ms</cell></row><row><cell>4</cell><cell>DLA-34</cell><cell>960 ? 512</cell><cell>66.20</cell><cell>96.34</cell><cell>99.47</cell><cell>20.35 mm</cell><cell>98.37</cell><cell>98.46</cell><cell>0</cell><cell>66.93 ms</cell></row><row><cell>3</cell><cell>DLA-34</cell><cell>960 ? 512</cell><cell>49.09</cell><cell>92.44</cell><cell>97.62</cell><cell>24.93 mm</cell><cell>95.77</cell><cell>93.08</cell><cell>0</cell><cell>54.99 ms</cell></row><row><cell>5</cell><cell>DLA-34</cell><cell>800 ? 448</cell><cell>70.66</cell><cell>97.26</cell><cell>99.70</cell><cell>19.99 mm</cell><cell>98.61</cell><cell>98.99</cell><cell>0</cell><cell>65.20 ms</cell></row><row><cell>5</cell><cell>DLA-34</cell><cell>640 ? 384</cell><cell>55.96</cell><cell>96.78</cell><cell>99.65</cell><cell>21.67 mm</cell><cell>98.37</cell><cell>98.45</cell><cell>0</cell><cell>45.37 ms</cell></row><row><cell>5</cell><cell>MobileNet-V2</cell><cell>960 ? 512</cell><cell>42.42</cell><cell>94.09</cell><cell>99.33</cell><cell>24.38 mm</cell><cell>97.61</cell><cell>97.82</cell><cell>0</cell><cell>27.50 ms</cell></row><row><cell>5</cell><cell>Higher-HRNet-W32</cell><cell>960 ? 512</cell><cell>85.88</cell><cell>98.31</cell><cell>99.54</cell><cell>16.97 mm</cell><cell>98.51</cell><cell>98.73</cell><cell>0</cell><cell>128.95 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Ablation study of number of views, 2D backbone and input image size on the Panoptic dataset.</figDesc><table><row><cell>Re-ID Features</cell><cell>Occlusion Mask</cell><cell>3D Poses</cell><cell>MOTA</cell><cell>IDF1</cell><cell>ID Switch</cell><cell>Tracking Time</cell></row><row><cell>? ? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>98.42 98.44 98.45 98.45</cell><cell>93.82 94.38 98.67 98.67</cell><cell>90 15 0 0</cell><cell>0.92 ms 0.96 ms 2.10 ms 2.16 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Ablation study of Re-ID features, occlusion mask and 3D poses on the Panoptic dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Committee Chair of IEEE ICME in 2010 and 2011, and has served as the General Chair or TPC Chair for several IEEE conferences (e.g., ICME'2018, ICIP'2017). He was the recipient of several best paper awards. He is a Fellow of the IEEE.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context modeling in 3d human pose estimation: A unified perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6238" to="6247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="466" to="481" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detectand-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="350" to="359" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="480" to="496" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fairmot: On the fairness of detection and re-identification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01888</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Towards real-time multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12605</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A baseline for 3d multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.03961" />
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time multiple people tracking with deeply learned candidate selection and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haizhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zijie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="941" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards multi-person pose tracking: Bottom-up and topdown methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation for posetrack with enhanced part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04596</idno>
		<title level="m">Joint flow: Temporal flow fields for multi person tracking</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient online multiperson 2d pose tracking with recurrent spatio-temporal affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Raaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4620" to="4628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multiple people tracking using body and joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Explicit spatiotemporal joint relation learning for tracking human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iterative greedy matching for 3d human pose tracking from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="537" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cross-view tracking for multi-human 3d pose estimation at over 100 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03972</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05754</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ai challenger: A large-scale dataset for going deeper in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06475</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Humble teacher and eager student: Dual network learning for semi-supervised 2d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12498</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rethinking on multi-stage networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-view pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC. Citeseer</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2011" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6457" to="6465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pose Flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Lighttrack: A generic framework for online topdown human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02822</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A simple baseline for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01888</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Progressive search space reduction for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="3464" to="3468" />
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">High-speed tracking-bydetection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Features for multi-target multi-camera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6036" to="6046" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Non-markovian globally consistent multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maksai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE interna</title>
		<meeting>the IEEE interna</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2544" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Multi-camera multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07065</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking algorithm for multi-target multi-camera tracking with disjoint views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1175" to="1184" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multicamera people tracking with a probabilistic occupancy map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lengagne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="267" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Robust multiple cameras pedestrian detection with multi-view bayesian network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1760" to="1772" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep multi-camera people detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chavdarova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="848" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep occlusion reasoning for multicamera multi-target detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baqu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Wildtrack: A multi-camera hd dataset for dense unscripted pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chavdarova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baqu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maksai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lettry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5030" to="5039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multi-view 3d human tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">A bayesian filter for multi-view 3d multi-object tracking with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nordholm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04118</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Efficient decomposition of image and mesh graphs by lifted multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lavou?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1751" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1201" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Lighttrack: A generic framework for online top-down human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1034" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2403" to="2412" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5079" to="5088" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Multiple human pose estimation with temporally consistent 3d pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="742" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Multiple human 3d pose estimation from multiview images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ershadi-Nasab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="15" to="573" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
