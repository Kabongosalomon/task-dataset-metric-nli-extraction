<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Efficient Training Approach for Very Large Scale Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panpan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Shenzhen Technology University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Efficient Training Approach for Very Large Scale Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face recognition has achieved significant progress in deep learning era due to the ultra-large-scale and welllabeled datasets. However, training on the outsize datasets is time-consuming and takes up a lot of hardware resource. Therefore, designing an efficient training approach is indispensable. The heavy computational and memory costs mainly result from the million-level dimensionality of the fully connected (FC) layer. To this end, we propose a novel training approach, termed Faster Face Classification (F 2 C), to alleviate time and cost without sacrificing the performance. This method adopts Dynamic Class Pool (DCP) for storing and updating the identities' features dynamically, which could be regarded as a substitute for the FC layer. DCP is efficiently time-saving and cost-saving, as its smaller size with the independence from the whole face identities together. We further validate the proposed F 2 C method across several face benchmarks and private datasets, and display comparable results, meanwhile the speed is faster than state-of-the-art FC-based methods in terms of recognition accuracy and hardware costs. Moreover, our method is further improved by a well-designed dual data loader including indentity-based and instancebased loaders, which makes it more efficient for updating DCP parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Neural Networks (DNNs) has achieved many remarkable results in computer vision tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. Face recognition can be regarded as one of the most popular research topics in computer vision. Many large scale and well-labelled datasets have been released over the past decade <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref>. The training process * Equal contribution.</p><p>(kai.wang@comp.nus.edu.sg, wang-shuo514@sina.com) ? Corresponding author (youy@comp.nus.edu.sg). of face recognition aims to learn identity-related embedding space, where the intra-class distances are reduced and interclass distances are enlarged in the meanwhile. Previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> have proved that training on a large dataset can obtain a substantial improvement over a small dataset. To this end, academia and industry collected ultra-largescale datasets including 10 even 100 million face identities. Google collected 200 million face images consisting of 8 million identities <ref type="bibr" target="#b23">[24]</ref>. Tsinghua introduced WebFace260M <ref type="bibr" target="#b48">[49]</ref> including 260 million faces, which is the largest public face dataset and achieves state-of-the-art performance.</p><p>In general, these ultra-large-scale datasets boost the face recognition performance by a large margin. However, with the growth of face identities and limitations of hardware, there are mainly two problems in training phase. The first problem results from the training time and hardware resource occupancy. As shown in <ref type="figure" target="#fig_2">Fig 1,</ref> the time cost and GPU memory occupancy of the FC layer are much greater than those of the backbone when the face identities reach 10 million. To address these issues, many previous methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> focus on reducing the time and resource cost of the FC layer. Previous methods can be summarized into two categories. One <ref type="bibr" target="#b0">[1]</ref> tries to distribute the whole FC to different GPUs, introducing heavy communication costs. The other <ref type="bibr" target="#b45">[46]</ref> attempts to reduce the computing cost by selecting a certain ratio of neurons from the FC layer randomly, but it still needs to store the whole FC parameters. When the identities reach 10 or 100 million, storing the whole FC parameters is extremely expensive. How to effectively reduce the computational and memory costs caused by the high-dimensional FC layer? An intuitive idea is to decrease the size of FC or design an alternative paradigm, which is hardly explored before. The second problem is related to the update efficiency and speed of FC parameters. As pointed by <ref type="bibr" target="#b5">[6]</ref>, the optimal solution for the class center is actually the mean of all samples of this class. Identities that have rare samples with very low frequency of sampling will have very little opportunity to updat class centers through their samples, which may hamper feature representation.    <ref type="figure" target="#fig_2">Figure 1a</ref> shows the forward time comparison of backbone (ResNet50) and the FC layer. Given an image, the time cost of FC increases sharply with the growing number of face identities but the time of backbone stays unchanged. <ref type="figure" target="#fig_2">Figure 1b</ref> illustrates the GPU memory occupancy with the size of face identities. Even the V100 32G GPU can only store the FC parameters with the output size of about 6 millions (The dimension of face recognition is usually 512). Therefore, it is very necessary to design a method that reduces the training time and hardware cost of the FC layer.</p><p>To tackle aforementioned issues, we propose an efficient training approach for ultra-large-scale face datasets, termed as Faster Face Classification (F 2 C). In F 2 C, we first introduce twin backbones named Gallery Net (G-Net) and Probe Net (P-Net) to generate identity centers and extract face features, respectively. G-Net has the same structure with P-Net and inherits the parameters from P-Net in a moving average manner. Considering that the most time-consuming part of the ultra-large-scale training lies at the FC layer, we propose Dynamic Class Pool (DCP) to store the features from G-Net and calculate the logits with positive samples (whose identities appear in DCP) in each mini-batch. DCP can be regarded as a substitute for the FC layer and its size is much smaller than FC, which is the reason why F 2 C can largely reduce the time and resource cost compared to the FC layer. For negative samples (whose identities do not appear in the DCP), we minimize the cosine similarities between negative samples and DCP. To improve the update efficiency and speed of DCP parameters, we design a dual data loader including identity-based and instance-based loaders. The dual data loader loads images from given dataset by instances and identities to generate batches for training. Finally, we conduct sufficient experiments on several face benchmarks to prove F 2 C can achieve comparable results and a higher training speed than normal FC-based method. F 2 C also obtains superior performance than previous methods in term of recognition accuracy and hardware cost. Our contributions can be summarized as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>We propose an efficient training approach F 2 C for ultra-large-scale face recognition training, which aims to reduce the training time and hardware costs while keeping comparable performance to state-of-the-art FC-based methods.</p><p>2) We design DCP to store and update the identities' features dynamically, which is an alternative to the FC layer. The size of DCP is much smaller than FC and independent of the whole face identities, so the training time and hardware costs can be decreased substantially.</p><p>3) We design a dual data loader including identity-based and instance-based loaders to improve the update efficiency of DCP parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Face Recognition. Face recognition has witnessed dramatical progress due to the large scale datasets, advanced architectures and loss functions. Large scale datasets play the most crucial role in promoting the performance of face recognition <ref type="bibr" target="#b4">[5]</ref>. These datasets can be divided into three intervals according to the number of face identities: 1-10K, 11-100K, &gt;100K. VGGFace <ref type="bibr" target="#b20">[21]</ref>, VGGFace2 <ref type="bibr" target="#b2">[3]</ref>, UMD-Faces <ref type="bibr" target="#b1">[2]</ref>, CelebFaces <ref type="bibr" target="#b27">[28]</ref>, and CASIA-WebFace <ref type="bibr" target="#b44">[45]</ref> belong to the first interval. The face identities of the IMDB-Face <ref type="bibr" target="#b29">[30]</ref> and MS1MV2 <ref type="bibr" target="#b4">[5]</ref> are between 11K to 100K. Glint360k <ref type="bibr" target="#b0">[1]</ref> and Webface260M <ref type="bibr" target="#b48">[49]</ref> have about 0.36M and 4M identities. Many previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref> illustrate that training on larger face identities datasets can achieve better performance than on smaller ones. Therefore using WebFace260M as the training dataset obtains state-of-the-art performance on IJBC <ref type="bibr" target="#b18">[19]</ref> and top 3 in NIST-FRVT challenge. Based on these datasets, a variety of CNN architectures for improving the performances, such as VG-GNet <ref type="bibr" target="#b25">[26]</ref>, GoogleNet <ref type="bibr" target="#b28">[29]</ref>, ResNet <ref type="bibr" target="#b9">[10]</ref>, AttentionNet <ref type="bibr" target="#b31">[32]</ref> and MobileFaceNet <ref type="bibr" target="#b3">[4]</ref>, have been proposed. For the loss function, contrastive loss <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b43">44]</ref> and triplet loss <ref type="bibr" target="#b25">[26]</ref> might be good candidates. But they suffer from high computational cost and slow convergence. To this end, researchers attempt to explore new metric learning loss functions to boost the face recognition performance. Several marginbased softmax losses <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> have been exploited and obtained the state-of-the-art results. To sum up, current methods and large scale datasets have achieved excellent performance in face recognition, but the training time and hardware costs are still the bottleneck at training phase, especially for training on million scale or even more face identities datasets.</p><p>Acceleration for Large-Scale FC Layer. As illustrated in <ref type="figure" target="#fig_2">Figure 1a</ref>, the time cost mainly focuses on FC layer rather convolutional layer when the face identities reach 10M. Researchers try some attempts to accelerate the large scale FC training since 2001. An intuitive idea is to design an approximate function to reduce the computational cost, the Hierarchical Softmax (HSM) <ref type="bibr" target="#b6">[7]</ref> tries to reformulate the multiclass classifier into a hierarchy of binary classifiers. Therefore, the training cost can be reduced by means of the given sample only has to traverse along a path from the root to the corresponding class. However, all the class centers are stored in RAM and the retrieval time can not be ignored with the increase of face identities. Zhang et.al. <ref type="bibr" target="#b45">[46]</ref> proposed a method that can recognize a small number of "active classes" in each mini batch, which constructs the dynamic class hierarchies on the fly. However, recognizing the "active classes" is also time-consuming when the face identities is too large. Some companies, such as Google and Microsoft, try to divide all the categories into multi-GPUs averagely. The communication cost of inter-servers can not be ignored. To tackle this problem, Partial FC <ref type="bibr" target="#b0">[1]</ref> tries to train a large-scale dataset on a single GPU server using 10% identities randomly at each iteration. However it's still limited by the memory of the GPUs in a single machine. As shown in <ref type="figure" target="#fig_2">Figure 1b</ref>, Partial FC can only work when the number of face identities is not ultra-large (&lt;10M), otherwise the GPUs will still run out of memory. There are several pairwise based methods <ref type="bibr" target="#b11">[12]</ref> that utilize the face pairs to train large scale datasets, while the time complexity is O(N k ), where k represents the size of the pair. The latest related work VFC <ref type="bibr" target="#b15">[16]</ref> builds some virtual FC parameters to reduce the computation cost but its performance is much lower compared to normal FC. Different from previous works, our F 2 C can reduce the FC training cost largely and achieve comparable performance compared to normal FC-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Faster Face Classification</head><p>In this section, we first give an overview of F 2 C for a brief understanding of our method. Then we present our motivation and key modules for ultra-large-scale datasets training. After that, we show the theoretical/empirical analysis over these modules. Finally we demonstrate the training details for better reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of F 2 C</head><p>The problem we tackle is to accelerate the training speed and reduce the hardware costs of ultra-large-scale face datasets (face identities &gt; 10M) without obvious degradation of performance. To this end, we propose F 2 C framework for ultra-large-scale face datasets training. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, given ultra-large-scale face datasets, we utilize instance-based loader to generate an instance batch as data loader usually does. Meanwhile, identity-based loader selects two images randomly from the same identity to form the paired identities batch. Subsequently, we mix up the images from instance and pair identity batches as shown in <ref type="figure" target="#fig_3">Figure 2</ref> and feed them into G-Net and P-Net. Inspired by MoCo <ref type="bibr" target="#b8">[9]</ref>, G-Net has the same structure as P-Net and inherits parameters from P-Net in a moving average manner. G-Net and P-Net are used to generate identities' centers and extract face features for face recognition, respectively. Then, we introduce DCP as a substitute for the FC layer. DCP is randomly initialized and updated by the features from G-Net at each iteration. The update strategy of DCP follows the rule: using the current features to replace the most outdated part of features in DCP. For positive samples, we use the common cross entropy loss. For negative samples, we minimize the cosine similarities between negative samples and DCP. The whole F 2 C is optimized by cross entropy loss and cosine similarities simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motivation</head><p>Before digging into F 2 C, we provide some motivations by rethinking the loss function cooperated with FC layer. For convenience, we consider the Softmax as follows:</p><formula xml:id="formula_0">L = ? 1 N N i=1 log e W T y i xi nID j=1 e W T j xi<label>(1)</label></formula><p>where N is batchsize and n ID stands for the number of whole face identities. For each iteration of the training process, the update of the classifier {W j } nID j=1 is performed as the following equations: Obviously, all the classifiers {W j } nID j=1 will be updated in each iteration, which means each classifier has the same chance to be optimized. The goal of face recognition is to distinct persons from different identities with the mechanism where the features from the same identity are pulled together and features belonging to different identities are pushed away. As the main problem of training with ultralarge-scale dataset is the explosive size of FC layer, We can consider the whole FC as a set of classifiers. In order to reduce the computation cost, it is intuitive for us to optimize fixed ratio of the classifiers in each iteration during the training process. Specifically, we utilize a vector as follows to represent whether a given classifier is in optimization queue.</p><formula xml:id="formula_1">?L ?W k = ? 1 N N i=1 (? kyi ? e W T k xi nID j=1 e W T j xi )x i<label>(2)</label></formula><formula xml:id="formula_2">V = {? 1 , ..., ? nID }, ?i, ? i ? {0, 1} and #{? i |? i = 0} = C (3)</formula><p>where C is a constant stands for the length of the optimization queue, ? i = 0/1 denotes the classifier W i is (not in)/(in) optimization queue. We draw the corresponding objective for this setting.</p><formula xml:id="formula_3">L = ? 1 N N i=1 log e W T y i xi nID j=1 ? j e W T j xi<label>(4)</label></formula><p>The classifiers update on basis of the following equations:</p><formula xml:id="formula_4">?L ?W k = ? 1 N N i=1 (? kyi ? ? k e W T k xi nID j=1 ? j e W T j xi )x i<label>(5)</label></formula><p>Formally, equation 5 is similar to equation 2, the selection mechanism for vector V will influence the update process of the classifier directly. We should design feasible selection mechanism for better optimization of classifiers under the constraint that only partial classifiers will be updated at each iteration. However, this straightforward method still suffer the heavy pressure from storing the whole set of classifiers. As a matter of fact, in our novel framework, we only offer limited space to store a fixed ratio of classifier/features dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Identity-Based and Instance-Based Loaders</head><p>In this subsection, we introduce the details of our dual data loader. For convenience, we denote the batchsize as M. Practically, we utilize the instance-based loader to sample M images from a given face dataset randomly to get the instance batch. In the meanwhile, the identity-based loader is applied to provide identity batch by selecting M identities randomly without replacement from the whole identities and sample two images for each identity. We divide the instance batch into two parts, with M/2 images for each part. For paired identity batch, we split it by face identity to form two parts with same set of face identities. We mix up the four parts to get I ? III; II ? IV (as illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>), where ? represents the union operation for sets.</p><p>Why Dual data loaders? As aforementioned, we design dual data loader to improve the update efficiency of DCP parameters. To better understand our design, we analyze the different influences between identity-based and instancebased loaders as follows. Let M denote batch size, n ID be the total number of identities of the given the dataset, k min (k max ) as the minimum (maximum) number of images for one person in the dataset,k be the average number of im-ages per identity. Here the shape of DCP mentioned in the main paper is C ? K ? D. C is the magnitude of DCP, K is the capacity for each placeholder in DCP, D represents feature dimension. The total images of given dataset can be denoted askn ID . We evaluate the update speed by estimating the minimum of epochs for given face identity to updat? knID M .</p><p>? If we only use instance-based loader, the update speed of identities' centers ? [k n ID M kmax ,k n ID M kmin ]. So only using instance-based loader may lead to following problems. 1. If the number of identities is severely imbalanced, the update speed of the identities' centers that have rare number of images is too slow. 2. If we sample M images that belong to M different identities, the DCP may have no positive samples for this iteration. In this case, cross entropy, which is crucial for classification, cannot be calculated.</p><p>? If we only use identity-based loader, we can obtain the average fastest update speed ( n ID M ) of each identity. However, identity-based loader re-sample identities that have rare number of images too many times, so it needs to use about kmax kmin times more iterations than instance-based loader to sample all images from the dataset. Further, the sample probabilities for each instance of identities with rich intra-class images are too low, the identity-based loader can not sample plenty of intra-class images during the training phase.</p><p>? Using the dual data loaer can inherit the benefits from instance-based and identity-based loaders. First, dual data loader provides appropriate ratios between positive and negative images, which is very important for DCP. Second, dual data loader keeps high update efficiency (speed) of identities' centers and various intraclass images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head><p>We take I?III and II?IV as input to Probe and Gallery Nets respectively to extract the face features and generate the identities' centers. The process can be formulated as follows:</p><formula xml:id="formula_5">P ? (I ? III) = F DCP p ? F ?DCP p G ? (II ? IV ) = F g<label>(6)</label></formula><p>where the probe and gallery net are abbreviated as P ? and G ? with parameters denoted as ?, ? respectively. The symbol ? is set to split features whose identities belong to DCP (subsection 3.4) from those not belong to DCP. F g represent the features extracted by the Gallery Net. For each batch, we denote number of identities in DCP as I and number of identities not in DCP as M ? I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dynamic Class Pool</head><p>In this subsection, we introduce the details of the Dynamic Class Pool (DCP). Inspired by sliding window <ref type="bibr" target="#b13">[14]</ref> in object detection task, we can utilize a dynamic identity group that slides the whole face identities by iterations.</p><p>We called this sliding identity group as DCP, which can be regarded as a substitute for the FC layer. Firstly, we define a tensor T with size of C ? K ? D which is initialized with Gaussian distribution, where C is the capacity or the number of face identities the DCP can hold, K represents the number of features that belong to the same identity (we set the default as K = 2). We store F g in DCP and update the most outdated features of DCP using the F g in each iteration. The updating rule is similar to least recently used (LRU) 1 policy which can be formulated as, , we can calculate its logits by the following equation,</p><formula xml:id="formula_6">P = 1 K K i=1 F DCP p , T [:, i, :] ? R I?C<label>(8)</label></formula><p>where ?, ? denotes the inner product operation, P represents the logits of F DCP p . Therefore, we can formulate the Cross-Entropy loss as follows:</p><formula xml:id="formula_7">L ce = ? 1 I I i=1 log e W T y i Pi C j=1 e W T j Pi ,<label>(9)</label></formula><p>where W j is the j-th classifier, y i is the identity of P i . For features F ?DCP p whose IDs are not in DCP, we add a constraint to minimize the cosine similarity between F ?DCP p and T , which can be formulated as,</p><formula xml:id="formula_8">L cos = 1 M ? I M ?I i=1 ?(F ?DCP p ,T ),<label>(10)</label></formula><p>where ? is the operation of calculating the cosine similarity, T represents the average operation along the axis of K in DCP. The total loss is L total = L ce + L cos .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Empirical Analysis</head><p>DCP As shown in equation 4 and 9, the cross entropy loss we utilize for DCP is similar to the loss for FC formally. With special setting of vector V in equation 3, we can represent L ce in the form of equation 4. For further verification of the effect of this mechanism on the training with DCP, we provide some empirical analysis. As mentioned in subsection 3.3 and equation 7, the identities in DCP are updated in an LRU mechanism as shown in Algorithm 1. As identity-based loader goes through the dataset in terms of identities, partial components( M 2 ) of vector V can be determined by shuffling the whole face identities and taking the corresponding t-th part of it, where 1 ? t ? n ID M . When we use identity-based loader, then by the setting of V and property of LRU rules, each classifier/pseudo feature center can be updated at least [ C M ] times. This means that every classifier can have the similar chance to be optimized in our settings. DCP may have the following benefits: 1) The size of DCP is independent from magnitude of face identities, which can be far smaller than FC. Therefore the computational cost is greatly reduced; 2) The hardware especially storage occupancy of DCP is also smaller than FC and the communication cost can be reduced dramatically. These benefits are the reasons why we call our method as Faster Face Classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Experimental Details</head><p>We train our F 2 C on a single server with 8 Tesla V100 32G GPUs. We utilize ResNet100, ResNet50 and Mobile-FaceNet as our backbones to evaluate the efficiency of F 2 C. The learning rate is initialized as 0.1 with SGD optimizer and divided by 10 at 10, 14, 17 epochs. The training is terminated at 20 epochs. The length (number of ID) of DCP is defaulted as 10% of total face identities. The batch size is 512 i.e., 256 images from identity-based loader and 256 images from instance-based loader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first review several benchmark datasets in face recognition area briefly. Then, we conduct ablation studies to evaluate the effectiveness of each module and the settings of hyper-parameters in F 2 C. Finally, we compare F 2 C to related state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets.</head><p>We utilize MobileFaceNet, ResNet50 and ResNet100 to train F 2 C on MS1MV2, Glint360k and Webface42M( Webface42M is the cleaned version of the original Web-face260M and it has 2M ID and about 42M images), respectively. We mainly show the performances of F 2 C in following 9 academic datasets: LFW <ref type="bibr" target="#b10">[11]</ref>, SLFW <ref type="bibr" target="#b10">[11]</ref>, CFP <ref type="bibr" target="#b24">[25]</ref>, CALFW <ref type="bibr" target="#b47">[48]</ref>, CPLFW <ref type="bibr" target="#b46">[47]</ref>, AGEDB <ref type="bibr" target="#b19">[20]</ref>, YTF <ref type="bibr" target="#b41">[42]</ref>, IJBC <ref type="bibr" target="#b18">[19]</ref>, and MegaFace <ref type="bibr" target="#b12">[13]</ref>. LFW is collected from the Internet which contains 13,233 images with 5,749 IDs. SLFW is similar to the LFW but the scale of SLFW is smaller than LFW. CFP collects celebrities' images including frontal and profile views. CALFW is a cross-age version of LFW. CPLFW is similar to CALFW, but CPLFW contains more pose variant images. AGEDB contains images annotated with accurate to the year, noise-free labels. YTF includes 3425 videos from YouTube with 1595 IDs. IJBC is updated from IJBB and includes 21294 images of 3531 objects. MegaFace aims at evaluating the face recognition performance at the million scale of distractors, which includes a large gallery set and a probe set. In this work, we use the Facescrub as the probe set of MegaFace as gallery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparisons between FC and F 2 C</head><p>We choose 3 different backbones and evaluate the performance on 9 academic benchmarks between FC and F 2 C using MS1MV2, Glint360k and Webface42M as training datasets. As shown in <ref type="table" target="#tab_0">Table 1</ref>, F 2 C can achieve comparable performance compared to FC. We also provide the average performance among these datasets and demonstrate it in the last column where F 2 C is only lower than FC within 1%. Note that, the size of DCP is only 10% of the total face identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We conduct ablation studies of hyper parameters and settings of F 2 C. Here we demonstrate the experiments on MS1MV2 using MobileFaceNet and ResNet50.</p><p>Single Loader or Dual Loaders? As mentioned in methodology section, dual loaders can improve the update efficiency of DCP. To evaluate the influence of loaders in F 2 C, we use different combinations of identity-based and instance-based loaders and show the results in <ref type="table" target="#tab_1">Table 2</ref>. The Small Datasets represent LFW, SLFW, CFP, CALFW, MoCo treates the two augmented images of the same image as positive samples and achieved impressive performance in unsupervised learning. Therefore, pictures with the same ID can naturally be regarded as positive samples, thus it is intuitive to use twin backbones in the same way as MoCo to generate the identities' centers and extract the face features respectively. However we intend to reduce the training cost further, so we compare the performance of single net to dual nets in <ref type="table" target="#tab_2">Table 3</ref>. The dual nets performs better than single net on all the datasets, which illustrates only using single net may fall into the trivial solution as explained in Semi-Siamese Training <ref type="bibr" target="#b5">[6]</ref>.</p><p>Exploring the Influence of K in DCP. K represents the number of the features that belong to the same identity. We evaluate the K = 1 and K = 2 in <ref type="table" target="#tab_3">Table 4</ref>. As the features in DCP represent the category centers, an intuitive sense is that a larger K can provide more reliable center estimation. The experiments results also support our intuition. However we must make a trade-off between performance and storage. A larger K means better performance at the cost of GPU memory and communication among severs. Therefore, we set K = 2 in DCP by default.   Ratios within dual data loader. We set the ratio of the size between instance-based and identity-based loaders as 1:1 by default. To further explore the influence of the ratios within dual data loader, we show the experiments in Table 5. We utilize ResNet50 as backbone to train MS1MV2 dataset. We find that the default ratio within dual data loader achieves the highest results on most datasets, especially on  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with SOTA Methods.</head><p>We compare our F 2 C to other 6 state-of-the-art methods and show the results in <ref type="table" target="#tab_5">Table 6</ref>. We can observe that F 2 C outperforms lower-boundary, N-pair, Multi-similarity, and TCP by a large margin, especially on IJBC and MegaFace datasets. As claimed in VFC <ref type="bibr" target="#b15">[16]</ref>, Upper-boundary represents training with normal FC using the 100% face identities. F 2 C has a little degradation of performance than upper-boundary. It can also achieve comparable results with Partial-FC, but Partial-FC requires hardware space to store the total identities' centers while VFC doesn't. However the performance of VFC drops obviously compared to F 2 C.</p><p>Visualizations of Resource Cost and Training Efficiency. The GPU memory occupancy and throughput are two crucial factors to evaluate the practicability of a method in distributed parallel training. To better understand the efficiency of F 2 C, <ref type="figure" target="#fig_6">Figure 3</ref> visualizes the GPU memory occupancy and throughput of F 2 C and other training methods. The results are obtained on a 8 V100 32G GPUs. GPU memory occupancy is illustrated in <ref type="figure" target="#fig_6">Figure 3a</ref>, Data Parallel and Model Parallel are out-of-memory (OOM) when the identities reach to 16 millions. The memory of Partial-FC increases with growth of the identities and it also OOM when the identities reach to 32 millions. Besides, we show the throughput comparisons in <ref type="figure" target="#fig_6">Figure 3b</ref>, only F 2 C can keep the high-level throughput among different number of identities. Therefore, the proposed F 2 C is practical in ultralarge-scale face recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an efficient training approach F 2 C for ultra-large-scale face recognition training, the main innovation is Dynamic Class Pool (DCP) for store and update of face identities' feature as an substitute of FC and dual loaders for helping DCP update efficiently. The results of comprehensive experiments and analysis show that our approach can reduce hardware cost and time for training as well as obtaining comparable performance to state-of-theart FC-based methods.</p><p>Broader impacts. The proposed method is validated on face training datasets due to the wide variety, the scheme could be expanded to other datasets and situations . However, it does not contain any studies involving affecting ethics or human rights performed by any of the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledge</head><p>This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-08-008). This work is supported by Alibaba Group through Alibaba Research Intern Program. We thank Google TFRC for supporting us to get access to the Cloud TPUs. We thank CSCS (Swiss National Supercomputing Centre) for supporting us to get access to the Piz Daint supercomputer. We thank TACC (Texas Advanced Computing Center) for supporting us to get access to the Longhorn supercomputer and the Frontera supercomputer. We thank LuxProvide (Luxembourg national supercomputer HPC organization) for supporting us to get access to the MeluXina supercomputer. This work is also supported by the Chinese National Natural Science Foundation Projects 62106264 and National Natural Science Foundation of China (62176165). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Comparison of backbone and FC time cost (ms).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The memory occupancy of the FC layer at training phase (G).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of training time and GPU memory occupancy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The pipeline of F 2 C. We use instance and id data loader to generate mixed batches (I ? III, II ? IV), which are later fed into G-Net and P-Net respectively. The features from G-Net will update DCP in the manner of LRU, and features from P-Net will be used to compute loss together with DCP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>T [ 1 :</head><label>1</label><figDesc>C ? M, :, :] = T [M + 1 : C, :, :] ? R (C?M )?K?D T [C ? M + 1 : C, 0, :] = F g ? R M ?K?D (7) For the current batch, with the update of the DCP, we obtain pseudo feature center for each identity in DCP, including the identities contained in II ? IV . As claimed in equation 6, features from P-Net can be divided into two types compared to DCP. One is F DCP p , the other is F ?DCP p . For F DCP p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 :; 3 if 1 ? t ? C M : 4 store</head><label>1314</label><figDesc>Update Mechanism of DCP Input: DCP: T ? R C?K?D initialized with Gaussian distribution. Index for the identity batch: t. Batch Size: M . 1 for 1 ? t ? n ID M do 2utilize the G-Net to extract features from t-th batch as the pseudo feature centers denoted as F g F g sequentially in those unoccupied position in DCP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Visualizations of the hardware resource occupancy of different training methods. challenging IJBC and MegaFace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results (%) on 9 face recognition benchmarks. All models are trained from scratch on MS1MV2, Glint360k and Webface42M. The TPR@FAR=1e-4 metric is used for IJBC. MegaFace is TPR@FAR=1e-6Method   LFW SLFW CFP CALFW CPLFW AGEDB YTF IJBC MegaFace Avg.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Training on MS1MV2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FC-Mobile 99.04 98.80 96.94</cell><cell>94.37</cell><cell>88.37</cell><cell>96.73</cell><cell>97.04 92.29</cell><cell>90.69</cell><cell>94.92</cell></row><row><cell cols="2">F 2 C-Mobile 98.93 98.57 97.16</cell><cell>94.53</cell><cell>87.80</cell><cell>96.47</cell><cell>97.24 91.06</cell><cell>89.30</cell><cell>94.56</cell></row><row><cell>FC-R50</cell><cell>99.78 99.55 98.80</cell><cell>95.76</cell><cell>92.01</cell><cell>98.13</cell><cell>98.03 95.74</cell><cell>97.82</cell><cell>97.29</cell></row><row><cell>F 2 C-R50</cell><cell>99.50 99.45 98.46</cell><cell>95.58</cell><cell>90.58</cell><cell>97.83</cell><cell>98.16 94.91</cell><cell>96.74</cell><cell>96.80</cell></row><row><cell></cell><cell></cell><cell cols="3">Training on Glint360k</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FC-R50</cell><cell>99.83 99.71 99.07</cell><cell>95.71</cell><cell>93.48</cell><cell>98.25</cell><cell>97.92 96.48</cell><cell>98.64</cell><cell>97.67</cell></row><row><cell>F 2 C-R50</cell><cell>99.71 99.53 98.30</cell><cell>95.23</cell><cell>91.60</cell><cell>97.88</cell><cell>97.76 94.75</cell><cell>96.73</cell><cell>96.83</cell></row><row><cell></cell><cell></cell><cell cols="3">Training on Webface42M</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FC-R100</cell><cell>99.83 99.81 99.38</cell><cell>96.11</cell><cell>94.90</cell><cell>98.58</cell><cell>98.51 97.68</cell><cell>98.57</cell><cell>98.15</cell></row><row><cell>F 2 C-R100</cell><cell>99.83 98.80 99.33</cell><cell>95.92</cell><cell>94.85</cell><cell>98.33</cell><cell>98.23 97.31</cell><cell>98.53</cell><cell>97.90</cell></row><row><cell cols="3">CPLFW, AGEDB and YTF in this subsection. We show</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">the average accuracy on Small Datasets. Unless specified,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">TPR@FAR=1e-4 metric is used for IJBC and Megafce is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">FPR@FAR=1e-6 by default. Training with instance-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">loader or identity-based loader can obtain comparable re-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">sults on small datasets. Instance-based loader outperforms</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">identity-based loader on IJBC and MegaFace by a large</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">margin. It could be explained that only using identity loader</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">can not ensure all the images are sampled. Using dual data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">loaders can improve the performance compared with each</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">single loader obviously, which is consistent to our analysis.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Note that, to make fair comparison, the results are obtained</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">with the same number of samples fed to the model, not with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">the same number of epoch.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Single Net or Dual Nets?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of single or dual data loaders.ID.L, Ins.L and Dua.L represent id loader, instance loader and dual loaders respectively.</figDesc><table><row><cell cols="5">Backbone Method Small Datasets IJBC MegaFace</cell></row><row><cell></cell><cell>ID.L</cell><cell>94.20</cell><cell>82.30</cell><cell>79.19</cell></row><row><cell>Mobile</cell><cell>Ins.L</cell><cell>94.24</cell><cell>89.30</cell><cell>86.40</cell></row><row><cell></cell><cell>Dua.L</cell><cell>95.29</cell><cell>91.06</cell><cell>89.30</cell></row><row><cell></cell><cell>ID.L</cell><cell>96.70</cell><cell>91.75</cell><cell>93.65</cell></row><row><cell cols="2">ResNet50 Ins.L</cell><cell>96.08</cell><cell>92.06</cell><cell>92.74</cell></row><row><cell></cell><cell>Dua.L</cell><cell>97.07</cell><cell>94.91</cell><cell>96.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of single net or dual nets.</figDesc><table><row><cell cols="5">Backbone Method Small Datasets IJBC MegaFace</cell></row><row><cell></cell><cell>Single</cell><cell>93.90</cell><cell>88.07</cell><cell>82.69</cell></row><row><cell>Mobile</cell><cell>Dual</cell><cell>95.29</cell><cell>91.06</cell><cell>89.30</cell></row><row><cell></cell><cell>Single</cell><cell>95.55</cell><cell>92.26</cell><cell>92.98</cell></row><row><cell>ResNet50</cell><cell>Dual</cell><cell>97.07</cell><cell>94.91</cell><cell>96.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of the number of K.</figDesc><table><row><cell cols="5">Backbone K Small Datasets IJBC MegaFace</cell></row><row><cell></cell><cell>1</cell><cell>95.19</cell><cell>90.75</cell><cell>88.31</cell></row><row><cell>Mobile</cell><cell>2</cell><cell>95.29</cell><cell>91.06</cell><cell>89.30</cell></row><row><cell></cell><cell>1</cell><cell>96.58</cell><cell>94.38</cell><cell>96.49</cell></row><row><cell>ResNet50</cell><cell>2</cell><cell>97.07</cell><cell>94.91</cell><cell>96.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Evaluation the ratios within dual data loader. ResNet50 is used here.</figDesc><table><row><cell></cell><cell cols="11">Ins.L ID.L Small Datasets IJBC MegaFace</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="2">96.77</cell><cell cols="2">91.75</cell><cell cols="2">93.65</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell cols="2">96.23</cell><cell cols="2">92.06</cell><cell cols="2">92.74</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="2">97.08</cell><cell cols="2">94.91</cell><cell cols="2">96.74</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="2">96.29</cell><cell cols="2">94.21</cell><cell cols="2">96.43</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell cols="2">95.40</cell><cell cols="2">90.80</cell><cell cols="2">90.56</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Out of Memory</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 16 32 64 Face Identities (Million)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32 4 8 16 Face Identities (Million)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FFC</cell></row><row><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>0</cell><cell>1000</cell><cell>2000</cell><cell>3000</cell><cell>4000</cell><cell>5000</cell></row><row><cell></cell><cell></cell><cell cols="3">Memory on Per GPU (GB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Throughput (Images/Sec.)</cell><cell></cell><cell></cell></row><row><cell cols="8">Model Parallel+FP16 Model Parallel Data Parallel PartialFC+FP16 FFC+FP16</cell><cell cols="6">FFC+FP16 PartialFC+FP16 Model Parallel+FP16 Model Parallel Data Parallel</cell></row><row><cell cols="8">(a) Comparison of GPU mem-</cell><cell cols="6">(b) Comparisons of Throughput</cell></row><row><cell cols="5">ory Occupancy (GB).</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(Images/Sec.).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparisons to state-of-the-art methods. To make fair comparison, Partial-FC, VFC, DCQ and F 2 C only use 1% of identities of MS1M for training. Megaface refers to rank-1 identification. IJBC is TPR@FAR=1e-4. The lower-boundary results are excerpted from VFC paper. The upper-boundary results are reproduced by us.</figDesc><table><row><cell>Method</cell><cell cols="3">CALFW CPLFW SLFW YTF</cell><cell>CFP</cell><cell cols="2">IJBC MegaFace</cell></row><row><cell>lower-boundary</cell><cell>87.43</cell><cell>75.45</cell><cell cols="3">93.52 93.78 91.66 65.19</cell><cell>79.28</cell></row><row><cell>upper-boundary</cell><cell>95.75</cell><cell>90.85</cell><cell cols="3">99.55 97.76 98.39 95.48</cell><cell>97.56</cell></row><row><cell>N-pair[27]</cell><cell>87.32</cell><cell>72.80</cell><cell>92.28 92.62</cell><cell>-</cell><cell>61.75</cell><cell>82.56</cell></row><row><cell>Multi-similarity[37]</cell><cell>85.40</cell><cell>73.60</cell><cell>91.03 92.76</cell><cell>-</cell><cell>57.82</cell><cell>76.88</cell></row><row><cell>TCP[18]</cell><cell>88.05</cell><cell>76.00</cell><cell cols="3">93.23 93.92 93.27 43.58</cell><cell>88.18</cell></row><row><cell>Partial-FC[1]</cell><cell>95.40</cell><cell>90.33</cell><cell cols="3">99.28 97.76 98.13 94.40</cell><cell>94.13</cell></row><row><cell>VFC[16]</cell><cell>91.93</cell><cell>79.00</cell><cell cols="3">96.23 95.08 95.77 70.12</cell><cell>93.18</cell></row><row><cell>DCQ[15]</cell><cell>95.38</cell><cell>88.92</cell><cell cols="3">99.23 97.71 98.16 92.96</cell><cell>95.21</cell></row><row><cell>F 2 C</cell><cell>95.25</cell><cell>89.38</cell><cell cols="3">99.23 97.76 98.25 92.31</cell><cell>94.25</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.interviewcake.com/concept/java/lru-cache</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05222</idno>
		<title level="m">Bin Qin, Debing Zhang, and Ying Fu. Partial fc: Training 10 million identities on a single machine</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Umdfaces: An annotated face dataset for training deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Nanduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international joint conference on biometrics (IJCB)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="428" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-siamese training for shallow face learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="561" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on faces in&apos;Real-Life&apos;Images: detection, alignment, and recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pairwise relational networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bong-Nam</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daijin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="628" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond sliding windows: Object localization by efficient subwindow search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dynamic class queue for large scale face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Virtual fully-connected layer: Training a large-scale face recognition dataset with limited computational resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei Zhang</forename><surname>Biaowang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transductive centroid projection for semisupervised large-scale recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-c: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Biometrics (ICB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Agedb: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Suppressing mislabeled data via grouping and self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="786" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03278</idno>
		<title level="m">Zheng Zhu, and Yang You. Crafting better contrastive views for siamese representation learning</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The devil of face recognition is in the noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liren</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="765" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Suppressing uncertainties for large-scale facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6897" to="6906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4057" to="4069" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mask aware network for masked face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1456" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<title level="m">Learning to condense dataset by aligning features. CVPR2022</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5022" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Loss function search for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno>PMLR, 2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10029" to="10038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Co-mining: Deep face recognition with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9358" to="9367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Support vector guided softmax loss for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11317</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ensemble soft-margin softmax loss for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03922</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large scale similarity learning using similar pairs for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Accelerated training for massive classification via dynamic class selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cross-age lfw: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08197</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04098</idno>
		<title level="m">A benchmark unveiling the power of million-scale deep face recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
