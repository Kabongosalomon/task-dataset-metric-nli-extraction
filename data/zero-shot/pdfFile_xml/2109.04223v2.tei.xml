<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accepted at the ICLR 2022 Workshop on Deep Learning on Graphs for Natural Language Processing KELM: KNOWLEDGE ENHANCED PRE-TRAINED LAN- GUAGE REPRESENTATIONS WITH MESSAGE PASSING ON HIERARCHICAL RELATIONAL GRAPHS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinquan</forename><surname>Lu</surname></persName>
							<email>luyinquan@pjlab.org.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">OPPO Guangdong Mobile Telecommunications Co</orgName>
								<address>
									<addrLine>Ltd. 2 ByteDance 3 Huawei Noah&apos;s Ark Lab 4 Shanghai</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Lu</surname></persName>
							<email>luhaonan@oppo.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guirong</forename><surname>Fu</surname></persName>
							<email>fuguirong@bytedance.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<email>qun.liu@huawei.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Co., Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Accepted at the ICLR 2022 Workshop on Deep Learning on Graphs for Natural Language Processing KELM: KNOWLEDGE ENHANCED PRE-TRAINED LAN- GUAGE REPRESENTATIONS WITH MESSAGE PASSING ON HIERARCHICAL RELATIONAL GRAPHS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Incorporating factual knowledge into pre-trained language models (PLM) such as BERT is an emerging trend in recent NLP studies. However, most of the existing methods combine the external knowledge integration module with a modified pre-training loss and re-implement the pre-training process on the large-scale corpus. Re-pretraining these models is usually resource-consuming, and difficult to adapt to another domain with a different knowledge graph (KG). Besides, those works either cannot embed knowledge context dynamically according to textual context or struggle with the knowledge ambiguity issue. In this paper, we propose a novel knowledge-aware language model framework based on fine-tuning process, which equips PLM with a unified knowledge-enhanced text graph that contains both text and multi-relational sub-graphs extracted from KG. We design a hierarchical relational-graph-based message passing mechanism, which allows the representations of injected KG and text to mutually update each other and can dynamically select ambiguous mentioned entities that share the same text 1 . Our empirical results show that our model can efficiently incorporate world knowledge from KGs into existing language models such as BERT, and achieve significant improvement on the machine reading comprehension (MRC) tasks compared with other knowledge-enhanced models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>However, as summarized in <ref type="table" target="#tab_7">Table 4</ref> of Appendix, most of the existing knowledge-enhanced PLMs need to re-pretrain the models based on an additional large-scale corpus, they mainly encounter two problems below: (1) Incorporating external knowledge during pretraining is usually resourceconsuming and difficult to adapt to other domains with different KGs. By checking the third column of <ref type="table" target="#tab_7">Table 4</ref> in Appendix, one can see that most of the pretrain-based models use Wiki-related KG as their injected knowledge source. These models also use English Wikipedia as pre-training corpus. They either use an additional entity linking tool (e.g. TAGME <ref type="bibr" target="#b3">(Ferragina &amp; Scaiella, 2010)</ref>) to align the entity mention in the text to a single mentioned entity in a Wiki-related KG uniquely or directly treat hyperlinks in Wikipedia as entity annotations. These models depend heavily on the one-to-one mapping relationship between Wikipedia corpus and Wiki-related KG, thus they never consider handling knowledge ambiguity issue. (2) These models with explicit knowledge injection usually use algorithms like BILINEAR <ref type="bibr">(Yang et al., 2015)</ref> to obtain pre-trained KG embeddings, which contain information about graph structure. Unfortunately, their knowledge context is usually static and cannot be embedded dynamically according to textual context. Several works <ref type="bibr" target="#b14">(Qiu et al., 2019;</ref><ref type="bibr" target="#b28">Yang et al., 2019)</ref> concentrate on injecting external knowledge based on fine-tuning PLM on downstream tasks, which is much easier to change the injected KGs and adapt to relevant domain tasks. They either cannot consider multi-hop relational information, or struggle with knowledge ambiguity issue. How to fuse heterogeneous information dynamically based on the fine-tuning process on the downstream tasks and use the information of injected KGs more efficiently remains a challenge. <ref type="figure">Figure 1</ref>: Unified Knowledge-enhanced Text Graph (UKET) consists of three parts corresponding to our model: (1) KG only part, (2) Entity link to token graph, (3) Text only graph.</p><p>To overcome the challenges mentioned above, we propose a novel framework named KELM, which injects world knowledge from KGs during the fine-tuning phase by building a Unified Knowledge-enhanced Text Graph (UKET) that contains both injected sub-graphs from external knowledge and text. The method extends the input sentence by extracting subgraphs centered on every mentioned entity from KGs. In this way, we can get a Unified Knowledge-enhanced Text Graph as shown in <ref type="figure">Fig. 1</ref>, which is made of three kinds of graph: (1) The injected knowledge graphs, referred to as the "KG only" part; (2) The graph about entity mentions in the text and mentioned entities in KGs, referred to as the "entity link to token" part. Entity mentions in the text are linked with mentioned entities in KGs by string matching, so one entity mention may trigger several mentioned entities that share the same text in the injected KGs (e.g. "Ford" in <ref type="figure">Fig. 1);</ref> (3) The "text only" part, where the input text sequence is treated as a fully-connected word graph just like classical Transformer architecture <ref type="bibr" target="#b21">(Vaswani et al., 2017)</ref>.</p><p>Based on this unified graph, we design a novel Hierarchical relational-graph-based Message Passing (HMP) mechanism to fuse heterogeneous information on the output layer of PLM. The implementation of HMP is via a Hierarchical Knowledge Enhancement Module as depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>, which also consists of three parts, and each part is designed for solving the different problems above: (1) For reserving the structure information and dynamically embedding injected knowledge, we utilize a relational GNN (e.g. rGCN <ref type="bibr" target="#b18">(Schlichtkrull et al., 2017)</ref>) to aggregate and update representations of extracted sub-graphs for each injected KG (corresponding to the "KG only" part of UKET). All mentioned entities and their K-hop neighbors in sub-graphs are initialized by pre-trained vectors obtained from the classical knowledge graph embedding (KGE) method (we adopt BILINEAR here). In this way, knowledge context can be dynamically embedded, the structural information about the graph is also kept; (2) For handling knowledge ambiguity issue and selecting relevant mentioned entities according to the input context, we leverage a specially designed attention mechanism to weight these ambiguous mentioned entities by using the textual representations of words/tokens to query the representations of their related mentioned entities in KGs (corresponding to the "entity link to token" graph of UKET). The attention score can help to select knowledge according to the input sentence dynamically. By concatenating the outputs of this step with the original outputs of PLM, we can get a knowledge-enriched representation for each token; (3) For further interactions between knowledge-enriched tokens, we employ a self-attention mechanism that operates on the fully-connected word graph (corresponding to the "text only" graph of UKET) to allow the knowledge-enriched representation of each token to further interact with others.</p><p>We conduct experiments on the MRC task, which requires a system to comprehend a given text and answer questions about it. In this paper, to prove the generalization ability of our method, we evaluate KELM on both the extractive-style MRC task (answers can be found in a span of the given text) and the multiple-response-items-style MRC task (each question is associated with several choices for answer-options, the number of correct answer-options is not pre-specified). MRC is a challenging task and represents a valuable path towards natural language understanding (NLU). With the rapid increment of knowledge, NLU becomes more difficult since the system needs to absorb new knowledge continuously. Pre-training models on large-scale corpus is inefficient. Therefore, finetuning the knowledge-enhanced PLM on the downstream tasks directly is crucial in the application. 2 2 RELATED WORK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">KNOWLEDGE GRAPH EMBEDDING</head><p>We denote a directed knowledge graph as G(E, R), where E and R are sets of entities and relations, respectively. We also define F as a set of facts, a fact stored in a KG can be expressed as a triplet (h, r, t) ? F, which indicates a relation r pointing from the head entity h to tail entity t, where h, t ? E and r ? R. KGE aims to extract topological information in KG and to learn a set of low-dimensional representations of entities and relations by knowledge graph completion task <ref type="bibr">(Yang et al., 2015;</ref><ref type="bibr" target="#b10">Lu &amp; Hu, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MULTI-RELATIONAL GRAPH NEURAL NETWORK</head><p>Real-world KGs usually include several relations. However, traditional GNN models such as <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>, and GAT <ref type="bibr" target="#b22">(Veli?kovi? et al., 2018)</ref> can only be used in the graph with one type of relation. <ref type="bibr" target="#b18">(Schlichtkrull et al., 2017;</ref><ref type="bibr" target="#b4">Haonan et al., 2019)</ref> generalizes traditional GNN models by performing relation-specific aggregation, making it possible to encode relational graphs. The use of multi-relational GNN makes it possible to encode injected knowledge embeddings dynamically in SKG and CokeBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">JOINT LANGUAGE AND KNOWLEDGE MODELS</head><p>Since BERT was published in 2018, many efforts have been made for further optimization, basically focusing on the design of the pre-training process and the variation of the encoder. For studies of knowledge-enhanced PLMs, they also fall into the above two categories or combine both of them sometimes. Despite their success in leveraging external factual knowledge, the gains are limited by computing resources, knowledge ambiguity issue, and the expressivity of their methods for the fusion of heterogeneous information, as summarized in <ref type="table" target="#tab_7">Table 4</ref> of Appendix and the introduction part.</p><p>Recent studies notice that the architecture of Transformer treats input sequences as fully-connected word graphs, thus some of them try to integrate injected KGs and textual context into a unified data structure. Here we argue that UKET in our KELM is different from the WK graph proposed in CoLAKE/K-BERT. These two studies heuristically convert textual context and entity-related sub-graph into input sequences, both entities and relations are treated as input words of the PLM, then they leverage a Transformer with a masked attention mechanism to encode those sequences from the embedding layer and pre-train the model based on the large-scale corpus. Unfortunately, it is not trivial for them to convert the second or higher order neighbors related to textual context <ref type="bibr" target="#b19">(Su et al., 2020)</ref>, the structural information about the graph is lost. UKET differs from the WK graph of CoLAKE/K-BERT in that, instead of converting mentioned entities, relations, and text into a sequence of words and feeding them together into the input layer of PLM (they unify text and KG into a sequence), UKET unifies text and KG into a graph. Besides, by using our UKET framework, the knowledge fusion process of KELM is based on the representation of the last hidden layer of PLM, making it possible to directly fine-tune the PLM on the downstream tasks without re-pretraining the model. SKG also utilizes relational GNN to fuse information of KGs and text representation encoded by PLM. However, SKG only uses GNN to dynamically encode the injected KGs, which corresponds to part one of <ref type="figure">Fig. 1</ref>. Outputs of SKG are made by directly concatenating outputs of graph encoder with the outputs of PLM. It cannot select ambiguous knowledge and forbids the interactions between knowledge-enriched tokens corresponding to part two and part three of <ref type="figure">Fig. 1</ref>, respectively. KT-NET uses a specially designed attention mechanism to select relevant knowledge from KGs. For example, it treats all synsets of entity mentions within the WN18 3 as candidate KB concepts. This limits the ability of KT-NET to select the most relevant mentioned entities 4 . Moreover, the representations of injected knowledge are static in KT-NET, they cannot dynamically change according to textual context, the information about the original graph structure in KG is also lost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>The architecture of KELM is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. It consists of three main modules: (1) PLM Encoding Module; (2) Hierarchical Knowledge Enhancement Module; (3) Output Module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PLM ENCODING MODULE</head><p>This module utilizes PLM (e.g.BERT) to encode text to get textual representations for passages and questions. An input example of the MRC task includes a paragraph and a question with a candidate answer, represented as a single sequence of tokens of the length n:</p><formula xml:id="formula_0">T ={[CLS], Q, (A), [SEP ], P, [SEP ]}={t i } n i=1</formula><p>, where Q, A and P represent all tokens for question, candidate answer and paragraph, respectively 5 .</p><p>[SEP ] and [CLS] are special tokens in BERT and defined as a sentence separator and a classification token, respectively. i-th token in the sequence is represented by h t i ? R dt , where d t is the last hidden layer size of used PLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HIERARCHICAL KNOWLEDGE ENHANCEMENT MODULE</head><p>This module is the implementation of our proposed HMP mechanism to fuse information of textual and graph context. We will formally introduce graph construction for UKET, and the three subprocesses of HMP in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">CONSTRUCTION OF UKET GRAPH</head><p>(1) Given a set with |Q| elements:</p><formula xml:id="formula_1">{G q k (E q k , R q k )} |Q| q=1</formula><p>and input text, where |Q| is the total number of injected KGs, and q indicates the q-th KG. We denote the set of entity mentions related to the q-th</p><formula xml:id="formula_2">KG as X q ={x q i } |X q | i=1</formula><p>, where |X q | is the number of entity mentions in the text. The corresponding mentioned entities are shared by all tokens in the same entity mention. All mentioned entities</p><formula xml:id="formula_3">M q ={m q i } |M q | i=1</formula><p>are linked with their relevant entity mentions in the text, where |M q | is the number of mentioned entities in the q-th KG. We define this "entity link to token graph" in <ref type="figure">Fig. 1</ref> as we have N 0</p><formula xml:id="formula_4">G q m (E q m , R q m ), where E q m =X q ? M q is</formula><formula xml:id="formula_5">m q i ={m q i }. We define "KG-only graph": G q s (E q s , R q s ), where E q s = |M q | i=0 K x=0 N x m q i</formula><p>is the union of all mentioned entities and their neighbors within the K-hops sub-graph, and R q s is a set of all relations in the extracted sub-graph of q-th KG. <ref type="formula" target="#formula_11">(3)</ref> The text sequence can be considered as a fully-connected word graph as pointed out previously. This "text-only graph" can be denoted as</p><formula xml:id="formula_6">G t (E t , R t ),</formula><p>where E t is all tokens in text and R t is a set with only one element that connects all tokens. Finally, we define the full hierarchical graph consisting of all three parts</p><formula xml:id="formula_7">{G q s } |Q| q=1 , {G q m } |Q| q=1</formula><p>, and G t , as Unified Knowledge-enhanced Text Graph (UKET).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">DYNAMICALLY EMBEDDING KNOWLEDGE CONTEXT</head><p>We use pre-trained vectors obtained from the KGE method to initialize representations of entities in G q s (E q s , R q s ). Considering the structural information of injected knowledge graph forgotten during training, we utilize |Q| independent GNN encoders (i.e. g 1 (.), g 2 (.) in <ref type="figure" target="#fig_0">Fig. 2</ref>, which is the case of injecting two independent KGs in our experiment setting) to dynamically update entity embeddings of |Q| injected KGs. We use rGCN to model the multi-relational nature of the knowledge graph. To update i-th node of q-th KG in l-th rGCN layer:</p><formula xml:id="formula_8">s q(l+1) i = ?( r?R q s j?N r i 1 |N r i | W q(l) r s q(l) j ) (1) Where N r i is a set of neighbors of i-th node under relation r ? R q s . W q(l) r</formula><p>is trainable weight matrix at l-th layer and s q(l+1) i is the hidden state of i-th node at (l+1)-th layer. After L updates, |Q| sets of node embeddings are obtained. The output of the q-th KG can be represented as S q ? R |E q s |?dq , where |E q s | and d q are the numbers of nodes of extracted sub-graph and the dimension of pre-trained KGE, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">DYNAMICALLY SELECTING SEMANTICS-RELATED MENTIONED ENTITIES</head><p>To handle the knowledge ambiguity issue, we introduce an attention layer to weight these ambiguous mentioned entities by using the textual representations of tokens (outputs of Section 3.1) to query their semantics-related mentioned entities representations in KGs. Here, we follow the attention mechanism of GAT to update each entity mention embedding in G q m :</p><formula xml:id="formula_9">x q i = ?( j?N q i ? q ij W q s q j )<label>(2)</label></formula><p>Where s q j is the output embeddings from the q-th rGCN in the previous step. x q i is the hidden state of i-th entity mention x q i in X q , and N q i is a set of neighbors of</p><formula xml:id="formula_10">x q i in G q m . W q ? R dout?din is a trainable weight matrix, we set d in =d out =d q (thus x q i ? R dq ).</formula><p>? is a nonlinear activation function. ? q ij is the attention score that weights ambiguous mentioned entities in the q-th KG:</p><formula xml:id="formula_11">? q ij = exp(LeakyReLU( ? T q [W q h t i ||W q s q j ])) k?N q i exp(LeakyReLU( ? T q [W q h t i ||W q s q k ]))<label>(3)</label></formula><p>The representation h t i with a dimension of d t is projected to the dimension of d q , before using it to query the related mentioned entity embeddings of S q :</p><formula xml:id="formula_12">h t i = W q proj h t i , where W q proj ? R dq?dt . ? q ? R 2dq</formula><p>is a trainable weight vector. ? T is the transposition operation and || is the concatenation operation.</p><p>Finally, we concatenate outputs of |Q| KGs with textual context representation to get final knowledgeenriched representation:</p><formula xml:id="formula_13">h k i = [ h t i , x 1 i , . . . , x |Q| i ] ? R dt+d1+???+d |Q|<label>(4)</label></formula><p>If token t i can't match any entity in q-th KG (say t i / ? X q ), we fill x q i in Eq.4 with zeros. Note that mentioned entities in KGs are not always useful, to prevent noise, we follow (Yang &amp; Mitchell, 2017)'s work and add an extra sentinel node linked to each entity mention in G q m . The sentinel node is initialized by zeros and not trainable, which is the same as the case of no retrieved entities in the KG. In this way, according to the textual context, KELM can dynamically select mentioned entities and avoid introducing knowledge noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">INTERACTION BETWEEN KNOWLEDGE-ENRICHED TOKEN EMBEDDINGS</head><p>To allow knowledge-enriched tokens' representations to propagate to each other in the text, we use a fully-connected word graph G t , with knowledge-enriched representations from outputs of the previous step, and employ the self-attention mechanism similar to KT-NET to update token's embedding. The final representation for i-th token in the text is h f i ? R 6 * (dt+d1+???+d |Q| ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">OUTPUT MODULE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">EXTRACTIVE-STYLE MRC TASK</head><p>A simple linear transformation layer and softmax operation are used to predict start and end positions of answers. For i-th token, the probabilities to be the start and end position of answer span are:</p><formula xml:id="formula_14">p s i = exp(w T s h f i ) n j=1 exp(w T s h f j ) , p e i = exp(w T e h f i ) n j=1 exp(w T e h f j )</formula><p>, where w s , w e ? R 6 * (dt+d1+???+d |Q| ) are trainable vectors and n is the number of tokens. The training loss is calculated by the log-likelihood of the true start and end positions: External Knowledge We adopt knowledge sources the same as used in KT-NET: WordNet and NELL <ref type="bibr" target="#b1">(Carlson et al., 2010)</ref>. Representations of injected knowledge are initialized by resources provided by <ref type="bibr">(Yang &amp; Mitchell, 2017)</ref>. The size of these embeddings is 100. We retrieve related knowledge from the two KGs in a given sentence and construct UKET graph (as shown in Section 3.2.1). More details about entity embedding and concepts retrieval are available in Appendix B.</p><formula xml:id="formula_15">L = ? 1 N N i=1 (log p s y s i + log p e y e i ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTAL SETUPS</head><p>Baselines and Comparison Setting Because we use BERT large as the base model in our method, we use it as our primary baseline for all tasks. For fair comparison, we mainly compare our results with two fine-tune-based knowledge-enhanced models: KT-NET and SKG, which also evaluate their results on ReCoRD with BERT large as the encoder part. As mentioned in the original paper of KT-NET, KT-NET mainly focuses on the extractive-style MRC task. We also evaluate KT-NET on the multiple-response-items-style MRC task and compare the results with KELM. We evaluate our approach in three different KB settings: KELM WordNet , KELM NELL , and KELM Both , to inject KG from WordNet, NELL, and both of the two, respectively (The same as KT-NET). Implementation details of our model are presented in Appendix C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RESULTS</head><p>The results for the extractive-style MRC task and multiple-response-items-style MRC task are given in <ref type="table">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref>, respectively. The scores of other models are taken directly from the leaderboard of SuperGLUE 6 and literature <ref type="bibr" target="#b14">(Qiu et al., 2019;</ref><ref type="bibr" target="#b28">Yang et al., 2019)</ref>. In this paper, our implementation is based on a single model, and hence comparing with ensemble based models is not considered. Best results are labeled in bold and the second best are underlined.</p><p>Results on the dev set of ReCoRD show that: (1) KELM outperforms BERT large , irrespective of which external KG is used. Our best KELM offers a 5.2/3.7 improvement in EM/F1 over BERT large .</p><p>(2) KELM outperforms previous SOTA knowledge-enhanced PLM (KT-NET) by +3.8 EM/+2.3 F1.</p><p>In addition, KELM outperforms KT-NET significantly in all three KB settings. On the dev set of MultiRC, the best KELM offers a 3.6 improvement in EM over KT-NET. Although the performance on F1 drop a little compared with KT-NET, we still get a gain of +2.9 (EM+F1) over the former SOTA model 7 .</p><p>Results on the test set further demonstrate the effectiveness of KELM and its superiority over the previous works. On ReCoRD, it significantly outperforms the former SOTA knowledge-enhanced PLM (finetuning based model) by +3.2 EM/+1.9 F1. And on MultiRC, KELM offers a 3.1/0.8 improvement in EM/F1 over BERT large , and achieves a gain of +1.5 (EM+F1) over KT-NET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDY</head><p>This section uses an example in ReCoRD to show how KELM avoids knowledge ambiguity issue and selects the most relevant mentioned entities adaptively w.r.t the textual context. Recall that given a token t i , the importance of a mentioned entity m q j in q-th KG is scored by the attention weight ? q ij in Eq.2. To illustrate how KELM can select the most relevant mentioned entities, we analyze the example that was also used in the case study part of KT-NET. The question of this example is "Sudan remains a XXX-designated state sponsor of terror and is one of six countries subject to the Trump administration's ban", where the "XXX" is the answer that needs to be predicted. The case study in KT-NET shows the top 3 most relevant concepts from WordNet for the word "ban" are "forbidding.n.01", "proscription.n.01", and "ban.v.02", with the weights of 0.861, 0.135, and 0.002, respectively. KT-NET treats all synsets of a word as candidate KG concepts, both "forbidding.n.01" and "ban.v.02" will be the related concepts of the word "ban" in the text. Although KT-NET can select relevant concepts and suppress the knowledge noise through its specially designed attention mechanism, we still observe two problems from the previous case study: (1) KT-NET cannot select the most relevant mentioned entities in KG that share the same string in the input text.</p><p>(2) Lack of ability to judge the part of speech (POS) of the word (e.g. "ban.v.02" gets larger weights than "ban.n.04").  For KELM, by contrast, we focus on selecting the most relevant mentioned entities to solve the knowledge ambiguity issue (based on the "entity link to token graph" part of UKET). For injecting WordNet, by allowing message passing on the extracted sub-graphs ("KG only" part of UKET), knowledge context can be dynamically embedded according to the textual context. Thus the neighbors' information of mentioned entities in WordNet can be used to help the word in a text to correspond to a particular POS based on its context. The top 3 most relevant mentioned entities in WordNet for the word "ban" in the above example are "ban.n.04", "ban.v.02", and "ban.v.01", with the weights of 0.715, 0.205, and 0.060, respectively.</p><p>To vividly show the effectiveness of KELM, we analyze ambiguous words in the motivating example show in <ref type="figure">Fig. 1</ref> (The example comes from ReCoRD):</p><p>"President Ford then pardoned Richard Nixon, leading to a further firestorm of outrage." <ref type="table">Table.</ref> 3 presents 5 words in the above passage. For each word, the most relevant mentioned entity in WordNet with the highest score is given. The golden mentioned entity for each word is labeled by us. Definitions of mentioned entities in WordNet that correspond to the word examples are listing in <ref type="table" target="#tab_8">Table 5</ref> of Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have proposed KELM for MRC, which enhances PLM representations with structured knowledge from KGs based on the fine-tuning process. Via a unified knowledge-enhanced text graph, KELM can embed the injected knowledge dynamically, and select relevant mentioned entities in the input KGs. In the empirical analysis, KELM shows the effectiveness of fusing external knowledge into representations of PLM and demonstrates the ability to avoid knowledge ambiguity issue. Injecting emerging factual knowledge into PLM during finetuning without re-pretraining the whole model is quite important in the application of PLMs and is still barely investigated. Improvements achieved by KELM over vanilla baselines indicate a potential direction for future research. A SUMMARY AND COMPARISON OF RECENT KNOWLEDGE-ENHANCED PLMS <ref type="table" target="#tab_7">Table 4</ref> shows a brief summary and comparison of recent knowledge-enhanced PLMs. Most of recent work concentrated on injecting external knowledge graphs during pre-training phase, which makes them inefficient in injecting external knowledge (e.g. LUKE takes about 1000 V100 GPU days to re-pretraining the RoBERTa based PLM model). Also, nearly all of them uses an additional entity linking tool to align the mentioned entities in the Wikidata to the entity mentions in the pre-trained corpus (English Wikipedia) uniquely. These methods never consider to resolve the knowledge ambiguity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATATSET AND KNOWLEDGE GRAPH DETAILS</head><p>ReCoRD (an acronym for the Reading Comprehension with Commonsense Reasoning Dataset) is a large-scale dataset for extractive-style MRC requiring commonsense reasoning. There are 100,730, 10,000, and 10,000 examples in the training, development (dev), and test set, respectively. An example of the ReCoRD consists of three parts: passage, question, and answer. The passage is formed by the first few paragraphs of an article from CNN or Daily Mail, with named entities recognized and marked. The question is a sentence from the rest of the article, with a missing entity specified as the golden answer. The model needs to find the golden answer among the entities marked in the passage. Questions that can be easily answered by pattern matching are filtered out. By the design of the process of data collection, one can see that to answer the questions, external background knowledge and ability of reasoning are required.</p><p>MultiRC (Multi-Sentence Reading Comprehension) is a multiple-response-items-style MRC dataset of short paragraphs and multi-sentence questions that can be answered from the content of the paragraph. Each example of MultiRC includes a question that associates with several choices for answer-options, and the number of correct answer-options is not pre-specified. The correct answer is not required to be a span in the text. The dataset consists of 10K questions ( 6k multiple-sentence questions), about 60% of this data make training/dev data. Paragraphs in the dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more complicated in their contents as compared to single-domain datasets.  WordNet contains 151,442 triplets with 40,943 synsets459and 18 relations. We look up mentioned entities in the WordNet by string matching operation, and link all tokens in the same word to the retrieved mentioned entities (tokens are tokenized by Tokenizer of BERT). Then, we extract all 1-hop neighbors for each mentioned entity and construct sub-graphs. In this paper, our experiment results are based on the 1-hop case. However, our framework can be generalized to multi-hop easily, and we leave this for future work.</p><p>NELL contains 180,107 entities and 258 concepts. We link entity mentions to the whole KG, and return associated concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IMPLEMENTATION DETAILS</head><p>Our implementation is based on HuggingFace <ref type="bibr" target="#b27">(Wolf et al., 2020)</ref> and DGL <ref type="bibr" target="#b24">(Wang et al., 2020b)</ref>. For all three settings of KELM, parameters of the encoding layer of BERT large are initialized with pre-trained model released by Google. Other trainable parameters in HMP are randomly initialized. The total number of trainable parameters of KELM is 340.4M (Roughly the same as BERT large , which has 340M parameters). Since including all neighbors around mentioned entities of WordNet is not efficient, for simplicity, we use top 3 most common relations in WordNet in our experiment (i.e. hyponym, hypernym, derivationally_related_form). For both datasets, we use a "two stage" fine-tune strategy to achieve our best performance, the FullTokenizer built in BERT is used to segment input words into wordpieces.</p><p>For ReCoRD, the maximum length of answer during inference is set to 30, and the maximum length of question is set to 64. Questions longer than that are truncated. The maximum length of input sequence T 8 is set to 384. Input sequences longer than that are segmented into chunks with a stride of 128. Fine-tuning our model on ReCoRD costs about 18 hours on 4 V100 GPU with a batch size of 48. We freeze parameters of BERT and use Adam optimizer with a learning rate of 1e-3 to train our knowledge module in the first stage. The maximum number of training epochs of the first stage is 10. The purpose of this is to provide a good weight initialization for our HMP. For the second stage, the pre-trained BERT parameters and our HMP part will be fine-tuned together. The max number of training epochs is chosen from {4, 6, 8}. The learning rate is set to be 2e-5 with a warmup over the first 6% of max steps, and linear decay until up to max epochs. For both two stages, early stopping is applied according to the best EM+F1 score on the dev set every 500 steps.</p><p>For MultiRC, the maximum length of input sequence T is set to 256. The summation of length of question (Q) and length of candidate answer (A) is not limited. Paragraph (P ) is truncated to fit the maximum length of input sequence. Fine-tuning KELM on MultiRC needs about 12 hours on 4 V100 GPU with a batch size of 48. For the first stage finetuning, learning rate is 1e-4 and the maximum number of training epochs is 10. For the second stage, the max number of training steps is chosen from {10000, 15000, 20000}. The learning rate is set to be 2e-5 with a warmup over the first 10% of max steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D SUPPLEMENTATION OF THE CASE STUDY SECTION</head><p>We provide definitions of the top 3 most relevant mentioned entities in WordNet that correspond to the word examples mentioned in Case Study Section. Descriptions are obtained by using NLTK <ref type="bibr" target="#b9">(Loper &amp; Bird, 2002)</ref>. By comprehending the motivating example in the case study section, we can see that KELM can correctly select the most relevant mentioned entities in the KG.  Each question is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. Similar to the previous two MRC tasks, the development set is publicly available, but the test set is hidden. One has to submit the predicted results for the test set to SuperGLUE to retrieve the final test score. Since the implementation of KELM is based on BERT large , we use it as our baseline for the comparison. The result of BERT large is directly taken from the leaderboard of SuperGLUE. <ref type="table" target="#tab_10">Table 6</ref> shows the experiment results. The injected KG is WordNet here, and we use accuracy as the evaluation metric.</p><p>The huge improvement over the baseline in this task demonstrates that knowledge in WordNet is indeed helpful for BERT to improve the generalization ability to the out-of-domain downstream task.   We also list the results of LUKE <ref type="bibr" target="#b29">(Yamada et al., 2020)</ref> in <ref type="table" target="#tab_11">Table 7</ref>. LUKE is a pretrain-based knowledge enhanced PLM and uses Wiki-related golden entities (one-to-one mapping) as the injected knowledge source (about 500k entities 9 ). It has more 128 M parameters than the total number of parameters of the vanilla RoBERTa. As we summarized in <ref type="table" target="#tab_7">Table 4</ref>    <ref type="figure">1 ? 88.9)</ref>. The rest of the improvement is because RoBERTa uses 10 times of training corpus than BERT and different pre-training strategies they used.</p><p>Interestingly, we find the performance of KELM on SQuAD 1.1 is sub-optimal compared with KT-NET. As we mentioned in the last paragraph of the Related Work Section, KT-NET treats all synsets of entity mentions within the WN18 as candidate KB concepts. Via a specially designed attention mechanism, KT-NET can directly use all 1-hop neighbors of the mentioned entities. Although this limits the ability of KT-NET to select the most relevant mentioned entities (as we discussed in Case Study Section), information of these neighbors can be directly considered. Using neighbors of the mentioned entities indirectly via the HMP mechanism makes it possible for KELM to dynamically embed injected knowledge and to select semantics-related mentioned entities. However, SQuAD is an in-domain downstream task for BERT, the problem of ambiguous meanings of words can be alleviated by pretraining model on the in-domain corpus. Compared with KT-NET, a longer message passing path in KELM may lead to sub-optimal improvement on the in-domain task.</p><p>H FURTHER DISCUSSIONS ABOUT THE NOVELTY W.R.T SKG/KT-NET UKET defined in KELM consists of three subgraphs in a hierarchical structure, each subgraph corresponds to one sub-process of our proposed HMP mechanism and solves one problem presented in the Hierarchical Knowledge Enhancement Module part of Methodology Section. SKG only uses GNN to dynamically encode the extracted KG which corresponds to the first part of UKET, it can not solve the knowledge ambiguity issue and forbids interactions among knowledge-enriched tokens. KT-NET defines a similar graph as the third part of UKET. However, the first and second subgraphs of UKET are absent. The second subgraph of UKET is independent of ideas of KT-NET and SKG, thus KELM is not a simple combination of these two methods. We are the first to unify text and KG into a graph and to propose this hierarchical message passing framework to incorporate two heterogeneous information. SKG/KT-NET can be interpreted as parts of the ablation study of components of KELM. The result of SKG is ablation with the component only related to the first subgraph of UKET. While KT-NET only contains the third subgraph with a modified knowledge integration module. KELM uses a dedicatedly designed HMP mechanism to let the information of farther neighbors to be considered. However, longer information passing path than KT-NET makes it less efficient. In our experiments, KELM takes more 30% training time than KT-NET on both ReCoRD and MultiRC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I LIMITATIONS AND FURTHER IMPROVEMENTS OF KELM</head><p>Limitations for KELM are two-fold: (1) Meanings of mentioned entities in different KGs that share the same entity mentions in the text may conflict with each other. Although HMP can help to select the most relevant mentioned entities in a single KG, there's no mechanism to guarantee the selections across different KGs; (2) Note the knowledge-enriched representation in Eq.3 is obtained by simple concatenation of the embeddings from different KGs. Too much knowledge incorporation may divert the sentence from its correct meaning (Knowledge noise issue). We expect these two potential improvements to be a promising avenue for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J FURTHER ANALYSIS AND DISCUSSION</head><p>KELM incorporates knowledge in KGs into the representations in the last hidden layer of PLM (Refer to Methodology Section). It is essentially a model-agnostic, KG-agnostic, and task-agnostic framework for enhancing language model representations with factual knowledge from KGs. It can be used to enhance any PLM, with any injected KGs, on any downstream task. Besides the two Q&amp;A-related MRC tasks we mentioned in the main text, we also evaluate KELM on COPA and SQuAD 1.1 based on BERT large , results are presented in Appendix E and Appendix G, respectively. To demonstrate KELM is a model-agnostic framework, we also implement KELM based on RoBERTa large and evaluate it on ReCoRD. The experiment is presented in Appendix F. Improvements achieved by KELM over all vanilla base PLM models indicate the effectiveness of injecting external knowledge.</p><p>However, the improvements of KELM over RoBERTa on ReCoRD and BERT on SQuAD 1.1 are marginal compared with the ones on ReCoRD/MultiRC/COPA (BERT large based). The reason behind this is that pretraining model on in-domain unlabeled data could boost performance on downstream tasks. Passages in ReCoRD are collected from articles in CNN/Daily Mail, while BERT is pre-trained on BookCorpus and English Wikipedia. RoBERTa not only uses the corpus that used in BERT (16 GB), but also an additional corpus collected from the CommonCrawl News dataset (76 GB). ReCoRD is in-domain for RoBERTa but is out-of-domain for BERT. Similarly, SQuAD 1.1 is created from Wikipedia, it is an in-domain downstream task for both BERT and RoBERTa. This partially explains why RoBERTa achieves a much larger improvement over BERT on the result of ReCoRD (71.3 ? 88.4 in EM on test set) than the one on SQuAD 1.1 (84.1 ? 88.9). A similar analysis can be also found in T5 <ref type="bibr" target="#b15">(Raffel et al., 2020)</ref>. From our empirical results, we can summarize that general KG (e.g. WordNet) can not help too much for the PLMs pretrained on in-domain data. But it can still improve the performance of the model when the downstream tasks are out-of-domain. Further detailed analysis can be found in our appendix. Finding a popular NLP task/dataset that is not related to the training corpus of modern PLMs is difficult. Pre-training on large-scale corpus is always good if we have unlimited computational resources and plenty of in-domain corpus. It has been evident that the simple finetuning of PLM is not sufficient for domain-specific applications. KELM can provide people another choice when they do not have such a large-scale in-domain corpus and want to incorporate incremental domain-related structural knowledge into the domain-specific applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Framework of KELM (left) and illustrates how to generate knowledge-enriched token embeddings (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the union of entity mentions and their relevant mentioned entities, R q m is a set with only one element that links mentioned entities and their relevant entity mentions. (2) For i-th mentioned entity m q i in M q , we retrieve all its K-hop neighbors {N x m q i } K x=0 from the q-th knowledge graph, where N x m q i is a set of i-th mentioned entity's x-hop neighbors, hence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.04 (0.72) an official prohibition or edict against something ban.v.02 (0.21) prohibit especially by legal means or social pressure ban.v.01 (0.06) forbid the public distribution of ( a movie or a newspaper)ford ford.n.05 (0.56) 38th President of the United States; appointed vice president and succeeded Nixon when Nixon resigned (1913-) ford.n.07 (0.24) a shallow area in a stream that can be forded ford.v.01 (0.08) cross a river where it's shallow pardon pardon.v.02 (0.86) a warrant granting release from punishment for an offense sentinel (0.10)pardon.n.02 (0.04) grant a pardon to nixon nixon.n.01 (0.74) vice president under Eisenhower and 37th President of the United States; resigned after the Watergate scandal in 1974 (1913-1994) sentinel (0.26)lead lead.v.03 (0.73) tend to or result in lead.n.03 (0.12) evidence pointing to a possible solution lead.v.04 (0.05) travel in front of; go in advance of others outrage outrage.n.02 (0.62) a wantonly cruel act sentinel (0.38) -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where N is the total number of examples in the dataset, y s i and y e i are the true start and end positions of i-th query's answer, respectively. During inference, we pick the span (a, b) with maximum p s a p e b where a ? b as predicted anwser.3.3.2 MULTIPLE-RESPONSE-ITEMS-STYLE MRC TASKSince answers to a given question are independent of each other, to predict the correct probability of each answer, a fully connected layer followed by a sigmoid function is applied on the final representation of [CLS] token in BERT. Khashabi et al., 2018) (multiple-response-items-style). Detailed descriptions of the two datasets can be found in Appendix B. On both datasets, the test set is not public, one has to submit the predicted results to the organization to get the final test score. Since frequent submissions to probe the unseen test set are not encouraged, we only submit our best model once for each of the datasets, thus the statistics of the results (e.g., mean, variance, etc.) are not applicable. We use Exact Match (EM) and (macro-averaged) F1 as the evaluation metrics.</figDesc><table><row><cell>4 EXPERIMENTS</cell></row><row><cell>4.1 DATASETS</cell></row></table><note>In this paper, we empirically evaluate KELM on both two types of MRC benchmarks in Super- GLUE (Wang et al., 2020a): ReCoRD (Zhang et al., 2018) (extractive-style) and MultiRC (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1: Result on ReCoRD.</figDesc><table><row><cell></cell><cell>Dev</cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">EM F1 EM F1</cell><cell></cell><cell>Dev</cell><cell>Test</cell></row><row><cell>BERTlarge</cell><cell cols="3">70.2 72.2 71.3 72.0</cell><cell>Model</cell><cell>EM F1 EM F1</cell></row><row><cell cols="4">SKG+BERTlarge 70.9 71.6 72.2 72.8</cell><cell>BERTlarge</cell><cell>-</cell><cell>-</cell><cell>24.1 70.0</cell></row><row><cell cols="2">KT-NETWordNet 70.6 72.8 KT-NETNELL 70.5 72.5</cell><cell>--</cell><cell>--</cell><cell cols="2">KT-NET  *  BOTH 26.7 71.7 25.4 71.1</cell></row><row><cell>KT-NETBOTH</cell><cell cols="3">71.6 73.6 73.0 74.8</cell><cell cols="2">KELMWordNet 29.2 70.6 25.9 69.2</cell></row><row><cell>KELMWordNet KELMNELL</cell><cell cols="3">75.4 75.9 75.9 76.5 74.8 75.3 75.9 76.3</cell><cell>KELMNELL KELMBoth</cell><cell>27.3 70.4 26.5 70.6 30.3 71.0 27.2 70.8</cell></row><row><cell>KELMBoth</cell><cell cols="3">75.1 75.6 76.2 76.7</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Result on MultiRC. [*] are from our implementation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Case study. Comparisons between the golden label with the most relevant mentioned entity in WordNet. The importance of selected mentioned entities is provided in the parenthesis.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, Hua Wu, Qiaoqiao She, and Sujian Li. Enhancing pre-trained language representations with rich knowledge for machine reading comprehension. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2346-2357, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1226. URL https://www.aclweb.org/anthology/P19-1226.</figDesc><table><row><cell>Bishan Yang and Tom Mitchell. Leveraging knowledge bases in LSTMs for improving machine</cell></row><row><cell>reading. In Proceedings of the 55th Annual Meeting of the Association for Computational Lin-</cell></row><row><cell>guistics (Volume 1: Long Papers), pp. 1436-1446, Vancouver, Canada, July 2017. Association for</cell></row><row><cell>Computational Linguistics. doi: 10.18653/v1/P17-1132. URL https://www.aclweb.org/</cell></row><row><cell>anthology/P17-1132.</cell></row><row><cell>Bishan Yang, Wen tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and</cell></row><row><cell>relations for learning and inference in knowledge bases, 2015.</cell></row><row><cell>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le.</cell></row><row><cell>Xlnet: Generalized autoregressive pretraining for language understanding, 2020.</cell></row></table><note>Donghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. Jaket: Joint pre-training of knowledge graph and language understanding, 2020. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension, 2018. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced language representation with informative entities, 2019.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>A brief summary and comparison of recent knowledge-enhanced PLMs. The full names of some abbreviations are as follows. MLM: masked language model, NSP: next sentence prediction, Ent: entity, Rel: relation, CLS: classification, Sent: sentence, ATT: attention. Comments/descriptions of features are written in parentheses. Desired properties are written in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Definitions of mentioned entities in WordNet corresponding to the word examples in the case study. The importance of mentioned entities is provided in the parenthesis. "sentinel" is meaningless, which is used to avoid knowledge noise.E EXPERIMENT ON COMMONSENSE CAUSAL REASONING TASKTo further explore the generalization ability of KELM, we also evaluate our method on COPA (Roemmele et al., 2011) (Choice of Plausible Alternatives), which is also a benchmark dataset in SuperGLUE and can be used for evaluating progress in open-domain commonsense causal reasoning. COPA consists of 1000 questions, split equally into development and test sets of 500 questions each.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc>RoBERTa large too much, since RoBERTa is pre-trained on a much larger corpus than BERT, knowledge in WordNet/NELL has been learned in RoBERTa.</figDesc><table><row><cell cols="4">: Performance comparison on COPA. The effectiveness of injecting knowledge (WordNet) are</cell></row><row><cell>shown.</cell><cell></cell><cell></cell></row><row><cell cols="4">F KELM: A FRAMEWORK OF FINETUNE-BASED MODEL-AGNOSTIC</cell></row><row><cell>KNOWLEDGE-ENHANCED PLM</cell><cell></cell><cell></cell></row><row><cell cols="4">We implement KELM based on the RoBERTa large , which has a similar number of trainable parameters</cell></row><row><cell cols="4">as BERT large but uses nearly 10 times of training corpus than BERT large . Since the performances of</cell></row><row><cell cols="4">RoBEATa on the leaderboard of SuperGLUE are based on ensembling, we also finetune RoBERTa large</cell></row><row><cell cols="4">on ReCoRD to produce the results of a single model. Comparisons of the results can be found</cell></row><row><cell cols="4">in Table 7, where you can also see an improvement there. However, that improvement is not</cell></row><row><cell cols="4">as significant as we observed in BERT large . Reasons are two-fold: (1) Passages in ReCoRD are</cell></row><row><cell cols="4">collected from articles in CNN/Daily Mail, while BERT is pre-trained on BookCorpus and English</cell></row><row><cell cols="4">Wikipedia. RoBERTa not only uses the corpus that used in BERT (16 GB), but also an additional</cell></row><row><cell cols="4">corpus collected from the CommonCrawl News dataset (76 GB). ReCoRD dataset is in-domain</cell></row><row><cell cols="4">for RoBERTa but is out-of-domain for BERT. It seems that the improvements of KELM with</cell></row><row><cell cols="4">injecting general KGs (e.g. WordNet) on the in-domain downstream tasks are not as large</cell></row><row><cell cols="4">as the out-of-domain downstream tasks. A similar phenomenon can be also observed in the</cell></row><row><cell cols="4">experiment of SQuAD 1.1 (Refer to Appendix E). (2) The same external knowledge (WordNet,</cell></row><row><cell cols="3">NELL) can not help Dev</cell><cell>Test</cell></row><row><cell></cell><cell>Model</cell><cell cols="2">EM F1 EM F1</cell></row><row><cell>PLM w/o</cell><cell>BERTlarge</cell><cell cols="2">70.2 72.2 71.3 72.0</cell></row><row><cell>external knowledge</cell><cell>RoBERTa  *  large</cell><cell cols="2">87.9 88.4 88.4 88.9</cell></row><row><cell>knowledge enhanced PLM (finetune-based)</cell><cell>KELM BERT large Both KELM RoBERTa large Both</cell><cell cols="2">75.1 75.6 76.2 76.7 88.2 88.7 89.1 89.6</cell></row><row><cell>knowledge enhanced PLM</cell><cell>LUKE</cell><cell cols="2">90.8 91.4 90.6 91.2</cell></row><row><cell>(pretrain-based)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison of the effectiveness of injecting external knowledge between BERT and RoBERTa. [*] Results are from our implementation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>in the main text, the pretraining task is also different compared with RoBERTa. Although LUKE gets better results compared with vanilla RoBERTa and KELM, it needs 16 NVIDIA Tesla V100 GPUs and the training takes approximately 30 days. Relying on hyperlinks in Wikipedia as golden entity annotations, lacking the flexibility to adapt the external knowledge of other domains, and needing re-pretraining when incorporating knowledge, these limitations hinder the abilities of applications.<ref type="bibr" target="#b16">Rajpurkar et al., 2016</ref>) is a well known extractive-style MRC dataset that consists of questions created by crowdworkers for Wikipedia articles. It contains 100,000+ question-answer pairs on 536 articles. We implement KELM based on the BERT large , and compare our results on the development set of SQuAD 1.1 with KT-NET (Best result of KT-NET is based on injecting WordNet only). Results are shown inTable 8</figDesc><table><row><cell>G EXPERIMENT ON SQUAD 1.1 PLM w/o external knowledge BERTlarge Model SQuAD1.1 (Dev EM F1 84.4 91.2 knowledge enhanced PLM (finetune-based) KT-NET BERT large 85.1 91.7 WordNet KELM BERT large WordNet 84.7 91.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Performance comparison on the development set of SQuAD 1.1.Results on KELM show an improvement over vanilla BERT. Both BERT and RoBERTa use English Wikipedia as the corpus for pretraining. Since SQuAD is also created from Wikipedia, it is an in-domain downstream task for both BERT and RoBERTa (while ReCoRD dataset is in-domain for RoBERTa but is out-of-domain for BERT). This explains why RoBERTa achieves a much larger improvement over BERT on the result ofReCoRD (71.3 ? 88.4  in EM on test set) than the one on SQuAD 1.1 (84.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code is available at here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A subset of WordNet.4  Refer the example given in the case study of KT-NET, the most relevant concept for the word "ban" is "forbidding_NN_1" (with the probability of 86.1%), but not "ban_NN_4".5  Depending on the type of MRC task (extractive-style v.s. multiple-response-items-style), candidate answer A is not required in the sequence of tokens for the extractive-style MRC task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://super.gluebenchmark.com/leaderboard (Nov.14th, 2021) 7 The best model is chosen according to the EM+F1 score (same as KT-NET).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Refer to the PLM Encoding Module of Methodology Section.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">For KELM, we only use 40943 entities in WordNet and 258 concepts in NELL.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors thank Ms. X. Lin for insightful comments on the manuscript. We also thank Dr. Y. Guo for helpful suggestions in parallel training settings. We also thank all the colleagues in AI Application Research Center (AARC) of Huawei Technologies for their supports.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1306" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Information and knowledge management</title>
		<meeting>the 19th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Graph star net for generalized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Haonan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Xiuyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Looking beyond the surface:a challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Haotang Deng, and Ping Wang. K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nltk: The natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118108.1118117</idno>
		<ptr target="https://doi.org/10.3115/1118108.1118117" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics</title>
		<meeting>the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
	<note>ETMTNLP &apos;02</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dense: An enhanced non-abelian group representation for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COMMUNICATIONS OF THE ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bert is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Poerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulli</forename><surname>Waltinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno>abs/1911.03681</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Machine reading comprehension using structural knowledge graph-aware network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangwen</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1602</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1602" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="5896" to="5901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Cosmin Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
		<idno>SS-11-06</idno>
		<ptr target="http://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418" />
	</analytic>
	<monogr>
		<title level="m">Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium</title>
		<meeting><address><addrLine>Stanford, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cokebert: Contextual knowledge selection and embedding towards enhanced pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Colake</surname></persName>
		</author>
		<title level="m">Contextualized language and knowledge embedding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Graph attention networks</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">K-adapter: Infusing knowledge into pre-trained models with adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Kepler: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luke</surname></persName>
		</author>
		<title level="m">Deep contextualized entity representations with entity-aware self-attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
