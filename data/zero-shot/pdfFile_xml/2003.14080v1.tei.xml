<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X-Linear Attention Networks for Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">X-Linear Attention Networks for Image Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress on fine-grained visual recognition and visual question answering has featured Bilinear Pooling, which effectively models the 2 nd order interactions across multi-modal inputs. Nevertheless, there has not been evidence in support of building such interactions concurrently with attention mechanism for image captioning. In this paper, we introduce a unified attention block -X-Linear attention block, that fully employs bilinear pooling to selectively capitalize on visual information or perform multimodal reasoning. Technically, X-Linear attention block simultaneously exploits both the spatial and channel-wise bilinear attention distributions to capture the 2 nd order interactions between the input single-modal or multi-modal features. Higher and even infinity order feature interactions are readily modeled through stacking multiple X-Linear attention blocks and equipping the block with Exponential Linear Unit (ELU) in a parameter-free fashion, respectively. Furthermore, we present X-Linear Attention Networks (dubbed as X-LAN) that novelly integrates X-Linear attention block(s) into image encoder and sentence decoder of image captioning model to leverage higher order intraand inter-modal interactions. The experiments on COCO benchmark demonstrate that our X-LAN obtains to-date the best published CIDEr performance of 132.0% on COCO Karpathy test split. When further endowing Transformer with X-Linear attention blocks, CIDEr is boosted up to 132.8%. Source code is available at https://github. com/Panda-Peter/image-captioning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image captioning is the task of automatically producing a natural-language sentence to describe the visual content of an image. The essential practice of neural captioning models follows encoder-decoder paradigm <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33]</ref>, which is derived from neural machine translation <ref type="bibr" target="#b29">[30]</ref>. In between, Convolutional Neural Network (CNN) is utilized to encode an input image and Recurrent Neural Network (RNN) is adopted as sentence decoder to generate the output sentence, one word at each time step. Despite involving two  <ref type="figure">Figure 1</ref>. Comparison between conventional attention mechanism and our X-Linear attention block for image captioning. (a) Conventional attention mechanism linearly fuses query (Q) and key (K) via element-wise sum to compute spatial attention weight for each value (V), which characterizes the 1 st order interaction between query and key. (b) X-Linear attention block fully capitalizes on bilinear pooling to capture the 2 nd order feature interaction in between, and measures both spatial and channel-wise attention distributions. The two attention weights are adopted to accumulate the enhanced values of bilinear pooling on query and value. different major modalities (visual content and textual sentence) in image captioning, such paradigm of approaches seldom explores the multi-modal interactions particularly at the early stage. In other words, vision and language are treated independently. That prompts the recent stateof-the-art methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref> to adopt visual attention mechanisms which trigger the interaction between visual content and natural sentence. Concretely, these visual attention mechanisms boost performance by learning to identify selective spatial regions in an image conditioning on current hidden state of language decoder, and in turn accumulating encoded region features with attention weights to guide decoding process. <ref type="figure">Figure 1(a)</ref> illustrates the most conventional attention measure which estimates attention weights via linearly fusing the given query (hidden state of sentence decoder) and key (encoded image features) from different modalities. The attention is then applied to the value (encoded image features) to derive a weighted sum. Nevertheless, we argue that the design of conventional attention inherently exploits only the 1 st order feature interaction and is still lacking in efficacy. That severely limits the capacity of complex multi-modal reasoning in image captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2003.14080v1 [cs.CV] 31 Mar 2020</head><p>A natural way to mitigate the problem is to capture higher order interactions. We start our exploration from 2 nd order interaction through the unique design of a unified attention block, namely X-Linear attention block, as shown in <ref type="figure">Figure 1(b)</ref>. Technically, the outer product of key and query is computed through bilinear pooling to take all pairwise interactions between query and key into account. After bilinear pooling, two embedding layers are exploited to predict attention weights for each spatial region, followed by a softmax layer to normalize the spatial attention vector. In the meantime, the embedded outer product (feature map) is passed through a squeeze-excitation operation. The squeeze operation aggregates the feature map across spatial regions to produce a channel descriptor and the excitation operation performs the self-gating mechanism with a sigmoid on the channel descriptor to obtain the channel-wise attention vector. Finally, the outer product of query and value via bilinear pooling is weighted summated with the spatial attention vector, and we take the channel-wise multiplication of the sum and the channel-wise attention vector as the attended features. As such, our X-Linear attention block builds the 2 nd order interactions and infers the joint representations for image features and hidden states. It is also appealing in view that a stack of the blocks is readily grouped to go beyond bilinear models and extract higher order interactions. In the extreme case, our model could create infinity order interactions by stacking numerous X-Linear attention blocks and we implement this via the kernel trick, e.g., Exponential Linear Unit (ELU), in practice.</p><p>By integrating X-Linear attention block(s) into image captioning structures, we present a new X-Linear Attention Networks (X-LAN) to leverage high order intra-and inter-modal interactions, respectively, in the encoder and decoder. Specifically, for image encoder, Faster R-CNN is firstly utilized to detect a set of image regions. After that, a stack of X-Linear attention blocks are adopted to encode the region-level features with the higher order intramodal interaction in between, leading to a set of enhanced region-level and image-level features. Conditioned on the enhanced visual features induced by image encoder, we further employ X-Linear attention block in sentence decoder to perform multi-modal reasoning. This encourages the exploration of high order inter-modal interactions between visual content and natural sentence to boost sentence generation.</p><p>The main contribution of this work is the proposal of a unified X-Linear attention block that models the 2 nd order interactions with both spatial and channel-wise bilinear attention. This also leads to the elegant view of how the block should be extended for mining higher or even infinity order interactions and how to integrate such block(s) into image captioning structure. Through an extensive set of experiments, we demonstrate that our new X-LAN model achieves new state-of-the-art performances on COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image Captioning. Image captioning is an active research area <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. The early attempts <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33]</ref> exploit the encoder-decoder paradigm that firstly utilizes CNN to encoder image and then adopts RNN based decoder to generate the output word sequence, leading to promising results for this task. After that, a series of innovations have been proposed to boost image captioning by encouraging more interactions between the two different modalities via attention mechanism <ref type="bibr" target="#b4">[5]</ref>. In particular, <ref type="bibr" target="#b34">[35]</ref> integrates soft and hard attention mechanism into LSTM based decoder, aiming to select the most relevant image regions for word prediction at each decoding stage. <ref type="bibr" target="#b40">[41]</ref> presents semantic attention that learns to selectively focus on the semantic attributes in image for sentence generation. Instead of fully performing visual attention as in <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b22">[23]</ref> proposes an adaptive attention model that dynamically decides whether to attend to image regions at each decoding stage. Furthermore, bottom-up and topdown attention mechanism <ref type="bibr" target="#b1">[2]</ref> exploits visual attention at object level via bottom-up mechanism, and all salient image regions are associated with the output words through top-down mechanism for image captioning. <ref type="bibr" target="#b25">[26]</ref> presents the look back method to integrate attention weights from previous time step into the measurement of attention at current time step, which suits visual coherence of human. Later on, the most recently proposed attention on attention module <ref type="bibr" target="#b11">[12]</ref> enhances visual attention by further measuring the relevance between the attention result and the query.</p><p>Much of existing attention mechanisms in image captioning have concentrated on the exploration of only the 1 st order feature interaction between image content and sentence, reflecting limited capacity of multi-modal reasoning. In contrast, we design a novel X-Linear attention block to capture higher and even infinity order interactions, which facilitate both single-modal feature enhancement and multimodal reasoning for image captioning.</p><p>Bilinear Pooling. Bilinear pooling is an operation to calculate outer product between two feature vectors. Such technique can enable the 2 nd order interaction across all elements in feature vectors and thus provide more discriminative representations than linear pooling. An early pioneering work <ref type="bibr" target="#b21">[22]</ref> demonstrates the advantage of bilinear pooling for fine-grained visual recognition task. Local pairwise feature interactions are thus modeled by leveraging bilinear pooling over the outputs of two CNNs. Later on, <ref type="bibr" target="#b8">[9]</ref> proposes compact bilinear pooling that efficiently compresses the high-dimensional bilinear pooling feature into compact one with a few thousand dimensions, but retains the same discriminative power in the meantime. <ref type="bibr" target="#b7">[8]</ref> further extends compact bilinear pooling into multi-modal scenario where visual and textual representations are combined for visual question answering task. Instead of compact bilinear pool-ing that needs complex computations, <ref type="bibr" target="#b15">[16]</ref> proposes a flexible low-rank bilinear pooling structure with linear mapping and Hadamard product. Recently, <ref type="bibr" target="#b41">[42]</ref> presents a hierarchical bilinear pooling model to aggregate multiple cross-layer bilinear pooling features for fine-grained visual recognition. <ref type="bibr" target="#b14">[15]</ref> exploits low-rank bilinear pooling to construct bilinear attention network, aiming to learn bilinear attention distributions for visual question answering.</p><p>The aforementioned bilinear pooling techniques are mainly designed for fine-grained visual recognition or visual question answering. Instead, our X-Linear attention block is applicable to image encoder and sentence decoder to exploit higher order intra and inter-modal interactions for image captioning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">X-linear Attention Networks (X-LAN)</head><p>In this section, we introduce a novel unified formulation of attention module, named X-Linear attention block, that fully capitalizes on bilinear pooling to capture the 2 nd order feature interactions with both spatial and channel-wise bilinear attention. Moreover, we show a specific integration of X-Linear attention block into image encoder and sentence decoder to capture higher order intra-and inter-modal interactions, aiming to enhance visual information and perform complex multi-modal reasoning for image captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conventional Attention Module</head><p>We first provide a brief review of the most conventional attention module <ref type="bibr" target="#b34">[35]</ref> applied in image captioning, which learns to selectively attend to salient image regions for sentence generation. Formally, at decoding time step t, conditioned on the query Q (current hidden state of sentence decoder h t ), we can obtain the attention distribution ? t over a set of keys</p><formula xml:id="formula_0">K = {k i } N i=1 (N local image features): a t i = Wa [tanh (W k ki + WqQ)] , ? t = sof tmax a t ,<label>(1)</label></formula><p>where W a , W k , and W q are embedding matrices, and a t i denotes the i-th element in a t . In this sense, the normalized attention weight ? t i for each local image feature (i-th element in ? t ) is derived from the linear fusion of the given query and key via element-wise sum. Such way inherently exploits only the 1 st order feature interaction between natural sentence and visual content for attention measurement. Next, attention module produces the attended image featur? v t by accumulating all values</p><formula xml:id="formula_1">V = {v i } N i=1 (N local image features) with spatial attention weights:v t = N i=1 ? t i v i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">X-Linear Attention Block</head><p>Though conventional attention module nicely triggers the interaction between different modalities, only the 1 st order feature interaction is exploited, which reflects limited capacity of complex multi-modal reasoning in image captioning. Inspired by the recent successes of bilinear pooling applied in fine-grained visual recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b41">42]</ref> or visual question answering <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>, we fully capitalize on bilinear pooling techniques to construct a unified attention module (X-Linear attention block) for image captioning, as depicted in <ref type="figure">Figure 1</ref>(b). Such design of X-Linear attention block strengthens the representative capacity of the output attended feature by exploiting higher order interactions between the input single-modal or multi-modal features.</p><p>In particular, suppose we have the query Q ? R Dq , a set of keys</p><formula xml:id="formula_2">K = {k i } N i=1 , and a set of values V = {v i } N i=1 , where k i ? R D k and v i ? R Dv</formula><p>denote the i-th key/value pair. X-Linear attention block firstly performs low-rank bilinear pooling to achieve a joint bilinear query-key representation B k i ? R D B between query Q and each key k i :</p><formula xml:id="formula_3">B k i = ? (W k ki) ? W k q Q ,<label>(2)</label></formula><p>where W k ? R D B ?D k , and W k q ? R D B ?Dq are embedding matrices, ? denotes ReLU unit, and represents element-wise multiplication. As such, the learnt bilinear query-key representation B k i conveys the 2 nd order feature interactions between query and key.</p><p>Next, depending on all bilinear query-key representa-</p><formula xml:id="formula_4">tions {B k i } N i=1</formula><p>, two kinds of bilinear attention distributions are obtained to aggregate both spatial and channel-wise information within all values. Most specifically, the spatial bilinear attention distribution is introduced by projecting each bilinear query-key representation into the corresponding attention weight via two embedding layers, followed with a softmax layer for normalization:</p><formula xml:id="formula_5">B k i = ? W k B B k i , b s i = W b B k i , ? s = sof tmax (b s ) ,<label>(3)</label></formula><p>where W k B ? R Dc?D B and W b ? R 1?Dc are embedding matrices, B k i is the transformed bilinear query-key representation, and b s i is the i-th element in b s . Here each element ? s i in ? s denotes the normalized spatial attention weight for each key/value pair. Meanwhile, we perform a squeeze-excitation operation <ref type="bibr" target="#b10">[11]</ref> over all transformed bilinear query-key representations {B k i } N i=1 for channelwise attention measurement. Concretely, the operation of squeeze aggregates all transformed bilinear query-key representations via average pooling, leading to a global channel descriptorB:B</p><formula xml:id="formula_6">= 1 N N i=1 B k i .<label>(4)</label></formula><p>After that, the followed excitation operation produces channel-wise attention distribution ? c by leveraging the self-gating mechanism with a sigmoid over the global channel descriptorB:  where W e ? R D B ?Dc is embedding matrix. Finally, our X-Linear attention block generates the attended value featurev by accumulating the enhanced bilinear values with spatial and channel-wise bilinear attention:</p><formula xml:id="formula_7">b c = WeB, ? c = sigmoid (b c ) ,<label>(5)</label></formula><formula xml:id="formula_8">v = FX?Linear (K, V, Q) = ? c N i=1 ? s i B v i , B v i = ? (Wvvi) ? W v q Q ,<label>(6)</label></formula><p>where B v i denotes the enhanced value of bilinear pooling on query Q and each value v i , W v ? R D B ?Dv , and W v q ? R D B ?Dq are embedding matrices. Accordingly, compared to conventional attention modules that simply explore 1 st order interaction between query and key, X-Linear attention block produces the more representative attended feature since higher order feature interactions are exploited via bilinear pooling.</p><p>Extension with higher order interactions. In order to exploit higher order feature interactions, we further iterate the above process of bilinear attention measurement and feature aggregation using a stack of our X-Linear attention blocks. Formally, for the m-th X-Linear attention block, we firstly take the pervious output attended featurev (m?1) as input query, coupled with current input keys</p><formula xml:id="formula_9">K (m?1) = {k (m?1) i } N i=1 , and values V (m?1) = {v (m?1) i } N i=1 : v (m) = FX?Linear K (m?1) , V (m?1) ,v (m?1) ,<label>(7)</label></formula><p>wherev (m) is the output new attended feature.v (0) , K (0) , and V (0) denotes Q, K, and V, respectively. Then, all keys/values are further updated conditioned on the output new attended featurev (m) :</p><formula xml:id="formula_10">k (m) i = LayerN orm(?(W k m [v (m) , k (m?1) i ]) + k (m?1) i ), v (m) i = LayerN orm(?(W v m [v (m) , v (m?1) i ]) + v (m?1) i ),<label>(8)</label></formula><p>where W k m and W v m are embedding matrices. Note that here each key/value is concatenated with the new attended feature, followed with a residual connection and layer normalization as in <ref type="bibr" target="#b30">[31]</ref>. We repeat the process (Eq. <ref type="formula" target="#formula_9">(7)</ref> and Eq. <ref type="formula" target="#formula_10">(8)</ref>) M times via stacking M X-Linear attention blocks, which captures higher (2M th ) order feature interactions.</p><p>Extension with infinity order interactions. One natural way to exploit more higher (even infinity) order feature interactions is to stack plenty of X-Linear attention blocks. Nevertheless, such way inevitably leads to a huge rise in memory demand and computational cost, not to mention the extreme case of stacking infinity blocks. Instead, we adopt a simple but effective method to enable our X-Linear attention block to model infinity order interactions by additionally encoding query Q, each key k i , and each value v i with Exponential Linear Unit (ELU) <ref type="bibr" target="#b3">[4]</ref>, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. That is, the infinity order feature interactions can be approximately modeled via performing bilinear pooling on two exponentially transformed features. Here we demonstrate that such approximation can be proved via Taylor expansion of each element in bilinear vector after exponential transformation. Specifically, given two feature vectors X and Y , the Taylor expansion of bilinear pooling over the exponentially transformed features can be expressed as:</p><formula xml:id="formula_11">exp(W X X) exp(W Y Y) = [exp(W 1 X X) exp(W 1 Y Y), ..., exp(W D X X) exp(W D Y Y)] = [exp(W 1 X X + W 1 Y Y), ..., exp(W D X X + W D Y Y)] = [ ? p=0 ? 1 p (W 1 X X + W 1 Y Y) p , ..., ? p=0 ? D p (W D X X + W D Y Y) p ],<label>(9)</label></formula><p>where W X and W Y are embedding matrices, D denotes the dimension of bilinear vector, W i X /W i Y is the i-th row in W X /W Y . Therefore, this expansion clearly shows that each element in bilinear vector after exponential transformation reflects infinity order interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">X-LAN for Image Captioning</head><p>Recall that our X-Linear attention is a unified attention block, it is feasible to plug X-Linear attention block(s) into image encoder and sentence decoder to capture higher order intra-and inter-modal interactions for image captioning. We next present how to integrate such block(s) into the encoder-decoder structure via our devised X-Linear Attention Networks (X-LAN), as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Notation and Training Strategy</head><p>In the standard task of image captioning, we are given an image I to be described with a natural-language sentence Y 1:T . The sentence Y 1:T = {w 1 , w 2 , ..., w T } is a sequence of T words, where w t is the textual feature of the t-th word. The image I is represented as a set of spatial image region features V = {v i } N i=1 by utilizing Faster R-CNN <ref type="bibr" target="#b26">[27]</ref>. During training, given the ground-truth sentence Y 1:T for image I, we first train our X-LAN by minimizing the cross entropy loss L CE (?) = ? T t=1 log(p ? (w t |Y 1:t?1 )), ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X-Linear Attention</head><p>Embed where ? denotes the parameters of X-LAN. Next, our X-LAN can be further optimized with sentence-level reward via Self-Critical Sequence Training <ref type="bibr" target="#b27">[28]</ref>.</p><formula xml:id="formula_12">Add &amp; Norm Faster R-CNN Embed K (0) V (0) = Q (0) v (0) v (1) V (1+M) ? ? ? vd vd vd v (m) V (m-1) K (m-1) Q (m-1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Encoder with X-Linear Attention</head><p>The image encoder is a module that transforms the input set of spatial image region features V into a series of intermediate states, which are enhanced with the contextual information in between. Here we fully employ our X-Linear attention block(s) to construct the image encoder. As such, the representative capacity of encoded image-level or regionlevel features are strengthened via capturing higher order intra-modal feature interactions. Formally, the image encoder in X-LAN is composed of a stack of (1 + M ) identical layers (M = 3). Each layer includes two components: X-Linear attention block as in Eq. <ref type="bibr" target="#b6">(7)</ref> and keys/values updating module as in Eq. <ref type="bibr" target="#b7">(8)</ref>. Specifically, for the first X-Linear attention block, we take the mean-pooled region featurev (0) = v = 1 N N i=1 v i as the initial input query, coupled with the initial keys/values (i.e., all region features K (0) = V (0) = V). The output is thus the attended image-level featurev <ref type="bibr" target="#b0">(1)</ref> , which will be further fed into the next X-Linear attention block as input query. Meanwhile, the keys/values are updated conditioned on the attended image-level featurev <ref type="bibr" target="#b0">(1)</ref> . After that, we repeat the updating process of query and keys/values in M times via the subsequence M stacked layers. Accordingly, by performing feature enhancement via the image encoder with (1 + M ) X-Linear attention blocks, we can obtain</p><formula xml:id="formula_13">(1 + M ) output attended image-level features {v (m) } 1+M</formula><p>m=1 . Moreover, we treat the updated values V (1+M ) after the final X-Linear attention block as the enhanced region-level features, which are endowed with the higher order intramodal feature interactions in between.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Decoder with X-Linear Attention</head><p>The sentence decoder aims to generate the output sentence conditioned on the enhanced image-level and region-level visual features induced by the image encoder. To further encourage high order inter-modal interactions between visual content and natural sentence, we integrate our X-Linear attention block into attention-based LSTM decoder to perform multi-modal reasoning. In particular, at each decoding time step t, we firstly concatenate the mean-pooled region featurev (0) and all attended image-level features {v (m) } 1+M m=1 , which is further transformed into the global image-level feature? through an embedding layer:</p><formula xml:id="formula_14">v = WG[v (0) ,v (1) , ...,v (1+M ) ],<label>(10)</label></formula><p>where W G is embedding matrix. The input of LSTM is thus set as the concatenation of current input word w t , the global image-level feature?, the previous LSTM hidden state h t?1 , and the pervious context vector c t?1 . After that, we take the output of LSTM h t as input query of X-Linear attention block, whose keys/values are set as the enhanced region-level features V (1+M ) from image encoder. In this way, the output attended featurev d of X-Linear attention block is more representative by capturing the 2 nd order interactions between image features and hidden state. Next, we measure current context vector c t by concatenating the attended featurev d with current LSTM hidden state h t , followed with an embedding layer and a Gated Linear Unit (GLU) <ref type="bibr" target="#b5">[6]</ref>. Such context vector c t is finally leveraged for the prediction of next word w t+1 via a softmax layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Implementation Details</head><p>All the experiments are conducted on the most popular image captioning benchmark COCO <ref type="bibr" target="#b20">[21]</ref>. The whole COCO dataset contains 123,287 images, which includes 82,783 training images, 40,504 validation images, and 40,775 testing images. Each image is equipped with five human-annotated sentences. Note that the annotations for official testing set are not provided and the evaluation over that testing set can only be conducted through online testing server. In addition, we adopt the widely adopted Karpathy split <ref type="bibr" target="#b13">[14]</ref> for offline evaluation. There are 113,287 training images, 5,000 validation images, and 5,000 testing images in the Karpathy split. We pre-process all training sentences by converting them into lower case and dropping the words that occur less than 6 times, leading to the final vocabulary with 9,488 unique words.</p><p>We leverage the off-the-shelf Faster-RCNN pre-trained on ImageNet <ref type="bibr" target="#b6">[7]</ref> and Visual Genome <ref type="bibr" target="#b17">[18]</ref> to extract image region features <ref type="bibr" target="#b1">[2]</ref>. Each original region feature is a 2,048-dimensional vector, which is transformed as the input region feature with the dimension D v = 1,024. Each word is represented as "one-hot" vector. The dimensions of the bilinear query-key representation and the transformed bilinear feature (D B and D c ) in X-Linear attention block is set as 1,024 and 512, respectively. We stack four X-Linear attention blocks (plus ELU) in the image encoder and the sentence decoder is equipped with one X-Linear attention block (plus ELU). The hidden layer size in LSTM decoder is set as 1,024. The whole image captioning architecture are mainly implemented with PyTorch, optimized with Adam <ref type="bibr" target="#b16">[17]</ref>. For the training stage, we follow the training schedule in <ref type="bibr" target="#b30">[31]</ref> to optimize the whole architecture with cross-entropy loss. The warmup steps are set as 10,000 and the mini-batch size is 40. Since low-rank bilinear pooling may lead to slow convergence rate as indicated in <ref type="bibr" target="#b15">[16]</ref>, we set the maximum iteration as 70 epoches. For the training with self-critical training strategy, as in <ref type="bibr" target="#b27">[28]</ref>, we first select the initialization model which is trained with crossentropy loss and achieves best CIDEr score on validation set. After that, the whole architecture is further optimized with CIDEr reward, when the learning rate is set as 0.00001 and the maximum iteration is 35 epoches. At the inference stage, we adopt the beam search strategy and set the beam size as 3. Five evaluation metrics, BLEU@N <ref type="bibr" target="#b24">[25]</ref>, ME-TEOR <ref type="bibr" target="#b2">[3]</ref>, ROUGE-L <ref type="bibr" target="#b19">[20]</ref>, CIDEr <ref type="bibr" target="#b31">[32]</ref>, and SPICE <ref type="bibr" target="#b0">[1]</ref>, are simultaneously utilized to evaluate our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison</head><p>Offline Evaluation. <ref type="table" target="#tab_1">Table 1</ref> summaries the performance comparisons between the state-of-the-art models and our proposed X-LAN on the offline COCO Karpathy test split. Note that for fair comparison, we report the results for each run optimized with both cross entropy loss and CIDEr Score. Meanwhile, we separately show the performances for single models and ensemble/fused models. In general, our X-LAN consistently exhibits better performances than other single models, which include the non-attention baselines (LSTM, LSTM-A) and attention-based methods (SCST, RFNet, and others). The CIDEr score of our X-LAN can achieve 132.0% with CIDEr score optimization, which is to-date the best performance without any model ensemble and makes the absolute improvement over the best competitor AoANet by 2.2%. The performance improvements generally demonstrate the key advantage of exploiting higher and even infinity order interactions via our X-Linear attention block, that facilitate both single-modal feature enhancement and multi-modal reasoning for image captioning. In particular, LSTM-A improves LSTM by em-  <ref type="table" target="#tab_1">B@1  B@2  B@3  B@4  M  R  C  c5  c40  c5  c40  c5  c40  c5  c40  c5  c40  c5  c40  c5</ref> c40 LSTM-A (ResNet-152) <ref type="bibr" target="#b39">[40]</ref> 78 phasising semantic attributes at decoding stage. RFNet and Up-Down further boost the performances by involving attention mechanism that learns to identify selective spatial regions for sentence generation. Moreover, by exploiting rich semantic information in images (e.g., visual relations between objects or scene graph) for sentence generation, GCN-LSTM and SGAE exhibit better performance than Up-Down. Nevertheless, the performances of SGAE are lower than AoANet that enhances conventional visual attention by further measuring the relevance between the attention result and the query. This confirms that improving attention measurement is an effective way to enhance the interaction between visual content and natural sentence and thus boost image captioning. In addition, by integrating X-Linear attention block(s) into encoder and decoder, our X-LAN outperforms AoANet, which demonstrates the merit of mining higher and even infinity intra-and inter-modal interactions. Similar to the observations over single models, an ensemble version of our X-LAN by fusing four models with different initialized parameters obtains better performances than other ensemble models.</p><p>To fully verify the generalizability of our X-Linear attention block for image captioning, we include a variant of our X-LAN (named X-Transformer) by plugging X-Linear attention blocks into Transformer based encoder-decoder structure. <ref type="table" target="#tab_1">Table 1</ref> also shows the performance comparison between Transformer and our X-Transformer. Note that here Transformer denotes our implementation of Transformer-based encoder-decoder structure as in <ref type="bibr" target="#b28">[29]</ref>. Similar to the observations in LSTMbased encoder-decoder structure, X-Transformer boosts up the performances by integrating X-Linear attention blocks into the Transformer-based encoder and decoder. The performance improvements again demonstrate the advantage of exploiting higher order interactions via our X-Linear attention block for image captioning. Online Evaluation. In addition, we evaluate our X-LAN and X-Transformer on the official testing set by submitting the ensemble versions to online testing server. <ref type="table" target="#tab_2">Table  2</ref> details the performances over official testing images with 5 reference captions (c5) and 40 reference captions (c40). Note that here we adopt two common backbones (ResNet-X-LAN: a group of people sitting in a room watching a television Up-Down: a group of people sitting in a room GT1: a group of kids viewing a television in a classroom GT2: a group of people sitting next to each other in front of a TV GT3: students in a classroom watching a lecture on television X-LAN: a coffee cup sitting next to a computer keyboard Up-Down: a computer keyboard and a mouse sitting on a desk GT1: a cup of coffee sitting next to a computer keyboard GT2: a coffee cup is next to a white keyboard GT3: black and white photograph of a cup of coffee and a keyboard X-LAN: a blue semi truck hauling logs on a road Up-Down: a blue truck is parked on the back of a road GT1: a large blue truck hauling many long logs GT2: a large truck is stacked with cut wooden logs GT3: a blue and silver truck with logs trees and wires X-LAN: two little girls eating donuts in a room Up-Down: two girls are eating a piece of pizza GT1: two young girls eating doughnuts together at a home GT2: two girls sitting inside a house while eating donuts GT3: two girls eating donuts in a house X-LAN: a group of cars stopped at a traffic light Up-Down: a truck is driving down a street with a traffic light GT1: the cars and trucks are all stopped at the traffic light GT2: a group of cars that are stopped at a traffic light GT3: cars wait at an intersection on a sunny day 101 <ref type="bibr" target="#b9">[10]</ref> and SENet-154 <ref type="bibr" target="#b10">[11]</ref>) for online evaluation. The results clearly show that compared to all the other published state-of-the-art systems, our X-LAN and X-Transformer exhibit better performances across most metrics. Qualitative Analysis. <ref type="figure" target="#fig_4">Figure 4</ref> showcases several image captioning results of Up-Down and our X-LAN, coupled with human-annotated ground truth sentences (GT). Generally, compared with the captions of Up-Down which are somewhat relevant to image content and logically correct, our X-LAN produces more accurate and descriptive sentences by exploiting higher order intra-and inter-modal interactions. For example, Up-Down generates the phrase of "a truck is driving" that is inconsistent with the visual content for the last image, while "a group of cars stopped" in our X-LAN depicts the visual content more precise. This again confirms the advantage of capturing the high order interactions among image regions and meanwhile triggering high order interactions between different modalities for multi-modal reasoning via our X-Linear attention block.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Analysis</head><p>Ablation Study. To fully examine the impact of X-Linear attention block(s) in both image encoder and sentence decoder for image captioning, we conduct ablation study by comparing different variants of our X-LAN in <ref type="table" target="#tab_4">Table 3</ref>. We start from a base model which is a degraded version of our X-LAN by simply encoding images with Faster R-CNN and further leveraging LSTM with conventional attention module for sentence generation. This ablated base model obtains similar results to Up-Down. Next, we extend the base model by replacing the conventional attention module with our X-Linear attention block in sentence decoder, which obtains better performances. The results indicate that compared to conventional attention module that only exploits 1 st order inter-modal interaction, our X-Linear attention block enhances the capacity of multi-modal reasoning via the modeling of higher order interactions. Furthermore, in order to show the relations between performance and the number of stacked X-Linear attention blocks in image encoder, we include a series of variants by integrating one more X-Linear attention blocks into encoder. In general, stacking more X-Linear attention blocks in image encoder can lead to performance improvements. That basically validates the effectiveness of modeling high order intra-modal interactions between image regions in encoder. However, no further significant performance improvement is observed when stacking four blocks. We speculate that the increased parameters by stacking more blocks might result in overfitting, which somewhat hinder the exploitation of more higher order interaction in this way. Recall that instead of stacking blocks to capture more higher order interactions, we present a simple but effective solution to enable even infinity order feature interactions by equip-ping X-Linear attention block with ELU in a parameter-free fashion. As such, when upgrading our X-Linear attention block with ELU in both encoder and decoder, a larger performance gain is attained. Attention Visualization. In order to better qualitatively evaluate the generated results with X-Linear attention block, we visualize the evolutions of attended image regions along the caption generation processes for Up-Down and X-LAN in <ref type="figure" target="#fig_5">Figure 5</ref>. We can observe that Up-Down sometimes focus on the irrelevant image region whose corresponding object should not be generated at that time step. For instance, at the 6 th decoding step for Up-Down, the region containing irrelevant object "apples" is attended, which misleads decoder to produce "apples" again. Instead, by exploiting higher order feature interactions for multi-modal reasoning via X-Linear attention block, our X-LAN consistently focuses on the correct regions for image captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We present a novel unified X-Linear attention block for image captioning, which models the 2 nd order interactions with both spatial and channel-wise bilinear attention. The higher and even infinity order feature interactions can be readily modeled via staking multiple X-Linear attention blocks and equipping the block with Exponential Linear Unit (ELU). To verify our claim, we devise X-Linear Attention Networks (X-LAN) by integrating X-Linear attention block(s) into image encoder and sentence decoder to exploit higher order intra and inter-modal interactions for image captioning. Extensive experiments conducted on COCO dataset demonstrate the efficacy of X-Linear attention block and X-LAN. More remarkably, we obtain new state-of-theart performances on this captioning dataset with X-LAN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A schematic diagram of X-Linear attention block plus ELU to capture infinity order feature interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Overview of our X-Linear Attention Networks (X-LAN) for image captioning. Faster R-CNN is firstly utilized to detect a set of image regions. Next, a stack of X-Linear attention blocks are leveraged in image encoder to encode the region-level features with the higher order intra-modal interaction in between, leading to a set of enhanced region-level and image-level features. Depending on the enhanced visual features, X-Linear attention block is further adopted in sentence decoder to perform multi-modal reasoning. This encourages the exploration of high order inter-modal interactions between visual content and natural sentence to boost sentence generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Examples of image captioning results by Up-Down and X-LAN, coupled with the corresponding ground truth sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The visualization of attended image regions along the caption generation processes for (a) Up-Down and (b) X-LAN. At the decoding step for each word, we outline the image region with the maximum attention weight in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance comparisons on COCO Karpathy test split, where B@N , M, R, C and S are short for BLEU@N , METEOR, ROUGE-L, CIDEr and SPICE scores. All values are reported as percentage (%). indicates model ensemble/fusion.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Cross-Entropy Loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CIDEr Score Optimization</cell><cell></cell><cell></cell></row><row><cell></cell><cell>B@1</cell><cell>B@2</cell><cell>B@3</cell><cell>B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell>S</cell><cell>B@1</cell><cell>B@2</cell><cell>B@3</cell><cell>B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell>S</cell></row><row><cell>LSTM [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.6</cell><cell>25.2</cell><cell>52.6</cell><cell>94.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31.9</cell><cell>25.5</cell><cell>54.3</cell><cell>106.3</cell><cell>-</cell></row><row><cell>SCST [28]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>30.0</cell><cell>25.9</cell><cell>53.4</cell><cell>99.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>34.2</cell><cell>26.7</cell><cell>55.7</cell><cell>114.0</cell><cell>-</cell></row><row><cell>LSTM-A [40]</cell><cell>75.4</cell><cell>-</cell><cell>-</cell><cell>35.2</cell><cell>26.9</cell><cell>55.8</cell><cell>108.8</cell><cell>20.0</cell><cell>78.6</cell><cell>-</cell><cell>-</cell><cell>35.5</cell><cell>27.3</cell><cell>56.8</cell><cell>118.3</cell><cell>20.8</cell></row><row><cell>RFNet [13]</cell><cell>76.4</cell><cell>60.4</cell><cell>46.6</cell><cell>35.8</cell><cell>27.4</cell><cell>56.5</cell><cell>112.5</cell><cell>20.5</cell><cell>79.1</cell><cell>63.1</cell><cell>48.4</cell><cell>36.5</cell><cell>27.7</cell><cell>57.3</cell><cell>121.9</cell><cell>21.2</cell></row><row><cell>Up-Down [2]</cell><cell>77.2</cell><cell>-</cell><cell>-</cell><cell>36.2</cell><cell>27.0</cell><cell>56.4</cell><cell>113.5</cell><cell>20.3</cell><cell>79.8</cell><cell>-</cell><cell>-</cell><cell>36.3</cell><cell>27.7</cell><cell>56.9</cell><cell>120.1</cell><cell>21.4</cell></row><row><cell>GCN-LSTM [38]</cell><cell>77.3</cell><cell>-</cell><cell>-</cell><cell>36.8</cell><cell>27.9</cell><cell>57.0</cell><cell>116.3</cell><cell>20.9</cell><cell>80.5</cell><cell>-</cell><cell>-</cell><cell>38.2</cell><cell>28.5</cell><cell>58.3</cell><cell>127.6</cell><cell>22.0</cell></row><row><cell>LBPF [26]</cell><cell>77.8</cell><cell>-</cell><cell>-</cell><cell>37.4</cell><cell>28.1</cell><cell>57.5</cell><cell>116.4</cell><cell>21.2</cell><cell>80.5</cell><cell>-</cell><cell>-</cell><cell>38.3</cell><cell>28.5</cell><cell>58.4</cell><cell>127.6</cell><cell>22.0</cell></row><row><cell>SGAE [36]</cell><cell>77.6</cell><cell>-</cell><cell>-</cell><cell>36.9</cell><cell>27.7</cell><cell>57.2</cell><cell>116.7</cell><cell>20.9</cell><cell>80.8</cell><cell>-</cell><cell>-</cell><cell>38.4</cell><cell>28.4</cell><cell>58.6</cell><cell>127.8</cell><cell>22.1</cell></row><row><cell>AoANet [12]</cell><cell>77.4</cell><cell>-</cell><cell>-</cell><cell>37.2</cell><cell>28.4</cell><cell>57.5</cell><cell>119.8</cell><cell>21.3</cell><cell>80.2</cell><cell>-</cell><cell>-</cell><cell>38.9</cell><cell>29.2</cell><cell>58.8</cell><cell>129.8</cell><cell>22.4</cell></row><row><cell>X-LAN</cell><cell>78.0</cell><cell>62.3</cell><cell>48.9</cell><cell>38.2</cell><cell>28.8</cell><cell>58.0</cell><cell>122.0</cell><cell>21.9</cell><cell>80.8</cell><cell>65.6</cell><cell>51.4</cell><cell>39.5</cell><cell>29.5</cell><cell>59.2</cell><cell>132.0</cell><cell>23.4</cell></row><row><cell>Transformer [29]</cell><cell>76.1</cell><cell>59.9</cell><cell>45.2</cell><cell>34.0</cell><cell>27.6</cell><cell>56.2</cell><cell>113.3</cell><cell>21.0</cell><cell>80.2</cell><cell>64.8</cell><cell>50.5</cell><cell>38.6</cell><cell>28.8</cell><cell>58.5</cell><cell>128.3</cell><cell>22.6</cell></row><row><cell>X-Transformer</cell><cell>77.3</cell><cell>61.5</cell><cell>47.8</cell><cell>37.0</cell><cell>28.7</cell><cell>57.5</cell><cell>120.0</cell><cell>21.8</cell><cell>80.9</cell><cell>65.8</cell><cell>51.5</cell><cell>39.7</cell><cell>29.5</cell><cell>59.1</cell><cell>132.8</cell><cell>23.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ensemble/Fusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SCST [28]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.8</cell><cell>26.7</cell><cell>55.1</cell><cell>106.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>35.4</cell><cell>27.1</cell><cell>56.6</cell><cell>117.5</cell><cell>-</cell></row><row><cell>RFNet [13]</cell><cell>77.4</cell><cell>61.6</cell><cell>47.9</cell><cell>37.0</cell><cell>27.9</cell><cell>57.3</cell><cell>116.3</cell><cell>20.8</cell><cell>80.4</cell><cell>64.7</cell><cell>50.0</cell><cell>37.9</cell><cell>28.3</cell><cell>58.3</cell><cell>125.7</cell><cell>21.7</cell></row><row><cell>GCN-LSTM [38]</cell><cell>77.4</cell><cell>-</cell><cell>-</cell><cell>37.1</cell><cell>28.1</cell><cell>57.2</cell><cell>117.1</cell><cell>21.1</cell><cell>80.9</cell><cell>-</cell><cell>-</cell><cell>38.3</cell><cell>28.6</cell><cell>58.5</cell><cell>128.7</cell><cell>22.1</cell></row><row><cell>SGAE [36]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.0</cell><cell>-</cell><cell>-</cell><cell>39.0</cell><cell>28.4</cell><cell>58.9</cell><cell>129.1</cell><cell>22.2</cell></row><row><cell>HIP [39]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.0</cell><cell>28.6</cell><cell>57.8</cell><cell>120.3</cell><cell>21.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>39.1</cell><cell>28.9</cell><cell>59.2</cell><cell>130.6</cell><cell>22.3</cell></row><row><cell>AoANet [12]</cell><cell>78.7</cell><cell>-</cell><cell>-</cell><cell>38.1</cell><cell>28.5</cell><cell>58.2</cell><cell>122.7</cell><cell>21.7</cell><cell>81.6</cell><cell>-</cell><cell>-</cell><cell>40.2</cell><cell>29.3</cell><cell>59.4</cell><cell>132.0</cell><cell>22.8</cell></row><row><cell>X-LAN</cell><cell>78.8</cell><cell>63.4</cell><cell>49.9</cell><cell>39.1</cell><cell>29.1</cell><cell>58.5</cell><cell>124.5</cell><cell>22.2</cell><cell>81.6</cell><cell>66.6</cell><cell>52.3</cell><cell>40.3</cell><cell>29.8</cell><cell>59.6</cell><cell>133.7</cell><cell>23.6</cell></row><row><cell>X-Transformer</cell><cell>77.8</cell><cell>62.1</cell><cell>48.6</cell><cell>37.7</cell><cell>29.0</cell><cell>58.0</cell><cell>122.1</cell><cell>21.9</cell><cell>81.7</cell><cell>66.8</cell><cell>52.6</cell><cell>40.7</cell><cell>29.9</cell><cell>59.7</cell><cell>135.3</cell><cell>23.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Leaderboard of the published state-of-the-art image captioning models on the COCO online testing server, where B@N , M, R, and C are short for BLEU@N , METEOR, ROUGE-L, and CIDEr scores. All values are reported as percentage (%).</figDesc><table><row><cell>Model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on the use of X-Linear attention block(s) in image encoder and sentence decoder (optimized with cross-entropy loss), where B@N , M, R, and C are short for BLEU@N , METEOR, ROUGE-L, and CIDEr. All values are reported as percentage (%).</figDesc><table><row><cell>Image Encoder</cell><cell></cell><cell></cell><cell>Sentence Decoder</cell><cell></cell><cell>B@1</cell><cell>B@2</cell><cell>B@3</cell><cell>B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell>S</cell></row><row><cell>Faster R-CNN</cell><cell></cell><cell></cell><cell cols="2">LSTM + Conventional attention</cell><cell>76.4</cell><cell>60.3</cell><cell>46.7</cell><cell>36.1</cell><cell>27.9</cell><cell>56.7</cell><cell>114.1</cell><cell>20.9</cell></row><row><cell>Faster R-CNN</cell><cell></cell><cell></cell><cell cols="2">LSTM + X-Linear attention</cell><cell>76.9</cell><cell>60.9</cell><cell>47.3</cell><cell>36.6</cell><cell>28.2</cell><cell>57.0</cell><cell>117.0</cell><cell>21.2</cell></row><row><cell cols="2">Faster R-CNN + 1?X-Linear attention</cell><cell></cell><cell cols="2">LSTM + X-Linear attention</cell><cell>77.3</cell><cell>61.5</cell><cell>47.9</cell><cell>37.1</cell><cell>28.5</cell><cell>57.3</cell><cell>118.2</cell><cell>21.6</cell></row><row><cell cols="2">Faster R-CNN + 2?X-Linear attention</cell><cell></cell><cell cols="2">LSTM + X-Linear attention</cell><cell>77.5</cell><cell>61.9</cell><cell>48.4</cell><cell>37.7</cell><cell>28.6</cell><cell>57.7</cell><cell>119.4</cell><cell>21.6</cell></row><row><cell cols="2">Faster R-CNN + 3?X-Linear attention</cell><cell></cell><cell cols="2">LSTM + X-Linear attention</cell><cell>77.7</cell><cell>62.2</cell><cell>48.6</cell><cell>37.8</cell><cell>28.6</cell><cell>57.7</cell><cell>120.0</cell><cell>21.6</cell></row><row><cell cols="2">Faster R-CNN + 4?X-Linear attention</cell><cell></cell><cell cols="2">LSTM + X-Linear attention</cell><cell>77.8</cell><cell>62.3</cell><cell>48.7</cell><cell>37.8</cell><cell>28.6</cell><cell>57.8</cell><cell>120.4</cell><cell>21.6</cell></row><row><cell cols="3">Faster R-CNN + 4?X-Linear attention (+ELU)</cell><cell cols="2">LSTM + X-Linear attention (+ELU)</cell><cell>78.0</cell><cell>62.3</cell><cell>48.9</cell><cell>38.2</cell><cell>28.8</cell><cell>58.0</cell><cell>122.0</cell><cell>21.9</cell></row><row><cell>A</cell><cell>bowl</cell><cell>of</cell><cell>apples</cell><cell>and</cell><cell>apples</cell><cell></cell><cell>on</cell><cell></cell><cell>a</cell><cell>table</cell></row><row><cell></cell><cell></cell><cell cols="5">(a) Up-Down: A bowl of apples and apples on a table</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell>bowl</cell><cell>of</cell><cell>apples</cell><cell>and</cell><cell cols="2">oranges</cell><cell>on</cell><cell></cell><cell>a</cell><cell>table</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">(b) X-LAN: A bowl of apples and oranges on a table</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07483</idno>
		<title level="m">Continuously differentiable exponential linear units</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoderdecoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent fusion network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointing novel objects in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Explain images with multimodal recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Look back and predict forward in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional auto-encoding of sentence topics for image paragraph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in image captioning for learning novel objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchy parsing for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Boosting image captioning with attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
