<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PHNNs: Lightweight Neural Networks via Parameterized Hypercomplex Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Graduate Student Member, IEEE</roleName><forename type="first">Eleonora</forename><surname>Grassucci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aston</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Danilo</forename><forename type="middle">Comminiello</forename></persName>
						</author>
						<title level="a" type="main">PHNNs: Lightweight Neural Networks via Parameterized Hypercomplex Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Hypercomplex Neural Networks</term>
					<term>Kronecker Decomposition</term>
					<term>Lightweight Neural Networks</term>
					<term>Quaternions</term>
					<term>Ef- ficient models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hypercomplex neural networks have proven to reduce the overall number of parameters while ensuring valuable performance by leveraging the properties of Clifford algebras. Recently, hypercomplex linear layers have been further improved by involving efficient parameterized Kronecker products. In this paper, we define the parameterization of hypercomplex convolutional layers and introduce the family of parameterized hypercomplex neural networks (PHNNs) that are lightweight and efficient large-scale models. Our method grasps the convolution rules and the filter organization directly from data without requiring a rigidly predefined domain structure to follow. PHNNs are flexible to operate in any user-defined or tuned domain, from 1D to nD regardless of whether the algebra rules are preset. Such a malleability allows processing multidimensional inputs in their natural domain without annexing further dimensions, as done, instead, in quaternion neural networks for 3D inputs like color images. As a result, the proposed family of PHNNs operates with 1/n free parameters as regards its analog in the real domain. We demonstrate the versatility of this approach to multiple domains of application by performing experiments on various image datasets as well as audio datasets in which our method outperforms real and quaternion-valued counterparts. Full code is available at: https://github.com/eleGAN23/HyperNets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>4-channel signals <ref type="bibr" target="#b29">[30]</ref>, while the octonion one is suitable for 8D inputs. Unfortunately, most common color image datasets contain RGB images and some tricks are required to process this data type with QNNs. Among them, the most employed are padding a zero channel to the input in order to encapsulate the image in the four quaternion components, or remodelling the QNN layer with the help of vector maps <ref type="bibr" target="#b30">[31]</ref>. Additionally, while quaternion neural operations are widespread and easy to be integrated in pre-existing models, very few attempts have been made to extend models to different domain orders. Accordingly, the development of hypercomplex convolutional models for larger multidimensional inputs, such as magnitudes and phases of multichannel audio signals or 16-band satellite images, still remains painful. Moreover, despite the significantly lower number of parameters, these models are often slightly slow with respect to real-valued baselines <ref type="bibr" target="#b31">[32]</ref> and adhoc algorithms may be necessary to improve efficiency <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p><p>Recently, a novel literature branch aims at compress neural networks leveraging Kronecker product decomposition <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, gaining considerable results in terms of model efficiency <ref type="bibr" target="#b35">[36]</ref>. Lately, a parameterization of hypercomplex multiplications have been proposed to generalize hypercomplex fully connected layers by sum of Kronecker products <ref type="bibr" target="#b36">[37]</ref>. The latter method obtains high performance in various natural language processing tasks by also reducing the number of overall parameters. Other works extended this approach to graph neural networks <ref type="bibr" target="#b37">[38]</ref> and transfer learning <ref type="bibr" target="#b38">[39]</ref>, proving the effectiveness of Kronecker product decomposition for hypercomplex operations. However, no solution exists for convolutional layers yet, which remain the most employed layers when dealing with multidimensional inputs, such as images and audio signals <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>.</p><p>In this paper, we devise the family of parameterized hypercomplex neural networks (PHNNs), which are lightweight large-scale hypercomplex neural models admitting any multidimensional input, whichever the number of dimensions. At the core of this novel set of models, we propose the parameterized hypercomplex convolutional (PHC) layer. Our method is flexible to operate in domains from 1D to nD, where n can be arbitrarily chosen by the user or tuned to let the model performance lead to the most appropriate domain for the given input data. Such a malleability comes from the ability of the proposed approach to subsume algebra rules to perform convolution regardless of whether these regulations are preset or not. Thus, neural models endowed with our approach adopt 1/n of free parameters with respect to their real-valued counterparts, and the amount of parameter reduction is a user choice. This makes PHNNs adaptable to a plethora of applications in which saving storage memory can be a crucial aspect. Additionally, PHNNs versatility allows processing multidimensional data in its natural domain by simply setting the dimensional hyperparameter n. For instance, color images can be analyzed in their RGB domain by setting n = 3 without adding any useless information, contrary to standard processing for quaternion networks with the padded zero-channel. Indeed, PHC layers are able to grasp the proper algebra from input data, while capturing internal correlations among the image channels and saving 66% of free parameters.</p><p>On a thorough empirical evaluation on multiple benchmarks, we demonstrate the flexibility of our method that can be adopted in different domains of applications, from images to audio signals. We devise a set of PHNNs for large-scale image classification and sound event detection tasks, letting them operate in different hypercomplex domain and with various input dimensionality with n ranging from 2 to 16.</p><p>The contribution of this paper is three-fold. <ref type="bibr">?</ref> We introduce a parameterized hypercomplex convolutional (PHC) layer which grasps the convolution rules directly from data via backpropagation exploiting the Kronecker product properties, thus reducing the number of free parameters to 1/n. <ref type="bibr">?</ref> We devise the family of parameterized hypercomplex neural networks (PHNNs), lightweight and more efficient large-scale hypercomplex models. Thanks to the proposed PHC layer and to the method in <ref type="bibr" target="#b36">[37]</ref> for fully connected layers, PHNNs can be employed with any kind of input and pre-existing neural models. To show the latter, we redefine common ResNets, VGGs and Sound Event Detection networks (SEDnets), operating in any user-defined domain just by choosing the hyperparameter n, which also drives the number of convolutional filters. ? We show how the proposed approach can be employed with any kind of multidimensional data by easily changing the hyperparameter n. Indeed, by setting n = 3 a PHNN can process RGB images in their natural domain, while leveraging the properties of hypercomplex algebras, allowing parameter sharing inside the layers and leading to a parameter reduction to 1/3. To the best of our knowledge, this is the first approach that processes color images with hypercomplex-based neural models without adding any padding channel. As well, multichannel audio signals can be analysed by simply considering n = 4 for standard first-order ambisonics (which has 4 microphone capsules), n = 8 for an array of two ambisonics microphones, or even n = 16 if we want to include the information of each channel phase.</p><p>The rest of the paper is organized as follows. In Section II, we introduce concepts of hypercomplex algebra and we recapitulate real and quaternion-valued convolutional layers. Section III rigorously introduces the theoretical aspects of the proposed method. Sections IV and V reveal how the approach can be adopted in different neural models and in two different domains, the images and audio one, expounding how to <ref type="figure">Fig. 1</ref>. Example of hypercomplex multiplication table for n = 2 i.e., complex, among others (green line), n = 4 i.e., quaternions, tessarines, (blue line) and n = 8, i.e., octonions, bi-quaternions, and so on (red line). While for these domains algebra rules exist and are predefined, no regulations are set for other domains such as n = 3, 5, 6, 7 (dashed grey lines). The parameterized hypercomplex approaches are able to learn these missing algebra rules from data, thus defining hypercomplex multiplication and convolution for any desired domain. process RGB images with n = 3 and multichannel audio with n up to 8. The experimental evaluation is presented in Section VI for image classification and in Section VII for sound event detection. Finally, Section VIII reports the ablation studies we conduct and in Section IX we draw conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HYPERCOMPLEX NEURAL NETWORKS A. Hypercomplex Algebra</head><p>Hypercomplex neural networks rely in a hypercomplex number system based on the set of hypercomplex numbers H and their corresponding algebra rules to shape additions and multiplications <ref type="bibr" target="#b23">[24]</ref>. These operations should be carefully modelled due to the interactions among imaginary units that may not behave as real-valued numbers. For instance, <ref type="figure">Figure 1</ref> reports an example of a multiplication table for complex (green), quaternion (blue) and octonion (red) numbers. However, this is just a small subset of the hypercomplex domain that exist. Indeed, for n = 4 there exist quaternions, tessarines, among others, while for n = 8 octonions, dual-quaternions, and so on. Each of these domains have different multiplication rules due to dissimilar imaginary units interactions. A generic hypercomplex number is defined as</p><formula xml:id="formula_0">h = h 0 + h i?i + . . . + h n?n , i = 1, . . . , n<label>(1)</label></formula><p>being h 0 , . . . , h n ? R and? i , . . . ,? n imaginary units. Different subsets of the hypercomplex domain exist, including complex, quaternion, and octonion, among others. They are identified by the number of imaginary units they employ and by the properties of their vector multiplication. The quaternion domain is one of the most popular for neural networks thanks to the Hamilton product properties. This domain has its foundations in the quaternion number q = q 0 + q 1? + q 2? + q 3? , in which q c , c ? {0, 1, 2, 3} are real coefficients and?,?,? the imaginary untis. A quaternion with its real part q 0 equal to 0 is named pure quaternion. The imaginary units comply with the property? 2 =? 2 =? 2 = ?1 and with the noncommutative products?? = ???;?? = ???;?? = ???. Due to the non-commutativity of vector multiplication, the Hamilton product has been introduced to properly model the multiplication between two quaternions. . . . <ref type="figure">Fig. 2</ref>. The quaternion convolution rule can be expressed as sum of Kronecker products between the matrices A i that subsume the algebra rules and the matrices F i that contain the convolution filters, with i = 1, 2, 3, 4. In this example, the parameters of A i are fixed for visualization purposes, but in PHC layers they are learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real and Quaternion-Valued Convolutional Layers</head><p>A generic convolutional layer can be described by</p><formula xml:id="formula_1">y = Conv(x) = W * x + b,<label>(2)</label></formula><p>where the input x ? R t?s is convolved ( * ) with the filters tensor W ? R s?d?k?k to produce the output y ? R d?t , where s is the input channels dimension, d the output one, k is the filter size, and t is the input and output dimension. The bias term b does not heavily influence the number of parameters, thus the degrees of freedom for this operation are essentially O(sdk 2 ). Quaternion convolutional layers, instead, build the weight tensor W ? R s?d?k?k by following the Hamilton product rule and organize filters according to it:</p><formula xml:id="formula_2">W * x = ? ? ? ? W 0 ?W 1 ?W 2 ?W 3 W 1 W 0 ?W 3 W 2 W 2 W 3 W 0 ?W 1 W 3 ?W 2 W 1 W 0 ? ? ? ? * ? ? ? ? x 0 x 1 x 2 x 3 ? ? ? ? (3) where W 0 , W 1 , W 2 , W 3 ? R s 4 ? d 4</formula><p>?k?k are the real coefficients of the quaternion weight matrix W = W 0 + W 1? + W 2? + W 3? and x 0 , x 1 , x 2 , x 3 are the coefficients of the quaternion input x with the same structure.</p><p>As done for real-valued layers, the bias can be ignored and the degree of freedom computations of the quaternion convolutional layer can be approximated to O(sdk 2 /4). The lower number of parameters with respect to the real-valued operation is due to the reuse of filters performed by the Hamilton product in Eq. 3. Also, sharing the parameter submatrices forces to consider and exploit the correlation between the input components <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PARAMETERIZING HYPERCOMPLEX CONVOLUTIONS</head><p>In the following, we delineate the formulation for the proposed parameterized hypercomplex convolutional (PHC) layer. We also show that this approach is capable of learning the Hamilton product rule when two quaternions are convolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Parameterized Hypercomplex Convolutional Layers</head><p>The PHC layer is based on the construction, by sum of Kronecker products, of the weight tensor H which encapsulates and organizes the filters of the convolution. The proposed method is formally defined as:</p><formula xml:id="formula_3">y = PHC(x) = H * x + b,<label>(4)</label></formula><p>whereby, H ? R s?d?k?k is built by sum of Kronecker products between two learnable groups of matrices. Here, s is the input dimensionality to the layer, d is the output one, and k is the filter size. More concretely,</p><formula xml:id="formula_4">H = n i=1 A i ? F i ,<label>(5)</label></formula><p>in which A i ? R n?n with i = 1, ..., n are the matrices that describe the algebra rules and F i ? R s n ? d n ?k?k represents the i-th batch of filters that are arranged by following the algebra rules to compose the final weight matrix. It is worth noting that s n ? d n ? k ? k holds for squared kernels, while s n ? d n ? k should be considered instead for 1D kernels. The core element of this approach is the Kronecker product <ref type="bibr" target="#b43">[44]</ref>, which is a generalization of the vector outer product that can be parameterized by n. The hyperparameter n can be set by the user who wants to operate in a pre-defined real or hypercomplex domain (e.g., by setting n = 2 the PHC layer is defined in the complex domain, or in the quaternion one if n is set equal to 4, as <ref type="figure">Figure 2</ref> illustrates), or tuned to obtain the best performance from the model. The matrices A i and F i are learnt during training and their values are reused to build the definitive tensor H.</p><p>The degree of freedom of A i and F i are n 3 and sdk 2 /n, respectively. Usually, real world applications employ a large number of filters in layers (s, d = 256, 512, ...) and small values for k. Therefore, frequently sdk 2 n 3 holds. Thus, the degrees of freedom for the PHC weight matrix can be approximated to O(sdk 2 /n). Hence, the PHC layer reduces the number of parameters by 1/n with respect to a standard convolutional layer in real world problems.</p><p>Moreover, when processing multidimensional data with correlated channels, such as color images, rather than mulichannel audio or multisensor signals, PHC layers bring benefits due to the weight sharing among different channels. This allows capturing latent intra-channels relations that standard convolutional networks ignore because of the rigid structure of the weights <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b44">[45]</ref>. The PHC layer is able to subsume hypercomplex convolution rules and the desired domain is specified by the hyperparameter n. Interestingly, by setting n = 1 a real-valued convolutional layer can be represented too. Indeed, standard real layers do not involve parameter sharing, therefore the algebra rules are solely described by the single A ? R 1?1 and the complete set of filters are included in F s?d?k?k . Therefore, the PHC layer fills the gaps left by pre-existing hypercomplex algebras in <ref type="figure">Fig. 1</ref> and subsumes the missing algebra rules directly from data, i.e., the dashed grey lines in <ref type="figure">Fig. 1</ref>. Thus, a neural model equipped with PHC layers can grasp the filter organization also for n = 3, 5, 6, 7 and so on. Moreover, any convolutional model can be endowed with our approach, since PHC layers easily replace standard convolution / transposed convolution operations and the hyperparameter n gives high flexibility to adapt the layer to any kind of input, such as color images, multichannel audio or multisensor signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Tests on Toy Examples</head><p>We test the receptive ability of the PHC layer in two toy problems building an artificial dataset. We highly encourage the reader to take a look at the section tutorials of the GitHub repository https://github.com/eleGAN23/HyperNets for more insights and results on toy examples, including the learned matrices A i . The first task aims at learning the right matrix A to build a quaternion convolutional layer which properly follows the Hamilton rule in Eq. 3. That is, we set n = 4 and the objective is to learn the four matrices A i as they are in the quaternion product in <ref type="figure">Fig. 2</ref>. We build the dataset by performing a convolution with a matrix of filters W ? H, which are arranged following the regulation in Eq. 3, and a quaternion x ? H in input. The target is still a quaternion, named y ? H. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref> (right), the MSE loss of the PHC layer converges very fast, meaning that the layer properly learns the matrix A and the Hamilton convolution.</p><p>The second toy example is a modification of the previous dataset target. Here, we want to learn the matrix A which describes the convolution among two pure quaternions. Therefore, when setting n = 4, the matrix A 1 of a pure quaternion should be complete null. Pure quaternions may be, as an example, an input RGB image and the weights of a hypercomplex convolutional layer since the first channel of RGB images is zero. <ref type="figure" target="#fig_0">Figure 3</ref> (left) displays the convergence of the PHC layer loss during training, proving that the proposed method is able of subsuming hypercomplex convolutional rules when dealing with pure quaternions too.</p><p>[A]</p><formula xml:id="formula_5">(1?1) ? ? ? ? ? ? ? ? ? ? ? F ? ? ? ? ? ? ? ? ? ? (s?d?k?k) = ? ? ? ? ? ? ? ? ? ? H ? ? ? ? ? ? ? ? ? ? (s?d?k?k) [A 1 ] (2?2) ? ? ? F 1 ? ? ( s 2 ? d 2 ?k?k) + [A 2 ] (2?2) ? ? ? F 2 ? ? ( s 2 ? d 2 ?k?k) = ? ? ? ? ? ? ? ? ? ? H ? ? ? ? ? ? ? ? ? ? (s?d?k?k)</formula><p>. . .</p><p>. . .</p><formula xml:id="formula_6">[A 1 ] (n?n) ? F 1 ( s n ? d n ?k?k) + [A 2 ] (n?n) ? F 2 ( s n ? d n ?k?k) + . . . + [A n ] (n?n) ? F n ( s n ? d n ?k?k) = ? ? ? ? ? ? ? ? ? ? H ? ? ? ? ? ? ? ? ? ? (s?d?k?k) .<label>(6)</label></formula><p>5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Demystifying Parameterized Hypercomplex Convolutional Layers</head><p>We provide a formal explanation of the PHC layer to better understand the Kronecker product and how it organizes convolution filters to reduce the overall number of parameters to 1/n. In Eq. 6, we show how the PHC layer generalizes from 1D to nD domains. When subsuming real-valued convolutions in the first line of Eq. 6, the Kronecker product is performed between a scalar A and the filter matrix F, whose dimension is the same as the final weight matrix H, which is s?d?k?k.</p><p>Considering the complex case with n = 2 in the second line of Eq. 6, the algebra is defined in A 1 and A 2 while the filters are contained in F 1 and F 2 , each of dimension 1/2 the final matrix H. Therefore, while the size of the weight matrix H remains unchanged, the parameter size is approximately 1/2 the real one. In the last line of Eq. 6, we can see the generalization of this process, in which the size of matrices F i , i = 1, ..., n is reduced proportionally to n. It is worth noting that, while the parameter size is reduced with growing values of n, the dimension of H remains the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PARAMETERIZED HYPERCOMPLEX NEURAL NETWORKS FOR COLOR IMAGES</head><p>In this section, we describe how PHNNs can be applied for processing color images in hypercomplex domains without needing any additional information to the input and we propose examples of parameterized hypercomplex versions of common computer vision models such as VGGs and ResNets. In order to be consistent with literature, we perform each experiment with a real-valued baseline model, then we compare it with its complex and quaternion counterparts and with the proposed PHNN. Furthermore, we assess the malleability of the proposed approach testing different values of the hyperparameter n, therefore defining parameterized hypercomplex models in multiple domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Process Color Images with PHC Layers</head><p>Different encodes exist to process color images, however, the most common computer vision datasets are comprised of three-channel images in R 3 . In the quaternion domain, RGB images are enclosed into a quaternion and processed as single elements <ref type="bibr" target="#b41">[42]</ref>. The encapsulation is performed by considering the RGB channels as the real coefficients of the imaginary units and by padding a zeros channel as the first real component of the quaternion.</p><p>Here, we propose to leverage the high malleability of PHC layers to deal with RGB images in hypercomplex domains without embedding useless information to the input. Indeed, the PHC can directly operate in R 3 by easily setting n = 3 and process RGB images in their natural domain while exploiting hypercomplex network properties such as parameters sharing. Indeed, the great flexibility of PHC layers allows the user to choose whether processing images in R 4 or R 3 . On one hand, by setting n = 4, the zeros channel is added to the input even so the layer saves the 75% of free parameters. On the other hand, by choosing n = 3 the network does not handle any useless information, notwithstanding, it reduces the number of parameters by solely 66%. This is a trade-off which may depend on the application or on the hardware the user needs. Furthermore, the domain on which processing images can be tuned by letting the performance of the network indicates the best choice for n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameterized Hypercomplex VGGs</head><p>A family of popular methods for image processing is based on the VGG networks <ref type="bibr" target="#b45">[46]</ref> that stack several convolutional layers and a closing fully connected classifier. To completely define models in the desired hypercomplex domain, we propose to endow the network with PHC layers as convolution components and with Parameterized Hypercomplex Multiplication (PHM) layers <ref type="bibr" target="#b36">[37]</ref> as linear classifier. The backbone of our PHVGG is then</p><formula xml:id="formula_7">h t = ReLU (PHC t (h t?1 )) t = 1, ..., j y = ReLU (PHM(h j )) .<label>(7)</label></formula><p>C. Parameterized Hypercomplex ResNets</p><p>In recent literature, a copious set of high performance in image classification is obtained with models having a residual structure. ResNets <ref type="bibr" target="#b46">[47]</ref> pile up manifold residual blocks composed of convolutional layers and identity mappings. A generic PHResNet residual block is defined by</p><formula xml:id="formula_8">y = F(x, {H j }) + x,<label>(8)</label></formula><p>whereby H j are the PHC weights of layer j = 1, 2 in the block, and F is</p><formula xml:id="formula_9">F(x, {H j }) = PHC (ReLU (PHC(x))) ,<label>(9)</label></formula><p>in which we omit batch normalization to simplify notation. The backward phase of a PHNNs reduces to a backpropagation similar to the quaternion neural networks one, which has been already developed in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PARAMETERIZED HYPERCOMPLEX NEURAL NETWORKS FOR MULTICHANNEL SIGNALS</head><p>In the following, we expound how PHNNs can be employed to deal with multichannel audio signals and we introduce, as an example, the parameterized hypercomplex Sound Event Detection networks (PHSEDnets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Process multichannel audio with PHC layers</head><p>A first-order Ambisonics (FOA) signal is composed of 4 microphone capsules, whose magnitude representations can be enclosed in a quaternion <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. However, the quaternion algebra may be restrictive if more than one microphone is employed for registration or whether the phase information has to be included too. Indeed, quaternion neural networks badly fit with multidimensional input with more than 4 channels <ref type="bibr" target="#b50">[51]</ref>.</p><p>Conversely, the proposed method can be easily adapted to deal with these additional dimensions by handily setting the hyperparameter n and thus completely leveraging each information in the n-dimensional input.   <ref type="figure">Fig. 4</ref>. CIFAR10 accuracy against number of network parameters for VGG and ResNet models. The larger is the point, the higher is the standard deviation over the runs. PHC-based models obtain better accuracies in both the families while far reducing the number of parameters. We do not display Complex VGGs as their accuracy is very low with respect to other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameterized Hypercomplex SEDnets</head><p>Sound Event Detection networks (SEDnets) <ref type="bibr" target="#b51">[52]</ref> are comprised of a core convolutional component which extracts features from the input spectrogram. The information is then passed to a gated recurrent unit (GRU) module and to a stack of fully connected (FC) layers with a closing sigmoid ? which outputs the probability the sound is in the audio frame. Formally, the PHSEDnet is described by</p><formula xml:id="formula_10">h t = PHC t (h t?1 ) t = 1, ..., j y = ? (FC (GRU (h j ))) .<label>(10)</label></formula><p>After the GRU model, We employ standard fully connected layers, that can be also implemented as PHM layers with n = 1, since the so processed signal loses its multidimensional original structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL EVALUATION ON IMAGE CLASSIFICATION</head><p>To begin with, we test the PHC layer on RGB images and we show how, exploiting the correlations among channels, the proposed method saves parameters while ensuring high performance. We perform each experiment with a real-valued baseline model and then we compare it with its complex and quaternion counterparts and with the proposed PHNNs. Furthermore, we assess the malleability of the proposed approach testing different values of the hyperparameter n, therefore defining parameterized hypercomplex models in multiple domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We perform the image classification task with five baseline models. We consider ResNet18, ResNet50 and ResNet152 from the ResNet family and VGG16 and VGG19 from the VGG one. Each hyperparameter is set according to the original papers <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. We investigate the performance in four  <ref type="table">Table II</ref> in each of the runs. The PHC-based models with n = 3 (red bar) far exceeds other configurations being the more performing choice for RGB image classification task. different color images datasets at different scales. We employ SVHN, CIFAR10, CIFAR100, and ImageNet and any kind of data augmentation is applied to these datasets in order to guarantee a fair comparison. We modify the number of filters for ResNets in order to be divisible by 3 and thus having the possibility of testing a configuration with n = 3. The modified versions of the ResNets are built with an initial convolutional layer of 60 filters. Then, the subsequent blocks have 60, 120, 240, 516 filters. The number of layers in the blocks depends on the ResNet chosen, whether 18, 50 or 152. Instead, VGG19 convolution component comprise two 24, two 72, four 216, and eight 648 filter layers, with batch normalization. The classifier is composed of three fully connected layers of 648, 516 and 10, 100 or 1000 depending on the number of classes in the dataset. The rest of the hyperparameters are set as suggested in the original papers. The batch size is fixed to 128 and training is performed via SGD optimizer with momentum equal to 0.9, weight decay 5e ?4 and a cosine annealing scheduler. For ResNets, the initial learning rate is set to 0.1. For VGG is equal to 0.01. Models on CIFAR10 and CIFAR100 are trained for 200 epochs whereas on SVHN networks run for 50 epochs. For the ImageNet dataset, we follow the recipes in <ref type="bibr" target="#b52">[53]</ref>, so we resize the images for training at 160 ? 160 while keeping the standard size of 224 ? 224 for validation and test. We employ a step learning rate decay every 30 epochs with ? = 0.1, the SGD optimizer and an initial learning rate of 0.1 with weight decay 0.0001. The training is performed for 300k iterations with a batch size of 256 employing four Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>We execute initial experiments with VGGs against Quaternion VGGs and two versions of PHVGGs with n equal to 2 and to 4. Average and standard deviation accuracy over three runs are reported for SVHN and CIFAR10 datasets in <ref type="table">Table I</ref>. We experiment also additional runs but any <ref type="table">significant  TABLE I  IMAGE CLASSIFICATION RESULTS FOR VGG. THE ACCURACY MEAN AND STANDARD DEVIATION OVER THREE RUNS WITH DIFFERENT SEEDS IS  REPORTED. TRAINING (T) TIME AND INFERENCE (I) TIME REQUIRED ON CIFAR10. FOR TRAINING TIME WE REPORT, IN SECONDS PER 100   ITERATIONS, THE MEAN AND THE STANDARD DEVIATION OVER THE ITERATIONS IN ONE EPOCH, WHILE THE INFERENCE TIME IS THE TIME REQUIRED   TO DECODE THE TEST</ref>  difference emerges as the randomness only affects the network initialization. Both the PHVGG16 and PHVGG19 versions clearly outperform real, complex and quaternion counterparts while being built with more than a half the number of parameters of the baseline. Additionally, PH-based models extraordinarily reduce the number of training and inference time (computed on an NVIDIA Tesla-V100) required with respect to the quaternion model which operates in a hypercomplex domain as well. Furthermore, when scaling up the experiment with VGG19, the proposed methods are more efficient at inference time with respect to the real-valued VGG19. Therefore, PHNNs can be easily adopted in applications with disk memory limitations, due to the reduction of parameters, and for fast inference problems thanks to the efficiency at testing time. Although the sum of Kronecker products in PHC layers requires additional computations, the increase is insignificant with respect to the FLOPs computated for the whole network, so the overall number of FLOPs is not heavily affected by our method and the count remains almost the same. Our approach has high malleability, indeed, when dealing with color images, we can the domain in which operating thanks to the hyperparameter n. Therefore, we test PHNNs in the complex (n = 2), quaternion (n = 4) or H 3 (n = 3) domain, where in the latter we do not concatenate any zero padding and process the RGB channels of the image in their natural domain. <ref type="table">Table II</ref> presents average and standard deviation accuracy over three runs with different seeds for ResNet-based models. We perform extensive experiments and the PH models with n = 4 always outperform the quaternion counterpart gaining a higher accuracy and being more robust. This underlines the effectiveness of the PHC architectural flexibility over the predefined and rigid structure of quaternion layers. Furthermore, our method distinctly far exceeds the corresponding real-valued baselines across the experiments while saving from 50% to 75% parameters. Focusing on the latter result, the PHResNets with n = 3 results to be the most suitable choice in many cases, proving the validity of processing RGB images in their natural domain leveraging hypercomplex algebra. However, performance with n = 3 and n = 4 are comparable, thus the choice of this hyperparameter may depend on the application or on the hardware employed. On one hand, n = 4 may sometimes lead to lower performance, nevertheless it allows saving disk memory, as shown in the third column of <ref type="table">Table II</ref>, thus it may be more appropriate for edge applications.</p><p>On the other hand, processing color images with n = 3 may bring higher accuracy even so it requires more parameters. Therefore, such a flexibility makes PHNNs adaptable to a large range of applications. Likewise, PHResNets with n = 2 gain considerable accuracy scores with respect to the realvalued corresponding models and, due to the larger number of parameters with respect to the PH model with n = 3, sometimes outperform it too. Finally, the PHResNet with n = 4 obtains the overall best accuracy in the largest experiment of this set. Indeed, considering a ResNet152 backbone on CIFAR100, our method exceeds the real-valued baseline by more than 4%. This is the empirical proof that, PHNNs well scale to large real-world problems by notably reducing the overall number of parameters. These results are summarized for ResNets and VGGs models on CIFAR10 in <ref type="figure">Fig. 4</ref>. The plot displays models accuracies against models parameters. The PH-based models, either ResNets or VGGs exceed their real and quaternion-valued baselines while consistently reduce the number of parameters. What is more, in <ref type="table">Table II</ref>, we also report the memory required to store models checkpoints for inference. Our method crucially reduces the amount of disk memory demand with respect to the heavier real-valued model. Further, we perform the image classifcation task on the ImageNet dataset. We compute the percentage of successes of ResNet-based models in each run for which we report the average accuracies in <ref type="table">Table II</ref>. As <ref type="figure" target="#fig_2">Fig. 5</ref> shows, the largest parcentage of successes is reached by the PHResNet with n = 3 which has been demonstrated to be the most valuable choice for n when dealing with RGB images. Therefore, we test the PHResNet with n = 3 against the real-valued counterpart. <ref type="table" target="#tab_3">Table III</ref> shows that the proposed method achieves comparable, and even slightly superior, performance than the real-valued baseline, while involving 66% fewer parameters. Additionally, in <ref type="figure" target="#fig_3">Fig.6</ref>, we provide Grad-CAM visualizations <ref type="bibr" target="#b53">[54]</ref> for a sample of predictions by our method in the ImageNet dataset to further prove the correct behavior of the PHRes-Net50 n = 3 in this scenario. This proves the robustness of the proposed approach, which can be adopted and implemented in models at different scales. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTAL EVALUATION ON SOUND EVENT DETECTION</head><p>Sound event detection (SED) is the task of recognizing the sounds classes and at what temporal instances these sounds are active in an audio signal <ref type="bibr" target="#b54">[55]</ref>. We prove that the PHC layer is adaptable to n-dimensional input signals and, due to parameter reduction and hypercomplex algebra, is more performing in terms of efficiency and evaluation scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>For sound event detection models we consider the augmented version of the SELDnet <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b51">[52]</ref> which was proposed as baseline for of the L3DAS21 Challenge Task 2 <ref type="bibr" target="#b55">[56]</ref> and we perform our experiments with the corresponding released dataset 1 . We consider as our baselines the SEDnet (without the localization part) and its quaternion counterpart. The L3DAS21 Task 2 dataset contains 15 hours of MSMP Bformat Ambisonics audio recordings, divided in 900 1-minutelong data points sampled at a rate of 32 kHz, where up to 3 acoustic events may overlap. The 14 sounds classes have been selected from the FSD50K dataset and are representative for an office sounds: computer keyboard, drawer open/close, cupboard open/close, finger snapping, keys jangling, knock, laughter, scissors, telephone, writing, chink and clink, printer, female speech, male speech. In this dataset, the volume difference between the sounds is in the range 0 and 20 dB full scale (dBFS). Considering the array of two microphones 1, 2, the channels order is [W1, Z1, Y1, X1, W2, Z2, Y2, X2], where W, X, Y, Z are the B-format ambisonics channels if the phase (p) information is not considered. Whether we want to include also this information, the order will be [W1, Z1, Y1, X1, W1p, Z1p, Y1p, X1p, W2, Z2, Y2, X2, W2p, Z2p, Y2p, X2p] up to 16 channels. In <ref type="figure" target="#fig_4">Fig.7</ref>, we show the 8-channel input when considering one microphone and the phase information. Magnitudes and phases are normalized to be centered in 0 with standard deviation 1.</p><p>We perform experiments with multiple configurations of this dataset. We first test the recordings from one microphone considering the magnitudes only (4 channels input), then we test the networks with the signals recorded by two microphones and magnitudes only (8 channels input). The extracted features by the preprocessing are fed to the four-layer convolutional stack with 64, 128, 256, 512 filters, with batch normalization, ReLU activation, max pooling and dropout (probability 0.3), with pooling sizes (8, 2), (8, 2), (2, 2), <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1)</ref>. The bidirectional GRU module has three layers, each with an hidden size of 256. The tail is a four-layer fully connected classifier with 1024 filters alternated by ReLUs and with a final dropout and a sigmoid activation function. The initial learning rate is set to 0.00001. To be consistent with pre-existing literature metrics , we define True Positives as TP, False Positives as FP and False Negatives as FN. These are computed according to the detection metric <ref type="bibr" target="#b55">[56]</ref>. Moreover, in order to compute the Error Rate (ER), we consider: S = min(FN, FP), D = max(0, FN ? FP) and I = max(0, FP ? FN), as in <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b54">[55]</ref>. Therefore, we consider:</p><formula xml:id="formula_11">F score = 2TP 2TP + FP + FN , ER = S + D + I N ,</formula><p>whereby N is the total number of active sound event classes in the reference. The SED score is defined by: For ER and SED score , the lower scores, the better the performance, while for the F score higher values stand for better accuracy.</p><formula xml:id="formula_12">SED score = ER + 1 ? F score 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>We investigate PHSEDnets in complex, quaternion and octonion domain with n = 2, 4, 8 and train each network for 1000 epochs with a batch size of 16. The proposed parameterized hypercomplex SEDnets distinctly outperform real and quaternion-valued baselines, as reported in <ref type="table" target="#tab_5">Table  IV and Table V</ref>. Indeed, the PHSEDnet with n = 2 gains the best results for each score and in both one and two microphone datasets, proving that the weights sharing due to the hypercomplex parameterization is able to capture more information regardless the lower number of parameters. It is interesting to note that the PHSEDnet n = 4, which operates in the quaternion domain, achieves improved scores with respect to the Quaternion SEDnet that follows the rigid predefined algebra rules. Further, the malleability of PHC layers allows gaining comparable performance with respect to the quaternion baseline even so reducing convolutional parameters by 87%, just setting n = 8. In Section VIII-B, we show additional experimental results of PH models able to save 94% of convolutional parameters while operating in the sedonion domain by involving n = 16.</p><p>Furthermore, PHSEDnets are more efficient in terms of time required for training and inference. <ref type="table">Table V</ref> shows also that each tested version of the proposed method is faster regards as the real SEDnet and the quaternion one, both at training and at inference time. Time efficiency is crucial in audio applications where networks are usually trained for thousands of epochs and datasets are very large and require protracted computations. <ref type="figure">Figure 8</ref> summarises number of parameters, metrics scores and computational time in a radar plot from which it is clear that PHSEDnet n = 2 gains the best scores and a large time saving at a cost of more parameters with respect to other </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ABLATION STUDIES A. Less parameters do not lead to higher generalization</head><p>In the following, we demonstrate that higher accuracies achieved by our method are not caused by the parameter reduction which may lead to more generalization. To this end, we perform multiple experiments. First, we test lighter ResNets that were originally built for the CIFAR10 dataset <ref type="bibr" target="#b46">[47]</ref>: ResNet20, ResNet56 and ResNet110. Second, we consider also the smallest VGG network, that is the VGG11 which has 14M parameters. Finally, we perform experiments Finally, in order to further remove the hypothesis that smaller number of neural parameters leads to higher generalization capabilities, we perform experiments with real-valued baselines with a number of parameters reduced by 75%. <ref type="table" target="#tab_3">Table  VIII</ref> shows that reducing the number of filters downgrades the performance and thus it is not sufficient to improve the generalization capabilities of a model. We do not include standard deviations for values in the ablation studies as the values are similar to the previous examples so we aim at favoring paper readability.</p><p>B. Push the hyperparameter n up to 16</p><p>In the following, we perform additional experiments for the sound event detection task. We conduct a test considering two microphones and the phase information, so to have an input with 16 channels. For this purposes, we consider as baseline the quaternion model and PHNNs with n = 4, 8, 16 so to test higher order domains. Quaternion and PHSEDnet with n = 4 manage the 16 channels by grouping them in four components, thus assembling them in 4 channels: one channel containing the magnitudes of the first microphone, one channel the phases of the same microphone, and so on. Therefore, the details coming from the magnitudes, which are the most important for sound event detection, are grouped together without properly exploiting this information. On the contrary, employing PHC layers allows the model to process information without roughly grouping channels while instead leveraging every information by easily setting n equal to the number of channels, that is in this case 16. From Table IX, it is clear that employing a 4-channel model such as Quaternion or PHC with n = 4 does not lead to higher performance, despite the higher number of parameters. Indeed, the best scores are obtained with PHC models involving n = 8 and n = 16 that are able to grasp information from each channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>In this paper, we introduce a parameterized hypercomplex convolutional (PHC) layer which grasps the convolution rule directly from data and can operate in any domain from 1D to nD, regardless the algebra regulations are preset. The proposed approach reduces the convolution parameters to 1/n with respect to real-valued counterparts and allows capturing internal latent relations thanks to parameter sharing among input dimensions. Employing this method, jointly with the one in <ref type="bibr" target="#b36">[37]</ref>, we devise the family of parameterized hypercomplex neural networks (PHNNs), a set of lightweight and efficient neural models exploiting hypercomplex algebra properties for increased performance and high flexibility. We show our method is flexible to operate in different fields of application by performing experiments with images and audio signals. We also prove the malleability and the robustness of our approach to learn convolution rules in any domain by setting different values for the hyperparameter n from 2 to 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CO2 Emission Related to Experiments</head><p>Experiments were conducted using a private infrastructure, which has a carbon efficiency of 0.445 kgCO 2 eq/kWh. A cumulative of 2000 hours of computation was performed on hardware of type Tesla V100-SXM2-32GB (TDP of 300W). Total emissions are estimated to be 267 kgCO 2 eq of which 0 percents were directly offset. Estimations were conducted using the MachineLearning Impact calculator presented in <ref type="bibr" target="#b56">[57]</ref>.</p><p>More in detail, considering an experiment for the sound event detection (SED) task, according to <ref type="table">Table V</ref>, the realvalued baseline requires approximately 20 hours for training and validation, with a corresponding carbon emissions of 2.71 kgCO 2 eq. Conversely, the proposed PH model takes approximately 17 hours with a reduction of carbon emissions of 16%, being 2.28 kgCO 2 eq.</p><p>In conclusion, we believe that the improved efficiency of our method with respect to standard models may be a little step towards reducing carbon emissions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Loss plots for toy examples. The PHC layer is able to learn the matrix A describing the convolution rule for pure (left) and full quaternions (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Bar plot of number of successes achieves by the models in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Grad-CAM visualization for the PHResNet50 n = 3 on the ImageNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Sample spectrograms from L3DAS21 dataset recorded by one microphone with four capsules.The first four figures represent the magnitudes while the last four contain the corresponding phases information. The black sections represent silent instants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>SET. THE PHNN WITH n = 4 OUTPERFORMS THE QUATERNION COUNTERPART BOTH IN TERMS OF ACCURACY AND TIME. THE PHVGG WITH n = 2 FAR EXCEEDS THE REAL-VALUED BASELINE IN THE CONSIDERED DATASETS, WHILE BOTH THE PHVGG19 VERSIONS WITH n = 2, 4 ARE MORE EFFICIENT THAN THE REAL AND QUATERNION-VALUED BASELINES AT INFERENCE TIME. p-VALUE UNDER THE T-TEST 0.0002. RESULTS WITH RESNET MODELS. EACH EXPERIMENT IS RUN THREE TIMES WITH DIFFERENT SEEDS AND MEAN WITH STANDARD DEVIATION IS REPORTED. THE PROPOSED MODELS FAR EXCEED REAL-VALUED AND QUATERNION BASELINES ALMOST IN EACH EXPERIMENT WE CONDUCT. INTERESTINGLY, THE PHNN OUTPERFORM THE REAL-VALUED COUNTERPART BY 4% POINTS IN THE LARGEST-SCALE EXPERIMENT ON CIFAR100. THE TIME IS SIMILAR TO THE CLAIMS IN TABLE I SO WE DO NOT ADD HERE TO AVOID REDUNDANCY.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Params</cell><cell></cell><cell>SVHN</cell><cell>CIFAR10</cell><cell>Time (T)</cell><cell>Time (I)</cell></row><row><cell>VGG16</cell><cell></cell><cell>15M</cell><cell></cell><cell>94.364 ? 0.394</cell><cell>85.067 ? 0.765</cell><cell>2.2 ? 0.02</cell><cell>1.2</cell></row><row><cell>Complex VGG16</cell><cell></cell><cell cols="2">7.6M (-50%)</cell><cell>93.555 ? 0.392</cell><cell>76.927 ? 0.511</cell><cell>5.2 ? 0.02</cell><cell>1.5</cell></row><row><cell cols="4">Quaternion VGG16 3.8M (-75%)</cell><cell>93.887 ? 0.292</cell><cell>83.997 ? 0.493</cell><cell>5.2 ? 0.02</cell><cell>2.2</cell></row><row><cell cols="2">PHVGG16 n = 2</cell><cell cols="2">7.6M (-50%)</cell><cell cols="2">94.831 ? 0.257 86.510 ? 0.216</cell><cell>3.2 ? 0.02</cell><cell>1.4</cell></row><row><cell cols="2">PHVGG16 n = 4</cell><cell cols="2">3.8M (-75%)</cell><cell>94.639 ? 0.121</cell><cell>85.640 ? 0.205</cell><cell>3.2 ? 0.02</cell><cell>1.4</cell></row><row><cell>VGG19</cell><cell></cell><cell>29.8M</cell><cell></cell><cell>94.140 ? 0.129</cell><cell>85.624 ? 0.257</cell><cell>3.2 ? 0.02</cell><cell>16.0</cell></row><row><cell>Complex VGG19</cell><cell></cell><cell cols="2">14.8M (-50%)</cell><cell>90.469 ? 0.222</cell><cell>76.979 ? 0.345</cell><cell>5.2 ? 0.02</cell><cell>16.2</cell></row><row><cell cols="4">Quaternion VGG19 7.5M (-75%)</cell><cell>93.983 ? 0.190</cell><cell>83.914 ? 0.129</cell><cell>6.2 ? 0.02</cell><cell>16.3</cell></row><row><cell cols="2">PHVGG19 n = 2</cell><cell cols="4">14.9M (-50%) 94.553 ? 0.229 85.750 ? 0.286</cell><cell>4.0 ? 0.02</cell><cell>15.4</cell></row><row><cell cols="2">PHVGG19 n = 4</cell><cell cols="2">7.4M (-75%)</cell><cell>94.169 ? 0.296</cell><cell>84.830 ? 0.733</cell><cell>4.2 ? 0.02</cell><cell>15.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell></row><row><cell>IMAGE CLASSIFICATION Model</cell><cell></cell><cell>Params</cell><cell cols="2">Storage Memory</cell><cell>SVHN</cell><cell>CIFAR10</cell><cell>CIFAR100</cell></row><row><cell>ResNet18</cell><cell cols="2">10.1M</cell><cell cols="2">39MB</cell><cell>93.992 ? 1.317</cell><cell>89.543 ? 0.340</cell><cell>62.634 ? 0.600</cell></row><row><cell>Complex ResNet18</cell><cell cols="2">5.2M (-50%)</cell><cell cols="2">20MB (-50%)</cell><cell>89.902 ? 0.322</cell><cell>89.541 ? 0.412</cell><cell>60.417 ? 0.811</cell></row><row><cell>Quaternion ResNet18</cell><cell cols="2">2.8M (-75%)</cell><cell cols="2">10MB (-75%)</cell><cell>93.661 ? 0.413</cell><cell>88.240 ? 0.377</cell><cell>59.850 ? 0.607</cell></row><row><cell>PHResNet18 n = 2</cell><cell cols="2">5.4M (-50%)</cell><cell cols="2">20MB (-50%)</cell><cell>94.359 ? 0.187</cell><cell>89.260 ? 0.625</cell><cell>60.320 ? 2.249</cell></row><row><cell>PHResNet18 n = 3</cell><cell cols="2">3.6M (-66%)</cell><cell cols="2">13MB (-66%)</cell><cell>94.303 ? 1.234</cell><cell>89.603 ? 0.563 62.660 ? 1.067</cell></row><row><cell>PHResNet18 n = 4</cell><cell cols="2">2.7M (-75%)</cell><cell cols="2">10MB (-75%)</cell><cell>94.234 ? 0.161</cell><cell>88.847 ? 0.874</cell><cell>61.780 ? 0.689</cell></row><row><cell>ResNet50</cell><cell cols="2">22.5M</cell><cell cols="2">86MB</cell><cell>94.546 ? 0.269</cell><cell>89.630 ? 0.305</cell><cell>65.514 ? 0.569</cell></row><row><cell>Complex ResNet50</cell><cell cols="4">11.1M (-50%) 43MB (-50%)</cell><cell>89.004 ? 0.215</cell><cell>89.699 ? 0.485</cell><cell>65.104 ? 0.598</cell></row><row><cell>Quaternion ResNet50</cell><cell cols="2">5.7M (-75%)</cell><cell cols="2">22MB (-75%)</cell><cell>93.685 ? 0.389</cell><cell>89.670 ? 0.383</cell><cell>63.760 ? 0.717</cell></row><row><cell>PHResNet50 n = 2</cell><cell cols="4">11.1M (-50%) 43MB (-50%)</cell><cell>93.849 ? 0.249</cell><cell>89.750 ? 0.386</cell><cell>65.884 ? 0.333</cell></row><row><cell>PHResNet50 n = 3</cell><cell cols="2">7.6M (-66%)</cell><cell cols="2">29MB (-65%)</cell><cell>93.617 ? 0.497</cell><cell>90.423 ? 0.145 66.497 ? 1.256</cell></row><row><cell>PHResNet50 n = 4</cell><cell cols="2">5.7M (-75%)</cell><cell cols="2">23MB (-74%)</cell><cell>94.558 ? 0.754</cell><cell>88.897 ? 0.645</cell><cell>66.240 ? 1.165</cell></row><row><cell>ResNet152</cell><cell cols="2">52.6M</cell><cell cols="2">201MB</cell><cell>94.625 ? 0.355</cell><cell>89.580 ? 0.173</cell><cell>62.053 ? 0.385</cell></row><row><cell>Complex ResNet152</cell><cell cols="4">26.3M (-50%) 101MB (-50%)</cell><cell>90.332 ? 0.129</cell><cell>89.792 ? 0.427</cell><cell>63.125 ? 0.681</cell></row><row><cell cols="5">Quaternion ResNet152 13.2M (-75%) 51MB (-75%)</cell><cell>93.638 ? 0.098</cell><cell>89.227 ? 0.287</cell><cell>61.267 ? 0.784</cell></row><row><cell>PHResNet152 n = 2</cell><cell cols="4">26.6M (-50%) 103MB (-49%)</cell><cell>93.915 ? 0.512</cell><cell>90.540 ? 0.401</cell><cell>65.817 ? 0.327</cell></row><row><cell>PHResNet152 n = 3</cell><cell cols="4">17.8M (-66%) 70MB (-65%)</cell><cell>93.955 ? 0.152</cell><cell>90.077 ? 0.436</cell><cell>66.347 ? 0.567</cell></row><row><cell>PHResNet152 n = 4</cell><cell cols="4">13.4M (-75%) 53 MB (-74%)</cell><cell>94.290 ? 0.237</cell><cell>89.897 ? 0.097</cell><cell>66.437 ? 0.064</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III IMAGENET</head><label>III</label><figDesc>CLASSIFICATION WITH REAL-VALUED BASELINE AGAINST OUR BEST MODEL PH n = 3. OUR APPROACH OUTPERFORM THE BASELINE WHILE SAVING THE 66% OF PARAMETERS.</figDesc><table><row><cell>Model</cell><cell>Params</cell><cell>ImageNet</cell></row><row><cell>ResNet50</cell><cell>25.7M</cell><cell>67.990</cell></row><row><cell>PHResNet50 n = 3</cell><cell>9.6M (-66%)</cell><cell>68.584</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV SEDNETS</head><label>IV</label><figDesc>RESULTS WITH ONE MICROPHONE (4 CHANNELS INPUT). SCORES ARE COMPUTED OVER THREE RUNS WITH DIFFERENT SEEDS AND WE REPORT THE MEAN. THE PROPOSED METHOD WTIH n = 2 FAR EXCEEDS THE BASELINES IN EACH METRIC CONSIDERED. CHANNELS INPUT). SCORES ARE COMPUTED OVER THREE RUNS WITH DIFFERENT SEEDS AND WE REPORT THE MEAN. THE PHSEDNET n = 2 OUTPERFORM THE BASELINES. FOR TRAINING TIME (SECONDS/ITERATION) THE MEAN AND THE STANDARD DEVIATION OVER ONE EPOCH IS REPORTED, FOR INFERENCE TIME WE REPORT THE TIME REQUIRED TO PERFORM AN ITERATION ON THE VALIDATION SET. PH-BASED MODELS FAR EXCEED BASELINES BOTH IN TRAINING AND INFERENCE TIME. SVHN DATASET WITH THE SMALLEST NETWORKS FROM EACH FAMILY, RESNET20 AND VGG11, THE LATTER WITH MODIFIED NUMBER OF FILTERS IN ORDER TO BE DIVIDED BY EACH VALUE OF n AND FC LAYERS IN THE CLOSING CLASSIFIER. WE TEST ALSO THE PHNN WITH n = 1 TO REPLICATE THE REAL DOMAIN WHICH OUTPERFORM THE REAL-VALUED RESNET20. real one. A good trade-off is brought by the PH model n = 4 which further reduces the number of parameters at the cost of slightly worse SED score and ER. Moreover, the real-valued SEDnet is capable of obtaining fair scores while having the largest parameters amount and high computational time demanding.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="6">Conv Params Fscore ? ER ? SEDscore ?</cell><cell>P ?</cell><cell>R ?</cell></row><row><cell cols="2">SEDnet</cell><cell>1.6M</cell><cell cols="2">0.637</cell><cell>0.450</cell><cell>0.406</cell><cell></cell><cell cols="2">0.756 0.5505</cell></row><row><cell cols="3">Quaternion SEDnet 0.4M (-75%)</cell><cell cols="2">0.580</cell><cell>0.516</cell><cell>0.468</cell><cell></cell><cell>0.724</cell><cell>0.484</cell></row><row><cell cols="2">PHSEDnet n = 2</cell><cell>0.8M (-50%)</cell><cell cols="2">0.680</cell><cell>0.389</cell><cell>0.355</cell><cell></cell><cell>0.767</cell><cell>0.611</cell></row><row><cell cols="2">PHSEDnet n = 4</cell><cell>0.4M (-75%)</cell><cell cols="2">0.638</cell><cell>0.453</cell><cell>0.407</cell><cell></cell><cell>0.765</cell><cell>0.547</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SEDNETS RESULTS WITH TWO MICROPHONES (8 Model Conv Params Fscore ? ER ?</cell><cell cols="2">SEDscore ?</cell><cell>P ?</cell><cell>R ?</cell><cell></cell><cell>Time (T)</cell><cell>Time (I)</cell></row><row><cell>SEDnet</cell><cell>1.6M</cell><cell>0.663</cell><cell>0.428</cell><cell cols="2">0.383</cell><cell>0.788</cell><cell>0.572</cell><cell cols="2">1.242 ? 0.088</cell><cell>1.198</cell></row><row><cell cols="2">Quaternion SEDnet 0.4M (-75%)</cell><cell>0.559</cell><cell>0.556</cell><cell cols="2">0.499</cell><cell cols="2">0.754 0.444</cell><cell cols="2">1.308 ? 0.088</cell><cell>1.298</cell></row><row><cell>PHSEDnet n = 2</cell><cell>0.8M (-50%)</cell><cell>0.669</cell><cell>0.406</cell><cell cols="2">0.368</cell><cell>0.767</cell><cell cols="3">0.594 1.091 ? 0.074</cell><cell>1.085</cell></row><row><cell>PHSEDnet n = 4</cell><cell>0.4M (-75%)</cell><cell>0.638</cell><cell>0.433</cell><cell cols="2">0.397</cell><cell cols="2">0.729 0.567</cell><cell cols="2">1.091 ? 0.032</cell><cell>1.077</cell></row><row><cell>PHSEDnet n = 8</cell><cell>0.2M (-87%)</cell><cell>0.553</cell><cell>0.560</cell><cell cols="2">0.503</cell><cell cols="2">0.747 0.439</cell><cell cols="2">1.142 ? 0.042</cell><cell>1.173</cell></row><row><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EXPERIMENTS ON Model</cell><cell>Params</cell><cell>SVHN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet20</cell><cell>0.27M</cell><cell>90.463</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Quaternion ResNet20 0.07M (-75%)</cell><cell>93.535</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PHResNet20 n = 1</cell><cell>0.27M</cell><cell>93.796</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PHResNet20 n = 2</cell><cell>0.14M (-50%)</cell><cell>93.708</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PHResNet20 n = 4</cell><cell>0.07M (-75%)</cell><cell>93.669</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VGG11</cell><cell>13.8M</cell><cell>93.488</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Quaternion VGG11</cell><cell>3.9M (-71%)</cell><cell>92.888</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PHVGG11 n = 2</cell><cell>7.2M (-48%)</cell><cell>93.958</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PHVGG11 n = 3</cell><cell>5.0M (-64%)</cell><cell>93.804</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PHVGG11 n = 4</cell><cell>3.9M (-71%)</cell><cell>93.919</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>versions but the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII THE</head><label>VII</label><figDesc>FIRST LINES REPORT VGG16 RESULTS WITH REAL-VALUED CLASSIFIER FOR QUATERNION AND PHNNS. EXTENSION OF TABLE I. ADDITIONAL EXPERIMENTS WITH RESNET56 AND RESNET110, THE LATTER WITH MODIFIED NUMBER OF FILTERS IN ORDER TO BE DIVIDED BY EACH VALUE OF n. ACCURACY SCORE IS THE MEAN OVER THREE RUNS WITH DIFFERENT SEEDS. ResNet152 reducing the number of filters by 75% so to have the same number of parameters of quaternion and PHNN with n = 4 counterparts. Table VI reports experiments with ResNet20 where we test also n = 1 to replicate the real-valued model, outperforming it. Experiments with VGG11 with modified number of filters in order to be divided by each value of n is also reported in the same table. Finally, inTable VIIwe report experiments on SVHN and CIFAR10 with ResNet56 and ResNet110, the latter with modified number of filters. PH models gain good performance in each test we conduct while reducing the amount of free parameters. Indeed, the PHResNet20s gain almost 94% of accuracy on the SVHN dataset involving just 70k parameters.</figDesc><table><row><cell>Model</cell><cell>Params</cell><cell cols="2">SVHN CIFAR10</cell></row><row><cell>Quaternion VGG16</cell><cell>4.2M (-72%)</cell><cell>94.086</cell><cell>84.126</cell></row><row><cell>PHVGG16 n = 2</cell><cell>7.9M (-62%)</cell><cell>94.885</cell><cell>86.147</cell></row><row><cell>PHVGG16 n = 4</cell><cell>4.2M (-72%)</cell><cell>94.562</cell><cell>85.710</cell></row><row><cell>ResNet56</cell><cell>0.9M</cell><cell>94.116</cell><cell>83.700</cell></row><row><cell>Quaternion ResNet56</cell><cell>0.2M (-75%)</cell><cell>93.664</cell><cell>81.687</cell></row><row><cell>PHResNet56 n = 2</cell><cell>0.4M (-50%)</cell><cell>93.722</cell><cell>83.413</cell></row><row><cell>PHResNet56 n = 4</cell><cell>0.2 (-75%)</cell><cell>94.122</cell><cell>82.720</cell></row><row><cell>ResNet110</cell><cell>16.7M</cell><cell>93.461</cell><cell>84.810</cell></row><row><cell cols="2">Quaternion ResNet110 4.2M (-75%)</cell><cell>92.788</cell><cell>83.920</cell></row><row><cell>PHResNet110 n = 2</cell><cell>8.4M (-50%)</cell><cell>93.746</cell><cell>83.220</cell></row><row><cell>PHResNet110 n = 3</cell><cell>5.6M (-66%)</cell><cell>94.712</cell><cell>85.200</cell></row><row><cell>PHResNet110 n = 4</cell><cell>4.2M (-75%)</cell><cell>94.885</cell><cell>85.280</cell></row><row><cell cols="4">on SVHN, CIFAR10 and CIFAR100 with the larger ResNet18,</cell></row><row><cell>ResNet50 and</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII REAL</head><label>VIII</label><figDesc>-VALUED RESNETS WITH CONVOLUTIONAL FILTERS REDUCED BY 75%, DENOTED BY (S). FULL MODELS EXCEEDS REDUCED VERSIONS IN EACH OF THE EXPERIMENT, PROVING THAT A SMALLER NUMBER OF PARAMETERS DO NOT LEAD TO HIGHER GENERALIZATION CAPABILITIES.</figDesc><table><row><cell>Model</cell><cell>Params</cell><cell cols="3">SVHN CIFAR10 CIFAR100</cell></row><row><cell>ResNet18</cell><cell>10.1M</cell><cell>93.992</cell><cell>89.543</cell><cell>62.634</cell></row><row><cell>ResNet18 (s)</cell><cell>2.7M (-75%)</cell><cell>93.842</cell><cell>88.310</cell><cell>59.590</cell></row><row><cell>ResNet50</cell><cell>22.5M</cell><cell>94.546</cell><cell>89.630</cell><cell>65.514</cell></row><row><cell>ResNet50 (s)</cell><cell>5.7M (-75%)</cell><cell>93.915</cell><cell>89.370</cell><cell>62.450</cell></row><row><cell>ResNet152</cell><cell>52.6M</cell><cell>94.625</cell><cell>89.580</cell><cell>62.053</cell></row><row><cell cols="2">ResNet152 (s) 13.2M (-75%)</cell><cell>94.400</cell><cell>89.001</cell><cell>60.850</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX SED</head><label>IX</label><figDesc>RESULTS WITH TWO MICROPHONE: MAGNITUDES AND PHASES (16 CHANNELS INPUT). WE TEST HIGHER ORDER HYPERCOMPLEX DOMAINS UP TO SEDONIONS BY SETTING n = 16. ALTHOUGH THE INCREDIBLE REDUCTION OF THE NUMBER OF PARAMETERS WITH RESPECT TO THE REAL-VALUED BASELINE IN TABLE V, THE PHNN WITH n = 16 STILL HAS COMPARABLE PERFORMANCE WITH OTHER MODELS. FURTHERMORE, THE PHSEDNET WITH n = 8 OUTPERFORM ALSO THE QUATERNION BASELINE WHICH HAS MORE DEGREES OF FREEDOM.</figDesc><table><row><cell>Model</cell><cell cols="4">Conv Params Fscore ? ER ? SEDscore ?</cell><cell>P ?</cell><cell>R ?</cell></row><row><cell>Quaternion SEDnet</cell><cell>0.4M (-75%)</cell><cell>0.580</cell><cell>0.480</cell><cell>0.450</cell><cell cols="2">0.655 0.520</cell></row><row><cell>PHSEDnet n = 4</cell><cell>0.4M (-75%)</cell><cell>0.585</cell><cell>0.470</cell><cell>0.443</cell><cell cols="2">0.653 0.530</cell></row><row><cell>PHSEDnet n = 8</cell><cell>0.2M (-87%)</cell><cell>0.607</cell><cell>0.466</cell><cell>0.430</cell><cell>0.702</cell><cell>0.534</cell></row><row><cell>PHSEDnet n = 16</cell><cell>0.1M (-94%)</cell><cell>0.588</cell><cell>0.509</cell><cell>0.461</cell><cell>0.734</cell><cell>0.491</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">L3DAS21 dataset and code are available at: https://github.com/l3das/ L3DAS21.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conf. on Artificial Intelligence</title>
		<meeting>the AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wide-sense markov signals on the tessarine domain. a study under properness conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Navarro-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ruiz-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page">108022</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tessarine signal processing under the t-properness condition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Navarro-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Fern?ndez-Alcal?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Jim?nez-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ruiz-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="10" to="100" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quaternion adaptive line enhancer based on singular spectrum analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Took</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Enshaeifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2876" to="2880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simultaneous diagonalisation of the covariance and complementary covariance matrices in quaternion widely linear signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Enshaeifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Stott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Took</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Mandic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="193" to="204" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quaternion projection rule for rotor hopfield neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="900" to="908" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the existence of the exact solution of quaternion-valued neural networks based on a sequence of approximate solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modal regression-based graph representation for noise robust face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the dynamics of hopfield neural networks on unit quaternions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>De Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2464" to="2471" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stability analysis of quaternion-valued neural networks: Decomposition and direct approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4201" to="4211" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quaternion-valued recurrent projection neural networks on unit quaternions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">843</biblScope>
			<biblScope unit="page" from="136" to="152" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A broad class of discrete-time hypercomplex-valued Hopfield neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="54" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A kernel adaptive algorithm for quaternion-valued inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogunfunmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2422" to="2439" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Guest editorial special issue on complex-and hypercomplex-valued neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Aizenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Mandic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1597" to="1599" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using quaternion convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muppidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6309" to="6313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quaternion recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linar?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quaternion generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generative Adversarial Learning: Architectures and Applications</title>
		<editor>R. Razavi-Far, A. Ruiz-Garcia, V. Palade, and J. Schmidhuber</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="57" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lightweight and efficient neural natural language processing with quaternion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1494" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast algorithms for deep octonion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cariow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cariowa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2021-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep octonion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Senhadji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">397</biblScope>
			<biblScope unit="page" from="179" to="191" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hypercomplex-valued recurrent correlation neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">432</biblScope>
			<biblScope unit="page" from="111" to="123" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quaternion factorization machines: A lightweight solution to intricate feature interaction modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A quaternion-valued variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An information-theoretic perspective on proper quaternion variational autoencoders</title>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reduced biquaternion convolutional neural network for color image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extreme learning machines on Cayley-Dickson algebra applied for color image auto-encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Joint Conf. on Neural Netw. (IJCNN)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multichannel quaternion least mean square algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Took</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8524" to="8527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Removing dimensional restrictions on complex/hyper-complex neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Gaudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Int. Conf. on Image Process. (ICIP)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="319" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Algebranets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07360</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast algorithms for quaternion-valued convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cariow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cariowa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="462" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic neural network with Kronecker flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Touati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SKFAC: Training neural networks with faster Kroneckerfactored approximate curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kronecker CP decomposition with fast multiplication for compressing RNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Parameterized hypercomplex graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clevert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16584</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Compacter: Efficient lowrank hypercomplex adapter layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Mahabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04647</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C F</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2103.15808</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoustics, Speech and Signal Process</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey of quaternion neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linar?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep quaternion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gaudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Joint Conf. on Neural Netw. (IJCNN)</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the history of the kronecker product</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pukelsheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Searle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear and Multilinear Algebra</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Quaternion convolutional neural networks for heterogeneous image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linar?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="8514" to="8518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A quaternary version of the back-propagation algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nitta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="2753" to="2756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Quaternion convolutional neural networks for detection and localization of 3D sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="8533" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Quaternion neural networks for 3D sound source localization in reverberant environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Celsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Workshop on Machine Learning for Signal Process</title>
		<meeting><address><addrLine>Espoo, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Dual quaternion ambisonics array for six-degree-of-freedom acoustic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brignone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01851</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sound event localization and detection of overlapping sources using convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nikunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sound event detection: A tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="67" to="83" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">L3DAS21 Challenge: Machine learning for 3D audio signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Gramaccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jamili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marinoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Massaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Medaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nachira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nucciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paglialunga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pennese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Int. Workshop on Machine Learning for Signal Process</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Quantifying the carbon emissions of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dandres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09700</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
