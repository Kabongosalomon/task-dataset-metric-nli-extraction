<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Title Position-wise optimizer: A nature-inspired optimization algorithm Author</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>M.D</roleName><forename type="first">Amir</forename><surname>Valizadeh</surname></persName>
							<email>thisisamirv@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Neuroscience Institute</orgName>
								<orgName type="institution" key="instit2">Tehran University of Medical Sciences</orgName>
								<orgName type="institution" key="instit3">ORCID Keshavarz Blvd</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Title Position-wise optimizer: A nature-inspired optimization algorithm Author</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The human nervous system utilizes synaptic plasticity to solve optimization problems. Previous studies have tried to add the plasticity factor to the training process of artificial neural networks, but most of those models require complex external control over the network or complex novel rules. In this manuscript, a novel nature-inspired optimization algorithm is introduced that imitates biological neural plasticity. Furthermore, the model is tested on three datasets and the results are compared with gradient descent optimization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Neural plasticity in biological neural networks "Neural plasticity" refers to the capacity of the nervous system to modify itself, functionally and structurally, in response to experience or injury 1 . The concept of neural plasticity is mostly built upon the principle of Hebb's rule: if a neuron repeatedly takes part in making another neuron fire, the connection between them is strengthened ("neurons that fire together, wire together") 2 . Neural plasticity is possible through synaptic plasticity. Synaptic plasticity refers to the activity-dependent modification of the strength of synaptic transmission at preexisting synapses (i.e., change in synaptic weight) <ref type="bibr" target="#b2">3</ref> . Synaptic plasticity has long been considered to have a central role in learning and memory formation <ref type="bibr" target="#b3">4</ref> . While in the central nervous system plasticity is mainly based on adaptive changes in neural circuitries and synaptic reorganization, in the peripheral nervous system it is predominantly based on axonal regrowth and neuron addition 5 , which is a much slower, restricted process. Considering this fact, it is proposed that the learning process and formation of new memories are mostly due to modifications in "deeper" (i.e., central) synapses, and the impact of synaptic plasticity of "shallower" (i.e., peripheral) synapses is less influential in those processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization problem</head><p>An optimization problem is a problem of finding the best solution from all feasible solutions 6 . Machine learning (ML) problems, once formulated, can be viewed as optimization problems. For supervised learning problems, the goal would be to find an optimal mapping function ( ) to minimize the loss function ( ) of the training samples 7 , <ref type="bibr" target="#b0">( 1 )</ref> where is the number of training samples, is the parameter of the mapping function, " is the feature vector of the th samples, and " is the corresponding label. The gradient descent method is the earliest and most common optimization method. The idea is to update parameters iteratively in the opposite direction of the gradients of the objective function 8 . The objective function could be written as,</p><formula xml:id="formula_0">min ! 1 + , " , / " , 12 # "$% ,</formula><formula xml:id="formula_1">( ) = 1 2 + , " ? ! / " 12 &amp; # "$% ,<label>( 2 )</label></formula><p>where, <ref type="bibr" target="#b2">( 3 )</ref> and is the number of input features. The gradient descent method alternates between the following two steps until it converges 7 : 1) Derive ( ) for ' to get the gradient corresponding to each ' :</p><formula xml:id="formula_2">! ( ) = + ' ' ( '$% ,</formula><formula xml:id="formula_3">( ) ' = ? 1 + , " ? ! / " 12 ' " # "$% .<label>( 4 )</label></formula><p>2) Update each ' in the opposite of the gradient direction to minimize the objective function:</p><formula xml:id="formula_4">' ) = ' + . 1 + , " ? ! / " 12 ' " , # "$% ( 5 )</formula><p>where is the learning rate. Previous research has proposed that the human brain might also be using a gradient-based method to address optimization problems 9 . Yet again, there is a huge gap between human-level performance and machine performance in a variety of tasks, such as visual classification where a machine requires dozens of images of an object to be able to accurately recognize it in unseen examples, while a human agent probably needs just a couple of examples. Such astonishing learning rates raise the question of how could human brain's solution to optimization problems differs from that of machines (most notably, gradient descent)?</p><p>Adding the plasticity factor to artificial neural networks Artificial neural networks which employ backpropagation based on the gradient descent method have been used extensively in ML <ref type="bibr" target="#b9">10</ref> . There have been previous studies that have tried to add the plasticity factor to the training process of artificial neural networks <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> . However, most of these models require complex external control over the network or complex novel rules, which in turn increase the computational burden of the model. In this manuscript, it is intended to implement synaptic plasticity in ANNs in a similar manner to that of the human nervous system. This model is supposed to be easier to implement, while also achieving faster learning rates.</p><p>To better implement such a model, a set of criteria were defined for the model to satisfy: 1. Local computation: An ANN performs computation only based on the inputs it receives from other neurons. This input is weighted by the strengths of its synaptic connections. 2. Global plasticity: The amount of change in synaptic weights is not only dependent on the activity of the two neurons the synapse connects, but also on the relative position of the ANN. 3. Minimal external control: The ANNs perform computations with as little external control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position-wise optimizer</head><p>The new model proposed in this research was named "position-wise optimizer" as its most prominent feature is to take the ANN's relative position to other ANNs into account for updating the parameters. Due to the chain rule, updating the weights of one layer requires computing the gradients of the weights of all the deeper layers <ref type="bibr" target="#b13">14</ref> . In the standard gradient descent model, each parameter of the model (i.e., weights of the synapses, ' ) gets an update after each epoch (when the learning algorithm sees the complete dataset once). This process is not completely in line with the synaptic plasticity model in biological networks where synaptic weight change is more restricted for the peripheral synapses than the central synapses. To imitate the synaptic plasticity function of biological neural networks, the model must be altered for the deeper ANNs to be able to change their synaptic weight more frequently than the shallower ANNs. The proposed model in this paper achieves that goal in the following steps:</p><p>I. Following a complete forward pass of the network, only the gradients for the last layer ( ) will be computed. Afterward, the parameters (synaptic weights) of the last layer get an update. The reason behind this is to train the ANNs in the last layer to minimize the objective function based on input from shallower layers while holding them constant. II.</p><p>Another forward pass only using the last layer is done to be able to calculate the new gradients following the update in the previous step. III.</p><p>Gradients for the layer before the last layer ( ? 1) and the last layer ( ) will be computed based on the new objective function value. Afterward, these two layers' parameters (synaptic weights) get an update. Again, the reason behind this is to train the ANNs in these layers to minimize the objective function based on input from shallower layers while holding them constant. IV.</p><p>The same process is repeated in a layer-wise manner up to the first layer of the network. In this process, deeper ANNs are trained to achieve a low objective function while the parameters of shallower ANNs (and thus their output to the deeper ANNs) change less frequently, compared to the times of change in the parameters of the deeper layers. A simplified illustration of the proposed model is compared to the gradient descent model in <ref type="figure" target="#fig_0">Figure 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model implementation</head><p>The implementation of the model, compared with the gradient descent model, is presented in the following table: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>The model was implemented in Python 3.9. The network consisted of 3 hidden layers of dimensions 20, 7, and 5 respectively. Weights were randomly initialized based on normal Xavier initialization:</p><formula xml:id="formula_5">[+] ~ ( , ); ? = G 2 [+] + [+-%] ,<label>( 6 )</label></formula><p>where <ref type="bibr">[+]</ref> is the weight matrix corresponding to layer . For binary classification tasks, the objective function was the cross-entropy loss, which is computed as follows:</p><formula xml:id="formula_6">= ? 1 +K " log/ " 1 + /1 ? " 1. log/1 ? " 1O # "$% .<label>( 7 )</label></formula><p>For multi-class classification tasks, the objective function was the additive margin softmax loss. The bias term (b) for each layer was initially set to zero. After initiating the weights, each weight was copied into two new variables, each used for one algorithm (gradient descent optimizer and position-wise optimizer). The network had 4 activation units. Starting from the activation unit of the first layer to the last, they included: ReLU, ReLU, ReLU, and Sigmoid activation functions. While training each model, at the end of each iteration, the loss for the model was computed and stored in a list data structure. Both models were trained using the same constant learning rate ( ). Both algorithms were set to terminate the training process when the loss fell below a desirable value. After the termination of each training process, the time elapsed to train the model was measured using the "time" module. Finally, calculated losses were plotted against the number of epochs for each algorithm, using the "matplotlib" module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1; In-house dataset for cat vs. non-cat classification</head><p>This experiment was a binary classification task. The dataset consisted of 209 images. Each image was of size: <ref type="figure" target="#fig_3">(64, 64, 3)</ref>. At first, the data were imported into Python using the "h5py" module. Images were reshaped into one big vector, used for the training process. The input layer was a matrix with the dimensions of (12288, 209). The training process results for both models are presented in <ref type="figure" target="#fig_2">Figure 2</ref>. It took 2243 epochs for the standard gradient descent method to reach a loss value of 0.1. This process took 50 seconds. Using the position-wise optimization method only took 881 epochs to reach the same value for the loss. It took only 23 seconds. Given the significant difference in the execution time of the models (50 vs 23 seconds), the position-wise model managed to reach the desired loss significantly faster than the gradient descent model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2; CIFAR-10 dataset</head><p>This experiment was a multi-class classification task (10 classes). The CIFAR-10 15 dataset consists of 50000 training images. Each image is of size: (32, 32, 3). Images were reshaped into one big vector, used for the training process. The input layer was a matrix with dimensions of (3072, 50000). The training process results for both models are presented in <ref type="figure" target="#fig_3">Figure 3</ref>. It took 121 epochs for the standard gradient descent method to reach a loss value of 3.23. This process took 462 seconds. Using the position-wise optimization method only took 97 epochs to reach the same value for the loss. It took only 359 seconds. Given the significant difference in the execution time of the models (462 vs 359 seconds), the position-wise model managed to reach the desired loss significantly faster than the gradient descent model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 3; MNIST dataset</head><p>This experiment was a multi-class classification task (10 classes). The MNIST dataset <ref type="bibr" target="#b15">16</ref> consists of 60000 training images. Each image is of size: (28, 28). Images were reshaped into one big vector, used for the training process. The input layer was a matrix with dimensions of (784, 60000). The training process results for both models are presented in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4;</head><p>The loss of each model plotted against the number of epochs for the MNIST dataset. The time elapsed to train each model to achieve the desired loss value is presented above the plots.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 ;</head><label>1</label><figDesc>One epoch of the gradient descent optimizer (left) vs one epoch of the position-wise optimizer (right). "X" indicates weight update for the corresponding layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Batch gradient descent Algorithm 2 Position-wise optimizer Require: input , weight , output , desired output , number of layers , loss , learning rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 ;</head><label>2</label><figDesc>The loss of each model plotted against the number of epochs for the in-house cat classification dataset. The time elapsed to train each model to achieve the desired loss value is presented above the plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 ;</head><label>3</label><figDesc>The loss of each model plotted against the number of epochs for the CIFAR-10 dataset. The time elapsed to train each model to achieve the desired loss value is presented above the plots.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It took 195 epochs for the standard gradient descent method to reach a loss value of 3.248. This process took 282 seconds. Using the position-wise optimization method only took 139 epochs to reach the same value for the loss. It took only 227 seconds. Given the significant difference in the execution time of the models (282 vs 227 seconds), the position-wise model managed to reach the desired loss significantly faster than the gradient descent model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this manuscript, a novel nature-inspired optimization algorithm was introduced. This model imitates the pattern of synaptic plasticity of biological neural networks. The results of testing the model on three distinct datasets showed the superiority of the model over the traditional gradient descent optimization algorithm. Synaptic plasticity in biological networks is not just about how much weights change in the synapses, but also about forming new informative synapses and terminating synapses that do not contribute to solving the optimization problem. In the model introduced in this manuscript, this phenomenon was not implemented. Future research might try to use the techniques of previous models to implement such phenomenon in the model and thus, improve upon it. It should also be noted that it is not yet proven that the human brain uses gradient methods to solve optimization problems. Thus, future researchers are recommended to try to implement synaptic plasticity behavior in other forms of optimization models as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Administrative information</head><p>Funding No sources of support were provided for this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations of interest</head><p>The author declares no conflicts of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics approval</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent to participate</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent to publication</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and material</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>A simplified implementation of the model is available at the following online repository: https://github.com/thisisamirv/Position-wise-optimizer Author's contributions AV was the sole author.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What Is Neural Plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Von Bernhardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Bernhardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eugen?n</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-62817-2_1</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Organization of Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Hebb</surname></persName>
		</author>
		<idno type="DOI">10.4324/9781410612403</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Citri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Malenka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synaptic Plasticity: Multiple Forms, Functions, and Mechanisms</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synaptic Plasticity Forms and Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grienberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="95" to="117" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Plasticity and regeneration in the peripheral nervous system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fornaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raimondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giacobini-Robecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ital J Anat Embryol</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="91" to="95" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nishizaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Numao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D L</forename><surname>Caro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T C</forename><surname>Suarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory And Practice Of Computation -Proceedings Of Workshop On Computation: Theory And Practice</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>World Scientific Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Survey of Optimization Methods From a Machine Learning Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="3668" to="3681" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Backpropagation and the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Akerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Neftci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An approximate backpropagation learning rule for memristor based neural networks using synaptic plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Negrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="193" to="199" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C R</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bogacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1229" to="1262" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural Networks: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Jansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analytical Chemistry</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The MNIST Database of Handwritten Digit Images for Machine Learning Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Best of the Web</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
