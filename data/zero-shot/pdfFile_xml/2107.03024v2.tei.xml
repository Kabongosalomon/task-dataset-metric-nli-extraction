<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Sampling Strategies for Unsupervised Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumeng</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehui</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
						</author>
						<title level="a" type="main">Rethinking Sampling Strategies for Unsupervised Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person Re-identification</term>
					<term>Unsupervised Learn- ing</term>
					<term>Group Sampling</term>
					<term>Representation Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised person re-identification (re-ID) remains a challenging task. While extensive research has focused on the framework design or loss function, we show in this paper that sampling strategy plays an equally important role. We analyze the reasons for differences in performance between various sampling strategies under the same framework and loss function. We suggest that deteriorated over-fitting is an important factor causing poor performance, and enhancing statistical stability can rectify this issue. Inspired by that, a simple yet effective approach is proposed, known as group sampling, which gathers groups of samples from the same class into a mini-batch. The model is thereby trained using normalized group samples, which helps to alleviate the effects associated with a single sample. Group sampling updates the pipeline of pseudo label generation by guaranteeing that samples are more efficiently divided into the correct classes. Group sampling regulates the representation learning process, which enhances statistical stability for feature representation in a progressive fashion. Qualitative and quantitative experiments on Market-1501, DukeMTMC-reID, and MSMT17 show that group sampling improves upon state-of-theart methods by between 3.3% ? 6.1%. Code has been available at https://github.com/ucas-vg/GroupSampling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>At present, pseudo label methods have achieved comparable performance to UDA methods without using labeled data.</p><p>During unsupervised person re-ID research, most works focus on the training framework design <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> or loss function <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. In this paper, we show that the process of sample selection plays an equally important role. For example, different sampling strategies can create substantially different results under the same framework and loss function. The representative pseudo label method SpCL <ref type="bibr" target="#b24">[25]</ref> is adopted as the baseline framework. Unexpectedly, using this framework, the most commonly used random sampling leads to training collapse, while triplet sampling <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> originally used in the baseline can greatly alleviate this problem. In order to address this anomaly, we analyze the sampling differences, and show why triplet sampling works and random sampling does not.</p><p>When using random sampling for training, the randomness of a single sample and its own trend may affect the overall trend of the class, causing in the features expressed by the class to shift, which is called deteriorated over-fitting. For triplet sampling, samples in the same class are selected within a mini-batch, which is called a grouping operation. Our analysis suggests that this grouping operation reduces the impact of a single sample, thereby strengthening statistical stability, overall. This appears to hold across the class and suppresses deteriorated over-fitting. However, triplet loss <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> is not used in our baseline framework, so the application of triplet sampling is not necessary in this framework and also causes a number of disadvantages. Therefore, this paper proposes a novel group sampling for pseudo-label-based unsupervised person re-ID, which utilizes the grouping operation and solves the shortcomings in triplet sampling. Grouping samples helps to optimize the model in a direction consistent with the trend of the whole class and to reduce the impact of a single sample, which facilitates similarity structure maintenance within each class. At the same time, using the overall trend of the class also helps to maintain discrimination between classes, thereby preventing many classes from being merged, which inhibits the model from deteriorated over-fitting. In this way, the model has access to exploit more subtle differences from the existing similarity structure so as to extract the unique identity similarity. Extensive experiments have been conducted, and the results demonstrate the effectiveness of group sampling on maintaining statistical stability and improving feature representation capability of the model. The contributions of this paper are summarized as follows: <ref type="bibr">?</ref> We provide evidence that sampling strategy plays an important role in the training process of unsupervised person re-ID, and investigate the mechanism of why random sampling leads to training collapse and why triplet sampling works. <ref type="bibr">?</ref> We highlight the shortcomings involved in triplet sampling, and further propose a novel group sampling strategy for unsupervised person re-ID, which addresses the negative effect of deteriorated over-fitting and enhances statistical stability related to the unsupervised model. ? Group sampling achieves mAP of 79.2% and 69.1%</p><p>on Market-1501 <ref type="bibr" target="#b30">[31]</ref> and DukeMTMC-reID <ref type="bibr" target="#b31">[32]</ref>, significantly outperforming the state-of-the-art <ref type="bibr" target="#b24">[25]</ref> by 6.1% and 3.8%. Group sampling also achieves competitive performance on the challenging MSMT17 V2 <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Unsupervised Person re-ID</head><p>Unsupervised person re-ID aims to learn effective features for unlabeled person image datasets. Existing unsupervised person re-ID methods can be roughly categorized into transferring knowledge from the labeled source domain to the unlabeled target domain <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and using representation features to estimate pseudo-labels <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. The transfer-based methods follow the UDA strategy, which transfers knowledge learned from the source domain dataset as model initialization to the target domain or uses the style transfer method to transfer labeled images to the target domain.</p><p>Recently, researchers have paid more attention to pseudolabel methods that do not require source domain data. Pseudo labels can be generated by a pseudo label generator or by mining soft label information from feature similarities. In this way, pseudo labels are used to fine-tune the re-ID model. MMT <ref type="bibr" target="#b5">[6]</ref> proposed to generate more robust soft labels via mutual mean-teaching. HCT <ref type="bibr" target="#b23">[24]</ref> combined hierarchical clustering with hard-batch triplet loss to improve the quality of pseudo labels. MMCL <ref type="bibr" target="#b22">[23]</ref> formulated unsupervised person re-ID as a multi-label classification task to progressively seek true labels. SpCL <ref type="bibr" target="#b24">[25]</ref> adopted the self-paced contrastive learning strategy to form more reliable clusters.</p><p>In contrast, our method attempts to boost the performance of unsupervised person re-ID on the unlabeled dataset without any source domain data or source domain pre-trained model. The pseudo-label-based strategy is adopted, combining with the training method of contrastive learning. Here, a more reasonable sampling strategy is proposed, which achieves better performance than existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sampling Strategy</head><p>Sampling is a basic operation for reducing bias during model learning <ref type="bibr" target="#b33">[34]</ref>, and it has been studied for stochastic optimization <ref type="bibr" target="#b34">[35]</ref> with the goal of accelerating convergence to the same global loss function. A commonly used way is random sampling <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. In the literature, different sampling methods are proposed to facilitate the learning of various loss functions. For contrastive loss, it is common to select randomly from all possible pairs <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b36">[37]</ref>, and sometimes with hard negative mining <ref type="bibr" target="#b39">[40]</ref>. For triplet loss, the semi-hard negative mining method was first utilized in FaceNet <ref type="bibr" target="#b27">[28]</ref> and is now widely adopted <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. For the person re-ID task, triplet sampling <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> is widely used for learning with triplet loss. For each training batch, a certain number of identities are randomly selected, and then several images are sampled from each selected identity. This sampling strategy guarantees informative positive and negative mining. In this paper, we study the impact of sampling strategy on unsupervised person re-ID learning, and further propose a new sampling strategy, termed group sampling, to ensure model convergence and to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Unsupervised Feature Learning</head><p>Unsupervised feature learning usually focuses on selecting appropriate self-supervised pretext tasks, whose targets are automatically generated without manual labeling. The performance of this invented task is usually not crucial, but the learned intermediate representation is concerned, which is expected to carry good semantic or structural meanings and can be beneficial to practical downstream tasks. Recently, stateof-the-art methods on unsupervised feature learning are based on contrastive learning <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. They adopt a specific pretext task for instance discrimination by treating every image as a distinct class, which demonstrates better performance than other pretext tasks. However, the strategy for instance discrimination is not suitable for unsupervised person re-ID, because this task requires measuring the interclass affinities. Similar to DeepCluster <ref type="bibr" target="#b47">[48]</ref>, our framework utilizes feature clustering and pseudo label optimization as its pretext task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HOW IMPORTANT IS SAMPLING?</head><p>In this section, we investigate the importance of sampling strategies for unsupervised person re-ID. In Sec. III-A, we first briefly introduce the baseline framework. Then, in Sec. III-B and III-C, we illustrate how important sampling is from an experimental point of view. The experiment settings and implementation details are described in Sec. VI-A and VI-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline Framework</head><p>We adopt the person re-ID contrastive learning framework proposed in SpCL <ref type="bibr" target="#b24">[25]</ref> as our baseline framework, which combines the self-supervised contrastive learning method <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> and the pseudo-label-based unsupervised person re-ID strategy <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b22">[23]</ref> to achieve state-of-the-art performance. It consists of a CNN-based encoder f ? with parameters ? and a feature memory bank M. The encoder maps the unlabeled person re-ID dataset X = {x 1 , x 2 , . . . , x n } to obtain a feature set V = {v 1 , v 2 , . . . , v n }, i.e.,</p><formula xml:id="formula_0">V = f ? (X ).<label>(1)</label></formula><p>The memory bank is utilized to store instance features extracted from the encoder. In self-supervised contrastive learning, each instance is treated as a distinct class. Nevertheless, Update the memory bank M with vi and m by Eq. (4); <ref type="bibr">8:</ref> end for 9: end while the difference is that in our baseline framework, following the pseudo-label-based unsupervised person re-ID strategy, the features in the memory bank are clustered by a pseudo label generator G. Adopting G, the feature set V is divided into an outliers set O and a cluster set C. The outlier set contains the samples which are classified separately, and the cluster set contains N c clusters,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Baseline Framework</head><formula xml:id="formula_1">C = {C 1 , C 2 , . . . , C Nc },<label>(2)</label></formula><p>in which each cluster C k contains at least two samples. Each instance in outliers set O is assigned a pseudo label, and instances in cluster C k are set with the same pseudo label. After being assigned a pseudo label, the instance is used to construct the unified contrastive loss. The unified contrastive loss function in <ref type="bibr" target="#b24">[25]</ref> is defined as,</p><formula xml:id="formula_2">Lv = ? log exp( v, c + /? ) Nc k=1 exp( v, c k /? ) + No k=1 exp( v, o k /? ) ,<label>(3)</label></formula><p>where c + indicates the positive class prototype corresponding to v, the temperature ? is empirically set as 0.05, ?, ? denotes the inner product between two feature vectors to measure their similarity, N c is the number of clusters and N o is the number of outliers. More specifically, if v is a clustered instance, c + = c k is the centroid of the cluster C k that v belongs to. If v is an un-clustered outlier, we would have c + = o k as the outlier instance feature corresponding to v. When unified contrastive calculating the loss, c k is calculated from the features belonging to the cluster C k stored in the memory bank M, and o k is the feature directly retrieved from M. The training process includes two steps: (1) using the features in the memory bank to cluster the training set samples, and (2) optimizing the encoder with the unified contrastive loss and dynamically updating the memory bank with encoded features, where the update formula is defined as,</p><formula xml:id="formula_3">M[x] ? m ? M[x] + (1 ? m) ? v,<label>(4)</label></formula><p>where m ? [0, 1] is the momentum coefficient for updating the sample features in memory bank is empirically set as 0.2. Alg. 1 shows the implementation process of the person re-ID contrastive learning framework. ? Unless otherwise specified, the parameter K of triplet sampling in this paper adopts the default value of 4. Moreover, triplet sampling does not mean that triplet loss is used. We uniformly use the unified contrastive loss <ref type="bibr" target="#b24">[25]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Description of Sampling Strategies</head><p>In this section, we describe in detail the two sampling strategies used in the following experiments and analyses. They are the most commonly used random sampling, and triplet sampling that is widely used in supervised person re-ID task. Triplet sampling is also the sampling strategy used in the baseline framework.</p><p>1) Random sampling: Random sampling is the simplest and most commonly used sampling strategy in deep learning, which randomly selects all possible samples from the dataset. The sampling list S can be regarded as the result of randomly shuffling the dataset X combined with the pseudo label set Y, i.e.,</p><formula xml:id="formula_4">S = (X ,?) = shuf f le(X , Y),<label>(5)</label></formula><p>whereX and? are the shuffled image sequence and the pseudo label sequence respectively.</p><p>2) Triplet sampling: Triplet sampling is designed to be used in conjunction with triplet loss <ref type="bibr" target="#b27">[28]</ref>, which tries to promote closer distances between samples belonging to the same class and farther distances between samples of different classes in the embedding space. In order to calculate the triplet loss, it needs to sample a certain number of positive samples and negative samples in each mini-batch. It samples P person identities and a fixed number K of instances for each person identity in each mini-batch. When the number of instances of a person identity is greater than K, only K instances are randomly sampled from them, and the rest will be discarded. On the contrary, when the number is less than K, there will be instances that are repeatedly sampled.</p><p>In baseline SpCL <ref type="bibr" target="#b24">[25]</ref>, implementation of triplet sampling is slightly different from that in supervised person re-ID <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Specifically, it still samples K instances from each cluster C i . However, for un-clustered outliers, each of them is regarded as a special class. They are not re-sampled during sampling, but only sampled once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison of Sampling Strategies</head><p>We use random sampling and triplet sampling <ref type="bibr" target="#b28">[29]</ref> to train the baseline framework, and the experimental performance is given in the first two rows of Tab. I. It can be clearly seen that compared to random sampling, triplet sampling produces a great performance gain. However, random sampling is a common choice in contrastive self-supervised learning <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, and triplet sampling is not essential for unified contrastive loss in the baseline framework. Therefore, why random sampling leads to training collapse, and why triplet sampling can be used to address training collapse to a certain extent is counter-intuitive, and is discussed in the latter sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. WHY TRIPLET SAMPLING WORKS?</head><p>In this section, we study why random sampling causes our baseline framework to fail to converge, and why triplet sampling can alleviate training collapse and ensure good performance. In Sec. IV-A, we proposed a concept called deteriorated over-fitting, and explained that the reason for the training collapse caused by random sampling is the deteriorated over-fitting in the pseudo label class. Then, in Sec. IV-B, we propose a corresponding concept called statistical stability, and explain that statistical stability is an important factor to restrict deteriorated over-fitting. Finally, in Sec. IV-C, we explained and analyzed that the grouping operation in triplet sampling helps to maintain statistical stability, which is why triplet sampling can suppress training collapse.</p><p>A. Deteriorated Over-fitting 1) Description of deteriorated over-fitting: Pseudo label methods utilize feature similarity and difference between samples to divide samples into different classes, and assign a pseudo label to each class. However, due to inadequate feature representation capabilities of the model and insufficient semantic information associated with features extracted from samples, noise is inevitable in the generated pseudo labels <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b6">[7]</ref>. When using such pseudo labels as supervision information for model training, the model cannot be trained to complete convergence. Instead, new pseudo labels are regenerated after a certain number of iterations to continue training <ref type="bibr" target="#b24">[25]</ref>. Therefore, under this training policy, the update of samples in the feature space has strong randomness.</p><p>One characteristic of the classes generated by the pseudo label method is that they can be roughly divided into strong and weak classes. The characteristic of the strong class is that the samples are more compact in the feature space and the number of samples is relatively large, which means that it has a more stable structure. On the contrary, the sample composition of the weak class is looser, which means that its structure is not stable and easy to be broken.</p><p>Due to the randomness of sample feature expression and the personality of each sample, during the training process, the sample will have a chance to break away from the original class and merge into another class. According to the characteristics of the strong and the weak class, we believe that the samples in the weak class are more likely to be separated from the class and then be swallowed by the strong class. When the strong class continues to swallow the weak class or other scattered samples, the group gradually grows in size. There are various samples with different ground-truth labels mixed in a class, and the overall feature expression of the class is more general and rough. We call this phenomenon deteriorated over-fitting. 2) Negative effects caused by deteriorated over-fitting: The samples in the same class have a certain similarity structure, which encourages them to gather together in the feature space. However, this similarity structure contains not only the unique feature similarity of each person identity, but also many general similarities. We call general similarities undesirable similarities, as shown by the dotted lines in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The consequence of deteriorated over-fitting is that various samples with different ground-truth labels are mixed in a class, which indicates that the undesirable similarity structures in the class are constantly gathering. In this way, the similarity structures of person identity are submerged in the class and cannot be expressed. Once such a class is formed, the model learns in the direction of feature deterioration. That is to say, the model can only learn deteriorated semantic information and loses the ability to mine more detailed features between samples, which is particularly disadvantageous for unsupervised person re-ID task <ref type="bibr" target="#b48">[49]</ref>. Additionally, the knowledge learned from deteriorated classes aggravates the noise in the regenerated pseudo labels, thus forming a vicious circle.</p><p>3) Random sampling leads to deteriorated over-fitting: When using random sampling, samples are randomly selected, so that the influence of randomness on a single sample and its own trend on model training are intensified. This may cause the sample to break away from the class and break up the original structure in the class, and also may lead to a single sample dominating the optimization direction of the entire class. These situations cause the original feature expression of the class to shift, resulting in undesirable similarity structure being strengthened. For example, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the features of samples in the class are biased towards undesirable similarity. Once the semantic information represented by this undesirable similarity structure is strengthened, those samples that have been correctly classified but do not have such similarity will be gradually repelled, thereby destroying the person identity similarity structure within the class. Simultaneously, structural discrimination between different classes will not be maintained, which leads to similarity structure in many classes being destroyed, many weak classes being swallowed, and undesirable similarities accumulating within classes. This ultimately results in the deterioration of semantic information. The direct manifestation of this result is that the noise in the pseudo labels will increase, that is, the quality of the pseudo label will continue to decrease. This is the reason for training collapse caused by random sampling. 4) Degree of purity and chaos: In order to visually illustrate the phenomenon of deteriorated over-fitting and to prove that random sampling does cause deteriorated over-fitting, we define the degree of purity and chaos.</p><p>Intuitively speaking, when most of the samples in a class have the same ground-truth label, it means that the class is relatively pure. By contrast, if there are many samples in a class associated with different ground-truths, the class is comparatively messy. We use the average number of person identities in a class to define the degree of chaos D c , i.e.,</p><formula xml:id="formula_5">C i = I i 1 ? I i 2 ? ? ? ? ? I i ni ,<label>(6)</label></formula><formula xml:id="formula_6">D c = 1 N c Nc i=1 n i ,<label>(7)</label></formula><p>where I i j indicates the j-th person identity set in class set C i , n i represents the total number of person identities in class set C i . Then we use the average proportion of person identities with the largest number of samples in the class to define the degree of purity D p , denoted as,</p><formula xml:id="formula_7">D p = 1 N c Nc i=1 max j |I i j | |C i | ,<label>(8)</label></formula><p>where |?| denotes the number of samples in the set. The degree of purity and chaos reflects the distribution of samples within a class from two perspectives, and they complement each other to reflect the quality of the class. We calculate the degree of purity and chaos of random sampling, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. It can be seen that random sampling causes many person identities to gather in a class, and no one can dominate the feature representation of this class, which leads to the consequence of deteriorated overfitting as described in Sec. IV-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Statistical Stability</head><p>In the previous section, we explained that the weak class being continuously swallowed by the strong class will lead to deteriorated over-fitting. Therefore, in order to restrict this deteriorated over-fitting phenomenon, it is necessary for the weak class to remain stable and not easy to be broken up during the training process. We call this process statistical stability maintenance.</p><p>The statistical stability of a class guarantees that the similarity structure within the class will not be greatly destroyed, and discrimination between other classes can be maintained to a certain extent. Only when the class maintains its statistical stability, the model can explore more essential and complex similarities between samples in the continuous learning process. At the same time, the model is capable of distinguishing more detailed differences from rough similarities. In this way, the model will learn sufficient inter-class discriminative representations and intra-class affinities to meet the needs of person re-ID task.</p><p>C. Grouping Samples Can Achieve Statistical Stability 1) Grouping samples is an effective way: In the previous section, we explained that random sampling leads to deteriorated over-fitting. The main reason is that a single sample affects the overall trend of the class. Therefore, in order to prevent the occurrence of deteriorated over-fitting, it is necessary to reduce randomness and weaken the influence of a single sample. Triplet sampling appears to provide a reasonable solution to this issue.</p><p>As described in Sec. III-B2, there is an important parameter K in triplet sampling, that is, number of instances for each person identity in each mini-batch. This operation enables a group of samples belonging to the same class to be trained at the same time, so the overall trend of the group is emphasized. The randomness of each sample in the group and its own tendency are weakened, and they are not easy to disperse from the group. The performance gain produced by triplet sampling illustrated in Sec. III-C shows that it can alleviate training collapse, which confirms that the grouping operation in triplet sampling is an effective method to achieve statistical stability.</p><p>2) Number of instances in triplet sampling affects performance: In the previous works of person re-ID <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b24">[25]</ref>, the parameter K is usually set to 4. In this part, we use different values of K to train the baseline framework, and the results are shown in Tab. II. It can be seen that the number of instances is a very important parameter that affects the performance of the model. This proves that grouping samples during sampling can relieve the issues in random sampling, and when setting K = 16, a very competitive performance can be obtained in unsupervised person re-ID task. It is worth noting that when K &gt; 16, the performance has dropped, which is discussed in detail in Sec. V-A3.</p><p>3) Verification: In order to visually verify the conclusion that the grouping operation can achieve statistical stability, we calculate the degree of purity and chaos obtained by training with triplet sampling, and also select different values of parameter K, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>The results show that the qualities of the classes obtained by triplet sampling are consistently better than random sampling. That is, the purity is higher, and chaos is lower. This indicates that the number of person identities in a class is generally small, and one person identity (or a small number) dominates the class. Therefore, the class can better represent the feature of person identity and achieve the desired effect of suppressing deteriorated over-fitting. In addition, it can be seen from the results of different K values that when performance is higher, the degree of purity is higher and chaos is lower. This confirms that when the class suppresses deteriorated over-fitting, it can better characterize person identity feature representation, so better performance can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. WHAT IS A BETTER SAMPLING STRATEGY?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shortcomings in Triplet Sampling</head><p>Compared with random sampling, triplet sampling has achieved great performance gains in our baseline framework, but there are still a number of shortcomings.</p><p>1) Triplet sampling is designed for triplet loss: Even though triplet sampling realizes the grouping of instances of the same label in the form of implementation, its original design intention is to match triplet loss, that is, to sample a certain number of positive samples in a mini-batch. The loss function used in the baseline framework is unified contrastive loss, therefore, it is not completely consistent with our goal.</p><p>2) The grouping results of triplet sampling have drawbacks: During a round of sampling process, triplet sampling may resampling or cause some instances to fail to be sampled, as described in Sec. III-B. When re-sampling occurs, it indicates that the number of samples in this class is small, and the weight of these samples will be enlarged, which leads to the problem of sample imbalance. When insufficient sampling occurs, some samples cannot be updated in time, which is undoubtedly detrimental to the training process.</p><p>3) Number of instances is not the bigger the better: In Sec. IV-C2, the performance obtained by selecting different values of the number of instances K is shown. However, the choice of K is not the bigger the better. When K is set to 32 and 64, there is a significant performance degradation, as shown in Tab. II. We believe that the drawbacks of grouping results are the main reason for this phenomenon. That is to say, when the value of K increases, K is greater than the number of samples in many classes, which causes a lot of re-sampling. Re-sampling leads to sample imbalance, which is reflected in the calculation of loss and the update of sample features in memory bank. Such sample imbalance also increases the impact of a single sample, resulting in poor performance.</p><p>B. Group Sampling 1) Description of group sampling: In order to improve these shortcomings of triplet sampling, we propose a sampling method that is more suitable for the baseline framework, termed group sampling. In this part, we introduce the implementation of group sampling.</p><p>First, in order to increase the randomness, all samples belonging to C i are shuffled. Then we pack each N adjacent samples into a group in each C i , where the hyper-parameter N is the number of samples in each group, named the group size. It is worth emphasizing that when the number of samples in C i is not divisible by N or less than N , the re-sampling strategy is not used, but the remaining samples are directly packed into a group. Finally, all the groups are clustered together, and the order of these groups is shuffled. Considering that outliers also have a great impact on model training, but single or several outliers are not expected to dominate the optimization process, we pack the samples in the outlier set O according to the batch size, and shuffle them with all groups. The influence of outliers on model training is analyzed in Sec. VI-D3 and VI-D4. Alg. 2 describes the implementation details of group sampling, and a schematic diagram is shown in <ref type="figure" target="#fig_2">Fig. 3</ref> with N = 2, in which we do not draw the outlier set O for the sake of simplicity.</p><p>2) Performance and analysis of group sampling: We use group sampling to train the baseline framework and compare it with random sampling and triplet sampling. The performance is shown in the last row of Tab. I. The results obtained by group sampling also prove that group operations can prevent training degradation and achieve good performance. Furthermore, in combination with the results in Tab. II, it can Take out all samples in Ci and put them into sample set Xi;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Shuffle the order of samples in Xi;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>while |Xi| &gt; N do <ref type="bibr">6:</ref> Pack {x1, x2, . . . , xN } ? Xi into group G;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Delete {x1, x2, . . . , xN } from Xi;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Add group G to sampling set S; <ref type="bibr">9:</ref> end while 10:</p><p>Group the remaining samples in Xi and add them to S; 11: end for 12: Shuffle the order of groups in S; <ref type="bibr">13:</ref> Shuffle the order of outliers in outlier set O and add O to S; <ref type="bibr">14:</ref> Divide each B adjacent samples in S into a batch in order; <ref type="bibr">15:</ref> Shuffle the order of batches in S; Return: Sampling list S. be seen that the performance of group sampling is better than that of triplet sampling, indicating that group sampling can improve the shortcomings of triplet sampling and better match our baseline framework.</p><p>Similarly, in <ref type="figure" target="#fig_1">Fig. 2</ref>, the degree of purity and chaos curves of the group sampling are shown. It can be seen that compared to random sampling and triplet sampling, group sampling can obtain a higher degree of purity and lower degree of chaos. This shows the reason why group sampling can achieve the best performance is that it can prevent the occurrence of deteriorated over-fitting, while maintaining statistical stability for each class. In this way, the class gradually learns identity similarity from the initial mixed similarity structure, and finally promote each class express the unique feature representation of person identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENT A. Datasets and Settings</head><p>Market MSMT17 <ref type="bibr" target="#b32">[33]</ref> is composed of 126,411 images from 4,101 identities collected by 15 cameras. These 15 cameras include 12 outdoor and 3 indoor ones. The dataset suffers from substantial variations of scene and lighting, and is quite challenging. To protect the privacy of pedestrians, the author released a second version of MSMT17, called MSMT17 V2, where the faces of pedestrians are masked, bringing more incredible difficulty and challenge.</p><p>Evaluation Metrics. Following <ref type="bibr" target="#b30">[31]</ref>, mean Average Precision (mAP) and Cumulative Matching Characteristic (CMC) are adopted to evaluate the performance. Moreover, results reported in this paper are under the single-query setting, and no post-processing technique is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We adopt ResNet-50 <ref type="bibr" target="#b35">[36]</ref> initialized with parameters pretrained on ImageNet as backbone to extract the feature. The subsequent layers after pooling-5 layer are removed, and a 1D batch normalization layer and an L 2 -normalization layer are added. DBSCAN <ref type="bibr" target="#b49">[50]</ref> is used as the pseudo label generator G and Jaccard distance with k-reciprocal nearest neighbors <ref type="bibr" target="#b50">[51]</ref> is used for clustering before each epoch, where we set k = 30. For DBSCAN, the maximum distance between neighbors is set as d = 0.6 and the minimal number of neighbors for a dense point is set as 4. The temperature ? in unified contrastive loss is set as 0.05, and the momentum coefficient in memory bank is set as 0.2. The number of training epochs is set to 50, the initial learning rate is set to 0.00035 and is divided by 10 after every 20 epochs. The batch size is set to 64. The input images are resized to 256?128. Strategies like randomly flipping, cropping and erasing <ref type="bibr" target="#b51">[52]</ref> are also introduced.</p><p>The group size N in group sampling is set differently on the three datasets. Specifically, N are set to 256, 128 and 1024 for Market-1501, DukeMTMC-reID and MSMT17, respectively. More ablation experiments and analysis on group size N are described in Sec. VI-D1.</p><p>The proposed method is implemented on PyTorch <ref type="bibr" target="#b52">[53]</ref>. On Market-1501 and DukeMTMC-reID dataset, it takes approximately 2 hours for training with a single GTX 2080TI GPU.</p><p>For the MSMT17 dataset, due to its large amount of data, we use 4 GPUs for training. At the same time, we adjusted some hyper-parameters in the framework to achieve better performance. Specifically, the initial learning rate is set to 0.00005, the batch size is set to 256, and the momentum coefficient in memory bank is set to 0.1. The total training time is approximately 5 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis</head><p>In Sec. IV and V, we describe the feature deterioration caused by random sampling and the statistical stability of group sampling. In this section, we further analyze and compare the deteriorated over-fitting phenomenon and statistical stability in four perspectives: number of clusters and Normalized Mutual Information (NMI) score, intra-class variance and inter-class variance, and correction rate and misleading rate, and feature visualization. All experiments are conducted with unsupervised person re-ID setting on Market-1501. The performance in terms of mAP using random sampling and group sampling is shown in <ref type="figure">Fig. 4(a)</ref>.</p><p>1) Number of clusters and NMI score: The number of clusters generated by clustering and NMI score can intuitively reflect the quality of pseudo labels. <ref type="figure">Fig. 4(b) and 4(c)</ref> show the curves of the number of clusters and NMI score using random sampling and group sampling. It can be seen that random sampling results in a sharp decrease in the number of clusters and NMI score. <ref type="figure">Fig. 4(b)</ref> illustrates that random sampling results in fewer clusters, indicating the samples are grouped in large-scale clusters, where components are messier. NMI score further shows that the reliability of the pseudo labels generated by random sampling is very low. By contrast, the number of clusters obtained by group sampling is closer to ground-truth identities of the dataset, and NMI score gradually increases, eventually reaching 0.95, showing higher reliability of the clusters and the effectiveness of group sampling.</p><p>2) Intra-class and inter-class variance: The intra-class variance of samples within a class and the inter-class variance between classes are calculated to quantitatively analyze the degree of aggregation and dispersion of samples in the feature space. <ref type="figure">Fig. 4(d)</ref> shows the curves for intra-class and inter-class variance during training. It can be seen that as the training progresses, the intra-class variance is constantly decreasing, and the inter-class variance is increasing. It shows that samples in the feature space are gradually grouped into compact clusters from a relatively dispersed state at the beginning, and the distance between clusters is constantly increasing.</p><p>However, when comparing the variance curves of group sampling and random sampling, it can be seen that the intraclass variance of group sampling is larger and the interclass variance is smaller. As previously mentioned, due to incorrectly assigned pseudo labels, random sampling makes the samples in each cluster tighter and the different clusters are more dispersed, resulting in incorrectly labeled samples being categorized into the wrong cluster and samples with the same ground-truth being scattered. Group sampling makes each cluster looser and the distance between adjacent clusters closer, so as to achieve relatively soft margin. The samples with wrong pseudo labels have more chances to break away from the wrong class and be classified into the correct class. In this way, the reliability of pseudo labels is improved by group sampling, and then the model's representation capability is improved iteratively.</p><p>3) Correction rate and misleading rate: In this part, we define the correction rate and misleading rate to further analyze the difference between random sampling and group sampling. We find out the person identity set I i j * with the largest proportion in each class C i , as shown in Eq. (8), and then use j * to represent the principal identity of this class. If a sample's person identity j is consistent with the principal identity j * of the class to which the sample belongs, we subjectively define that the sample is classified correctly. We compare the classification results of samples in two adjacent epochs. If the incorrectly classified sample in the previous epoch is classified correctly in this epoch, we call it correction. On the contrary, if the correctly classified sample in the previous epoch is incorrectly classified in this epoch, we call it misleading. The curves of correction rate and misleading rate are shown in <ref type="figure">Fig. 4(e)</ref> and 4(f).</p><p>It illustrates that random sampling causes the correction rate remains low during the training process, which makes the initial misclassification of samples almost impossible to rectify. At the same time, in the early stage of training, the misleading rate is relatively high, which causes an increasing number of errors to accumulate. This makes the overall semantic information of the class to deviate and leads to deteriorated over-fitting. This phenomenon confirms the description in Sec. IV-A. The decrease in the misleading rate in the latter stage of training is because the overall structure of a class has been formed, and there are few sample interactions between classes. In contrast, group sampling has a higher correction rate, indicating that samples which are initially misclassified have more chances to be corrected, which helps to gradually improve the quality of pseudo labels. At the same time, the misleading rate is always within a reasonable range, which indicates that group sampling inhibits the degradation of semantic information of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Visualization analysis of features:</head><p>We randomly select samples for four person identities, and visualize the distribution of these samples in the feature space during the training process, as shown in <ref type="figure">Fig. 5</ref>. For random sampling, the samples of these four person identities are divided into clusters, each of which contains multiple identity samples. It is worth noting that the number of clusters produced by random sampling is more than that observed in group sampling, which seems to be contrary to statistical results in Sec. VI-C1 which suggests that random sampling results in fewer clusters. In fact, this is because the samples of each person identities are scattered in many classes, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>(a) and 2(b), and there are many samples of other person identities in each cluster which are not drawn in the figure.</p><p>In contrast, it is obvious that group sampling can distinguish samples with different person identities, and samples with the same person identity can be correctly clustered. Although there are two person identities (red and blue points) which are classified into the same cluster, the samples are not mixed together and are still distinguishable. It is worth noting that each pseudo label cluster contains only the samples shown in the figure, and there are no samples with other identities.</p><p>Visualization results further validate that group sampling can prevent samples with the same ground-truth identity from being misclassified into different classes, while avoiding deteriorated over-fitting. Simultaneously, during the training process, the identity similarity structure is not destroyed, which helps the model gradually learn the characteristics of each identity, thereby verifying the importance of statistical stability described in Sec. IV-B.</p><p>D. Ablation Study 1) Group size N in group sampling: Tab. III reports the analysis of group size N in group sampling. As discussed in Sec. V-B, N is the number of samples in each group, which plays an important role in affecting the optimization trend of the entire group. When N is small, a small number of samples are grouped together, which does not enable the avoidance of consequences related to deteriorated over-fitting. When N gradually increases, the group can better guide the overall trend of the class, which avoids the destruction of the similarity structure and maintains statistical stability to a certain extent, leading to better performance.</p><p>Our ablation experiments are conducted on Market-1501 and DukeMTMC-reID datasets. For the characteristics of these two datasets, when N ? 64, the performance no longer significantly changes, and fluctuation remains at around about 1%. This shows that at that time each group has enough samples to represent the overall class trend, and increasing the number of samples in the group will not increase the performance significantly. We believe that for different datasets, the selection of N may be different. Because of the different number of images in the dataset, the number of samples in each class is not the same. Therefore, the value of N needs to be set according to a specific dataset. Samples in each group need to be able to characterize the overall trend of the entire class, so it is also feasible to set N to be large enough to pack all samples in the class into one group. 2) Shuffling degree: In Sec. VI-C, experimental analysis shows that random sampling has a negative impact on the reliability of pseudo labels and the generalization performance of the model, which indicates that it is unfavorable to shuffle samples with different labels during sampling. In this section, we show how shuffling impacts on the reliability of pseudo labels and performance. On the basis of group sampling, we packed samples in adjacent M mini-batches into a set, and then shuffle the samples in each set. When M is larger, the shuffling degree is higher. Obviously, M = 1 is equivalent to group sampling, and when all mini-batches are packed for shuffling, denoted as M = all, it is equivalent to random sampling. We take M = {1, 4, 16, 64, all}, and the mAP scores, number of clusters and NMI scores are shown in Tab. IV. This shows that with the increase of the shuffling degree, performance decreases. In addition, the greater the difference between the number of clusters and ground-truth, the lower the NMI score. Therefore, we argue that shuffling is not conducive to the feature learning, and increased shuffling creates an increased number of deficiencies.</p><p>3) Influence of outliers: We also investigated the impact of outliers on feature learning during training. Tab. V shows experimental results when only clustered instances are used for training and when un-clustered outliers are included. It can be seen from the experimental results that the performance obtained by only training using the samples in clusters is poor, which shows that outliers play a vital role in feature learning during the training process.</p><p>Poor performance is mainly due to the clustering process which is performed on features stored in the memory bank, and the memory bank requires all entries to be continuously updated. When outliers are discarded, their features cannot be updated and only a few samples take part in <ref type="table" target="#tab_1">the  TABLE VII  COMPARISON OF THE STATE-OF-THE-ART UNSUPERVISED PERSON RE-ID METHODS WITH A RESNET-50 BACKBONE ON MARKET-1501,  DUKEMTMC-REID AND MSMT17. BOLD INDICATES THE BEST PERFORMANCE, AND UNDERLINE INDICATES THE SECOND BEST</ref>  training, undoubtedly leading to training collapse. Note that previous unsupervised person re-ID methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref> which abandoned outliers did not fail, since they did not utilize a continuously updated memory bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>Outlier treatment in sampling: Sec. V-B1 introduces a form of outlier treatment during group sampling, that is, all outliers are placed into the outlier set O, and then treat O as a group, denoted as Tre. I. Similarly, Sec. III-B2 introduces the way that triplet sampling handles outliers, that is, each outlier is treated as an independent group, denoted as Tre. II. This part uses triplet sampling and group sampling, while adopting these two outlier treatment approaches to conduct comparative experiments, as shown in Tab. VI.</p><p>It can be seen that when outlier treatment in triplet sampling is replaced with Tre. I, higher performance cannot be achieved. This indicates that the outlier treatment is not the factor inhibiting triplet sampling performance. When a specific outlier treatment in group sampling is replaced with Tre. II, performance drops significantly, which indicates that this treatment is unsuitable for group sampling. We suggest that this treatment causes un-clustered outliers and clustered instances to be sampled in the same mini-batch. Therefore, when there are few samples in a cluster, the cluster is easily affected by outliers in the mini-batch. In this case, re-sampling clustered instances in triplet sampling alleviates the impact of the outliers to a certain extent, which is confirmed by the performance of K = 16 in the penultimate row of Tab. VI. Conversely, Tre. II ensures that there are only un-clustered outliers or only clustered instances in a mini-batch, which reduces the mutual influence of them, and is more suitable for group sampling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with the State-of-the-art Methods</head><p>We compare the proposed method with the state-of-theart unsupervised person re-ID methods <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> on Market-1501 <ref type="bibr" target="#b30">[31]</ref>, DukeMTMC-reID <ref type="bibr" target="#b31">[32]</ref> and MSMT17 <ref type="bibr" target="#b32">[33]</ref>. Following conventional settings, we use the same backbone, i.e. ResNet-50, as other methods. Results are reported in Tab. VII. Note that the method listed in this table do not use the labeled source dataset.</p><p>As shown in Tab. VII, we observe our method is competitive with all the state-of-the-art methods. For example, it outperforms the current state-of-the-art (SpCL <ref type="bibr" target="#b24">[25]</ref>) by 6.1% of mAP and 4.2% of rank-1 accuracy on Market-1501. On DukeMTMC-reID, our method is also 3.8% higher than SpCL in terms of mAP. In addition, we use the latest MSMT17 V2 dataset version for training and testing, which is more difficult and challenging than MSMT17 V1. On MSMT17 V2, our method is also effective and achieves a high performance of 24.6% in mAP and 56.2% in rank-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we give an in-depth analysis of sampling strategies and reveal that they play an important role in the task of unsupervised person re-ID. The new concepts introduced include deteriorated over-fitting and statistical stability. Different sampling strategies applied to the same framework obtain very different performance, e.g., random sampling easily leads to training collapse due to the deteriorated over-fitting, while triplet sampling obtains relatively good performance. In addition, inspired by the fact that grouping samples can effectively maintain statistical stability to against deterioration, we further propose a novel group sampling strategy for pseudo-labelbased unsupervised person re-ID. Group sampling rectifies the shortcomings of previous sampling strategies, and effectively alleviates the negative effect of a single sample for statistical stability of unsupervised models. Extensive experimental results illustrate that the proposed method achieves state-ofthe-art performance on Market-1501, DukeMTMC-reID and MSMT17 without introducing any additional costs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(Top) A single sample misleads the optimization trend of overall features of the class, leading to undesirable similarity structure being strengthened and identity similarity structure being destroyed. As a result, the feature representation tends to deteriorated over-fitting. (Bottom) We group the samples that belong to the same class and adopt the overall trend of the class to weaken the influence of a single sample, thereby forming statistical stability within the class. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The degree of purity and chaos for different sampling strategies on Market-1501. It intuitively reflects the deteriorated over-fitting phenomenon caused by random sampling, and triplet sampling and group sampling can suppress this phenomenon. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(Left) The process of our framework. The pseudo labels are generated using sample features, and then the samples are sent to the model for feature extraction after sampling, and the memory bank is updated at the same time. Finally, features in the memory bank are used to generate pseudo labels. (Right) Illustration of group sampling strategy with the group size N = 2, which groups samples belonging to the same class for training. More details are described in Sec. V-B and Alg. 2. (Best viewed in color.) Algorithm 2 Group Sampling Require: Cluster set C and outlier set O; Require: Group size N ; Require: Batch size B; Require: Initialize an empty set S; 1: Shuffle the order of clusters in C; 2: for each cluster Ci in C do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Comparison and analysis between group sampling and random sampling. It is verified that random sampling leads to the degradation of the quality of pseudo labels and the deterioration of semantic information of class features. It also shows that group sampling maintains the statistically stability within the class, thereby generating purer classes and higher quality pseudo labels. (Best viewed in color.) t-SNE visualization of the distribution of samples in the feature space during the training. Samples of the same color belong to the same person identity. Dotted circles indicate pseudo label clusters, where the samples that are not divided into clusters are outliers. It intuitively shows that compared to random sampling, group sampling gathers samples with the same identity, indicating that it facilitates improving the representation capability of the model. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Initialize f ? with ImageNet-pretrained ResNet-50; Require: Initialize M with features extracted by f ? ; Require: Pseudo label generator G; 1: while max epochs not reached do</figDesc><table><row><cell>2:</cell><cell>Y = G(M)</cell></row><row><cell>3:</cell><cell>S = Sampling(X , Y)</cell></row><row><cell>4:</cell><cell>for each mini-batch {xi, yi} in S do</cell></row><row><cell>5:</cell><cell>vi = f ? (xi)</cell></row><row><cell>6:</cell><cell>Compute Lv by Eq. (3) and update the encoder f ? ;</cell></row><row><cell>7:</cell><cell></cell></row></table><note>Require: Unlabeled training set X ; Require:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>COMPARISON BETWEEN SEVERAL SAMPLING STRATEGIES.</figDesc><table><row><cell>Sampling Strategy</cell><cell cols="2">Market-1501</cell><cell cols="2">DukeMTMC-reID</cell></row><row><cell></cell><cell>mAP</cell><cell>top-1</cell><cell>mAP</cell><cell>top-1</cell></row><row><cell>Random Sampling</cell><cell>6.1</cell><cell>15.1</cell><cell>4.2</cell><cell>12.3</cell></row><row><cell>Triplet Sampling  ?</cell><cell>48.8</cell><cell>70.5</cell><cell>44.1</cell><cell>64.4</cell></row><row><cell>Group Sampling</cell><cell>79.2</cell><cell>92.3</cell><cell>69.1</cell><cell>82.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>OF DIFFERENT NUMBER OF INSTANCES K IN TRIPLET SAMPLING.</figDesc><table><row><cell>K</cell><cell cols="2">Market-1501</cell><cell cols="2">DukeMTMC-reID</cell></row><row><cell></cell><cell>mAP</cell><cell>top-1</cell><cell>mAP</cell><cell>top-1</cell></row><row><cell>4</cell><cell>48.8</cell><cell>70.5</cell><cell>44.1</cell><cell>64.4</cell></row><row><cell>8</cell><cell>69.4</cell><cell>87.2</cell><cell>60.5</cell><cell>77.5</cell></row><row><cell>16</cell><cell>77.6</cell><cell>90.2</cell><cell>67.1</cell><cell>81.8</cell></row><row><cell>32</cell><cell>46.3</cell><cell>71.3</cell><cell>42.7</cell><cell>62.2</cell></row><row><cell>64</cell><cell>10.2</cell><cell>24.4</cell><cell>9.6</cell><cell>18.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>-1501 [31] contains 32,668 images of 1,501 identities, each captured by at most 6 cameras. Specifically, it consists of 12,936 training images with 751 identities and 19,732 testing images with 750 identities. All of the images were cropped by a pedestrian detector.</figDesc><table><row><cell>DukeMTMC-reID [32] contains 36,411 images with 1,404</cell></row><row><cell>person identities captured by 8 cameras. Specifically, it con-</cell></row><row><cell>tains 16,522 images of 702 person identities for training,</cell></row><row><cell>2,228 query images and 17,661 gallery images of 702 person</cell></row><row><cell>identities for test.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDY ON PARAMETER N IN GROUP SAMPLING.</figDesc><table><row><cell>N</cell><cell cols="2">Market-1501 mAP top-1</cell><cell cols="2">DukeMTMC-reID mAP top-1</cell></row><row><cell>1</cell><cell>6.1</cell><cell>15.1</cell><cell>4.2</cell><cell>12.3</cell></row><row><cell>2</cell><cell>13.1</cell><cell>28.2</cell><cell>5.6</cell><cell>12.2</cell></row><row><cell>4</cell><cell>20.2</cell><cell>37.4</cell><cell>8.0</cell><cell>16.7</cell></row><row><cell>8</cell><cell>44.6</cell><cell>66.4</cell><cell>15.1</cell><cell>26.0</cell></row><row><cell>16</cell><cell>64.5</cell><cell>82.9</cell><cell>54.3</cell><cell>72.0</cell></row><row><cell>32</cell><cell>76.7</cell><cell>90.2</cell><cell>66.8</cell><cell>81.6</cell></row><row><cell>64</cell><cell>78.1</cell><cell>90.7</cell><cell>68.7</cell><cell>83.1</cell></row><row><cell>128</cell><cell>78.5</cell><cell>91.5</cell><cell>69.1</cell><cell>82.7</cell></row><row><cell>256</cell><cell>79.2</cell><cell>92.3</cell><cell>68.9</cell><cell>82.0</cell></row><row><cell>512</cell><cell>78.9</cell><cell>91.9</cell><cell>68.4</cell><cell>82.0</cell></row><row><cell>1024</cell><cell>79.1</cell><cell>91.6</cell><cell>69.1</cell><cell>82.5</cell></row><row><cell></cell><cell></cell><cell>TABLE IV</cell><cell></cell><cell></cell></row><row><cell cols="5">ABLATION STUDY ON SHUFFLING DEGREE ON MARKET-1501.</cell></row><row><cell>M</cell><cell>mAP</cell><cell cols="2">Number of Clusters</cell><cell>NMI Score</cell></row><row><cell>1</cell><cell>79.2</cell><cell>607</cell><cell></cell><cell>0.95</cell></row><row><cell>4</cell><cell>65.0</cell><cell>436</cell><cell></cell><cell>0.79</cell></row><row><cell>16</cell><cell>16.6</cell><cell>210</cell><cell></cell><cell>0.81</cell></row><row><cell>64</cell><cell>5.9</cell><cell>106</cell><cell></cell><cell>0.66</cell></row><row><cell>all</cell><cell>6.1</cell><cell>104</cell><cell></cell><cell>0.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V ABLATION</head><label>V</label><figDesc>STUDY ON INFLUENCE OF OUTLIERS. CLUSTERS: ONLY CLUSTERED INSTANCES ARE USED FOR TRAINING. CLUSTERS + OUTLIERS: INCLUDING UN-CLUSTERED OUTLIERS INTO TRAINING DATA.TABLE VI ABLATION STUDY ON OUTLIER TREATMENT IN SAMPLING. TRE. I: THE OUTLIER SET INCLUDING ALL THE OUTLIERS IS TREATED AS A GROUP. TRE. II: EACH OUTLIER IS TREATED AS AN INDEPENDENT GROUP.</figDesc><table><row><cell cols="2">Method</cell><cell cols="2">Market-1501</cell><cell></cell><cell cols="2">DukeMTMC-reID</cell></row><row><cell></cell><cell></cell><cell>mAP</cell><cell cols="2">top-1</cell><cell>mAP</cell><cell>top-1</cell></row><row><cell cols="2">Clusters</cell><cell>9.7</cell><cell cols="2">23.2</cell><cell>4.1</cell><cell>9.6</cell></row><row><cell cols="2">Clusters + Outliers</cell><cell>79.2</cell><cell cols="2">92.3</cell><cell>69.1</cell><cell>82.7</cell></row><row><cell>Outliers</cell><cell>Sampling</cell><cell></cell><cell cols="2">Market-1501</cell><cell cols="2">DukeMTMC-reID</cell></row><row><cell></cell><cell></cell><cell></cell><cell>mAP</cell><cell>top-1</cell><cell>mAP</cell><cell>top-1</cell></row><row><cell></cell><cell>Triplet K = 4</cell><cell></cell><cell>52.9</cell><cell>75.3</cell><cell>46.7</cell><cell>65.5</cell></row><row><cell>Tre. I</cell><cell cols="2">Triplet K = 16</cell><cell>76.0</cell><cell>90.2</cell><cell>66.9</cell><cell>81.3</cell></row><row><cell></cell><cell>Group</cell><cell></cell><cell>79.2</cell><cell>92.3</cell><cell>69.1</cell><cell>82.7</cell></row><row><cell></cell><cell>Triplet K = 4</cell><cell></cell><cell>48.8</cell><cell>70.5</cell><cell>44.1</cell><cell>64.4</cell></row><row><cell>Tre. II</cell><cell cols="2">Triplet K = 16</cell><cell>77.6</cell><cell>90.2</cell><cell>67.1</cell><cell>81.8</cell></row><row><cell></cell><cell>Group</cell><cell></cell><cell>64.2</cell><cell>81.1</cell><cell>62.8</cell><cell>79.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imageimage domain adaptation with preserved self-similarity and domaindissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="994" to="1003" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Similarity-preserving image-image domain adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10551</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="172" to="188" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple expert brainstorming for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Instance-guided context rendering for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adcluster: Augmented discriminative clustering for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9021" to="9030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint visual and temporal consistency for unsupervised domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with noise resistible mutual-training for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="526" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint disentangling and adaptation for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="87" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cgan-tm: A novel domain-to-domain transferring method for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5641" to="5651" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervised agent learning for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8549" to="8560" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-domain adversarial feature generalization for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1596" to="1607" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Complementary pseudo labels for unsupervised domain adaptation on person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2898" to="2907" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual-refinement: Joint label and feature refinement for unsupervised domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selfsimilarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="8738" to="8745" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dispersion based clustering for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic graph comatching for unsupervised video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2976" to="2990" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification via cross-camera similarity exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5481" to="5490" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification via softened similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3390" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification via multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">990</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical clustering with hard-batch triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint noisetolerant learning and meta camera shift adaptation for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nicu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4855" to="4864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint generative and contrastive learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lagadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2004" to="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic optimization with importance sampling for regularized loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="132" to="149" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in KDD</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="226" to="231" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">He is currently pursuing the M.S. degree in electronic and communication engineering with University of Chinese Academy of Sciences. His research interests include machine learning and computer vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Xumeng Han received the B.E. degree in electronic and information engineering from University of Electronic Science and Technology of China, China</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">He is currently pursuing the Ph.D. degree in signal and information processing with University of Chinese Academy of Sciences. His research interests include machine learning and computer vision</title>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xuehui Yu received the B.E. degree in software engineering from Tianjin University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">and Ph.D. degree in technology of computer application from the Graduate University of the Chinese Academy of Sciences in 2012. Now, she is an associate professor at the University of Chinese Academy of Sciences. Her research interests include object tracking, video analysis, pattern recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Guorong Li received her B.S. degree in technology of computer application from Renmin University of China</orgName>
		</respStmt>
	</monogr>
	<note>and cross-media analysis</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">He has published over 40 cutting-edge papers. He has received &quot;2020-2022 Youth Talent Promotion Project&quot; from China Association for Science and Technology in 2021 and &quot;2021-2023 Beijing Youth Talent Promotion Project&quot; from Beijing Association for Science and Technology in 2020</title>
	</analytic>
	<monogr>
		<title level="m">He has won the Lee Hwee Kuan Award (Gold Award) on PREMIA 2019, the &quot;Best Student Paper Award on ACM MM 2018, and the top-3 awards several times on world-wide competitions. He is the SAC of VALSE, the committee member of CSIG-BVD, and the member of the board of directors of BSIG. He has served as the invited reviewer of NSFC, T-PAMI, IJCV, NeurIPS (one of the top 30% highest-scoring reviewers of NeurIPS 2018), CVPR, etc. Gang Pan received the B.S., M.S. and Ph.D. degrees in School of</title>
		<meeting><address><addrLine>Beijing, China; China; Tianjin University</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Jian Zhao received the Bachelors degree from Beihang University in 2012, the Masters degree from National University of Defense Technology in 2014, and the Ph.D. degree from National University of Singapore in 2019. He is currently an Assistant Professor with Institute of North Electronic Equipment ; Computer Software and School of Computer Science from Tianjin University ; University of Alberta</orgName>
		</respStmt>
	</monogr>
	<note>Now he is an associate professor with the College of Intelligence and Computing. His research interests include computer vision and image synthesis, especially in the fields of industrial robot and culture heritage</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">His research interests include image processing, visual object detection and machine learning. He pioneered the Kernel SVM based pyrolysis output prediction software which was put into practical application by SINOPEC in 2012. He developed two kinds of piecewise linear SVM methods which were successfully applied into visual object detection</title>
	</analytic>
	<monogr>
		<title level="m">He has published more than 100 papers in refereed conferences and journals including IEEE CVPR, ICCV, ECCV and PAMI, and received the Sony Outstanding Paper Award</title>
		<meeting><address><addrLine>University of Maryland, College Park</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Harbin Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Qixiang Ye (M&apos;10-SM&apos;15) received the B.S. and M.S. degrees in mechanical</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">he was an Associate Professor with HIT. Since 2006, he has been a Professor with the School of Electronic, Electrical, and Communication Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao Received The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ph</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>Harbin, China; Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of the Chinese Academy of Sciences</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include image processing. intelligent surveillance</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Since 2013, he has been an Associate Professor with the School of Electronic, Electrical, and Communication Engineering</title>
	</analytic>
	<monogr>
		<title level="m">2006 and the M.S. and Ph.D. degrees from University of Chinese Academy of Sciences</title>
		<meeting><address><addrLine>Tianjin, China, in; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Zhenjun Han received the B.S. degree in software engineering from Tianjin University ; University of Chinese Academy of Sciences</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include object tracking and detection</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
