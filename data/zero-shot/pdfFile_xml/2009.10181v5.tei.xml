<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Image-based Automatic Meter Reading in Unconstrained Scenarios: A Robust and Efficient Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayson</forename><surname>Laroca</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><forename type="middle">B</forename><surname>Araujo</surname></persName>
							<email>abaraujo@inf.ufpr.br</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">C</forename><surname>De Almeida</surname></persName>
							<email>eduardo@inf.ufpr.br</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Menotti</surname></persName>
							<email>menotti@inf.ufpr.br</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Image-based Automatic Meter Reading in Unconstrained Scenarios: A Robust and Efficient Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2021.3077415)</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing approaches for image-based Automatic Meter Reading (AMR) have been evaluated on images captured in wellcontrolled scenarios. However, real-world meter reading presents unconstrained scenarios that are way more challenging due to dirt, various lighting conditions, scale variations, in-plane and out-of-plane rotations, among other factors. In this work, we present an end-to-end approach for AMR focusing on unconstrained scenarios. Our main contribution is the insertion of a new stage in the AMR pipeline, called corner detection and counter classification, which enables the counter region to be rectified -as well as the rejection of illegible/faulty meters -prior to the recognition stage. We also introduce a publicly available dataset, called Copel-AMR, that contains 12,500 meter images acquired in the field by the service company's employees themselves, including 2,500 images of faulty meters or cases where the reading is illegible due to occlusions. Experimental evaluation demonstrates that the proposed system, which has three networks operating in a cascaded mode, outperforms all baselines in terms of recognition rate while still being quite efficient. Moreover, as very few reading errors are tolerated in real-world applications, we show that our AMR system achieves impressive recognition rates (i.e., ? 99%) when rejecting readings made with lower confidence values.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic Meter Reading (AMR) refers to the technology whose goal is to automatically record the consumption of electric energy, gas and water for both monitoring and billing <ref type="bibr" target="#b10">[1,</ref><ref type="bibr" target="#b11">2]</ref>. Although smart meters are gradually replacing old meters, in many regions (especially in developing countries) the reading is still done manually in the field, on a monthly basis, by an employee of the service company who takes a picture as reading proof <ref type="bibr" target="#b12">[3,</ref><ref type="bibr" target="#b13">4]</ref>.</p><p>As such a procedure is prone to errors <ref type="bibr" target="#b14">[5]</ref><ref type="bibr" target="#b15">[6]</ref><ref type="bibr" target="#b16">[7]</ref>, the picture needs to be verified by another employee in some situations, This is an author-prepared version. The published version is available at the IEEE Xplore Digital Library (DOI: 10.1109/ACCESS.2021.3077415).</p><p>for example, when the consumer makes a complaint about the amount charged and when the registered consumption differs significantly from that consumer's average. This offline checking is known to be a laborious task <ref type="bibr" target="#b17">[8,</ref><ref type="bibr" target="#b18">9]</ref>.</p><p>In this context, image-based techniques for AMR are much needed, especially taking into account that it is not feasible to quickly replace old meters with smart ones <ref type="bibr" target="#b19">[10]</ref><ref type="bibr" target="#b20">[11]</ref><ref type="bibr" target="#b21">[12]</ref>. The idea behind image-based AMR, which is an specific scenario of scene text detection and recognition, is that the aforementioned inspection can be carried out automatically, reducing mistakes introduced by the human factor and saving manpower <ref type="bibr" target="#b12">[3,</ref><ref type="bibr" target="#b22">13]</ref>. As pointed out by Salomon et al. <ref type="bibr" target="#b23">[14]</ref>, the consumers themselves can capture photos of meters using a mobile device (e.g., a cell phone or a tablet). This eliminates the need for employees of the service company traveling around to perform local meter reading at each consumer unit, resulting in cost savings (especially in rural areas).</p><p>Although AMR (hereinafter AMR refers to image-based AMR) has received great attention in recent years, most works in the literature are still limited in several ways. In general, the experiments were performed either on proprietary datasets <ref type="bibr" target="#b13">[4,</ref><ref type="bibr" target="#b19">10,</ref><ref type="bibr" target="#b24">15]</ref> or on datasets containing images captured on well-controlled environments <ref type="bibr" target="#b12">[3,</ref><ref type="bibr" target="#b20">11,</ref><ref type="bibr" target="#b25">16]</ref>. This is in stark contrast to related research areas, such as automatic license plate recognition, where in recent years the research focus shifted to unconstrained scenarios (with challenging factors such as blur, various lighting conditions, scale variations, inplane and out-of-plane rotations, occlusions, etc.) <ref type="bibr" target="#b26">[17]</ref>, helping to advance the state of the art considerably. In addition, there are many works focused on a single stage of the AMR pipeline <ref type="bibr" target="#b16">[7,</ref><ref type="bibr" target="#b21">12,</ref><ref type="bibr" target="#b27">18]</ref>, which makes it difficult to accurately evaluate the presented methods in an end-to-end manner (e.g., the results achieved by a recognition model may vary considerably depending on how accurate the counter region is detected). Another factor that makes it difficult to assess existing methods, or their applicability, is that the authors commonly do not report the execution time of the proposed approaches or the hardware in which they performed their experiments <ref type="bibr" target="#b20">[11,</ref><ref type="bibr" target="#b25">16,</ref><ref type="bibr" target="#b27">18]</ref>. Finally, to the best of our knowledge, no previous work dealt with cases where it is not possible to perform the meter reading due to occlusions or faulty meters, even though such cases are relatively common in practice.</p><p>Considering the above discussion, in this work we present a novel end-to-end approach for AMR that leverages the high capability of Convolutional Neural Networks (CNNs) to achieve impressive results on real-world scenarios while still being quite efficient -it is capable of processing 55 frames per second (FPS) on a high-end GPU. For our system to be both robust and efficient, we focused on achieving the best speed/accuracy trade-off at each stage when designing it. Our main contribution is the insertion of a new stage in the AMR pipeline, called corner detection and counter classification, where a multi-task network detects the four corners of the counter and simultaneously classifies it as legible/operational or illegible/faulty. Prior to the recognition stage, legible counters are rectified using the predicted positions of the corners, thus improving the results obtained in distorted/inclined counters due to oblique views, and illegible counters are rejected. We remark that while improving the recognition performance has an important role in reducing manual intervention, automatically classifying and filtering out illegible meter readings is of paramount importance to the service company, as such cases still require human review.</p><p>As part of this work, we introduce a publicly available dataset 1 , called Copel-AMR, that contains 12,500 meter images acquired in the field by the service company's employees themselves (i.e., the images were taken in real-world conditions), including 2,500 images of faulty meters or cases where the reading is illegible. More specifically, we consider as faulty the meters where it is not possible to perform the meter reading because no reading is displayed (e.g., electronic meters where the display screen is blank) and as illegible the meter images where it is not possible to perform the meter reading due to occlusions in the counter region (e.g., dirt, reflections, and/or water vapor on the meter glass). To the best of our knowledge, this is the first public dataset for end-to-end AMR captured "in the wild" and also the only one with images of illegible/faulty meters. The proposed dataset has six times more images -and contains a larger variety in different aspects -than the largest dataset found in the literature for the evaluation of end-to-end AMR methods.</p><p>We experimentally evaluate the proposed approach in two public datasets: UFPR-AMR <ref type="bibr" target="#b12">[3]</ref> and Copel-AMR (described in Section 3). Our system achieves state-of-the-art results by outperforming 10 deep learning-based baselines in both datasets (we are not aware of any work in the AMR literature where so many methods were evaluated in the experiments). The importance of the corner detection and counter classification stage is demonstrated, as our system made 34% fewer reading errors in the legible/operational meters of the 1 The Copel-AMR dataset is publicly available to the research community at https://web.inf.ufpr.br/vri/databases/copel-amr/. Access is granted upon request, i.e., interested parties must register by filling out a registration form and agreeing to the dataset's terms of use.</p><p>Copel-AMR dataset -where the images were captured in unconstrained scenarios -when feeding rectified counters into the recognition network. Moreover, simultaneously to the prediction of the counter corners, our network is able to filter out most of the illegible/faulty meters (i.e., 98.9%), thereby reducing the overall cost of the proposed system since the counter rectification and recognition tasks are skipped in such cases, while correctly accepting 99.82% of the legible/operational meters.</p><p>In summary, our paper has three main contributions:</p><p>? A robust and efficient approach for AMR that achieves state-of-the-art results in two public datasets and that significantly reduces the number of images that are sent to human review by filtering out most images containing illegible/faulty meters. Our system explores three carefully designed and optimized networks, operating in a cascaded mode, to achieve the best trade-off between accuracy and speed.</p><p>? A public dataset for end-to-end AMR with 12,500 fully-annotated images acquired on real-world scenarios by the service company's employees themselves, being 10,000 of them of legible/operational meters and 2,500 of illegible/faulty meters. The dataset contains a well-defined evaluation protocol to assist the development of new approaches for AMR as well as the fair comparison among published works;</p><p>? A comparative assessment of the proposed approach and 10 baseline methods based on deep learning, unlike most works in the literature that reported only the results obtained by the proposed methods or compared them exclusively with traditional approachesoften carrying out experiments exclusively on proprietary datasets. It is observed that most of the reading errors made by our AMR system occurred in challenging cases, where even humans can make mistakes, as one digit becomes very similar to another due to artifacts in the counter region.</p><p>The remainder of this paper is organized as follows. We review related works in Section 2. The Copel-AMR dataset is introduced in Section 3. In Section 4, we present the proposed approach in detail. The experiments carried out and the results achieved are described in Section 5. Conclusions and future works are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Image-based AMR is a specific application of scene text detection and recognition <ref type="bibr" target="#b24">[15,</ref><ref type="bibr" target="#b28">19]</ref>. However, there are some fundamental differences for the general task of detecting and recognizing scene text that should be highlighted: (i) in the AMR context, there is only one region of interest in each image (i.e., the counter region) and all digits are within it;</p><p>(ii) recognition networks for AMR need to learn 10 classes (digits 0 to 9) while networks for general scene text recognition need to learn 36 character classes (26 letters and 10 digits; the number of classes can be even higher depending on the language); and (iii) AMR presents an unusual challenge in Optical Character Recognition (OCR): rotating digits in electromechanical meters. Typically, rotating digits are a major cause of errors in such meters, even when robust approaches are employed for digit/counter recognition <ref type="bibr" target="#b12">[3,</ref><ref type="bibr" target="#b28">19]</ref>. This last point further emphasizes the importance of the proposed dataset, as recognition models trained exclusively on images from general datasets for robust reading (e.g., ICDAR 2013 <ref type="bibr" target="#b29">[20]</ref>) are likely to fail in these cases.</p><p>Over the last decade, a number of methods have been proposed for image-based AMR. Prior to the widespread adoption of deep learning in computer vision, most approaches to this task explored image enhancement techniques and handcrafted features with a similar pipeline, i.e., (i) counter detection followed by (ii) digit segmentation and (iii) digit recognition <ref type="bibr" target="#b14">[5,</ref><ref type="bibr" target="#b15">6]</ref>. Most limitations of such methods may be attributed to the fact that handcrafted features are easily affected by noise and are generally not robust to images captured under unconstrained environments <ref type="bibr" target="#b16">[7,</ref><ref type="bibr" target="#b20">11,</ref><ref type="bibr" target="#b30">21]</ref>.</p><p>In deep learning-based methods <ref type="bibr" target="#b13">[4,</ref><ref type="bibr" target="#b24">15,</ref><ref type="bibr" target="#b27">18]</ref>, on the other hand, usually the entire counter region is fed into the recognition network and all digits are predicted simultaneously (instead of first segmenting and then recognizing each of them). As major advances have been achieved in computer vision through deep learning <ref type="bibr" target="#b31">[22]</ref>, in this section we review works that employed deep learning-based approaches in the AMR context.</p><p>We also focus on studies related to digit-based meters, even though there are some recent works that addressed the recognition of dial meters <ref type="bibr" target="#b22">[13,</ref><ref type="bibr" target="#b23">14,</ref><ref type="bibr" target="#b32">23]</ref>. Such works usually explore the angle between the pointer and the dial to perform the reading.</p><p>Object detectors have been explored frequently to deal with counter detection. For example, Ko??evi? &amp; Suba?i? <ref type="bibr" target="#b19">[10]</ref> employed Faster R-CNN <ref type="bibr" target="#b33">[24]</ref> to detect counters and serial numbers on images of residential meters, whereas Tsai et al. <ref type="bibr" target="#b21">[12]</ref> applied and fine-tuned Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b34">[25]</ref> for counter detection in electricity meters. In both studies, only proprietary datasets were used to evaluate the detectors.</p><p>Similarly, Laroca et al. <ref type="bibr" target="#b12">[3]</ref> tackled counter detection using Fast-YOLOv2 <ref type="bibr" target="#b35">[26]</ref>. Considering that all counter regions were correctly detected in their experiments, the authors stated that very deep models are not necessary to handle this task. For counter recognition, three CNN-based approaches were evaluated, with the CR-NET model <ref type="bibr" target="#b36">[27]</ref> outperforming two segmentation-free models <ref type="bibr" target="#b37">[28,</ref><ref type="bibr" target="#b38">29]</ref> in terms of recognition rate. It should be noted that the images they used were acquired in a warehouse of the service company by one of the authors; in other words, the images are not as uncon-strained as those collected in the field by the service company's employees (e.g., there is no external lighting or occlusions caused by railings or vegetation).</p><p>Rather than exploring object detectors, Calefati et al. <ref type="bibr" target="#b39">[30]</ref> employed a Fully Convolutional Network (FCN) for semantic segmentation <ref type="bibr" target="#b40">[31]</ref> to handle the detection stage. Then, the counter region was aligned horizontally through the application of traditional image-processing techniques, such as contours extraction and mathematical morphology, in the segmentation mask. Finally, a CNN model was employed to produce the meter reading from the aligned counter region.</p><p>Although their experiments were carried out on real-world images, only a cropped version of their dataset is available for the research community (as only the region containing the digits was kept in each image, it is not possible to use the released dataset for the evaluation of end-to-end methods). In addition, the accuracy rates obtained in some digit positions were significantly lower than in others due to the low variability in such positions (their dataset is biased and so is their recognition model; this phenomenon was also observed by Laroca et al. <ref type="bibr" target="#b12">[3]</ref>). Such a limitation must be addressed before an AMR solution can be used in practice.</p><p>Yang et al. <ref type="bibr" target="#b27">[18]</ref> combined an FCN and Connectionist Temporal Classification (CTC) without any intermediate recurrent connections for counter recognition in water meter images. Their network achieved better recognition results than two baselines, showing that such a network is capable of learning contextual information and thus eliminating the need for recurring layers. However, it is important to note that their experiments were carried out only on manually-cropped counter regions and that such a segmentation-free approach may not be as robust in cases where the region of interest (here, the counter) is not detected as precisely <ref type="bibr" target="#b41">[32]</ref>.</p><p>Taking into account the importance of designing highly efficient methods in the AMR context, Li et al. <ref type="bibr" target="#b20">[11]</ref> proposed a light-weight CNN for counter recognition that splices a certain number of 1 ? 1 and 3 ? 3 kernels to reduce the network parameters with little loss in the recognition rate. The results reported by them are impressive considering the accuracy/speed trade-off obtained; nevertheless, their experiments were performed exclusively on a private dataset with wellcontrolled images quite similar to each other (i.e., the images were captured by a camera installed in the meter box and preprocessed manually by the authors; thus, they have no blur, scale variations, shadows, occlusions, significant rotations, among other challenging factors).</p><p>Marques et al. <ref type="bibr" target="#b13">[4]</ref> fine-tuned the Faster R-CNN and Reti-naNet <ref type="bibr" target="#b42">[33]</ref> object detectors for counter recognition. Although the authors reported mean Average Precision (mAP) rates above 90% with both detectors, only a small subset of counter images from a private dataset was employed in their experiments and the hardware used (i.e., the GPU) was not specified, making it difficult to compare their methodology with previous works both in terms of efficiency and recogni- <ref type="figure">Fig. 1</ref>. Some images extracted from the Copel-AMR dataset. Note that there are both electromechanical and electronic meters and that meters of different types/models often have different screen sizes and layouts. The last two images in each row are from faulty or illegible meters. As requested by Copel, the regions containing consumer identification were blurred on each image due to privacy constraints. tion rate. Waqar et al. <ref type="bibr" target="#b16">[7]</ref> also employed Faster R-CNN for counter recognition, however, a low recognition rate of 76% was reported in their experiments. As in <ref type="bibr" target="#b39">[30]</ref>, the accuracy achieved in some digit positions was considerably lower than in others, probably due to the fact that the authors did not take into account the bias in the distribution of the digit classes in the training set when fine-tuning the Faster R-CNN model. Despite the fact that the authors claimed that their method can be deployed in real-time applications, no experiments related to execution time were performed/reported.</p><p>There are some works in which the authors chose to perform the meter reading directly in the input image, i.e., without counter detection. For instance, Liao et al. <ref type="bibr" target="#b30">[21]</ref> simply employed YOLOv3 <ref type="bibr" target="#b43">[34]</ref> for this task, whereas G?mez et al. <ref type="bibr" target="#b24">[15]</ref> proposed a CNN model that directly outputs the meter reading in a segmentation-free manner. Although promising results were reported in these works, such approaches are not robust to severe perspective distortions and small-meter images <ref type="bibr" target="#b12">[3,</ref><ref type="bibr" target="#b39">30]</ref>.</p><p>Considering the many limitations of existing works, we propose a novel end-to-end AMR system that contains a unified approach for corner detection and counter classification in order to (i) improve the recognition results (especially in unconstrained scenarios) through counter rectification and (ii) significantly reduce the number of images that are sent to human review by filtering out images containing illegible/faulty meters. The proposed system is empirically evaluated in two public datasets that have well-defined evaluation protocols and that enable the evaluation of end-to-end AMR methods. One of the datasets, called UFPR-AMR <ref type="bibr" target="#b12">[3]</ref>, has 4K images collected in a warehouse of the service company by one of its authors, i.e., under controlled capture conditions, while the other (introduced in Section 3) contains 480p images acquired in the field by the service company's employees themselves, i.e., under unconstrained capture environments.</p><p>In our experiments, detailed information regarding both the hardware/frameworks used and the execution time required to run our AMR system is also provided in order to enable an accurate analysis of its speed/accuracy trade-off, as well as its applicability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE COPEL-AMR DATASET</head><p>The Copel-AMR dataset contains 12,500 meter images acquired in the field by the employees of the Energy Company of Paran? <ref type="bibr">(Copel)</ref>, which directly serves more than 4 million consuming units, across 395 cities and 1,113 locations (i.e., districts, villages and settlements), located in the Brazilian state of Paran? <ref type="bibr" target="#b44">[35]</ref>. Thus, Copel-AMR is composed of images captured in unconstrained scenarios, which typically include blur (due to camera motion), dirt, scale variations, in-plane and out-of-plane rotations, reflections, shadows, and occlusions. In 2,500 images (i.e., 20% of the dataset), it is not even possible to perform the meter reading due to occlusions or faulty meters. Although such situations are found on a daily basis by meter readers, there is no work in the literature addressing them or public datasets containing images of illegible/faulty meters, to the best of our knowledge. <ref type="figure">Fig. 1</ref> shows the diversity of the dataset. Note that as the model of the meters being installed/replaced has changed over the years, there is a wide variety of meter types in our dataset.</p><p>The images have a resolution of 480 ? 640 or 640 ? 480 pixels, depending on the orientation in which they were taken. Considering that the meter is operational and that there are no occlusions, these resolutions are enough for the meter reading to be legible.</p><p>For each image in our dataset, we manually labeled the meter reading, the position (x, y) of each of the four corners of the counter, and a bounding box (x, y, w, h) for each digit. Corner annotations -which can be converted to a bounding box -enable the counter to be rectified, while bounding boxes enable the training of object detectors as well as the application of a wider range of data augmentation techniques. As far as we are aware, the Copel-AMR dataset is the only one to provide so much labeled information for each image.</p><p>As the Copel-AMR dataset contains 10,000 images of legible/operational meters and each meter reading consists of 5 digits, we manually labeled a total of 50,000 digits. The distribution of the digit classes in the dataset is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We observed that the digit '0' has many more instances than the others, which was expected, due to the fact that a brand new meter starts with 00000 and the leftmost digit positions take longer to be increased.  In the AMR context, it is common that the digit '0' has many more instances than the others, as a brand new meter starts with 00000.</p><p>In electromechanical meters, it is possible that the digits (usually, the rightmost one) are rotating (see an example in the 3rd row and 4th column in <ref type="figure">Fig. 1</ref>). In such cases, following the protocol adopted at Copel, we considered the lowest digit as the ground truth (e.g., a digit rotating from '4' to '5' is labeled as '4'), except between digits '9' and '0' where the digit should be labeled as '9'.</p><p>With the advances of deep learning-based techniques and the availability of ever larger datasets, in many cases it is timeconsuming to divide the datasets multiple times and then average the results among multiple runs. Hence, public datasets introduced in recent years commonly have a single division of the images into training, validation and test sets <ref type="bibr" target="#b12">[3,</ref><ref type="bibr" target="#b23">14]</ref>. In the same direction, we randomly split the Copel-AMR dataset as follows: 5,000 images for training, 5,000 images for testing and 2,500 images for validation, following the split protocol (i.e., 40%/40%/20%) used in the UFPR-AMR dataset. We preserved the percentage of samples for illegible/faulty meters, that is, there are 1,000 images of illegible/faulty meters in each of the training and test sets, and 500 images in the validation one. For reproducibility purposes, the subsets gener-ated are explicitly available along with the proposed dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PROPOSED APPROACH</head><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, the proposed approach consists of three main stages: (i) counter detection, (ii) corner detection and counter classification, and (iii) counter recognition 2 . Given an input image, the counter region is located using a modified version of the Fast-YOLOv4 model, called Fast-YOLOv4-SmallObj. Then, in a single forward pass of the proposed Corner Detection and Counter Classification Network (CDCC-NET), the cropped counter is classified as legible/operational or illegible/faulty and the position (x, y) of each of its corners is predicted. Finally, illegible counters are rejected, while legible ones are rectified and fed into our recognition network, called Fast-OCR.</p><p>In the remainder of this section, each stage of the proposed system is better described. It is worth noting that, for each stage, we train a single network on images from both datasets in which we perform experiments (see Section 5.1). In this way, our networks become robust to images captured under different conditions with significantly less manual effort, as the network parameters are adjusted only once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Counter Detection</head><p>In unconstrained scenarios, locating the corners (2D points) of the counter directly in the input image is a challenging task for three main reasons: (i) one or more corners may not be visible due to occlusions caused by dirt, reflections, and other factors; (ii) the counter region may occupy a very small portion of the input image, as can be seen in <ref type="figure">Fig. 1</ref>; and (iii) some text blocks (e.g., meter specifications and serial number) are very similar to the counter region in certain meter models. Therefore, we first locate the counter in the input image and then detect its corners in the cropped patches.</p><p>As the counter region is rectified prior to the recognition stage in our system's pipeline, the counter detector does not need to be very sophisticated or rely on very deep models. In other words, our AMR system can tolerate less accurate detections of the counter region, as the corners will be later located and the counter rectified. Taking this into account as well as the importance of having an efficient system in real-world applications, we initially decided to use the Fast-YOLOv4 model <ref type="bibr" target="#b45">[36]</ref> for this task since, despite being much smaller than state-of-the-art object detectors, Fast-YOLO variants (also known as Tiny-YOLO) are still able to detect some objects quite precisely <ref type="bibr" target="#b46">[37]</ref> and have been employed in various research areas in recent years <ref type="bibr" target="#b23">[14,</ref><ref type="bibr" target="#b47">38,</ref><ref type="bibr" target="#b48">39]</ref>.</p><p>Nevertheless, we noticed in preliminary experiments that Fast-YOLOv4 failed in some cases where the meter was rela- tively far from the camera (usually in images where the reading is illegible). Therefore, we performed some modifications to the network in order to improve its performance in detecting small objects. More specifically, following insights from <ref type="bibr" target="#b43">[34,</ref><ref type="bibr" target="#b49">40,</ref><ref type="bibr" target="#b50">41]</ref>, we added a few layers to the network so that it predicts bounding boxes at 3 different scales instead of 2. This was done by (i) taking the feature map from the next-tolast layer and upsampling it by a factor of 2; (ii) concatenating a feature map from earlier in the network with the upsampled features; and (iii) adding some convolutional layers to process this combined feature map and predict a similar tensor but with twice the size. <ref type="table" target="#tab_0">Table 1</ref> shows the modified architecture, which hereinafter is referred to as Fast-YOLOv4-SmallObj.</p><p>Observe that the final feature map is now 48 ? 48 instead of 24 ? 24 pixels (for an input size of 384 ? 384 pixels), which makes fine details better visible; consequently, small objects can be detected more accurately. There are 18 filters (instead of 255) in layers 15, 22 and 29 so that the network predicts 1 class instead of 80.</p><p>The input size of 384 ? 384 pixels was chosen based on careful assessments carried out in the validation set, where we sought the best balance between speed and accuracy with different input dimensions (from 320 ? 320 to 608 ? 608 pixels). It is remarkable that, according to our experiments, for this task, Fast-YOLOv4-SmallObj performed better than the Fast-YOLOv4 model with larger input sizes, while requiring comparable or even less floating-point operations (FLOP) in each forward pass. As an example, Fast-YOLOv4-SmallObj, which requires 6.8 billion floating-point operations (BFLOP), achieved 99.88 mAP in the validation set, whereas Fast-YOLOv4 with an input size of 608 ? 608 pixels reached 99.13% mAP while requiring 14.5 BFLOP in each forward pass.</p><p>We exploit several data augmentation strategies to train our network, such as random cropping, shearing, conversion to grayscale, and random perturbations of hue, saturation and brightness. As each image contains a single meter, only the detection with the highest confidence value is considered in cases where more than one counter region is predicted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Corner Detection and Counter Classification</head><p>In order to rectify the cropped counter patch, we need to first locate the four corners of the counter. For this purpose, we designed a multi-task network <ref type="bibr" target="#b51">[42]</ref>, called CDCC-NET, that analyzes the counter region detected in the previous stage and predicts 9 outputs: eight float numbers referring to the corner positions (x 0 /w, y 0 /h, . . . , x 3 /w, y 3 /h) and an array containing two float numbers regarding the probability of the counter being legible/operational or illegible/faulty. We consider as input to the network a counter region slightly larger than the one detected in the previous stage in order to try to ensure that all corners are within the cropped patch even in less accurate detections. CDCC-NET's architecture is shown in <ref type="table" target="#tab_1">Table 2</ref>. As can be seen, there are three shared convolutional layers with 16/32/64 filters, each followed by a max-pooling layer with a 2 ? 2 kernel and stride = 2. There are also two fully connected (or dense) layers for each of the 9 outputs (i.e., these two layers are not shared). Observe that in the second nonshared dense layer there is a single unit for the prediction of each of the eight corner coordinates (a single float number is predicted for each task), and two units for the prediction of the probabilities of the counter being legible or illegible (here we employed the softmax function to enforce that the sum of the probabilities is equal to 1). The input size of the CDCC-NET model is 192 ? 64 pixels. These dimensions were defined by halving the input size used in the previous stage, as here the region of interest is already cropped, and by adapting it to the mean aspect ratio of the counters in the Copel-AMR dataset (w/h ? 3). Thus, all images are resized to 192 ? 64 pixels before being fed into the network. However, to avoid distortions when resizing the images, we first add black borders on them so that they have an aspect ratio (w/h) close/equal to 3.</p><p>The main difference between CDCC-NET and existing networks for corner detection in other applications, such as license plate recognition <ref type="bibr" target="#b52">[43,</ref><ref type="bibr" target="#b53">44]</ref>, is that the proposed network is relatively shallow and has a specific dense layer for predicting each output, while the existing models usually have more intermediate layers with many more filters and a single dense layer to predict all output values. Another approach worth mentioning is that proposed by Lyu et al. <ref type="bibr" target="#b54">[45]</ref> for multioriented scene text detection, where each corner point is rede-fined and represented by a horizontal square C = (x c , y c , ss, ss), where x c , y c are the coordinate of a corner point (such as x 1 , y 1 for top-left point) as well as the center of the horizontal square (ss is the length short side of the rotated rectangular bounding box). Then, the corner points are detected as default bounding boxes through a model with a backbone adapted from VGG16 <ref type="bibr" target="#b55">[46]</ref> containing several extra convolutional layers and a few deconvolution modules.</p><p>Considering that the number of training images is still limited to train such a multi-task network (i.e., there is no public dataset for AMR with hundreds of thousands of labeled images) and also the fact that the counter region is well-aligned in most cases (especially in the UFPR-AMR dataset <ref type="bibr" target="#b12">[3]</ref>), we created many artificial images through data augmentation in order to prevent overfitting. We performed random variations of hue, saturation and brightness to the original images, in addition to randomly rotating and cropping them. We also randomly permuted the position of the digits on the counters to eliminate undesirable biases in network learning related to the corner positions and certain classes of digits, for example, the network might learn a false correlation between the top-left/bottom-left corners and digits '0' since most occurrences of the class '0' are in the leftmost digit position (see <ref type="figure" target="#fig_1">Fig. 2</ref>). <ref type="figure" target="#fig_3">Fig. 4</ref> shows some examples of the images generated by us. Note that the bounding box of each digit, which is labeled in our dataset, is required to apply this data augmentation technique. For the sake of completeness, following <ref type="bibr" target="#b56">[47]</ref>, the 3 ? 3 matrix of the perspective transform is calculated so that:</p><formula xml:id="formula_0">? ? t i x i t i y i t i ? ? = map matrix ? ? ? x i y i 1 ? ? (1)</formula><p>where dst(i) = (x i , y i ), src(i) = (x i , y i ), i = 0, 1, 2, 3 <ref type="bibr" target="#b11">(2)</ref> and the counter region is rectified using the specified matrix:</p><formula xml:id="formula_1">dst(x, y) = src M11x+M12y+M13 M31x+M32y+M33 , M21x+M22y+M23 M31x+M32y+M33<label>(3)</label></formula><p>Counters classified by CDCC-NET as illegible/faulty, on the other hand, are rejected. To the best of our knowledge, in the AMR context, this is the first work in which the region of interest is rectified prior to the recognition stage. As illustrated in <ref type="figure" target="#fig_5">Fig. 5</ref>, rectified counters become more horizontal, tightly-bounded, and easier to read.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Counter Recognition</head><p>Once the counter region has been located and rectified, the digits must be recognized. To this end, we built a new lightweight detection network, called Fast-OCR, that incorporates features from existing models focused on the speed/accuracy trade-off, such as YOLOv2 <ref type="bibr" target="#b35">[26]</ref>, CR-NET <ref type="bibr" target="#b36">[27]</ref> and Fast-YOLOv4. In this work, we handle counter recognition as an object detection problem since object detectors have been successfully applied to several character recognition tasks in recent years <ref type="bibr" target="#b57">[48]</ref><ref type="bibr" target="#b58">[49]</ref><ref type="bibr" target="#b59">[50]</ref>. Accordingly, the Fast-OCR model is trained to predict 10 classes (i.e., 0-9) using the cropped counter patch as well as the class and bounding box (x, y, w, h) of each digit as inputs.</p><p>The architecture of Fast-OCR is shown in <ref type="table" target="#tab_2">Table 3</ref>. The input size is 384 ? 128 pixels considering both the number of max-pooling layers in the network (i.e., the dimensions of the output layer must be large enough to enable the detection of multiple digits horizontally spread side by side) and also the mean aspect ratio of the counters in the Copel-AMR dataset (w/h ? 3). As Fast-YOLOv4, the proposed network performs detection at 2 different scales (layers 14 and 21). This is particularly important for counter recognition in unconstrained scenarios due to the fact that the digits may occupy either a small or a large portion of the counter region depending on the meter model, as illustrated in <ref type="figure" target="#fig_6">Fig. 6</ref>, and also on how accurately the counter corners were detected in the previous stage. As in the networks introduced in <ref type="bibr" target="#b35">[26,</ref><ref type="bibr" target="#b36">27]</ref>, the convolutional layers in Fast-OCR mostly have 3 ? 3 kernels and the number of filters is doubled after each max-pooling layer. In addition, there are 1 ? 1 convolutional layers between 3 ? 3 convolutions to reduce the feature space from preceding layers.  In <ref type="table" target="#tab_2">Table 3</ref>, we also list the number of FLOP required in each layer to highlight how small the Fast-OCR network is compared to some deeper detection models. For example, for this task, our network requires 6.6 BFLOP while YOLOv3 <ref type="bibr" target="#b43">[34]</ref> and YOLOv4 <ref type="bibr" target="#b49">[40]</ref> require 65.4 and 59.5 BFLOP, respectively.</p><p>It is important to note that, thanks to the versatility and ability of detection networks to learn the general features of objects (here, the digits) regardless of their positions, and also to the confidence value tied to each prediction, Fast-OCR's output can be easily adapted/improved (i.e., without making any changes to its architecture) through post-processing heuristics. For example, a variable number of digits is predicted for each counter patch fed into the network, and some digits can be discarded based on the confidence values which they were predicted or through geometric constraints; in other words, Fast-OCR can be applied -without any modificationto counters with different numbers of digits. In fact, we believe that heuristic rules can also be explored to identify illegible/faulty meters erroneously classified as legible/operational in the previous stage, since it is very likely that counters classified as legible/operational should have been classified as illegible/faulty in cases where Fast-OCR has not predicted any digit with a high confidence value.</p><p>As the Fast-OCR model is trained from scratch, many training samples are needed for the network to generalize well. Therefore, in addition to using the original images of the counter region, we exploit the images artificially generated in the previous stage to train Fast-OCR and improve its robustness. Note that using the exactly same images in both stages is possible since patches of the counter region are fed as input into both CDCC-NET and Fast-OCR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setup and Baselines</head><p>In this work, we conducted experiments on images from the Copel-AMR and UFPR-AMR <ref type="bibr" target="#b12">[3]</ref> datasets. The UFPR-AMR dataset contains 2,000 images acquired in relatively well-controlled environments, with a resolution between 2,340 ? 4,160 and 3,120 ? 4,160 pixels, and is split into three subsets: training (800 images), validation (400 images) and testing (800 images). In order to train, validate and test our networks, we merge the respective subsets from both datasets (i.e., exactly the same networks are used regardless of which datasets we are running experiments on). As the UFPR-AMR dataset does not have any annotations related to the corners of the counters, we manually labeled their positions in its 2,000 images so that we can use images from both datasets to train/evaluate the CDCC-NET model. All annotations made by us are publicly available to the research community along with the Copel-AMR dataset.</p><p>It is important to point out that in several recent works in the literature <ref type="bibr" target="#b19">[10,</ref><ref type="bibr" target="#b24">15,</ref><ref type="bibr" target="#b30">21,</ref><ref type="bibr" target="#b39">30]</ref> the authors reported only the results obtained by the proposed methods or compared them exclusively with traditional approaches, overlooking deep learning-based approaches designed for AMR. This makes it difficult to make a fair comparison between recently published works. Taking this into account, in this work, the end-to-end results achieved by the proposed system are compared (both in terms of recognition rate and execution time) with those obtained by several baseline methods <ref type="bibr" target="#b12">[3,</ref><ref type="bibr" target="#b24">15,</ref><ref type="bibr" target="#b30">21,</ref><ref type="bibr" target="#b39">30,</ref><ref type="bibr" target="#b49">40]</ref> trained by us on exactly the same images as the proposed method <ref type="bibr" target="#b12">3</ref> . These specific methods, de- <ref type="bibr" target="#b12">3</ref> The architectures and weights of the baselines implemented/trained by scribed in Section 2, were chosen/implemented by us for two main reasons: (i) they were recently employed in the context of AMR (except <ref type="bibr" target="#b49">[40]</ref>, which was recently introduced) with promising/impressive results being reported, and (ii) we believe we have the knowledge necessary to train/adjust them in the best possible way in order to ensure fairness in our experiments, as the authors provided enough details about the architectures used, and also because we designed/employed similar networks (even the same ones in certain cases) in recent works in the context of license plate recognition and related areas <ref type="bibr" target="#b38">[29,</ref><ref type="bibr" target="#b41">32,</ref><ref type="bibr" target="#b59">50]</ref>. Note that, in our experiments, we adapted all networks so that their input layers have the same aspect ratio (w/h = 3).</p><p>Regarding the baselines, G?mez et al. <ref type="bibr" target="#b24">[15]</ref> evaluated their recognition network on a dataset containing mostly images where the counter is well centered and occupies a good portion of the image; therefore, in our experiments we first detect the counter region in the input image with the Fast-YOLOv4-SmallObj model and then apply their network to the detected region. The same was done with the recognition network proposed by Calefati et al. <ref type="bibr" target="#b39">[30]</ref>, as they dealt with the counter detection stage using an FCN for semantic segmentation <ref type="bibr" target="#b40">[31]</ref> that we do not have the knowledge necessary to train/adjust in the best possible way. We consider reimplementing/retraining it out of scope for this work since our detection model (i.e., Fast-YOLOv4-SmallObj) is able to achieve high F-measure rates in both datasets (see Section 5.2.1) and also due to the fact that the segmentation task is generally more time-consuming than the detection one.</p><p>The YOLO-based models were trained using the Darknet framework <ref type="bibr" target="#b13">4</ref> , while the other models were trained using Keras 5 . In Darknet, the following parameters were used: 65K iterations (max batches), batch size = 64, and learning rate = [10 -3 , 10 -4 , 10 -5 ] with decay steps at 26K and 45.5K iterations. In Keras, we employed the following parameters: initial learning rate = 10 -3 (with ReduceLROn-Plateau's patience = 3 and factor = 10 -1 ), batch size = 128, max epochs = 100, and patience = 7 (patience refers to the number of epochs with no improvement after which training will be stopped). All networks were trained using the Stochastic Gradient Descent (SGD) optimizer, and all experiments were carried out on a computer with an AMD Ryzen Threadripper 1920X 3.5GHz CPU, 48 GB of RAM, SSD (read: 535 MB/s; write: 445 MB/s), and an NVIDIA Titan V GPU. We remark that all parameter values were defined based on experiments performed in the validation set.</p><p>In addition to the baselines mentioned above, we trained and evaluated all models for scene text recognition implemented in <ref type="bibr" target="#b60">[51]</ref> (i.e., CRNN <ref type="bibr" target="#b37">[28]</ref>, RARE <ref type="bibr" target="#b61">[52]</ref>, R2AM <ref type="bibr" target="#b62">[53]</ref>, STAR-Net <ref type="bibr" target="#b63">[54]</ref>, GRCNN <ref type="bibr" target="#b64">[55]</ref>, Rosetta <ref type="bibr" target="#b65">[56]</ref> and TRBA <ref type="bibr" target="#b66">[57]</ref>), us are also publicly available at https://web.inf.ufpr.br/vri/ publications/amr-unconstrained-scenarios/ 4 https://github.com/AlexeyAB/darknet/ 5 https://keras.io/</p><p>which is the open source repository (PyTorch 6 ) of Clova AI Research used to record the 1st place of ICDAR2013 focused scene text and ICDAR2019 ArT, and 3rd place of ICDAR2017COCO-Text and ICDAR2019 ReCTS (task1). For reasons of space and clarity, instead of reporting the results obtained by all these models, we included in our overall evaluation only the results obtained by the model that performed faster (CRNN <ref type="bibr" target="#b37">[28]</ref>) and by the one that obtained the best recognition rates (TRBA <ref type="bibr" target="#b66">[57]</ref>) in our experiments. It is worth noting that, as stated in Section 2, recognition models trained exclusively on images from datasets for general robust reading (e.g., ICDAR 2013 <ref type="bibr" target="#b29">[20]</ref>) are likely to fail in AMR scenarios due to some domain-specific characteristics (e.g., rotating digits). However, we believe that first pretraining them using images from large-scale scene text recognition datasets and then fine-tuning them on images from AMR datasets can enable even better results to be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Discussion</head><p>In this section, we report the experiments carried out to verify the effectiveness of the proposed AMR system. We first assess the counter detection stage separately since the regions used in the following stages are extracted from the detection results, rather than cropped directly from the ground truthnote that a detection failure probably leads to another failure in the subsequent stages. Similarly, we then report the results reached by CDCC-NET in both corner detection and counter classification tasks. Finally, we evaluate the entire AMR system in an end-to-end manner (without any prior knowledge as to which dataset each test image belongs to or whether it is from a legible/operational or illegible/faulty meter) and compare the reading results achieved in legible/operational images with those obtained by 10 baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Counter Detection</head><p>Detection tasks in the AMR context are often evaluated by considering a predicted bounding box to be correct if its Intersection over Union (IoU) with the ground truth is greater than 50% <ref type="bibr" target="#b12">[3,</ref><ref type="bibr" target="#b23">14]</ref>. Nevertheless, such a low threshold (IoU &gt; 0.5) was deliberately defined by Everingham et al. <ref type="bibr" target="#b67">[58]</ref> to account for inaccuracies in bounding boxes in the training data, as defining the bounding box for a highly non-convex object (e.g., a person with arms and legs spread) is somewhat subjective. Taking into account that the counters are convex objects and that they were carefully labeled in both datasets, in <ref type="table" target="#tab_3">Table 4</ref> we report the performance (in terms of F-measure) of the Fast-YOLOv4-SmallObj model over different IoU thresholds, from 0.5 to 0.95, similarly to the COCO <ref type="bibr" target="#b68">[59]</ref> primary metric (mAP@IoU=[0.5:0.95]). As we consider only one meter per image, the precision and recall rates are identical. For comparison purposes, we also list in <ref type="table" target="#tab_3">Table 4</ref> the detection results obtained by the original Fast-YOLOv4 model. Observe that the results achieved by Fast-YOLOv4-SmallObj are considerably better at higher IoU thresholds (i.e., 0.8-0.95), which indicates that the bounding boxes predicted by the modified architecture are much better aligned with the ground truth. This is relevant since although our AMR system can tolerate less accurate detections at this stage, such imprecise predictions may still impair counter rectification and, consequently, counter recognition because one or more corners may not be within the detected bounding box. Considering the detections with IoU &gt; 0.5 with the ground truth as correct, as in some previous works, the Fast-YOLOv4-SmallObj failed in only one image from the UFPR-AMR's test set and in only 14 of the 5,000 test images of the Copel-AMR dataset. Nevertheless, we highlight that it is still possible to correctly perform the subsequent tasks in most cases where our network has failed at this stage, as the four corners are usually within the detected region (especially considering that we use as input to the next stage a counter region slightly larger than the one detected). For the record, we tried to further improve the results achieved at this stage by adding attention modules to each model; however, Fast-YOLOv4's results improved only marginally whereas Fast-YOLOv4-SmallObj's results have not improved at all.</p><p>Some detection results are shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. As can be seen, well-located predictions were attained on meters of different models and on images acquired under unconstrained conditions, that is, with significant reflections, rotations, and scale variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Corner Detection and Counter Classification</head><p>To assess the performance of CDCC-NET in the corner detection task, following <ref type="bibr" target="#b53">[44]</ref>, we report in <ref type="table">Table 5</ref> the mean pixel distance between the predicted corner positions and the ground truth on each dataset, in addition to how many FPS the proposed model is capable of processing (we report the average across 10 runs). We normalize the distances by dividing them by the respective image dimensions. To enable a comparative evaluation, we also list in <ref type="table">Table 5</ref> the results obtained by three CNN models recently proposed for the detection of corners on license plate images: Smaller-LocateNet <ref type="bibr" target="#b52">[43]</ref>, Lo-cateNet <ref type="bibr" target="#b52">[43]</ref> and Hybrid-MobileNetV2 <ref type="bibr" target="#b53">[44]</ref>. For a fair comparison and considering that the classification of the meters as legible/operational or illegible/faulty is essential in the proposed AMR pipeline, we added an output layer (softmax) to each baseline so that they can also perform such classification. We emphasize that, according to our experiments, this additional layer does not significantly affect the results obtained in the corner detection task (the normalized mean pixel distance achieved with and without that layer varied slightly in the fourth decimal place). <ref type="table">Table 5</ref>. Comparison of the corner detection results obtained by CDCC-NET and three baselines. The proposed model presents similar accuracy to Hybrid-MobileNetV2 but is twice as fast. Also, it performs almost as fast as Smaller-LocateNet, even though it predicts much more accurate corner positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model FPS</head><p>Mean pixel distance between the predicted corners and the ground truth (normalized)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UFPR-AMR Copel-AMR Average</head><p>Smaller-LocateNet <ref type="bibr" target="#b52">[43]</ref> 207 0.0059 0.0173 0.0116 LocateNet <ref type="bibr" target="#b52">[43]</ref> 166 0.0031 0.0098 0.0065 CDCC-NET (Ours) 191 0.0017 0.0055 0.0036 Hybrid-MobileNetV2 <ref type="bibr" target="#b53">[44]</ref> 97 0.0016 0.0046 0.0031</p><p>As can be seen, CDCC-NET presents the best balance between accuracy and speed among the evaluated models. More specifically, (i) Smaller-LocateNet is considerably less accurate in predicting the corner positions than the other networks, even though it runs faster; (ii) CDCC-NET is both faster and more accurate in locating the corners than LocateNet; and (iii) CDCC-NET predicts the positions of the corners almost as precisely as Hybrid-MobileNetV2, despite being able to process twice as many FPS. <ref type="figure" target="#fig_8">Fig. 8</ref> shows the evolution of the training and validation losses of CDCC-NET over time (we omitted the losses related to counter classification for better viewing). As it can be seen, CDCC-NET learns to predict the position of the four corners simultaneously and converged after 14/15 epochs.</p><p>It should be noted that we evaluated deeper networks in place of CDCC-NET (i.e., with more convolutional layers and/or more filters), however, the end-to-end reading re- For each corner, the loss plotted is the mean between the x and y coordinates, e.g., the 'top-left corner' loss is the mean between the losses of the x0 and y0 tasks. sults achieved by the proposed AMR system improved only slightly -not justifying the higher computational cost. In fact, we carried out an experiment in which we rectified all counter regions using the ground-truth annotations, as if the four corners were detected perfectly on each image, and the results improved less than we expected (from 95.87% to 96.53%), implying that most reading errors made by our AMR system were not caused by a poor rectification of the counter region, but by other challenging factors (see Section 5.2.3 for more information and qualitative results). <ref type="table" target="#tab_4">Table 6</ref> shows the results achieved by CDCC-NET in the counter classification task, which is significantly less challenging than corner detection. It is clear that CDCC-NET handles counter classification very well -in images from both datasets -since it correctly filtered out 989 of the 1,000 images in the Copel-AMR's test set (98.9%) where it is not possible to perform the meter reading due to occlusions or faulty meters, thereby reducing the overall cost of the proposed system by skipping the counter rectification and recognition tasks in such cases, while correctly accepting 799 of the 800 images in the UFPR-AMR's test set (99.88%) and 3,990 of the 4,000 legible/operational images in the Copel-AMR's test set (99.75%). According to <ref type="figure" target="#fig_9">Fig. 9</ref>, CDCC-NET is able to successfully predict the four corners of the counter and simultaneously classify it as legible/operational or illegible/faulty, regardless of the meter model and other factors that are common in images acquired in uncontrolled environments such as rotations, reflections and shadows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Overall Evaluation (end-to-end)</head><p>In this section, for each dataset, we report the number of correctly recognized counters divided by the number of legible/operational meters in the test set (legible/operational meters classified as illegible/faulty in the previous stage are considered as a reading error of the proposed method). A correctly recognized counter means that all digits on the counter were correctly recognized, as a single digit recognized incorrectly can result in a large reading/billing error. As in the previous stage, to enable an accurate analysis regarding the speed/accuracy trade-off of the evaluated methods, we report how many FPS each method is capable of processing (in the average of 10 runs), including the time required to load the respective models and weights.</p><p>Note that our end-to-end system classifies the counter regions as legible/operational or illegible/faulty in the UFPR-AMR dataset as well, even though it does not have images labeled as illegible/faulty. This procedure was adopted to better simulate a real-world scenario, where there are images of both legible/operational and illegible/faulty meters.</p><p>The results obtained by the proposed system and the baselines are shown in <ref type="table">Table 7</ref>. As can be seen, our system performed the correct reading of 94.75% of the meters in the UFPR-AMR's test set and 96.98% of the legible/operational meters in the Copel-AMR's test set, considerably outperforming all baselines in terms of recognition rate. It is remarkable that the proposed approach is able to process 55 FPS (i.e., it took ? 18 ms to process each image), meeting the efficiency requirements of real-world applications.</p><p>At first, we were surprised by the fact that the recognition rates reached in the proposed dataset were higher than those obtained in the UFPR-AMR dataset <ref type="bibr" target="#b12">[3]</ref>, where the images were acquired in relatively more controlled conditions. However, through an inspection of the reading errors made by all methods, we noticed some inconsistencies in the way rotating digits were labeled in the UFPR-AMR dataset (not always the lowest digit was chosen as the ground truth). We also observed that the UFPR-AMR's test set contains some images where it is very difficult to perform the correct reading, even for humans, due to factors such as water vapor, reflections and dirt on the meter glass, as well as the poor positioning of the camera by the person who took the photo, causing the digits to appear only partially in the image even though the counter region appears entirely -we emphasize that the UFPR-AMR dataset was collected by one of its authors and not by employees of the service company, unlike the Copel-AMR dataset. In fact, in a few cases, it is even difficult to verify if the labeled reading (i.e., the ground truth) is correct. Although we believe that such images should be rejected by the system due to the great possibility of reading errors (some of these images are shown in <ref type="figure">Fig. 10</ref>), we employed the original labels in our evaluations to enable fair comparisons with other works in the literature, since most authors tend to use the annotations originally provided as part of the dataset. <ref type="figure">Fig. 10</ref>. Some images from the UFPR-AMR dataset in which all evaluated methods failed to correctly perform the reading. In the first three images, there is dirt or water vapor on the meter glass, while in the fourth image the rightmost digit is labeled as '6' and not '5', contrary to the protocol normally adopted.</p><p>To highlight the importance of rectifying the counter region prior to the recognition stage, we included in <ref type="table">Table 7</ref> the results achieved by a modified version of our approach in which the detected counter region is fed directly into Fast-OCR (i.e., without counter rectification). As can be seen, the corner detection and counter classification stage is essential for accomplishing outstanding results in unconstrained scenarios, as our system made 34% fewer reading errors (i.e., (96.98% ? 95.43%)/(100% ? 95.43%)) in the legible/operational meters of the Copel-AMR dataset when feeding rectified counters into the recognition network.</p><p>The end-to-end results achieved by the baselines ranged from 86.09% to 94.73%. As some of them were originally evaluated only on private datasets, it is now possible to assess their applicability -both in terms of speed and accuracymore accurately. For example, G?mez et al. <ref type="bibr" target="#b24">[15]</ref> reported a promising recognition rate of 94.17% on a proprietary dataset using their segmentation-free network. Nevertheless, in our <ref type="table">Table 7</ref>. Recognition rates obtained by the proposed AMR system, a version without counter rectification of our system, and 10 baselines in both datasets used in our experiments. Note that exactly the same images were used for training the proposed methods and the baselines. comparative assessment, this model reached the lowest recognition rate among the baselines. In general, as observed in <ref type="bibr" target="#b12">[3]</ref>, the recognition models based on object detectors (e.g., CR-NET and Fast-OCR -which are based on YOLO <ref type="bibr" target="#b46">[37]</ref>) performed better than those where counter recognition is done holistically (e.g., <ref type="bibr" target="#b24">[15,</ref><ref type="bibr" target="#b37">28,</ref><ref type="bibr" target="#b39">30,</ref><ref type="bibr" target="#b66">57]</ref>). In terms of execution time, we noticed that all methods evaluated are relatively fast, i.e., they are all capable of processing more than 30 FPS on a high-end GPU, especially the version without counter rectification of our system, which is able to process 78 FPS. As can be seen clearly in <ref type="figure" target="#fig_11">Fig. 11</ref>, the proposed system outperformed all baselines in terms of average recognition rate while being relatively fast, and its version without counter rectification is much faster than the baselines that reached similar results.   We highlight that if an AMR system can run at 30+ FPS on a high-end GPU, it probably also works well on cheaper hardware (this is relevant to the service company). In this sense, for simpler/constrained scenarios, we believe that the proposed AMR system can be employed in low-end setups or even in some mobile phones (taking a few seconds). <ref type="figure" target="#fig_1">Fig. 12</ref> shows some meter readings performed correctly by the proposed system. It is noticeable that our end-to-end system is able to generalize well, being robust to meters of different models and images captured in unconstrained conditions (e.g., with various lighting conditions, reflections, shadows, scale variations, considerable rotations, etc.). Some reading errors made by our AMR system are shown in <ref type="figure" target="#fig_2">Fig. 13</ref>. As one may see, they occurred mainly in challenging cases, where one digit becomes very similar to another due to artifacts in the counter region. Another portion of the errors occurred on rotating digits (also called half digits), which is known to be a major cause of errors in electromechanical meters <ref type="bibr" target="#b12">[3,</ref><ref type="bibr" target="#b28">19]</ref>, even when robust approaches/models are employed for digit/counter recognition. It should be noted that (i) errors in the least significant digits are tolerable, as they do not significantly impact the amount charged to consumers; and (ii) reading errors in the most significant digits can be filtered by the service company through heuristic rules, for example, the reading must be greater than or equal to the reading taken in the previous month. <ref type="table">Table 8</ref>. Recognition rates reached by the proposed AMR system when discarding/rejecting the readings returned with lower confidence values by the Fast-OCR network. Our system achieves impressive recognition rates (i.e., ? 99% on average) when using a confidence threshold that rejects 15% of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Rejection  <ref type="figure" target="#fig_2">Fig. 13</ref>. Examples of reading errors made by our system. The ground truth is shown in parentheses. Observe that most of the errors occurred in challenging cases, where even humans can make mistakes, as one digit becomes very similar to another due to rotating digits or artifacts in the counter region. ated by Copel <ref type="bibr" target="#b44">[35]</ref> and other service companies, we present in <ref type="table">Table 8</ref> the end-to-end results achieved by the proposed system when discarding/rejecting the readings returned with lower confidence values by the Fast-OCR network (in practice, in a mobile application, the employee would have to capture another image). It is noteworthy that our AMR system achieved an average recognition rate above 98% by rejecting only 5% of the meter readings. Moreover, recognition rates above 99%, which are acceptable to service companies, are achieved by setting a confidence threshold that rejects 15% of the images. In this way, we consider that the proposed approach can be reliably employed on real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS AND FUTURE WORK</head><p>In this work, we presented an end-to-end, robust and efficient approach for AMR that achieves state-of-the-art results in two public datasets while being able to significantly reduce the number of images that are sent to human review by filtering out images in which it is not possible to perform the meter reading due to occlusions or faulty meters. Our main contribution is the insertion of a new stage in the AMR pipeline, called corner detection and counter classification, which enables the counter region to be rectified prior to the recognition stage. As the proposed system made 34% fewer reading errors in the legible/operational meters of the Copel-AMR dataset when feeding rectified counters into the recognition network, we consider this strategy (corner detection + counter rectification) essential for accomplishing outstanding results in unconstrained scenarios.</p><p>Our AMR system, which presents three new networks operating in a cascaded mode, performed the correct reading of 94.75% and 96.98% of the meters in the UFPR-AMR and Copel-AMR test sets (outperforming all baselines), respectively, while being able to process 55 FPS on a high-end GPU. It is notable that the proposed approach achieves impressive end-to-end recognition rates (i.e., ? 99%) when discarding/rejecting the readings made with lower confidence values, which is of paramount importance to the service companies since very few reading errors are tolerated in real-world applications due to the fact that a single digit recognized incorrectly can result in a large reading/billing error.</p><p>We also introduced a publicly available dataset for AMR with 12,500 fully-annotated images acquired on real-world scenarios by the service company's employees themselves, including 2,500 images of faulty meters or cases where the meter reading is illegible due to factors such as shadows and occlusions. The proposed dataset has six times more images and contains a larger variety in different aspects than the largest dataset found in the literature for the evaluation of endto-end AMR methods. It also contains a well-defined evaluation protocol to assist the development of new approaches and the fair comparison among published works.</p><p>As future work, we plan to design a methodology for the simultaneous detection of the counter region and its corners, aiming to perform counter rectification with an even better speed/accuracy trade-off. We also intend to explore the meter's model/type in the AMR pipeline and investigate in depth the cases where the counter has rotating digits, considering that this is a major cause of reading errors in electromechanical meters. Finally, we want to carry out an extensive assessment with various approaches/models for detecting and recognizing general scene text to compare how robust and efficient they are for the specific AMR scenario.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Distribution of the digit classes in the Copel-AMR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The pipeline of the proposed AMR system. Given an input image, the counter region is located using a modified version of the Fast-YOLOv4 model. Then, in a single forward pass of the proposed CDCC-NET, the cropped counter is classified as legible/operational or illegible/faulty and the position (x, y) of each of its corners is predicted. Finally, illegible counters are rejected, whereas legible ones are rectified and fed into our recognition network, called Fast-OCR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Some representative examples of the images generated by us for training CDCC-NET (and also Fast-OCR; see the next section). The images in the first column are the originals and the others were generated automatically.After locating the four corners, we rectify the counters classified as legible/operational by calculating and applying a perspective transform from the coordinates of the four corners in the source image (src) to the corresponding vertices in the "unwarped" image (dst). We defined these vertices asfollows: (0, 0) indicates the top-left corner; (max w ? 1, 0) corresponds to the top-right corner; (max w ? 1, max h ? 1) is the bottom-right corner; and (0, max h ? 1) refers to the bottom-left corner, where max w indicates the maximum distance between the bottom-right and bottom-left x coordinates or the top-right and top-left x coordinates, and max h is the maximum distance between the top-right and bottom-right y coordinates or the top-left and bottom-left y coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Two examples of counter regions before and after the rectification process. It should be observed that the rectified counters become more horizontal, tightly-bounded, and easier to read.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>The digits may occupy either a large (a) or a small (b) portion of the counter region depending on the meter model. The bounding boxes of the digits are outlined in red for better viewing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Representative samples of counters detected by the Fast-YOLOv4-SmallObj model -images taken under unconstrained conditions, with significant reflections, rotations, and scale variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>corner (training) top-right corner (training) bottom-left corner (training) bottom-right corner (training) top-left corner (validation) top-right corner (validation) bottom-left corner (validation) bottom-right corner (validation) Training and validation losses of CDCC-NET over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Some qualitative results achieved by CDCC-NET in corner detection and counter classification. For better visualization, we draw a polygon from the predicted corner positions. Counters classified as legible/operational are outlined in green while those classified as illegible/faulty are outlined in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>30</head><label>30</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Performance comparison of our AMR system with 10 deep learning-based baselines. For better viewing, both the proposed method (L) and its version without counter rectification (K) are depicted as red diamonds, whereas all the baselines (A-J) are depicted as blue circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The Fast-YOLOv4-SmallObj model, a modified version of Fast-YOLOv4 to improve the detection of small objects. We added layers 38-44 so that the network predicts bounding boxes at 3 different scales (layers 30, 37, and 44) instead of 2 (layers 30 and 37).</figDesc><table><row><cell>#</cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell></row><row><cell>0</cell><cell>conv</cell><cell>32</cell><cell cols="3">3 ? 3/2 384 ? 384 ? 3 192 ? 192 ? 32</cell></row><row><cell>1</cell><cell>conv</cell><cell>64</cell><cell cols="2">3 ? 3/2 192 ? 192 ? 32</cell><cell>96 ? 96 ? 64</cell></row><row><cell>2</cell><cell>conv</cell><cell>64</cell><cell>3 ? 3/1</cell><cell>96 ? 96 ? 64</cell><cell>96 ? 96 ? 64</cell></row><row><cell>3</cell><cell>route [2]</cell><cell></cell><cell></cell><cell>? 1/2</cell><cell>96 ? 96 ? 32</cell></row><row><cell>4</cell><cell>conv</cell><cell>32</cell><cell>3 ? 3/1</cell><cell>96 ? 96 ? 32</cell><cell>96 ? 96 ? 32</cell></row><row><cell>5</cell><cell>conv</cell><cell>32</cell><cell>3 ? 3/1</cell><cell>96 ? 96 ? 32</cell><cell>96 ? 96 ? 32</cell></row><row><cell>6</cell><cell>route [5, 4]</cell><cell></cell><cell></cell><cell></cell><cell>96 ? 96 ? 64</cell></row><row><cell>7</cell><cell>conv</cell><cell>64</cell><cell>1 ? 1/1</cell><cell>96 ? 96 ? 64</cell><cell>96 ? 96 ? 64</cell></row><row><cell>8</cell><cell>route [2, 7]</cell><cell></cell><cell></cell><cell></cell><cell>96 ? 96 ? 128</cell></row><row><cell>9</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 96 ? 96 ? 128</cell><cell>48 ? 48 ? 128</cell></row><row><cell>10</cell><cell>conv</cell><cell>128</cell><cell cols="2">3 ? 3/1 48 ? 48 ? 128</cell><cell>48 ? 48 ? 128</cell></row><row><cell>11</cell><cell>route [10]</cell><cell></cell><cell></cell><cell>? 1/2</cell><cell>48 ? 48 ? 64</cell></row><row><cell>12</cell><cell>conv</cell><cell>64</cell><cell>3 ? 3/1</cell><cell>48 ? 48 ? 64</cell><cell>48 ? 48 ? 64</cell></row><row><cell>13</cell><cell>conv</cell><cell>64</cell><cell>3 ? 3/1</cell><cell>48 ? 48 ? 64</cell><cell>48 ? 48 ? 64</cell></row><row><cell cols="2">14 route [13, 12]</cell><cell></cell><cell></cell><cell></cell><cell>48 ? 48 ? 128</cell></row><row><cell>15</cell><cell>conv</cell><cell>128</cell><cell cols="2">1 ? 1/1 48 ? 48 ? 128</cell><cell>48 ? 48 ? 128</cell></row><row><cell cols="2">16 route [10, 15]</cell><cell></cell><cell></cell><cell></cell><cell>48 ? 48 ? 256</cell></row><row><cell>17</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 48 ? 48 ? 256</cell><cell>24 ? 24 ? 256</cell></row><row><cell>18</cell><cell>conv</cell><cell>256</cell><cell cols="2">3 ? 3/1 24 ? 24 ? 256</cell><cell>24 ? 24 ? 256</cell></row><row><cell>19</cell><cell>route [18]</cell><cell></cell><cell></cell><cell>? 1/2</cell><cell>24 ? 24 ? 128</cell></row><row><cell>20</cell><cell>conv</cell><cell>128</cell><cell cols="2">3 ? 3/1 24 ? 24 ? 128</cell><cell>24 ? 24 ? 128</cell></row><row><cell>21</cell><cell>conv</cell><cell>128</cell><cell cols="2">3 ? 3/1 24 ? 24 ? 128</cell><cell>24 ? 24 ? 128</cell></row><row><cell cols="2">22 route [21, 20]</cell><cell></cell><cell></cell><cell></cell><cell>24 ? 24 ? 256</cell></row><row><cell>23</cell><cell>conv</cell><cell>256</cell><cell cols="2">1 ? 1/1 24 ? 24 ? 256</cell><cell>24 ? 24 ? 256</cell></row><row><cell cols="2">24 route [18, 23]</cell><cell></cell><cell></cell><cell></cell><cell>24 ? 24 ? 512</cell></row><row><cell>25</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 24 ? 24 ? 512</cell><cell>12 ? 12 ? 512</cell></row><row><cell>26</cell><cell>conv</cell><cell>512</cell><cell cols="2">3 ? 3/1 12 ? 12 ? 512</cell><cell>12 ? 12 ? 512</cell></row><row><cell>27</cell><cell>conv</cell><cell>256</cell><cell cols="2">1 ? 1/1 12 ? 12 ? 512</cell><cell>12 ? 12 ? 256</cell></row><row><cell>28</cell><cell>conv</cell><cell>512</cell><cell cols="2">3 ? 3/1 12 ? 12 ? 256</cell><cell>12 ? 12 ? 512</cell></row><row><cell>29</cell><cell>conv</cell><cell>18</cell><cell cols="2">1 ? 1/1 12 ? 12 ? 512</cell><cell>12 ? 12 ? 18</cell></row><row><cell>30</cell><cell>detection</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>31</cell><cell>route [27]</cell><cell></cell><cell></cell><cell></cell><cell>12 ? 12 ? 256</cell></row><row><cell>32</cell><cell>conv</cell><cell>128</cell><cell cols="2">1 ? 1/1 12 ? 12 ? 256</cell><cell>12 ? 12 ? 128</cell></row><row><cell>33</cell><cell>upsample</cell><cell></cell><cell cols="2">2? 12 ? 12 ? 128</cell><cell>24 ? 24 ? 128</cell></row><row><cell cols="2">34 route [33, 23]</cell><cell></cell><cell></cell><cell></cell><cell>24 ? 24 ? 384</cell></row><row><cell>35</cell><cell>conv</cell><cell>256</cell><cell cols="2">3 ? 3/1 24 ? 24 ? 384</cell><cell>24 ? 24 ? 256</cell></row><row><cell>36</cell><cell>conv</cell><cell>18</cell><cell cols="2">1 ? 1/1 24 ? 24 ? 256</cell><cell>24 ? 24 ? 18</cell></row><row><cell>37</cell><cell>detection</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>38</cell><cell>route [35]</cell><cell></cell><cell></cell><cell></cell><cell>24 ? 24 ? 256</cell></row><row><cell>39</cell><cell>conv</cell><cell>64</cell><cell cols="2">1 ? 1/1 24 ? 24 ? 256</cell><cell>24 ? 24 ? 64</cell></row><row><cell>40</cell><cell>upsample</cell><cell></cell><cell>2?</cell><cell>24 ? 24 ? 64</cell><cell>48 ? 48 ? 64</cell></row><row><cell cols="2">41 route [40, 15]</cell><cell></cell><cell></cell><cell></cell><cell>48 ? 48 ? 192</cell></row><row><cell>42</cell><cell>conv</cell><cell>128</cell><cell cols="2">3 ? 3/1 48 ? 48 ? 192</cell><cell>48 ? 48 ? 128</cell></row><row><cell>43</cell><cell>conv</cell><cell>18</cell><cell cols="2">1 ? 1/1 48 ? 48 ? 128</cell><cell>48 ? 48 ? 18</cell></row><row><cell>44</cell><cell>detection</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>CDCC-NET's layers and hyperparameters. It is relatively shallow and has two dense layers for each of the 9 outputs (i.e., x0/w, y0/h, x1/w, y1/h, x2/w, y2/h, x3/w, y3/h, [legible, illegible]).</figDesc><table><row><cell>#</cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell></row><row><cell>0</cell><cell>conv</cell><cell>16</cell><cell cols="3">3 ? 3/1 192 ? 64 ? 3 192 ? 64 ? 16</cell></row><row><cell>1</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 192 ? 64 ? 16 96 ? 32 ? 16</cell></row><row><cell>2</cell><cell>conv</cell><cell>32</cell><cell cols="2">3 ? 3/1 96 ? 32 ? 16</cell><cell>96 ? 32 ? 32</cell></row><row><cell>3</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 96 ? 32 ? 32</cell><cell>48 ? 16 ? 32</cell></row><row><cell>4</cell><cell>conv</cell><cell>64</cell><cell cols="2">3 ? 3/1 48 ? 16 ? 32</cell><cell>48 ? 16 ? 64</cell></row><row><cell>5</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 48 ? 16 ? 64</cell><cell>24 ? 8 ? 64</cell></row><row><cell>6</cell><cell>flatten</cell><cell></cell><cell></cell><cell>24 ? 8 ? 64</cell><cell>12288</cell></row><row><cell>#</cell><cell>Layer</cell><cell>Connected to</cell><cell>Units</cell><cell>Input</cell><cell>Output</cell></row><row><cell>7</cell><cell>non-shared dense [0..8]</cell><cell>#6</cell><cell>128</cell><cell>12288</cell><cell>128</cell></row><row><cell>8</cell><cell>non-shared dense corners [0..7]</cell><cell>#7</cell><cell>1</cell><cell>128</cell><cell>1</cell></row><row><cell cols="2">9 non-shared dense counter class [8]</cell><cell>#7</cell><cell>2</cell><cell>128</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The architecture of the Fast-OCR model, proposed for counter recognition.</figDesc><table><row><cell>#</cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell><cell>BFLOP</cell></row><row><cell>0</cell><cell>conv</cell><cell>32</cell><cell cols="3">3 ? 3/1 384 ? 128 ? 3 384 ? 128 ? 32</cell><cell>0.085</cell></row><row><cell>1</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 384 ? 128 ? 32 192 ? 64 ? 32</cell><cell>0.002</cell></row><row><cell>2</cell><cell>conv</cell><cell>64</cell><cell cols="2">3 ? 3/1 192 ? 64 ? 32</cell><cell>192 ? 64 ? 64</cell><cell>0.453</cell></row><row><cell>3</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 192 ? 64 ? 64</cell><cell>96 ? 32 ? 64</cell><cell>0.001</cell></row><row><cell>4</cell><cell>conv</cell><cell>128</cell><cell>3 ? 3/1</cell><cell>96 ? 32 ? 64</cell><cell>96 ? 32 ? 128</cell><cell>0.453</cell></row><row><cell>5</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 96 ? 32 ? 128</cell><cell>48 ? 16 ? 128</cell><cell>0.000</cell></row><row><cell>6</cell><cell>conv</cell><cell>256</cell><cell cols="2">3 ? 3/1 48 ? 16 ? 128</cell><cell>48 ? 16 ? 256</cell><cell>0.453</cell></row><row><cell>7</cell><cell>conv</cell><cell>128</cell><cell cols="2">1 ? 1/1 48 ? 16 ? 256</cell><cell>48 ? 16 ? 128</cell><cell>0.050</cell></row><row><cell>8</cell><cell>conv</cell><cell>256</cell><cell cols="2">3 ? 3/1 48 ? 16 ? 128</cell><cell>48 ? 16 ? 256</cell><cell>0.453</cell></row><row><cell>9</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 48 ? 16 ? 256</cell><cell>24 ? 8 ? 256</cell><cell>0.000</cell></row><row><cell>10</cell><cell>conv</cell><cell>512</cell><cell>3 ? 3/1</cell><cell>24 ? 8 ? 256</cell><cell>24 ? 8 ? 512</cell><cell>0.453</cell></row><row><cell>11</cell><cell>conv</cell><cell>256</cell><cell>1 ? 1/1</cell><cell>24 ? 8 ? 512</cell><cell>24 ? 8 ? 256</cell><cell>0.050</cell></row><row><cell>12</cell><cell>conv</cell><cell>512</cell><cell>3 ? 3/1</cell><cell>24 ? 8 ? 256</cell><cell>24 ? 8 ? 512</cell><cell>0.453</cell></row><row><cell>13</cell><cell>conv</cell><cell>45</cell><cell>1 ? 1/1</cell><cell>24 ? 8 ? 512</cell><cell>24 ? 8 ? 45</cell><cell>0.009</cell></row><row><cell>14</cell><cell>detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15</cell><cell>route [11]</cell><cell></cell><cell></cell><cell></cell><cell>24 ? 8 ? 256</cell><cell></cell></row><row><cell>16</cell><cell>conv</cell><cell>256</cell><cell>1 ? 1/1</cell><cell>24 ? 8 ? 256</cell><cell>24 ? 8 ? 256</cell><cell>0.025</cell></row><row><cell>17</cell><cell>upsample</cell><cell></cell><cell>2?</cell><cell>24 ? 8 ? 256</cell><cell>48 ? 16 ? 256</cell><cell></cell></row><row><cell cols="2">18 route [17, 6]</cell><cell></cell><cell></cell><cell></cell><cell>48 ? 16 ? 512</cell><cell></cell></row><row><cell>19</cell><cell>conv</cell><cell>512</cell><cell cols="2">3 ? 3/1 48 ? 16 ? 512</cell><cell>48 ? 16 ? 512</cell><cell>3.624</cell></row><row><cell>20</cell><cell>conv</cell><cell>45</cell><cell cols="2">1 ? 1/1 48 ? 16 ? 512</cell><cell>48 ? 16 ? 45</cell><cell>0.035</cell></row><row><cell>21</cell><cell>detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">(a) digits occupying a large portion of the counter</cell><cell></cell></row><row><cell></cell><cell cols="5">(b) digits occupying a small portion of the counter</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>F-measure values obtained over different IoU thresholds, from 0.5 to 0.95, in the counter detection stage. Note that Fast-YOLOv4-SmallObj achieves considerably better results at higher IoU thresholds (i.e., 0.8-0.95), which indicates that its predictions are much better aligned with the ground truth.</figDesc><table><row><cell>Model</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>IoU Threshold 0.8 0.9</cell><cell>0.95 0.5:0.95</cell></row><row><cell>UFPR-AMR [3]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Fast-YOLOv4 (608 ? 608) 98.1% 97.8% 97.6% 94.6% 75.9% 38.6% 83.8%</cell></row><row><cell cols="6">Fast-YOLOv4-SmallObj 99.9% 99.9% 99.9% 98.1% 80.0% 38.9% 86.1%</cell></row><row><cell>Copel-AMR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Fast-YOLOv4 (608 ? 608) 99.0% 98.3% 95.3% 84.2% 52.2% 17.0% 74.3%</cell></row><row><cell cols="6">Fast-YOLOv4-SmallObj 99.7% 99.5% 98.8% 96.1% 75.3% 28.8% 83.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Results achieved by CDCC-NET in the counter classification task. It is able to filter out 98.9% of the illegible/faulty meters, while correctly accepting 99.82% (on average of both datasets) of the legible/operational meters.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Class Legible/Operational Illegible/Faulty</cell></row><row><cell cols="2">UFPR-AMR</cell><cell>99.88%</cell><cell>-</cell></row><row><cell cols="2">Copel-AMR</cell><cell>99.75%</cell><cell>98.90%</cell></row><row><cell cols="2">Average</cell><cell>99.82%</cell><cell>98.90%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>AMR 94.75% 97.63% 98.47% 98.82% 99.22% Copel-AMR 96.98% 98.61% 99.00% 99.24% 99.44% Average 95.87% 98.12% 98.74% 99.03% 99.33%Finally, considering that very few reading errors are toler-Fig. 12. Examples of meter readings performed correctly by the proposed system. It is remarkable that it performed well in images of meters of different models and captured in unconstrained conditions (e.g., with various lighting conditions, reflections, shadows, scale variations, and considerable rotations).</figDesc><table><row><cell>25464 21270 18327 (18527) 10651 03931 UFPR-04241 57599 00841 25725 06578 07059 18101 01710 18740 (18748) 10469 (10459) 09379 (09979) 01815 63054 (07815) (03937) (63059)</cell><cell>0% 00353 (00864) 44671 03953 00264 02923 (02323)</cell><cell>5%</cell><cell>Rate 10% 04730 02059 28382 34125 (34124) 24091 (24097)</cell><cell>15%</cell><cell>20%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The entire system, i.e., the architectures and weights, is publicly available at https://web.inf.ufpr.br/vri/publications/ amr-unconstrained-scenarios/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://pytorch.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the Coordination for the Improvement of Higher Education Personnel (CAPES) (Grant 88887.516264/2020-00 and Social Demand Program), and in part by the National Council for Scientific and Technological Development (CNPq) (Grant 308879/2020-1). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. We also thank the Energy Company of Paran? (Copel), in particular the manager of the reading division Dihon Pereira Brand?o, for providing the images for the creation of the Copel-AMR dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Fast-Yolov4-Smallobj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>G?mez</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast-YOLOv4-SmallObj + CRNN</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laroca</surname></persName>
		</author>
		<imprint>
			<publisher>Multi-task</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Fast-Yolov4-Smallobj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baek</surname></persName>
		</author>
		<imprint>
			<publisher>TRBA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liao</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>larger input size</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Fast-Yolov4-Smallobj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Calefati</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laroca</surname></persName>
		</author>
		<imprint>
			<publisher>CR-NET</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ours -unrectified (Fast-YOLOv4-SmallObj + Fast-OCR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ours (Fast-YOLOv4-SmallObj + CDCC-NET + Fast-OCR) References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of communication protocols for automatic meter reading applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nayak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys Tutorials</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="182" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on smart metering and smart grid communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kabalci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Renewable and Sustainable Energy Reviews</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="302" to="318" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for automatic meter reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13023</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-based electric consumption recognition via multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C S</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian Conference on Intelligent Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="419" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gas meter reading from real world images using a multi-net system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vanetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nodari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="519" to="526" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust angle invariant GAS meter reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamberletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Noce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2015-11" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meter digit recognition via Faster R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waqar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Waris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yousaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation in Industry</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A mobile recognition system for analog energy meter scanning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shalunts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic consumption reading on electromechanical meters using HOG and SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B P</forename><surname>Quintanilha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latin American Conference on Networked and Electronic Media</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="57" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic visual reading of meters using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ko??evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suba?i?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Croatian Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Light-weight spliced convolution network-based automatic water meter reading in smart city</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="174" to="359" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Use SSD to detect the digital region in electricity meter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Cybernetics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A robust approach to reading recognition of pointer meters based on improved Mask-RCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">388</biblScope>
			<biblScope unit="page" from="90" to="101" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning for image-based automatic dial meter reading: Dataset and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cutting sayre&apos;s knot: Reading scene text without segmentation. Application to utility meters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusi?ol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Workshop on Document Analysis Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Text detection and recognition in raw image dataset of seven segment digital energy meter display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kanagarathinam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Reports</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="842" to="852" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">License plate detection and recognition in unconstrained scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="593" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional sequence recognition network for water meter number reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="679" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic watermeter digit recognition on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Internet Multimedia Computing and Service</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ICDAR 2013 Robust Reading Competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reading digital numbers of water meter with deep learning based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Pattern Recognition and Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="38" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A value recognition algorithm for pointer meter based on improved Mask-RCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Science and Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="108" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-time brazilian license plate detection and recognition using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for imagebased sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-task learning for low-resolution license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberoamerican Congress on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="251" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reading meter numbers in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calefati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nawaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Realtime automatic license plate recognition through deep multi-task networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Energy Company Of Paran?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Copel</surname></persName>
		</author>
		<ptr target="http://www.copel.com/hpcopel/english/" />
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Fast-YOLOv4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<ptr target="https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4-tiny.cfg" />
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A robust real-time automatic license plate recognition based on the YOLO detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mixed frame-/event-driven fast pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stechele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8332" to="8338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">YOLOv4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">YOLOv3 and YOLOv2 for Windows and Linux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<ptr target="https://github.com/AlexeyAB/darknet#how-to-improve-object-detection" />
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A survey on multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A robust and efficient method for license plate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="1713" to="1718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep corner prediction to rectify tilted license plate images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7553" to="7563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Geometric Image Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Opencv</surname></persName>
		</author>
		<ptr target="https://docs.opencv.org/master/da/d54/groupimgproctransform.html" />
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An end-to-end approach for recognition of modern and historical handwritten numeral strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hochuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Britto</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Barddal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Real-time license plate detection and recognition using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="page">102773</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An efficient and layout-independent automatic license plate recognition system based on the YOLO detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Intelligent Transport Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="483" to="503" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Clova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Research</surname></persName>
		</author>
		<ptr target="https://github.com/clovaai/deep-text-recognition-benchmark/" />
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">STAR-Net: A spatial attention residue network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Gated recurrent convolution neural network for OCR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Rosetta: Large scale system for text detection and recognition in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Borisyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">What is wrong with scene text recognition model comparisons? Dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014-09" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
