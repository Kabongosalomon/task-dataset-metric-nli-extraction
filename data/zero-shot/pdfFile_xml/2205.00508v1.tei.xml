<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Best of Both Worlds: Combining Model-based and Nonparametric Approaches for 3D Human Body Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
							<email>jimyang@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
							<email>fowlkes@ics.uci.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Best of Both Worlds: Combining Model-based and Nonparametric Approaches for 3D Human Body Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nonparametric based methods have recently shown promising results in reconstructing human bodies from monocular images while model-based methods can help correct these estimates and improve prediction. However, estimating model parameters from global image features may lead to noticeable misalignment between the estimated meshes and image evidence. To address this issue and leverage the best of both worlds, we propose a framework of three consecutive modules. A dense map prediction module explicitly establishes the dense UV correspondence between the image evidence and each part of the body model. The inverse kinematics module refines the key point prediction and generates a posed template mesh. Finally, a UV inpainting module relies on the corresponding feature, prediction and the posed template, and completes the predictions of occluded body shape. Our framework leverages the best of non-parametric and model-based methods and is also robust to partial occlusion. Experiments demonstrate that our framework outperforms existing 3D human estimation methods on multiple public benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The 3d estimation of the human body pose and shape from a monocular image is a fundamental task for various applications such as VR/AR, virtual try-on, metaverse and animations. It is challenging mostly due to the depth ambiguity and lack of evidence from single image. There are several ways to solve this ambiguity such as leveraging multi-view or video data to fuse image evidence from more images and infer occluded parts. For the case of single images, researchers used parametric models such as SMPL <ref type="bibr" target="#b22">[23]</ref> to fit 2D image evidence <ref type="bibr" target="#b14">[15]</ref> or use human pose prior <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref> to penalize problematic human pose / mesh prediction in combination with modern deep learning techniques. However, these model-based methods are prone to produce corrupted results when severe occlusion happens. <ref type="bibr">Figure 1</ref>. From left to right: Original image our mesh prediction overlay and alternative views mesh visualization. Images are from 3DOH <ref type="bibr" target="#b51">[52]</ref>, and LSP <ref type="bibr" target="#b9">[10]</ref> datasets. (Best viewed in Color) Nonparametric methods use non-compressed representations like voxels <ref type="bibr" target="#b32">[33]</ref>, heatmaps <ref type="bibr" target="#b29">[30]</ref> and joint location <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref> as the target for modern deep learning. However, to estimate dense meshes they are computationaly expensive and consume lots of memory. They either use integral methods to estimate normalized joint location <ref type="bibr" target="#b29">[30]</ref> or simplify meshes <ref type="bibr" target="#b20">[21]</ref> to reduce the number of vertices. Without post-processing, these methods also generate qualitatively non-pleasing results. The dense correspondence methods <ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref>, which are based on template SMPL human mesh surface and have been proven for various tasks.</p><p>Connecting nonparametric methods and model-based methods is hard due to the difficulty in localizing the corresponding feature. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b50">51]</ref> utilize bounding boxes or keypoints location to find the related features to estimate necessary SMPL parameters. While <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref> learn the feature-parameter correspondence (attention) implicitly through neural networks. <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b48">49]</ref> consider the correspondence between the mesh representation and pixel representation based on human surface mapping (UV coordinate system). However, they estimate the SMPL parameter through a light weight FC network and treat this simple optimization process as a post process. Their methods also do not convey the advantages of nonparametric methods such as robustness to occlusion.</p><p>To leverage the advantages from both worlds, we propose a 3d human body estimation framework that consists of three modules: Dense Map Prediction module (DMP), Inverse Kinematics module (IK) and UV Inpainting module (UVI). DMP explicitly predicts per-pixel human 3d joint location, 3d surface location in root relative coordinates, 3d displacement between the joint location and surface location, and also predicts UV coordinates which represent the human surface in a 2D grid. This module is robust to partial occlusion when predicting joint, as all the image evidence belongs to this part will contribute to the prediction explicitly. IK module connects the nonparametric prediction to model-based method. We first warp the DMP dense prediction to UV space and get the joint prediction based on the part-segmentation in UV space. Then we use a twostage multi-layer perceptron, where the first stage inpaints and refines the joint prediction, while the second stage estimates SMPL parameters and eventually produces a posed mesh. With all the predictions in UV space from DMP and IK, UVI inpaints and refines the 3d body pose and mesh in UV space. In summary, our contributions are three fold:</p><p>? We propose a 3d body estimation framework from single image that seamlessly leverages the best of the both worlds (model-based and nonparametric).</p><p>? The method is robust to occlusions and can self-correct wrong poses from Dense Map Prediction module.</p><p>? We achieve state-of-the-art performance on H36M and 3DOH datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D human shape estimation from monocular images SMPL <ref type="bibr" target="#b22">[23]</ref> has been widely used for 3D human mesh reconstruction. To boost its power in practice, a number of deep learning frameworks have been proposed by using SMPL as regression targets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref>. <ref type="bibr" target="#b11">[12]</ref> regresses SMPL parameters directly from input images by end-to-end training. Following this research direction, <ref type="bibr" target="#b29">[30]</ref> add spherical Gaussian attention joint based on initial joint estimation, and the use the the attended feature to learn the vertices location. <ref type="bibr" target="#b14">[15]</ref> combine learning and optimization <ref type="bibr" target="#b31">[32]</ref> in the same framework but cannot handle occlu-sions. <ref type="bibr" target="#b45">[46]</ref> uses the template UV mapping from SMPL and transforms 3d mesh reconstruction to decomposed UV estimation and position map inpainting problems. However, the way to get 3d human joint from SMPL mesh is based on the pre-trained joint regressor, which will induce intrinsic errors and usually does not generalize to other datasets.</p><p>3D human pose estimation from monocular images Deep learning approaches have shown success in regressing 3D pose from a single image <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref>. Basically, most current models can be categorized into two frameworks. The first is to directly estimate 3D pose from images, based on volumetric representation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref>. But these approaches may involve in high memory consumption and complex post-processing steps. Based on the explosive improvement in 2D pose estimation <ref type="bibr" target="#b43">[44]</ref>, another framework is to estimate 2D pose from images and then lift 2D pose to 3D pose <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b52">53]</ref>. Since these approaches take 2D joint locations as input, 3D human pose estimation simply focuses on learning depth of each joint. This releases learning difficulty and leads to better 3D pose. However, there are few methods on systematically handling occlusion in the first framework while the second framework cannot recover information if the joint detector fails. Additionally, how to get human surfaces from the joint prediction remains a problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverse Kinematics</head><p>The inverse kinematics (IK) problem has been extensively studied in robotics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref> and graphics <ref type="bibr" target="#b3">[4]</ref> and its techniques have been used in 3d human pose estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>. Numerical solutions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43]</ref> rely on time-consuming iterative optimization. <ref type="bibr" target="#b39">[40]</ref> uses temporal sequence to resolve IK ambiguity. <ref type="bibr" target="#b16">[17]</ref> decomposes the IK rotation to the product of swing rotation and twist rotation and solve swing rotation analytically from predicted joint locations. Feed forward solution like <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref> propose BodyIKNet to regress SMPL <ref type="bibr" target="#b22">[23]</ref> pose and shape parameters from 3d joint location, However, it leads to a sub-optimal solution when partial occlusion happens.</p><p>Occlusion <ref type="bibr" target="#b37">[38]</ref> presented a systematic study of various types of synthetic occlusions in 3D human pose estimation from a single RGB image. Since synthetic data can not fully depict the real occlusion, <ref type="bibr" target="#b4">[5]</ref> learns from real data and uses grammar models with explicit occluding templates to reason about occluded people. To avoid specific design for occlusion patterns, <ref type="bibr" target="#b2">[3]</ref> presents a method for modeling occlusion that aims at explicitly learning the appearance and statistics of occlusion patterns. They also synthesizes a large corpus of training data by compositing segmented objects at random locations over a base training image. <ref type="bibr" target="#b1">[2]</ref> utilizes a cylinder model and confidence maps to filter out the occluded joints and uses flow warped joint in the same video to approximate the missing joints. <ref type="bibr" target="#b33">[34]</ref> integrates depth information about occluded objects into 3D pose estimation.</p><p>To provide full-geometry information to handle occlusion scenarios, <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b6">[7]</ref> provide 3d scene geometry as multilayer depth maps or signed distance fields into the inference stage. <ref type="bibr" target="#b34">[35]</ref> proposes a simple but effeive self-training framework to adapt the model to highly occluded observations. To fully utilize the holistic human body model (e.g. SMPL <ref type="bibr" target="#b22">[23]</ref>), <ref type="bibr" target="#b51">[52]</ref> represents the target SMPL human mesh as UV location map and converts the full-body human estimation as an image inpainting problem. However, these frameworks either rely on nonparametric estimation or pure model-based regression, how to leverage the best of both worlds seamlessly remain an unexplored problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>As shown in <ref type="figure" target="#fig_0">Fig 2,</ref> our framework consists of three consecutive modules, including a dense map prediction module (DMP), which extract dense semantic maps (e.g. 3d joint location, surface location and their displacements) and correspondence UV position, an inverse kinematics and SMPL module (IK), which inpaint 3d joint location and estimate the smpl parameters, as well as a UV map inpainting module, which estimate the final joint location and mesh location in UV space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dense Map Prediction Module</head><p>Our dense map prediction module is an encoder-decoder architecture and is used to extract the IUV images M i , as well as dense semantic maps including dense joint map M j , dense location map M l and dense displacement maps M d . They are further illustrated in <ref type="figure" target="#fig_1">Fig 3.</ref> M i is generated from the continuous UV map from <ref type="bibr" target="#b45">[46]</ref>, it is continuous in both image space and UV space, thus, easier to learn compared with original UV map <ref type="bibr" target="#b22">[23]</ref>. It is used to convert the dense local features as well as these semantic maps to UV space. For location map M l , it represents the position of each vertices from the SMPL human mesh surface in root-relative coordinates. To construct M l groundtruth, we first use the SMPL model, SMPL parameters and camera parameters to generate the vertices location in root-relative coordinate, and generate the full UV space location map U V l using barycentric interpolation (The mesh faces correspondence is defined by <ref type="bibr" target="#b45">[46]</ref>). After that we use the M i to fetch values from U V l to get the dense location map in image space. For the generation of dense joint map M j , we first rely on T-pose SMPL mesh and assign each vertex to the nearest joints (14 LSP joints setting), after that we use barycentric interpolation to get the UV space assignment, and further refine the assignment by make it symmetric in UV space (e.g. left hip and right hip has symmetric shape in UV space, as illustrated in <ref type="figure" target="#fig_2">Fig 4)</ref>. We term the part assignment in UV space as A uv . After setting the assignment in UV space, we use the M i to query values from U V j to  get the dense joint map in image space. U V j stores the rootrelative joint location. We define displacement as the residual between vertex location and the assigned joint location,</p><formula xml:id="formula_0">thus U V d = U V l ? U V j and M d = M l ? M j .</formula><p>As our human are left-right symmetric (e.g. left hand has symmetric shape with right hand and the size and the distance between joint and surface is almost the same.), the magnitude of left part and right part of U V d should be the same.</p><p>These semantic maps are aligned with the human in the images. Thus we are able to train a encoder-decoder network to estimate directly from image space. Dense image space joint prediction shares the similar flavor with <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>The objective for the dense map prediction module is</p><formula xml:id="formula_1">? DM P = ? Mi + ? M l + ? Mj + ? M d<label>(1)</label></formula><p>? Mi is composed of two parts: a binary mask loss ? Mib of human body, which distinguishes pixels from those at the background, and the human pixels. The loss function of ? Mib is binary cross entropy loss. our CNN further outputs the UV coordinates and uses L1 loss ? Miuv .</p><formula xml:id="formula_2">? Mi = ? M ib + ? Miuv<label>(2)</label></formula><p>For ? M l , ? Mj and ? M d , we use L1 loss to directly regress the real value. As these values are already in root-relative coordinate and in unit meters, thus their data range is ?1 to +1, we do not further normalize them.</p><p>Our dense map prediction module not only predicts these semantic maps, but also extracts both global feature to estimate camera parameter and local feature for the UV impainting module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inverse Kinematics Module</head><p>Estimate Joint Location from DMP After warping the semantic maps (M l , M j , M d ) from image space to uv space, we get the incomplete uv joint map U V j . Based on the uv space joint assignment A uv (as shown in <ref type="figure" target="#fig_2">Fig 4)</ref>, we aggregate the dense prediction U V j for each joint and average them if they are not fully occluded. Thus we have a coarse prediction for each joint J initial .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Inpaint and Refine Module</head><p>Even though each human pixel contributes to joint prediction, there are still cases that some joints have no assigned vertex/pixel available from the image evidence. Thus we propose the joint inpainting module to inpaint these missing joints. This network is pretty flexible and can be MLP <ref type="bibr" target="#b26">[27]</ref>, GCN <ref type="bibr" target="#b52">[53]</ref> or even modern transformers <ref type="bibr" target="#b19">[20]</ref>. For the ease of implementation we use simple multi-layer perceptron. Our joint inpainting net is inspired by <ref type="bibr" target="#b26">[27]</ref>, which is simple, deep and a fullyconnected network with six linear layer with 256 output features. It includes dropout after every fully connected layer, batch-normalization and residual connections. The model contains approximately 400k training parameters. The goal of this network is not only to inpaint the joints but also to refine the joints prediction that is not occluded. It takes the J initial as input and the output of the network is the joint in root-relative coordinates J ref ine . We use L1 loss L ji to train joint inpaint and refine module. The structure of the joint inpainting and refine module is shown in <ref type="figure" target="#fig_4">Fig 6.</ref> Inverse Kinematics Module After getting the sparse 3d human keypoints. We want to repose the template SMPL meshes based on the predicted joints location. To solve this problem we leverage inverse kinematics (IK). Typically, the IK task is tackled with iterative optimization methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43]</ref>, which requires a good initialization, more time and case-by-case optimization method. Here we propose a global inverse kinematics neural network GIK-Net. This network is constructed by the basic fully connected neural network module with residual connection, batch normalization and relu activation similar to <ref type="bibr" target="#b26">[27]</ref>. In particular, GIK-Net takes the refined keypoint coordinates J ref ine in root-relative space and outputs joint rotations ? and ? which serve as the input for SMPL layer. As we also use the Mocap dataset (AMASS <ref type="bibr" target="#b25">[26]</ref>, SPIN <ref type="bibr" target="#b14">[15]</ref> and AIST++ <ref type="bibr" target="#b17">[18]</ref>), our GIK-Net can implicitly learn the realistic distribution of human kinematics rotation and human shape. The use of the additional Mocap dataset serves the same purpose as the factorized adversarial prior <ref type="bibr" target="#b11">[12]</ref>, variational human pose prior <ref type="bibr" target="#b31">[32]</ref> and motion discriminator <ref type="bibr" target="#b12">[13]</ref>. We use L1 loss L ? and L ? to train GIK-Net. The structure of GIK-Net is shown in <ref type="figure" target="#fig_4">Fig 6.</ref> SMPL revisits and Reposing Module SMPL <ref type="bibr" target="#b22">[23]</ref> represents the body pose and shape by pose ? ? R 72 and shape ? ? R 10 parameter. Here we use the gender-neural shape model following previous work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Given these parameters, the SMPL module is a differentiable function that outputs a posed 3D mesh M (?, ?) ? R 6890?3 . The 3D joint locations J 3D = W M ? R J?3 , while J are computed with a pretrained linear regressor W . After getting the ? and ? from the GIK-Net we send them to SMPL layer to get the body mesh prediction.</p><p>We also augment the joints input for GIK-Net from Mocap dataset with guassian noise and random synthetic occlusion (30%). The augmentation helps our GIK-Net generalize to more realistic noisy input. We use L1 loss L vi to train the mesh prediction from SMPL module.</p><p>The objective for the inverse kinematics and smpl module is</p><formula xml:id="formula_3">? IK = ? ? + ? ? + ? ji + ? vi<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">UV Inpainting Module</head><p>The goal of UV inpainting module is to regress 3d joint and mesh location directly based on the feature / seman-</p><formula xml:id="formula_4">tic output (U V l , U V j , U V d ) from DMP and semantic output (U V l , U V j , U V d ) from IK.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inevitable Fitting Error introduced by SMPL model and Joint regressor</head><p>The advantage of directly regressing joint/mesh location over model-based method is that modelbased method will introduce intrinsic fitting error. Specifically, we use the SMPL layer, groundtruth SMPL parameters (from Mosh), and the joint-regressor <ref type="bibr" target="#b14">[15]</ref> to obtain fitted joint for the whole Human3.6M dataset. We get average fitting error as 24.1 mm (MPJPE) when compared with the Human3.6M joint from Mocap system. It means that even we predict perfect SMPL mesh we still have about 24.1 mm fitting error. Thus we argue directly train and estimate joint location from UV space is a better alternative solution.</p><p>UV inpainting module After getting the refined joint location J ref ine from IK module, we distribute the refined joint location in UV space based on UV space joint assignment map A uv and generate refine UV joint map U V jref ine . We also have the reposed template mesh and the corresponding reposed UV location map U V l (through barycentric interpolation). Additionally, we have features U V f , location map U V l , joint map U V j and displacement U V d from DMP. We combine the best of both worlds ( DMP and IK) feature through aggregation and send it to our UV inpainting module. The UV inpainting module is a light UNet with skip connections. We can see the <ref type="figure" target="#fig_3">Fig 5 is</ref> the complete version of <ref type="figure" target="#fig_2">Fig 4 and</ref> serves as the groundtruth for the UV inpainting module.</p><p>For the training of the UV inpainting module, we have</p><formula xml:id="formula_5">? map = ?? V map ? U V map ? 1<label>(4)</label></formula><p>Note the 'map' represents location map, joint map and displacement map in uv space. Addtionally, we have 3d joints and 2d joint loss based on the predicted camera parameter.</p><p>Our camera parameters consist of scale and offset parameter to map the xy in J 3d to J 2d .</p><formula xml:id="formula_6">? j3d = ?? 3d ? J 3d ? 1 (5) ? j2d = ?? 2d ? J 2d ? 1<label>(6)</label></formula><p>As we know, the distance between the human surface to the joints are left-right symmetric, thus we also apply symmetric loss on the magnitude of displacement.</p><formula xml:id="formula_7">? dismag = ??? V d ? ? ?? V f lip d ?? 1<label>(7)</label></formula><p>To align the predicted mesh surface with image aligned IUV images M i , we also adopt consistent loss from <ref type="bibr" target="#b45">[46]</ref>. It is enabled by the camera parameter predicted by our model (scaling and offset parameter).</p><p>The objective for the uv inpainting module is</p><formula xml:id="formula_8">? U V I = ? dismag + ? j2d + ? j3d + ? map + ? con (8)</formula><p>Thus we have all the losses as</p><formula xml:id="formula_9">? all = ? DM P + ? IK + ? U V I<label>(9)</label></formula><p>Inference We do inference of 3d joint location from U V j and based on the uv assignment A uv for each joint. We average all the prediction for the specific joints if this pixel prediction is valid. For human mesh prediction we use the barycentric interpolation from the UV space location map U V l . <ref type="table">DMP  H36M, MPI-INF-3DHP, MPII, COCO, LSP  IK  H36M, MPI-INF-3DHP, AMASS, AIST++  UVI  H36M, 3DOH   Table 1</ref>. Training datasets for each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stages</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>The proposed framework is trained on the ResNet-50 <ref type="bibr" target="#b7">[8]</ref> backbone pre-trained on ImageNet. It takes a 224 ? 224 image as input, and input resolution for UVI is 64 ? 64 and the output resolution is 128 ? 128. We train three modules separately. We first train our DMP, and based on the output of DMP and Mocap data we train our IK; We finally fix and concat DMP and IK, and train UVI module. We apply synthetic occlusion <ref type="bibr" target="#b38">[39]</ref> when train DMP. The training data is augmented with randomly scaling, rotation, flipping and RGB channel noise. We use the Adam optimizer. The training data for each module is illustrated in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Metric</head><p>Human3.6M <ref type="bibr" target="#b8">[9]</ref> is commonly used as the benchmark dataset for 3D human pose estimation, consisting of 3.6 millions of video frames captured in the controlled environment. It has 11 subjects, 15 kinds of action sequences and 1.5 million training images with accurate 3D annotations. Similar to <ref type="bibr" target="#b11">[12]</ref>, we use MoSH to process the marker data in the original dataset, and obtain the ground truth SMPL parameters to generate the groundtruth for U V l . For a fair comparison, we use 300K data in S1, S5, S6, S7, S8 for network training, and test in S9, S11. 3DOH <ref type="bibr" target="#b51">[52]</ref> utilize multi-view SMPLify-X <ref type="bibr" target="#b31">[32]</ref> to get the 3d ground truth. The dataset is designed to have object occlusion for subjects. It contains 50,310 training images and 1,290 test images. It provides 2D, 3D annotations and SMPL parameters to generate meshes. We use the test set for evaluation purposes and the training set to train the UVI module. LSP <ref type="bibr" target="#b9">[10]</ref> dataset is a 2D human pose estimation benchmark. In our work, we use the <ref type="bibr" target="#b15">[16]</ref> SMPL parameter to render the M i to train DMP module. MPI-INF-3DHP <ref type="bibr" target="#b27">[28]</ref> is a dataset captured with a multiview setup mostly in indoor environments. No markers are used for the capture, so 3D pose data tend to be less accurate compared to other datasets. We use the provided training set (subjects S1 to S8) for training. We use the it to train DMP and IK module. Mocap dataset We use <ref type="bibr" target="#b25">[26]</ref> AMASS, AIST++ <ref type="bibr" target="#b17">[18]</ref> and SPIN <ref type="bibr" target="#b14">[15]</ref> dataset to train our occlusion aware GIKNet. Evaluation We evaluate our method on H36M <ref type="bibr" target="#b8">[9]</ref> dataset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H36M</head><p>Method MPJPE MPJPE-PA HMR <ref type="bibr" target="#b11">[12]</ref> 88.0 56.8 DaNet <ref type="bibr" target="#b47">[48]</ref> 61.5 48.6 HoloPose <ref type="bibr" target="#b5">[6]</ref> 60.3 46.5 SPIN <ref type="bibr" target="#b14">[15]</ref> 62. and 3DOH <ref type="bibr" target="#b51">[52]</ref> datasets. We report Procrustes-aligned mean per joint position error (MPJPE-PA) and mean per joint position error (MPJPE) in mm. For 3DOH we also report mean per vertex error (MPVE) in mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Results</head><p>Comparison with SOTA performance We can see our final stage (UVI-14) in <ref type="table">Table 2</ref> achieve state-of-the-art performance on common H36M benchmark. Our SOTA performance demonstrates the usefulness of proposed combi- nation of model-based and nonparametric approaches. In <ref type="table">Table 3</ref>, as our methods focus on both pose and mesh while <ref type="bibr" target="#b46">[47]</ref> focus more on meshes, they achieve SOTA performance on 3DOH dataset; PARE <ref type="bibr" target="#b13">[14]</ref> uses the EFT dataset <ref type="bibr" target="#b10">[11]</ref> with improved groundtruth thus outperforms us on MPJPE-PA metric.</p><p>14 joints vs 24 joints setting Another way to get 24 joints prediction from DMP is to have a 24 joints segmentation A uv in UV space following SMPL setting. As shown in <ref type="figure" target="#fig_5">Fig 7</ref> we define 14 joints setting and 24 joints setting. We run DMP-24 and DMP-14 and evaluate on the predicted J initial . We observe the error of DMP-24 is much higher than DMP-14 as in certain parts (feet, hand) and will lead to higher error.</p><p>Occlusion vs Non-occlusion When computing the MPJPE for J initial , the results for visible parts (part with any pixel belong to them visible) and invisible parts differ a lot. We compare the DMP-14 and DMP-14-Nonoccluded in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GIK-Net data augmentations</head><p>We also try to remove the gaussian noise or random mask out joints data augmentation techniques for MOCAP data, which serve as input for the GIK-Net, to see how is the MPJPE varying. As shown in <ref type="table" target="#tab_2">Table 4</ref>, IK-14 w/o gaussian noise and IK-14-w/o random zero yield larger error (2.8 mm and 3.9 mm ) compared with IK-14. It demonstrate these data augmentation makes the GIK-Net more robust to noise and helps generalize to real data input.</p><p>UVI ablations As the magnitude of our U V d should be symmetric, we introduce the magnitude error for U V d and its flip version. We run a model without this ? dismag and observe there is 4.5 mm error increase in MPJPE metric. This is shown in <ref type="table" target="#tab_2">Table 4</ref>.</p><p>Each stage performance DMP module is a nonparametric method, while IK module is a model-based method relying on the output of DMP and then correct it. UVI module </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Results</head><p>We present qualitative results in <ref type="figure" target="#fig_6">Fig 8</ref> including the joints prediction from DMP, IK, UVI modules and mesh prediction from IK, UVI modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>We also show failure cases in <ref type="figure" target="#fig_7">Fig 9.</ref> Typical failure cases can be attributed to challenging poses (a,b,d), and crowded scenarios (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a framework that combine the best of both worlds (nonparametric and SMPL model-based method). It predicts the initial 3d body pose from DMP module, refine the predicted pose and repose the template SMPL meshes using IK module. Based on the nonparametric prediction from DMP module and model-based prediction from IK module, the UVI module inpaint and refine the prediction. To alleviate the intrinsic error introduced by joint regressor (fitting), we regress joint (U V j ) and mesh (U V l ) separately in different maps in UV space. We also introduce the magnitude loss ? dismag to enforce the symmetric property of human (U V d ). Our framework achieves state-of-theart performance among 3D mesh-based methods on several public benchmarks. Future work can focus on extending the framework to the reconstruction of full body surfaces including hands and faces. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Our 3d body estimation framework consists of three part: Dense Map Prediction module (DMP), Inverse Kinematics and SMPL module (IK) and UV Inpainting Module (UVI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Semantic maps aligned with image space. From left to right: IUV image Mi, Dense jointmap Mj, dense location map M l and dense displacement map M d . (Best viewed in Color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Warped Images in UV space based on IUV images Mi. From left to right: Part segmentation in UV space Auv, UV space jointmap U Vj, UV space location map U V l and UV space displacement map U V d . (Best viewed in Color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Full groundtruth in UV space. From left to right: UV space jointmap U Vj, UV space location map U V l and UV space displacement map U V d . (Best viewed in Color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Structure of GIKNet. (Best viewed in Color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Different part segmentation choice in UV space. (Best viewed in Color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Pose and shape prediction from DMP module, IK module and UVI module. (Best viewed in Color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Failure cases. (Best viewed in color) relies on both nonparametric output and model-based output, and predict the final body joint and mesh. Based on Table 4, DMP-14 estimate from raw images and gives inferior performance. IK-14 corrects the output from DMP-14 and reduce the error by 15.5 mm. UVI-14 relies on both IK-14 and DMP-14 and further reduce MPJPE to 58.3 mm. However, if any of the previous stage output is missing, MPJPE increase by 11.1 mm (w/o IK-14) or 8.5 mm (w/o DMP-14).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>More qualitative results on COCO dataset. (Best viewed in Color) More qualitative results on 3DOH dataset. (Best viewed in Color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .Table 4 .</head><label>44</label><figDesc>The main reason is that oversegment of body parts may distribute less visible pixels to Ablation study about reconstruction errors on 3DOH test set. 14 and 24 denotes the number of joints setting for training and evaluations. Nonoccluded denotes when we calculate error we are not counting the part without any visible image evidence.</figDesc><table><row><cell></cell><cell></cell><cell>3DOH</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">MPVE MPJPE PMPJPE</cell></row><row><cell>DMP-24</cell><cell>-</cell><cell>246.4</cell><cell>208.5</cell></row><row><cell>DMP-14 w/o synthetic occlusion</cell><cell>-</cell><cell>135.4</cell><cell>115.7</cell></row><row><cell>DMP-14-Nonoccluded</cell><cell>-</cell><cell>87.3</cell><cell>64.7</cell></row><row><cell>DMP-14</cell><cell>-</cell><cell>128.4</cell><cell>109.8</cell></row><row><cell>IK-14 w/o gaussian noise</cell><cell>138.2</cell><cell>115.7</cell><cell>82.8</cell></row><row><cell>IK-14 w/o random zero</cell><cell>139.5</cell><cell>116.8</cell><cell>83.2</cell></row><row><cell>IK-14</cell><cell>133.5</cell><cell>112.9</cell><cell>80.8</cell></row><row><cell>UVI-14 w/o IK-14</cell><cell>82.9</cell><cell>69.4</cell><cell>58.1</cell></row><row><cell>UVI-14 w/o DMP-14</cell><cell>80.1</cell><cell>67.8</cell><cell>55.1</cell></row><row><cell>UVI-14 w/o ? dismag</cell><cell>75.5</cell><cell>63.8</cell><cell>47.3</cell></row><row><cell>UVI-14</cell><cell>72.3</cell><cell>58.3</cell><cell>44.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>We find visible parts with 87.3 mm MPJPE while the MPJPE counting invisible parts yield 128.4 mm. It tells us if the joints are visible, our DMP can predict relative good initial results. Thus, synthetic occlusion helps for our DMP module. When we remove the data augmentation techniques like synthetic occlusion<ref type="bibr" target="#b38">[39]</ref>, DMP-14 increase to 135.4 mm.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Qualitative Results</head><p>We show more qualitative results on COCO <ref type="bibr" target="#b21">[22]</ref> in <ref type="figure">Fig  10,</ref> and 3DOH <ref type="bibr" target="#b51">[52]</ref> in <ref type="figure">Fig 11.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Part Segmentation in UV Space</head><p>We first use the reference T-pose mesh and the LSP joint regressor provided by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b45">46]</ref> to get the T-pose 14 joint location. Then we calculate the joint-vertex euclidean distance and assign vertex to joint based on the smallest distance. After that, we use the barycentric interpolation (mapping between vertex triangle and UV space triangles) to get the UV space assignment probability (128 ? 128 ? 14). Following these operations, we use argmax to get the final assignment for each UV grid to the joint location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>For AMASS <ref type="bibr" target="#b25">[26]</ref> data, we only get SMPL-H <ref type="bibr" target="#b31">[32]</ref> fitting instead of SMPL fitting data, however, SMPL-H does not included hands rotations as in SMPL. We sample random rotations from SPIN <ref type="bibr" target="#b14">[15]</ref> fitting or the predictions from our DMP stages for its training data. For AIST++ <ref type="bibr" target="#b17">[18]</ref>, it does not included ? parameters, we sample ? from SPIN <ref type="bibr" target="#b14">[15]</ref> fitting or the predictions from our DMP stages for its training data. We use the original rotation representation from SMPL <ref type="bibr" target="#b22">[23]</ref> (axis-angle representation) for the fast training purpose.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust control of robotic manipulators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balestrino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">De</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sciavicco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFAC Proceedings Volumes</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parsing occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computational modeling for the computer animation of legged figures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony A</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object detection with grammar models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurips</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Resolving 3d human pose ambiguities with 3d scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exemplar fine-tuning for 3d human pose fitting towards in-thewild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Arxiv</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pare: Part attention regressor for 3d human body estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unite the people -closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ai choreographer: Music conditioned 3d dance generation with aist++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mesh graphormer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">siggraph</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pc-hmr: Pose calibration for 3d human mesh recovery from 2d images/videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transfusion: Cross-view fusion with transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deying</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Yao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AMASS: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A semantic occlusion model for human pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umer</forename><surname>Raf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Full-body awareness from partial observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Rockwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How robust is 3d human pose estimation to occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Arxiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Synthetic occlusion augmentation with volumetric heatmaps for the 2018 eccv posetrack challenge on 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Arxiv</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural kinematic networks for unsupervised motion retargetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geometric pose affordance: 3d human pose with scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaurya</forename><surname>Rathore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arxiv</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Daeyun Shin, and Charless Fowlkes</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Predicting camera viewpoint improves cross-dataset generalization for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 3DPW workshop</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A computational technique for inverse kinematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wolovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CDC</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A2j: Anchor-tojoint regression network for 3d articulated pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boshen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taidong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wangu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning 3d human shape and pose from dense body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning dynamical human-joint affinity for 3d pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Perceiving 3d human-object spatial arrangements from a single image in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Pepose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monocular realtime full body capture with inter-part correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Monocular realtime hand shape and motion capture using multi-modal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
