<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Iyer</surname></persName>
							<email>iyernikhil007@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Thejas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nipun</forename><surname>Kwatra</surname></persName>
							<email>nipun.kwatra@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramachandran</forename><surname>Ramjee</surname></persName>
							<email>ramjee@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muthian</forename><surname>Sivathanu</surname></persName>
							<email>muthian@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<country>Atlassian India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>generalization</term>
					<term>learning rate schedule</term>
					<term>optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several papers argue that wide minima generalize better than narrow minima. In this paper, through detailed experiments that not only corroborate the generalization properties of wide minima, we also provide empirical evidence for a new hypothesis that the density of wide minima is likely lower than the density of narrow minima. Further, motivated by this hypothesis, we design a novel explore-exploit learning rate schedule. On a variety of image and natural language datasets, compared to their original hand-tuned learning rate baselines, we show that our explore-exploit schedule can result in either up to 0.84% higher absolute accuracy using the original training budget or up to 57% reduced training time while achieving the original reported accuracy. For example, we achieve state-of-theart (SOTA) accuracy for IWSLT'14 (DE-EN) dataset by just modifying the learning rate schedule of a high performing model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the fascinating properties of deep neural networks (DNNs) is their ability to generalize well, i.e., deliver high accuracy on the unseen test dataset. It is well-known that the learning rate (learning rate) schedules play an important role in the generalization performance <ref type="bibr" target="#b20">(Keskar et al., 2016;</ref><ref type="bibr" target="#b40">Wu et al., 2018;</ref><ref type="bibr" target="#b13">Goyal et al., 2017)</ref>. In this paper, we study the question, what are the key properties of a learning rate schedule that help DNNs generalize well during training?</p><p>We start with a series of experiments training Resnet18 on Cifar-10 over 200 epochs. We vary the number of epochs trained at a high learning rate of 0.1, called the explore epochs, from 0 to 100 and divide up the remaining epochs equally for training with learning rates of 0.01 and 0.001. Note that the training loss typically stagnates around 50 epochs with 0.1 learning rate. Despite that, we find that as the number of explore epochs increase to 100, the average test accuracy also increases. We also find that the minima found in higher test accuracy runs are wider than the minima from lower test accuracy runs, corroborating past work on wide-minima and generalization <ref type="bibr" target="#b20">(Keskar et al., 2016;</ref><ref type="bibr" target="#b16">Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b17">Jastrzebski et al., 2017;</ref><ref type="bibr" target="#b39">Wang et al., 2018)</ref>. Moreover, what was particularly surprising was that, even when using fewer explore epochs, a few runs out of many trials still resulted in high test accuracies! Thus, we not only find that an initial exploration phase with a high learning rate is essential to the good generalization of DNNs, but that this exploration phase needs to be run for sufficient time, even if the training loss stagnates much earlier. Further, we find that, even when the exploration phase is not given sufficient time, a few runs still see high test accuracy values.</p><p>To explain these observations, we hypothesize that, in the DNN loss landscape, the density of narrow minima is significantly higher than that of wide minima. Intuitively, a large learning rate can escape narrow minima easily (as the optimizer can jump out of them with large steps). However, once it reaches a wide minima, it is likely to get stuck in it (if the "width" of the wide minima is large compared to the step size). With fewer explore epochs, a large learning rate might still get lucky occasionally in finding a wide minima but invariably finds only a narrower minima due to their higher density. As the explore duration increases, the probability of eventually landing in a wide minima also increases. Thus, a minimum duration of explore is necessary to land in a wide minimum with high probability.</p><p>An observation on the rarity of wide minima has been hinted at by prior work <ref type="bibr" target="#b40">(Wu et al., 2018;</ref><ref type="bibr" target="#b2">Baldassi et al., 2020)</ref> based on theoretical analysis of simple neural networks (see Section 2). In this paper, we add significant empirical evidence to these theoretical observations. We believe that all these results together constitute sufficient evidence for this observation to now be classified as a hypothesis, that we term the wide-minima density hypothesis.</p><p>The hypothesis helps explain not only our experiments but also the generalization out-performance of prior heuristic-based learning rate decay schemes such as cosine decay <ref type="bibr" target="#b27">(Loshchilov and Hutter, 2016)</ref>. Cosine decay implicitly maintains a higher learning rate during the first half of training compared to schemes like linear decay. Based on the hypothesis, the higher learning rate allows cosine decay to find wider minima with higher probability, resulting in cosine decay's better generalization compared to linear decay.</p><p>Apart from helping explain empirical observations, the hypothesis also enables a principled learning rate schedule design that explicitly accounts for the requisite explore duration. Motivated by the hypothesis, we design a novel Explore-Exploit learning rate schedule, where the initial explore phase optimizes at a high learning rate in order to arrive in the vicinity of a wide minimum. This is followed by an exploit phase which descends to the bottom of this wide minimum. We give explore phase enough time so that the probability of landing in a wide minima is high. For the exploit phase, we experimented with multiple schemes, and found a simple, parameter-less, linear decay to zero to be effective. Thus, our proposed learning rate schedule optimizes at a constant high learning rate for a given duration, followed by a linear decay to zero. We call this learning rate schedule the Knee schedule.</p><p>We extensively evaluate the Knee schedule across a wide range of models and datasets, ranging from NLP (BERT pre-training, Transformer on  and ) to CNNs (ImageNet on ResNet-50, Cifar-10 on ResNet18), and spanning multiple optimizers: SGD Momentum, Adam, RAdam, and LAMB. In all cases, Knee schedule improves the test accuracy of state-of-the-art hand-tuned learning rate schedules, when trained using the original training budget. The explore duration is a hyper-parameter in Knee schedule but even if we set the explore duration to a fixed 50% fraction of total training budget, we find that it still outperforms prior schemes.</p><p>We also experimented with reducing the training budget, and found that Knee schedule can achieve the same accuracy as the baseline under significantly reduced training budgets. For the BERT LARGE pretraining,  and ImageNet experiments, we are able to train in 33%, 57% and 44% less training budget, respectively, for the same test accuracy. This corresponds to significant savings in GPU compute, e.g. savings of over 1000 V100 GPU-hours for BERT LARGE pretraining.</p><p>The main contributions of our work 1 are: 1. A hypothesis of lower density of wide minima in the DNN loss landscape, backed by extensive experiments, that explains why a high learning rate needs to be maintained for sufficient duration to achieve good generalization. 2. The hypothesis explains the good performance of heuristic-based schemes such as cosine decay, and promotes a principled design of learning rate decay schemes. 3. Motivated by the hypothesis, we design an Explore-Exploit learning rate schedule called Knee schedule that outperforms prior heuristic-based learning rate schedules, including achieving state-of-the-art results on the IWSLT'14 (DE-EN) dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generalization. There has been a lot of work on understanding the generalization characteristics of DNNs. <ref type="bibr" target="#b19">Kawaguchi (2016)</ref> found that DNNs have many local minima, but all local minima were also the global minima. It has been observed by several authors that wide minima generalize better than narrow minima <ref type="bibr" target="#b0">(Arora et al., 2018;</ref><ref type="bibr" target="#b16">Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b20">Keskar et al., 2016;</ref><ref type="bibr" target="#b17">Jastrzebski et al., 2017;</ref><ref type="bibr" target="#b39">Wang et al., 2018)</ref> but there have been other works questioning this hypothesis as well <ref type="bibr" target="#b8">(Dinh et al., 2017;</ref><ref type="bibr" target="#b12">Golatkar et al., 2019;</ref><ref type="bibr" target="#b14">Guiroy et al., 2019;</ref><ref type="bibr" target="#b18">Jastrzebski et al., 2019;</ref><ref type="bibr" target="#b42">Yoshida and Miyato, 2017)</ref>. <ref type="bibr" target="#b20">Keskar et al. (2016)</ref> found that small batch SGD generalizes better and lands in wider minima than large batch SGD. However, recent work has been able to generalize quite well even with very large batch sizes <ref type="bibr" target="#b13">(Goyal et al., 2017;</ref><ref type="bibr" target="#b28">McCandlish et al., 2018;</ref><ref type="bibr" target="#b33">Shallue et al., 2018)</ref>, by scaling the learning rate linearly as a function of the batch size. Jastrzebski et al. (2019) analyze how batch size and learning rate influence the curvature of not only the SGD endpoint but also the whole trajectory. They found that small batch or large step SGD have similar characteristics, and yield smaller and earlier peak of spectral norm as well as smaller largest eigenvalue. <ref type="bibr" target="#b6">Chaudhari et al. (2019)</ref>;  propose methods to drive the optimizer to wide minima. <ref type="bibr" target="#b39">Wang et al. (2018)</ref> analytically show that generalization of a model is related to the Hessian and propose a new metric for the generalization capability of a model that is unaffected by model reparameterization of <ref type="bibr" target="#b8">Dinh et al. (2017)</ref>. <ref type="bibr" target="#b42">Yoshida and Miyato (2017)</ref> argue that regularizing the spectral norm of the weights of the neural network help them generalize better. On the other hand, <ref type="bibr" target="#b0">Arora et al. (2018)</ref> derive generalization bounds by showing that networks with low stable rank (high spectral norm) generalize better. <ref type="bibr" target="#b14">Guiroy et al. (2019)</ref> looks at generalization in gradientbased meta-learning and they show experimentally that generalization and wide minima are not always correlated. Finally, <ref type="bibr" target="#b12">Golatkar et al. (2019)</ref> show that regularization results in higher test accuracy specifically when it is applied during initial phase of training, similar to the importance of Knee schedule's explore phase during initial phase of training. In a similar vein,  explain the regularization benefits of the initial higher learning rate by showing that higher learning rate helps networks learn easier-to-fit general patterns.</p><p>Neural network loss landscapes. The landscape of loss in neural networks have been extensively studied <ref type="bibr" target="#b9">(Draxler et al., 2018;</ref><ref type="bibr" target="#b10">Freeman and Bruna, 2016;</ref><ref type="bibr" target="#b11">Garipov et al., 2018;</ref><ref type="bibr" target="#b32">Sagun et al., 2017)</ref>. These papers point out that the loss landscape contains both wide and narrow minima, and there may even exist a path from one minima to another without barriers. However, there are multiple paths between these minima and some paths indeed face barriers (e.g., see <ref type="figure" target="#fig_1">Figure 1</ref> in <ref type="bibr" target="#b9">Draxler et al. (2018)</ref>). Since we don't know which path SGD and other optimizers might follow, even if wide and narrow minima are part of a single basin, SGD and other optimizers might still require higher learning rates to navigate from narrow to wide minima.</p><p>Lower density of wide minima. <ref type="bibr" target="#b40">Wu et al. (2018)</ref> compares the sharpness of minima obtained by full-batch gradient descent (GD) with different learning rates for small neural networks on FashionMNIST and Cifar10 datasets. They find that GD with a given learning rate finds the theoretically sharpest feasible minima for that learning rate. Thus, in the presence of several flatter minimas, GD with lower learning rates does not find them, leading to the conjecture that density of sharper minima is perhaps larger than density of wider minima. <ref type="bibr" target="#b2">Baldassi et al. (2020)</ref> show analytically for simple, two-layer non-convex networks that wide minima exists and are rare, compared to narrow minima, local minima and saddle points. In this paper, we add significant evidence to these theoretical observations based on empirical results obtained on large-scale, state-of-the-art neural networks through carefully designed experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Wide-Minima Density Hypothesis</head><p>Many popular learning rate schedules, such as the step decay schedules for image datasets, start the training with high learning rate, and then reduce the learning rate periodically. For example, consider the case of Cifar-10 on Resnet-18, trained using a typical step learning rate schedule of 0.1, 0.01, and 0.001 for 100, 50, 50 epochs each. In many such schedules, even though training loss stagnates after several epochs of high learning rate, one still needs to continue training at high learning rate in order to get good generalization.</p><p>For example, <ref type="figure" target="#fig_1">Figure 1</ref> shows the training loss for Cifar-10 on Resnet-18, trained with a fixed learning rate of 0.1 (orange curve), compared to a model trained via a step schedule with learning rate reduced at epoch 50 (blue curve). As can be seen from the figure, the training loss stagnates after ? 50 epochs for the orange curve, and locally it makes sense to reduce the learning rate to decrease the loss. However, as shown in <ref type="table" target="#tab_0">Table 1</ref>, generalization is directly correlated with duration of training at high learning rate, with the highest test accuracy achieved when the high learning rate is used for 100 epochs, well past the point where training loss stagnates. Note that the final training loss remains similar for all runs.</p><p>To understand the above phenomena, we perform another experiment. We train Cifar-10 on Resnet-18 for 200 epochs, using a high learning rate of 0.1 for only 30 epochs and then use learning rate of 0.01 and 0.001 for 85 epochs each. We repeat this training 50 times with different random weight initializations. On an average, as expected, this training yields a low test accuracy of 94.81. However, in 1 of the 50 runs, we find that the test accuracy reaches 95.24, even higher than the average accuracy of 95.1 obtained while training at high learning rate for 100 epochs!   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hypothesis</head><p>To explain the above observations, i.e., using a high learning rate for short duration results in low average test accuracy with rare occurrences of high test accuracy, while using the same high learning rate for long duration achieves high average test accuracy and frequent occurrences of high test accuracy, we introduce a new hypothesis. We hypothesize that, in the DNN loss landscape, the density of narrow minima is significantly higher than that of wide minima. Intuitively, a large learning rate can escape narrow minima "valleys" easily (as the optimizer can jump out of them with large steps). However, once it reaches a wide minima "valley", it is likely to get stuck in it (if the "width" of the wide valley is large compared to the step size). This intuition is backed by theoretical results from <ref type="bibr" target="#b41">Xie et al. (2020)</ref> that show that the time to escape a minimum using SGD is exponential in the inverse of learning rate as well as inverse of the sharpness (measured by eigenvalue of the Hessian at the minima). Thus, large learning rates escape narrow minima exponentially faster than wide minima.</p><p>If wide and narrow minima were uniformly distributed, SGD with a large LR would be able to quickly escape the narrow minima, land on a wide minima and get stuck there. Yet, we see that we need to maintain large LR for significant duration for landing in a wide minima with high probability. On the other hand, if our hypothesis is true, i.e., wide   minima are much fewer than narrow minima, the probability of landing in a wide minima after escaping a narrow minima is low, and the optimizer needs to take a lot of steps to have a high probability of eventually landing in a wide minimum. Thus, the hypothesis is a better explanation for the observation in <ref type="table" target="#tab_0">Table 1</ref>, where the average accuracy continues to improve as we increase the number of high learning rate training steps. The hypothesis also explains why very few (just 1) of the 50 runs trained at 0.1 learning rate for just 30-epochs also manages to attain high accuracy-these runs just got lucky in a probabilistic sense and landed in a wide minimum even with a shorter duration of explore.</p><p>To validate this hypothesis further, we run experiments similar to the one in <ref type="table" target="#tab_0">Table 1</ref>. Specifically, we train Cifar-10 on Resnet-18 model for 200 epochs using a standard step schedule with learning rate of 0.1, 0.01, 0.001. We vary the number of epochs trained using the high learning rate of 0.1, called the explore epochs, from 0 to 100 epochs, and divide up the rest of the training equally between 0.01 and 0.001. For each experimental setting, we conduct 50 random trials and plot the distributions of final test accuracy and the minima sharpness as defined by the metric in Keskar et al. (2016) (see section 3.2). If our hypothesis is true, then the more you explore, the higher the probability of landing (and getting stuck) in a wide minima region, which should cause the distribution to tighten and move towards wider minima (lower sharpness), as the number of explore steps increase. This is exactly what is observed in <ref type="figure" target="#fig_2">Figure 2</ref>. Also since wide minima correlate with higher test accuracy, we should see the test accuracy distribution move towards higher accuracy and sharpen, as the number of explore steps increase. This is confirmed as well in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Longer training with low learning rate is not sufficient. Finally, to verify whether explore at high learning rate is essential, we train Cifar-10 for 10,000 epochs at a fixed lower learning rate of 0.001. The training loss converged but the final test accuracy was only 93.9, compared to an accuracy of over 95% in 200 epochs in <ref type="table" target="#tab_0">Table 1</ref>. Thus, even training 50? longer at low learning rate is not sufficient to achieve good generalization. Again, this observation ties in well with the theoretical results from <ref type="bibr" target="#b41">Xie et al. (2020)</ref> where the authors show that the time to escape a minimum using SGD is exponential in the inverse of learning rate. Thus, this result adds further evidence to our density hypothesis, since even training 50? longer at a low learning rate is not sufficient to land in a wide minima.</p><p>Multi-scale. Given the importance of explore at high learning rate, a natural question that may arise is whether explore is necessary at smaller learning rate as well. To answer this, we train the same network for a total of 200 epochs with an initial high learning rate of 0.1 for 100 epochs, but now we vary the number of epochs trained with the learning rate of 0.01 (we call this finer-scale explore), and train with learning rate of 0.001 for the remaining epochs. As can be seen from <ref type="table">Table 2</ref>, although the final training loss remains similar, we find that finer-scale explore also plays a role similar to the initial explore in determining the final test accuracy. This indicates that our hypothesis about density of wide/narrow regions indeed holds at multiple scales. <ref type="table">Table 2</ref>: Cifar-10 on Resnet-18 trained for 200 epochs. A learning rate of 0.1 is used for the first 100 epochs. We then vary the number of epochs trained with learning rate of 0.01 (called finer-scale explore), and train the remaining epochs with a learning rate of 0.001. We report averages values over 3 runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Minima Sharpness</head><p>Our hypothesis predicts that higher explore helps the optimizer land in a wider minimum, which in turn helps generalization. We demonstrated this empirically in <ref type="figure" target="#fig_2">Figure 2</ref>, where we plotted the distribution of the minima sharpness, as measured by the sharpness metric introduced by <ref type="bibr" target="#b20">(Keskar et al., 2016)</ref>. In this section, we describe Keskar's sharpness metric in detail. We also introduce a simple projected gradient ascent scheme to compute this metric efficiently, which scales well to large networks. Finally, we also evaluate our hypothesis with a different metric for minima sharpness, the Fisher Score, which is based on the Fisher information matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Keskar's Sharpness Metric</head><p>Keskar's sharpness metric is based on measuring the maximum jump in the network's output function F in a small neighborhood around the minimum. After a few simplifications, Keskar's metric for sharpness around a point x can be written as:</p><formula xml:id="formula_0">S x,F ( ) := (max y?C (x) F (x + y)) ? F (x) 1 + F (x) ? 100,<label>(1)</label></formula><p>where C (x) is an neighborhood around x.  <ref type="bibr" target="#b4">(Byrd et al., 2003)</ref> optimization scheme. However, in our experiments we found the method to be very slow. To combat this, <ref type="bibr" target="#b20">Keskar et al. (2016)</ref> limited their runs to 10 iterations but we found that results were suboptimal using few iterations. Instead, we employed a projected gradient ascent scheme to solve Equation 1. In each optimization step, we took a small step with a learning rate of 0.001 in the gradient direction and projected the updated point to lie inside C (x). Because of the first order nature, this method is much faster. We found that even 1000 iterations were fast to compute and the results were much better than the second order method in all cases we evaluated.</p><p>Using Keskar's sharpness metric, we had shown in <ref type="figure" target="#fig_2">Figure 2</ref> that the distribution of minima sharpness moves towards lower values as the number of explore epochs increase. In <ref type="table" target="#tab_3">Table 3</ref>, we also report the average sharpness of the minima for varying explores. As predicted by our hypothesis, average sharpness decreases as number of explore epochs increase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Fisher Score</head><p>The maximum Eigen value of the Fisher Information Matrix (FIM) estimates the highest curvature at a point, and is used as another metric to measure minima sharpness <ref type="bibr" target="#b37">(Sokol and Park, 2018)</ref>. We used an unbiased estimate of the true Fisher matrix (see <ref type="bibr" target="#b24">Kunstner et al. (2019)</ref>) using 10 unbiased samples per training data. <ref type="table" target="#tab_4">Table 4</ref> shows the average Fisher scores for the Cifar-10 experiments at varying explores. Again, the sharpness measured by the Fisher score decreases as the number of explore epochs increase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Explore-Exploit Learning Rate Schedule</head><p>Given that we need to explore at multiple scales for good generalization, how do we go about designing a good learning rate schedule? The search space of the varying learning rate steps and their respective explore duration is enormous.</p><p>Fortunately, since the explore at the initial scale is searching over the entire loss surface while explore at finer-scales is confined to exploring only the wide-minima region identified by the initial explore, the former is more crucial. In our experiments as well, we found that the initial portion of training is much more sensitive to exploration and needs a substantial number of explore steps, while after this initial phase, several decay schemes worked equally well. This is similar to the observations in <ref type="bibr" target="#b12">(Golatkar et al., 2019)</ref> where the authors found that regularization such as weight-decay and data augmentation mattered significantly only during the initial phase of training.</p><p>The above observations motivate our Explore-Exploit learning rate schedule, where the explore phase first optimizes at a high learning rate for some minimum time in order to land in the vicinity of a wide minima. We should give the explore phase enough time (a hyper-parameter), so that the probability of landing in a wide minima is high. After the explore phase, we know with a high probability, that the optimizer is in the vicinity of a wide region. We now start the exploit phase to descend to the bottom of this wide region while progressively decreasing the learning rate. Any smoothly decaying learning rate schedule can be thought of as doing micro explore-exploit at progressively reduced scales. A steady descent would allow more explore duration at all scales, while a fast descent would explore less at higher learning rates. We experimented with multiple schedules for the exploit phase, and found a simple linear decay to zero, that does not require any hyper-parameter, to be effective in all the models/datasets we tried. We call our proposed learning rate schedule which starts at a constant high learning rate for some minimum time, followed by a linear decay to zero, the Knee schedule.</p><p>Note that any learning rate decay scheme incorporates an implicit explore during the initial part, where the learning rate stays high enough. To evaluate the benefit of an explicit explore phase, we compare Knee schedule against several decay schemes such as linear and cosine. Interestingly, the results depend on the length of training. For long budget experiments, simple decay schemes perform comparable to Knee schedule in some experiments, since the implicit explore duration is also large, helping these schemes achieve good generalization. However for short budget experiments, these schemes perform significantly worse than Knee schedule, since the implicit explore duration is much shorter. See <ref type="table" target="#tab_5">Table 5</ref> , 6 and 7 for the comparison.</p><p>Warmup. Some optimizers such as Adam use an initial warmup phase to slowly increase the learning rate. However, as shown in <ref type="bibr" target="#b26">Liu et al. (2019)</ref>, learning rate warmup is needed mainly to reduce variance during initial training stages and can be eliminated with an optimizer such as RAdam. Learning rate warmup is also used for large-batch training <ref type="bibr" target="#b13">(Goyal et al., 2017)</ref>. Here, warmup is necessary since the learning rate is scaled to a very large value to compensate for the large batch size. This warmup is complementary and can be incorporated into Knee schedule. For example, we do this for BERT LARGE pretraining experiment where a large 16k batch size was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>In this section we present extensive empirical evaluation of Knee schedule on multiple models and datasets across various optimizers, and compare Knee schedule against the original hand-tuned learning rate baselines. We first provide an overview of our main results followed by detailed experimental results. We then run further experiments to validate our wideminima density hypothesis, as well as run sensitivity analysis of seed learning rate on the Knee schedule.</p><p>Note that, for completeness, we present a detailed comparison of Knee schedule with many other learning rate schedules in literature such as linear decay, cosine decay <ref type="bibr" target="#b27">(Loshchilov and Hutter, 2016)</ref>, one-cycle <ref type="bibr" target="#b36">(Smith, 2018)</ref> in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments</head><p>We evaluate Knee schedule on multiple models and datasets spanning both vision and NLP problems. The training of these models spanned various optimizers including SGD Momentum, Adam (Kingma and Ba, 2014a), RAdam <ref type="bibr" target="#b26">(Liu et al., 2019)</ref> and LAMB <ref type="bibr" target="#b43">(You et al., 2019)</ref>. For all experiments, we used an out of the box policy, where we only change the learning rate schedule, without modifying anything else. We evaluate on multiple image datasets -Imagenet on Resnet-50, Cifar-10 on Resnet-18; as well as various NLP datasets -pretraining BERT LARGE on Wikipidea+BooksCorpus and fine-tuning it on SQuADv1.1; and WMT'14 (EN-DE), IWSLT'14 (DE-EN) on Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results Overview</head><p>In all our experiments, we find that Knee schedule shows an improvement in test accuracy over the original hand-tuned learning rate baseline as well as various other learning rate schedules in the literature. Further, we also find that Knee schedule can achieve the same accuracy as the baseline with a much reduced training budget.  <ref type="table" target="#tab_5">Table 5</ref> shows the test accuracies of the various experiments, when trained with the original budget; while <ref type="table">Table 6</ref> shows the results when trained with a reduced budget. As <ref type="table">Table 6</ref>: Shorter budget training: Test accuracy on all learning rate schedules tried in this paper, but trained with a shortened budget. We report same metrics as <ref type="table" target="#tab_5">Table 5</ref>. Knee schedule achieves the same accuracy as baseline schedules using much lower budget, saving precious GPU-hours.  shown, for the original budget runs, Knee schedule improves on the test accuracies in all experiments. Note that in Knee schedule, the explore duration is a hyperparameter. To avoid tuning this hyperparameter, we experimented with a fixed 50% explore duration for the full budget runs. Even the fixed 50% explore Knee schedule outperforms all the other baselines. Also noteworthy is that Knee schedule is able to achieve the same test accuracies as the baseline's full budget runs with a much lower training budget, saving precious GPU cycles <ref type="table">(Table 6</ref>). While the difference in accuracy values between the various schedules might appear deceptively small in absolute terms, achieving these gains require a large amount of compute. For example, the number of epochs needed by each scheme to reach the target BLEU score for IWSLT'14 DE-EN and WMT'14 EN-DE with the Transformer network is shown in <ref type="table" target="#tab_7">Table 7</ref>. One can see that Knee schedule is significantly more efficient as compared to say Cosine Decay, which takes 100% more training time to achieve the same accuracy for WMT'14 EN-DE. Thus, the accuracy and/or compute gains achieved by Knee schedule is significant.</p><p>A summary of our main experimental results is as follows:</p><p>1. Imagenet on Resnet-50: We show an absolute gain of 0.8% in top-1 accuracy against the competitive step schedule baseline for this model. Also, Knee schedule can achieve the same accuracy as baseline in ?45% less training epochs.</p><p>2. BERT LARGE pre-training on Wikipedia+BooksCorpus dataset: Compared to the baseline of <ref type="bibr" target="#b43">You et al. (2019)</ref>, we improve the F1 score on SQuAD v1.1 fine-tuning task by 0.2% (91.51 compared to 91.34). Also, we were able to achieve similar accuracy as baseline in 33% less training steps (a saving of ?1002 V100 GPU-hours!).</p><p>3. WMT'14 and IWSLT machine translation on Transformers: Compared to competitive baselines, we were able to improve the BLEU scores by 0.24 and 0.56 points for the two tasks. Moreover, Knee schedule was able to achieve the same accuracy as baselines in 57% and 30% less training times.</p><p>4. State of the Art (SOTA) Results: We also attain state of the art results on the IWSLT'14(DE-EN) machine translation dataset by simply replacing the learning rate schedule of the current SOTA model <ref type="bibr" target="#b34">(Shen et al., 2020)</ref> with Knee. We were able to improve the BLEU score by 0.18, reaching a new SOTA score of 37.78. Moreover, Knee can achieve the current SOTA baseline value in 30% less training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Detailed Results</head><p>We now describe each of our main experimental results in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">ImageNet Image Classification on Resnet-50</head><p>We train ImageNet dataset <ref type="bibr" target="#b31">(Russakovsky et al., 2015)</ref> on Resnet-50 network <ref type="bibr" target="#b15">(He et al., 2016)</ref> which has 25 million parameters, with a batch size of 256 and a seed learning rate of 0.1. Random cropping and random horizontal flipping augmentations were applied to the training dataset. We use SGD optimizer with momentum of 0.9 and weight decay of 1e ?4 . For baseline runs, we used the standard hand-tuned step learning rate schedule of 0.1, 0.01 and 0.001 for 30 epochs each. For Knee schedule we used a seed learning rate of 0.1 (same as baseline). We trained with the original budget of 90 epochs as well as with a reduced budget of 50 epochs. We used 30 explore epochs for the two experiments. 2 <ref type="table" target="#tab_8">Table 8</ref> shows the training loss and test accuracies for our experiments. Knee schedule comfortably beats the test accuracy of baseline in the full budget run (with absolute gains of 0.8% and 0.4% in top-1 and top-5 accuracy, respectively), while meeting the baseline accuracy even with a much shorter budget. The fact that the baseline schedule takes almost 80% more training time than Knee schedule for the same test accuracy, shows the effectiveness of our Explore-Exploit scheme. See <ref type="figure">Figure 5</ref> in Appendix B for training curves. We train Cifar-10 dataset <ref type="bibr" target="#b23">(Krizhevsky et al., 2009</ref>) on Resnet-18 network <ref type="bibr" target="#b15">(He et al., 2016)</ref> which has around 11 million parameters. SGD optimizer is used with momentum of 0.9 and weight decay of 5e ?4 . Random cropping and random horizontal flipping augmentations were applied to the training dataset. 3 . For baseline, we used the hand-tuned step learning rate schedule of 0.1, 0.01 and 0.001 for 100, 50, 50 epochs, respectively. With Knee schedule, we train the network with the original budget of 200 epochs, as well as a reduced budget of 150 epochs. We used 100 explore epochs for both runs, and a seed learning rate of 0.1 (same as baseline). <ref type="table" target="#tab_9">Table 9</ref> shows the training loss and test accuracies for the experiments. Knee schedule beats the test accuracy of baseline in the full budget run, while meeting the baseline test accuracy in 25% less budget. Refer to figure 6 in Appendix B for detailed comparisons of training loss, test accuracy, and learning rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">BERT LARGE Pre-training</head><p>We pretrain on BERT LARGE on Wikipedia+BooksCorpus dataset with LAMB optimizer <ref type="bibr" target="#b43">(You et al. (2019)</ref>). BERT LARGE has around 330 million parameters and the pre-training is divided into two phases with different sequence lengths. The first phase consists of 90% steps with sequence length of 128 and the second phase consists of the remaining 10% steps with sequence length of 512 <ref type="bibr" target="#b7">(Devlin et al. (2018)</ref>). We used a batch size of 16384 in both phases of training 4 . We use the same training budget of 31250 steps mentioned in <ref type="bibr" target="#b43">(You et al. (2019)</ref>). We also train the model on a shortened training budget of 2/3 rd the original steps (20854 steps). Since large batch training requires learning rate warmup (see <ref type="bibr" target="#b13">Goyal et al. (2017)</ref>), we incorporate it into the Knee schedule by first doing a warmup of 10% as suggested in <ref type="bibr" target="#b43">(You et al., 2019)</ref> followed by the explore-exploit phases. We used an explore of 50% of the total steps available for both phases of BERT training. For baseline, we use the warmup (10%) + linear decay (90%) schedule <ref type="bibr" target="#b43">(You et al., 2019;</ref><ref type="bibr" target="#b7">Devlin et al., 2018)</ref>. The pre-trained models are evaluated on the SQuAD v1.1 <ref type="bibr" target="#b30">(Rajpurkar et al., 2016)</ref> dataset by fine-tuning on the dataset for 2 epochs. See <ref type="table" target="#tab_0">Table 10</ref> for the results. For the full budget run, Knee schedule improves the baseline by 0.2%, while for the reduced budget we achieved similar fine-tuning accuracy as baseline. The baseline schedule achieves a much lower accuracy with shorter budget training, showing the efficacy of Knee schedule. BERT pre-training is extremely compute expensive and takes around 47 hours on 64 V100 GPUs (3008 V100 GPU-hrs) on cloud VMs. The reduced budget amounts to a saving of approximately 1002 V100 GPU-hours!  <ref type="bibr" target="#b26">(Liu et al., 2019)</ref>. With Knee schedule, we trained with the original budget of 70 epochs, as well as a reduced budget of 30 epochs. We used 50 and 25 explore epochs for the two runs, respectively and a seed learning rate of 3e ?4 for both Knee schedule and baseline. In all cases we use the model checkpoint with least loss on the validation set for computing BLEU scores on the test set. <ref type="table" target="#tab_0">Table 11</ref> shows the training loss and test accuracy averaged over 3 runs. Knee schedule improves the test BLEU score of baseline in the full budget run by 0.24 points. In the shorter budget run, Knee schedule matches the test accuracy of the baseline while taking 57% less training time (a saving of 80 V100 GPU-hours!). See <ref type="figure">Figure 8</ref> in Appendix B for training curves.</p><p>IWSLT'14 (DE-EN): For IWSLT'14 (DE-EN) we use the same configuration as WMT'14 (EN-DE), except for a dropout of 0.3 following Fairseq's out-of-box implementation. Each training batch contains approximately 4000 tokens. For Knee schedule, we choose explore as 30 epochs for short budget runs and 40 epochs for full budget runs.</p><p>The baseline schedule uses a linear decay for 50 epochs <ref type="bibr" target="#b26">(Liu et al., 2019)</ref>. With Knee schedule, we trained with the original budget of 50 epochs, as well as a reduced budget of 35 epochs. We used 40 and 30 explore epochs for the two runs, respectively and a seed learning rate of 3e ?4 for both Knee schedule and baseline. In all cases we use the model checkpoint with least loss on the validation set for computing BLEU scores on the test set. Knee schedule improves the baseline test BLEU score by 0.56 points in the full budget run. In the shorter budget run, Knee schedule matches the test accuracy of the baseline schedule while taking 30% less training time. See <ref type="figure">Figure 9</ref> in Appendix B for training curves.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">https://github.com/pytorch/fairseq</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">SQuAD-v1.1 fine-tuning on BERT BASE</head><p>We also evaluate Knee schedule on the task of fine-tuning BERT BASE model <ref type="bibr" target="#b7">Devlin et al. (2018)</ref> on SQuAD v1.1 <ref type="bibr" target="#b30">Rajpurkar et al. (2016)</ref> with the Adam Kingma and Ba (2014b) optimizer 6 . BERT fine-tuning is prone to overfitting because of the huge model size compared to the small fine-tuning dataset, and is typically run for only a few epochs. For baseline we use the linear decay schedule mentioned in <ref type="bibr" target="#b7">Devlin et al. (2018)</ref>. We use a seed learning rate of 3e ?5 and train for 2 epochs. For Knee schedule, we train the network with 1 explore epoch with the same seed learning rate of 3e ?5 . <ref type="table" target="#tab_0">Table 13</ref> shows our results over 3 runs. We achieve a mean EM score of 81.4, compared to baseline's 80.9, a 0.5% absolute improvement. We don't do a short budget run for this example, as the full budget is just 2 epochs. Please refer to <ref type="figure" target="#fig_1">Figure 12</ref> in Appendix B for the training loss, test accuracy and learning rate curves. To further demonstrate the effectiveness of Knee schedule, we took a recent high performing model, Cutoff <ref type="bibr">(Shen et al., 2020) 7</ref> , which had reported state-of-the-art accuracy on the IWSLT'14 (DE-EN) dataset. They reported a BLEU score of 37.6 when trained with an inverse square root learning rate schedule for 100 epochs, with the first 6000 steps allocated for warmup. We simply retrained the model with our Knee schedule, and achieved a new SOTA BLEU score of 37.78 (an absolute increase of 0.18). See <ref type="table" target="#tab_0">Table 14</ref> for the BLEU scores, training and validation perplexities. We also show that Knee schedule can train the model in 30% less training time (70 epochs), while achieving slightly better accuracy of 37.66 BLUE score compared to the 100 epoch baseline. The baseline schedule when run for 70 epochs achieves a much worse accuracy of 37.31.</p><p>For both the full budget (100 epochs) and the short budget (70 epochs) Knee runs, we choose 50% of the total training epochs as explore epochs. We also perform warmup for the same number of steps as baseline. For all runs (Knee and baseline), we report the BLEU score obtained by averaging the last 5 checkpoints and computing on the test set. See <ref type="figure" target="#fig_1">Figure 10</ref> and 11 in Appendix B for training curves. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Hypothesis Validation with Knee schedule on Language Tasks</head><p>For validating our hypothesis on the density of wide minima vs narrow minima, we did multiple experiments on vision tasks, most of which were discussed in Section 3. To summarize, in <ref type="figure" target="#fig_2">Figures 2 and 3</ref>, we showed that for Cifar-10 on Resnet-18, as the number of explore steps increase, the distribution of minima width and test accuracy sharpens and shifts towards wider minima and better accuracy, respectively. We now perform similar experiments on the IWSLT'14 German to English dataset (Cettolo et al., 2014) trained on Transformer networks <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> to demonstrate that our hypothesis holds even on a completely different NLP dataset and network architecture. We train with the Knee schedule for a total budget of 50 epochs with explore lr as 3e ?4 , but keep varying the number of explore epochs. As shown in <ref type="table" target="#tab_0">Table 15</ref>, the test BLEU score increases as we increase the number of explore epochs. Further, we found that among multiple trials, a 20 epoch explore run had a high BLEU score of 35.29, suggesting 7. We used the code available at https://github.com/dinghanshen/Cutoff that the run got lucky. Thus, these results on the IWSLT'14 (DE-EN) dataset add more evidence to the wide-minima density hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Learning Rate Sensitivity for Knee schedule</head><p>We performed sensitivity analysis of the starting learning rate, referred to as the seed learning rate, for Knee schedule. We trained the Cifar-10 dataset on Resnet-18 with the Knee schedule for a shortened budget of 150 epochs, starting at different seed learning rates. For each experiment, we do a simple linear search to find the best explore duration. The test accuracies and optimal explore duration for the different seed learning rate choices is shown in <ref type="table" target="#tab_0">Table 16</ref>. As shown, the seed learning rate can impact the final accuracy, but Knee schedule is not highly sensitive to it. In fact, we can achieve the target accuracy of 95.1 with multiple seed learning rates of 0.05, 0.075, 0.0875 and 0.115, as compared to the original seed learning rate of 0.1, by tuning the number of explore epochs. Another interesting observation is that the optimal explore duration varies inversely with the seed learning rate. Since a bigger learning rate has higher probability of escaping narrow minima compared to a lower learning rate, it would, on an average, require fewer steps to land in a wide minima. Thus, larger learning rates can explore faster, and spend more time in the exploit phase to go deeper in the wide minimum. This observation is thus consistent with our hypothesis and further corroborates it.</p><p>We also note that by tuning both seed learning rate and explore duration, we can achieve the twin objectives of achieving a higher accuracy, as well as a shorter training time -e.g. here we are able to achieve an accuracy of 95.34 in 150 epochs (seed learning rate 0.075), compared to 95.1 achieved by the baseline schedule in 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we make an observation that an initial explore phase with a high learning rate is essential for good generalization of DNNs. Further, we find that a minimum explore duration is required even if the training loss stops improving much earlier. We explain this observation via our hypothesis that in the DNN loss landscape, the density of wide minima is significantly lower than that of narrow minima. Motivated by this hypothesis, we present </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Comparisons with Other Baseline Learning Rate Schedules</head><p>In this section we compare Knee schedule against several other learning rate schedulesone-cycle, linear decay and cosine decay. One-Cycle: The one-cycle learning rate schedule was proposed in Smith (2018) (also see Smith <ref type="formula" target="#formula_0">(2017)</ref>). This schedule first chooses a maximum learning rate based on an learning rate range test. The learning rate range test starts from a small learning rate and keeps increasing the learning rate until the loss starts exploding (see <ref type="figure">figure 4)</ref>. <ref type="bibr" target="#b36">Smith (2018)</ref> suggests that the maximum learning rate should be chosen to be bit before the minima, in a region where the loss is still decreasing. There is some subjectivity in making this choice, although some blogs and libraries 8 suggest using a learning rate one order lower than the one at minima. We go with this choice for all our runs.</p><p>Once the maximum learning rate is chosen, the one-cycle schedule proceeds as follows. The learning rate starts at a specified fraction 9 of the maximum learning rate and is increased linearly to the maximum learning rate for 45 percent of the training budget and then decreased linearly for the remaining 45. For the final 10 percent, the learning rate is reduced by a large factor (we chose a factor of 10). We used an opensource implementation 10 for our experiments.</p><p>Linear Decay: The linear decay learning rate schedule simply decays the learning rate linearly to zero starting from a seed learning rate.</p><p>Cosine Decay: The cosine decay learning rate schedule decays the learning rate to zero following a cosine curve, starting from a seed learning rate.</p><p>A.1 Cifar-10 <ref type="figure">Figure 4a</ref> shows the learning rate range test for Cifar-10 with the Resnet-18 network. The minima occurs around learning rate of 0.09, and we choose 9e ?3 as the maximum learning rate for the One-Cycle runs. For linear, cosine decay schedules we start with a seed learning rate of 0.1 as used in the standard baselines. The training loss and test accuracy for the various schedules are shown in <ref type="table" target="#tab_0">Table 17</ref> for the full budget runs (200 epochs), and in <ref type="table" target="#tab_0">Table 18</ref> for the short budget runs (150 epochs). <ref type="figure">Figure 4d</ref> shows the learning rate range test for ImageNet with the Resnet-50 network. The minima occurs around learning rate of 2.16, and we choose 0.216 as the maximum learning rate for One-Cycle runs. For linear, cosine decay schedules we start with a seed learning rate of 0.1 as used in the standard baselines. The training loss and test accuracy for the various schedules are shown in <ref type="table" target="#tab_0">Table 19</ref> for the full budget runs (90 epochs), and in <ref type="table" target="#tab_21">Table 20</ref> for the short budget runs (50 epochs).  <ref type="figure">Figure 4</ref>: learning rate range test for selecting the maximum learning rate. A good choice is the learning rate is a bit before the minima in a region where the loss is still decreasing.     <ref type="table" target="#tab_0">Table 21</ref> for the full budget runs (70 epochs), and in <ref type="table" target="#tab_23">Table 22</ref> for the short budget runs (30 epochs).  A.4 IWSLT'14 DE-EN <ref type="figure">Figure 4b</ref> shows the learning rate range test for IWSLT on the transformer networks. The minima occurs near 2.5e ?3 . For the maximum learning rate, we choose 2.5e ?4 for the default one-cycle policy. For linear, cosine decay schedules we start with a seed learning rate of 3e ?4 as used in the standard baselines The training, validation perplexity and BLEU scores for the various schedules are shown in <ref type="table" target="#tab_3">Table 23</ref> for the full budget runs (50 epochs), and in <ref type="table" target="#tab_4">Table 24</ref> for the short budget runs (35 epochs).  We choose 1e ?5 as the maximum learning rate for One-Cycle runs as the minima occurs close to 1e ?4 . For linear, cosine decays we start with a seed learning rate of 3e ?5 as used in standard baselines. <ref type="table" target="#tab_5">Table 25</ref> show the average training loss, average test EM and F1 scores for the various schedules. We did not do a short budget training for this dataset, as the full budget is just 2 epochs.  Learning Rate vs Epochs <ref type="figure">Figure 7</ref>: BERT LARGE pretraining for batch size of 16k with LAMB optimizer for the short budget runs. Shown are the training loss and learning rate as a function of steps, for the baseline scheme short budget (orange) vs the Knee schedule scheme short budget (blue). The plot is split into 2 parts to give a clear picture of the two phases of training <ref type="bibr" target="#b7">Devlin et al. (2018)</ref>. Note that even though the training loss curves look similar for the two runs, we see a significant gap in F1 score obtained when we fine-tune the model checkpoints on SQuAD-v1.1 <ref type="bibr" target="#b30">Rajpurkar et al. (2016)</ref>. See       <ref type="figure" target="#fig_1">Figure 12</ref>: SQuAD-v1.1 fine-tuning on BERT BASE trained with Adam. Shown are the training loss, test EM score, and learning rate as a function of epochs, for the baseline scheme (orange) vs the Knee schedule scheme (blue). The plot is split into 2 parts to permit higher fidelity in the y-axis range. It is clear that with Knee schedule the network starts to overfit after the 2nd epoch, where the testing loss continues to go down, but generalization suffers. We saw similar behavior with different seeds, and thus need to train with Knee schedule for only 2 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ImageNet</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Training loss for Cifar-10 on Resnet-18. Orange plot uses a fixed learning rate of 0.1, while in blue plot, the learning rate is reduced from 0.1 to 0.01 at epoch 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Histogram of minima sharpness<ref type="bibr" target="#b20">(Keskar et al., 2016)</ref> for 50 random trials of Cifar-10 on Resnet-18. Each figure shows histograms for runs with different number of explore epochs. The distribution moves toward lower sharpness and tightens as the number of explore epochs increase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Histogram of test accuracy for 50 random trials of Cifar-10 on Resnet-18. Each figure shows histograms for runs with different number of explore epochs. The distribution moves toward higher test accuracy and sharpens as the number of explore epochs increase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Keskar et al. (2016) mentions that under certain conditions and for small values of , S x,F is proportional to the largest eigenvalue of the Hessian. Please see Keskar et al. (2016) for more details. For our measurements we choose an of 1e ?4 . For solving the maximization problem in Equation 1, Keskar et al. (2016) uses a secondorder L-BFGS-B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Cifar-10 on Resnet-18 trained with Momentum. Shown are the training loss, test accuracy and learning rate as a function of epochs, for the baseline scheme (orange) vs the Knee schedule scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>WMT'14 (EN-DE) on Transformer BASE network trained with RAdam. Shown are the training perplexity, validation perplexity and learning rate as a function of epochs, for the baseline scheme (orange) vs the Knee schedule scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range. IWSLT'14 (DE-EN) on Transformer BASE network trained with RAdam. Shown are the training perplexity, validation perplexity and learning rate as a function of epochs, for the baseline scheme (orange) vs the Knee schedule scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>IWSLT'14 (DE-EN) on the SOTA model Cutoff<ref type="bibr" target="#b34">(Shen et al., 2020)</ref>, trained with Adam. Shown are the training perplexity, validation perplexity and learning rate as a function of epochs, for the baseline scheme (orange) vs the Knee schedule scheme (blue). IWSLT'14 (DE-EN) on the SOTA model Cutoff<ref type="bibr" target="#b34">(Shen et al., 2020)</ref>, trained with Adam with a reduced training budget of 70 epochs. Shown are the training perplexity, validation perplexity and learning rate as a function of epochs, for the baseline scheme (orange) vs the Knee schedule scheme (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">: Cifar-10 on Resnet-18 trained for 200</cell></row><row><cell cols="3">epochs with Momentum. A learning rate of</cell></row><row><cell cols="3">0.1 is used for the explore epochs. Half the</cell></row><row><cell cols="3">remaining epochs are trained at 0.01 and the</cell></row><row><cell cols="3">other half at 0.001. Reported results are av-</cell></row><row><cell cols="2">erage over 4 runs.</cell><cell></cell></row><row><cell>Epochs at</cell><cell>Test Accuracy</cell><cell>Train Loss</cell></row><row><cell>0.1 LR</cell><cell cols="2">Avg. (Std. Dev) Avg. (Std. Dev.)</cell></row><row><cell>0</cell><cell>94.34 (0.13)</cell><cell>0.0017 (8e-5)</cell></row><row><cell>30</cell><cell>94.81 (0.15)</cell><cell>0.0017 (8e-5)</cell></row><row><cell>40</cell><cell>94.91 (0.14)</cell><cell>0.0018 (9e-5)</cell></row><row><cell>60</cell><cell>95.01 (0.14)</cell><cell>0.0018 (1e-4)</cell></row><row><cell>80</cell><cell>95.05 (0.15)</cell><cell>0.0019 (1e-4)</cell></row><row><cell>100</cell><cell>95.10 (0.14)</cell><cell>0.0021 (1e-4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Keskar's sharpness metric for Cifar-10 on Resnet-18 trained for 200 epochs with Momentum. A learning rate of 0.1 is used for the explore epochs. Half the remaining epochs are trained at 0.01 and the other half at 0.001. We report the average sharpness over 50 different trials.</figDesc><table><row><cell cols="2">Explore Epochs Sharpness</cell></row><row><cell>0</cell><cell>10.56</cell></row><row><cell>30</cell><cell>5.43</cell></row><row><cell>60</cell><cell>3.86</cell></row><row><cell>100</cell><cell>3.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Fisher Score for Cifar-10 on Resnet-18 trained for 200 epochs with Momentum. A learning rate of 0.1 is used for the explore epochs. Half the remaining epochs are trained at 0.01 and the other half at 0.001. We report the average Fisher score over 10 different trials.</figDesc><table><row><cell cols="2">Explore Epochs FIM score</cell></row><row><cell>0</cell><cell>0.051</cell></row><row><cell>30</cell><cell>0.046</cell></row><row><cell>60</cell><cell>0.043</cell></row><row><cell>100</cell><cell>0.042</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>We report the top-1 accuracy for ImageNet and Cifar-10, BLEU score for IWSLT'14 and WMT'14 and F1 score for BERT on SQuAD. All values are averaged over multiple runs for each experiment. Experiment details are mentioned in the individual sections of the experiments.</figDesc><table><row><cell>Knee</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell>IWSLT</cell><cell>35.08</cell><cell>35</cell><cell>45</cell><cell>60</cell></row><row><cell>WMT'14</cell><cell>27.28</cell><cell>30</cell><cell>60</cell><cell>70</cell></row></table><note>Epochs required by different LR schedules to reach the target accuracy. The target accuracy is chosen based on Knee schedule's results with a reduced budget.Experiment Target BLEU Score Knee schedule Cosine Decay Linear Decay</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>ImageNet on Resnet-50 results. We report mean (stddev) over 3 runs.</figDesc><table><row><cell>LR Schedule</cell><cell cols="4">Test Top 1 Acc. Test Top 5 Acc. Training Loss Training Epochs</cell></row><row><cell>Baseline</cell><cell>75.87 (0.035)</cell><cell>92.90 (0.015)</cell><cell>0.74 (1e-3)</cell><cell>90</cell></row><row><cell>Knee</cell><cell>76.71 (0.097)</cell><cell>93.32 (0.031)</cell><cell>0.79 (1e-3)</cell><cell>90</cell></row><row><cell>Knee (short budget)</cell><cell>75.92 (0.11)</cell><cell>92.90 (0.085)</cell><cell>0.90 (3e-3)</cell><cell>50</cell></row><row><cell cols="3">5.3.2 Cifar-10 Image Classification on Resnet-18</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Training loss and Test accuracy for Cifar-10 on Resnet-18.</figDesc><table><row><cell>We report mean</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>BERT LARGE results. We report the pre-training train loss, and the test F1 accuracy on SQuAD v1.1 after fine-tuning. Seefigure 7in Appendix B for training curves.</figDesc><table><row><cell>LR Schedule</cell><cell cols="3">F1 score on SQuAD v1.1 Training loss Total Training Steps</cell></row><row><cell>Knee</cell><cell>91.51</cell><cell>1.248</cell><cell>31250</cell></row><row><cell>Baseline (You et al., 2019)</cell><cell>91.34</cell><cell>-</cell><cell>31250</cell></row><row><cell>Baseline (short budget)</cell><cell>90.64</cell><cell>1.336</cell><cell>20854</cell></row><row><cell>Knee (short budget)</cell><cell>91.29</cell><cell>1.275</cell><cell>20854</cell></row><row><cell cols="4">5.3.4 Machine Translation on Transformer Network with WMT'14 and</cell></row><row><cell>IWSLT</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">In the second NLP task, we train the Transformer (base model) (Vaswani et al., 2017) on</cell></row><row><cell cols="4">the IWSLT'14 (De-En) (Cettolo et al., 2014) and WMT'14 (En-De) (Bojar et al., 2014)</cell></row><row><cell cols="2">datasets with the RAdam (Liu et al., 2019) optimizer.</cell><cell></cell><cell></cell></row></table><note>WMT'14 (EN-DE): We use the default implementation provided by the fairseq package (Ott et al., 2019) 5 . We train WMT'14 (EN-DE) dataset on the Transformer BASE (Vaswani et al., 2017) model which has around 86 million parameters and use the RAdam (Liu et al., 2019) optimizer with ? 1 of 0.9 and ? 2 of 0.999. Label smoothed cross entropy was used as the objective function with an uncertainty of 0.1. A dropout of 0.1, clipping norm of 25 and weight decay of 1e ?4 is used. Each training batch contains approximately 30000 tokens. The baseline schedule uses a linear decay for 70 epochs</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Results for WMT'14 (EN-DE) on Transformer networks. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report mean (stdev) over 3 runs.</figDesc><table><row><cell>LR Schedule</cell><cell>Test BLEU Score</cell><cell>Train Perplexity</cell><cell>Validation Perplexity</cell><cell>Training Epochs</cell></row><row><cell>Baseline</cell><cell cols="3">27.29 (0.06) 3.87 (0.017) 4.89 (0.02)</cell><cell>70</cell></row><row><cell>Knee</cell><cell cols="3">27.53 (0.12) 3.89 (0.017) 4.87 (0.006)</cell><cell>70</cell></row><row><cell cols="4">Knee (short budget) 27.28 (0.17) 4.31 (0.02) 4.92 (0.007)</cell><cell>30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Training, validation perplexity and test BLEU scores for IWSLT on Transformer networks. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report the mean and standard deviation over 3 runs.</figDesc><table><row><cell>LR Schedule</cell><cell>Test BLEU Score</cell><cell>Train Perplexity</cell><cell>Validation Perplexity</cell><cell>Training Epochs</cell></row><row><cell>Baseline</cell><cell cols="3">34.97 (0.035) 3.36 (0.001) 4.91 (0.035)</cell><cell>50</cell></row><row><cell>Knee</cell><cell cols="3">35.53 (0.06) 3.00 (0.044) 4.86 (0.02)</cell><cell>50</cell></row><row><cell cols="4">Knee (short budget) 35.08 (0.12) 3.58 (0.049) 4.90 (0.063)</cell><cell>35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>SQuAD fine-tuning on BERT BASE . We report the average training loss, and average test EM, F1 scores over 3 runs.</figDesc><table><row><cell>LR Schedule</cell><cell>EM</cell><cell>F1</cell><cell>Train Loss</cell><cell>Training Epochs</cell></row><row><cell>Baseline</cell><cell cols="3">80.89 (0.15) 88.38 (0.032) 1.0003 (0.004)</cell><cell>2</cell></row><row><cell cols="4">Knee schedule 81.38 (0.02) 88.66 (0.045) 1.003 (0.002)</cell><cell>2</cell></row><row><cell cols="5">6. We used the implementation at: https://github.com/huggingface/transformers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Training, validation perplexity and test BLEU scores for IWSLT'14 DE-EN on Cutoff. The test BLEU scores are computed by averaging the last 5 checkpoints</figDesc><table><row><cell>LR Schedule</cell><cell>Test BLEU Score</cell><cell cols="3">Train Perplexity Perplexity Epochs Validation Training</cell></row><row><cell>Inv. Sqrt</cell><cell>37.60</cell><cell>3.46</cell><cell>4.24</cell><cell>100</cell></row><row><cell>Knee</cell><cell>37.78</cell><cell>3.29</cell><cell>4.13</cell><cell>100</cell></row><row><cell>Inv. Sqrt (short budget)</cell><cell>37.31</cell><cell>3.76</cell><cell>4.29</cell><cell>70</cell></row><row><cell>Knee (short budget)</cell><cell>37.66</cell><cell>3.48</cell><cell>4.18</cell><cell>70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>IWSLT'14 (DE-EN) on the Transformer network trained with the Knee schedule. The explore duration is varied, while keeping the total training budget fixed at 50 epochs. We report averages over 3 runs.</figDesc><table><row><cell cols="3">Explore Epochs Test BLEU score Training Perplexity</cell></row><row><cell>5</cell><cell>34.93</cell><cell>3.29</cell></row><row><cell>10</cell><cell>35.02</cell><cell>3.22</cell></row><row><cell>15</cell><cell>35.08</cell><cell>3.11</cell></row><row><cell>20</cell><cell>35.10</cell><cell>3.08</cell></row><row><cell>25</cell><cell>35.23</cell><cell>3.02</cell></row><row><cell>30</cell><cell>35.28</cell><cell>2.99</cell></row><row><cell>40</cell><cell>35.53</cell><cell>3.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>Seed learning rate sensitivity analysis. Cifar-10 on Resnet-18 trained for 150 epochs with Knee schedule. We vary the seed learning rate and explore epochs to get the best test accuracy for the particular setting. We report averages over 3 runs. Exploit based learning rate schedule, called the Knee schedule. We do extensive evaluation of Knee schedule on multiple models and datasets. In all experiments, the Knee schedule outperforms prior hand-tuned baselines, including achieving SOTA test accuracies, when trained with the original training budget, and achieves the same test accuracy as the baseline when trained with a much shorter budget.</figDesc><table><row><cell cols="3">Seed LR Test Accuracy Optimal Explore Epochs</cell></row><row><cell>0.03</cell><cell>95.07</cell><cell>120</cell></row><row><cell>0.05</cell><cell>95.12</cell><cell>120</cell></row><row><cell>0.0625</cell><cell>95.15</cell><cell>120</cell></row><row><cell>0.075</cell><cell>95.34</cell><cell>100</cell></row><row><cell>0.0875</cell><cell>95.22</cell><cell>100</cell></row><row><cell>0.1</cell><cell>95.14</cell><cell>100</cell></row><row><cell>0.115</cell><cell>95.20</cell><cell>60</cell></row><row><cell>0.125</cell><cell>95.06</cell><cell>60</cell></row><row><cell>0.15</cell><cell>95.04</cell><cell>30</cell></row><row><cell>an Explore-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>8. See e.g. https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6 and https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html. Also see https: //docs.fast.ai/callbacks.lr_finder.html and https://docs.fast.ai/callbacks.one_cycle.html 9. See div factor in https://docs.fast.ai/callbacks.one_cycle.html.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">13.5</cell></row><row><cell></cell><cell cols="2">2.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">13.0</cell></row><row><cell></cell><cell cols="2">2.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">12.0 12.5</cell></row><row><cell>loss</cell><cell cols="2">2.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Losses</cell><cell cols="2">11.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">11.0</cell></row><row><cell></cell><cell cols="2">2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">10.5</cell></row><row><cell></cell><cell cols="2">1.9</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2 learning_rate</cell><cell>0.3</cell><cell>0.4</cell><cell cols="2">9.5 10.0</cell><cell>Learning Rate 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">(a) LR range test for CIFAR-10</cell><cell></cell><cell cols="2">(b) LR range test for IWSLT'14 DE-EN</cell></row><row><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7.2</cell></row><row><cell></cell><cell></cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7.0</cell></row><row><cell></cell><cell></cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.8</cell></row><row><cell cols="2">Losses</cell><cell>12 13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">loss</cell><cell>6.6 6.4</cell></row><row><cell></cell><cell></cell><cell>11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.2</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.0</cell></row><row><cell></cell><cell></cell><cell>9</cell><cell cols="5">0.000000.000250.000500.000750.001000.001250.001500.001750.00200 Learning Rate</cell><cell></cell><cell>5.8</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15 learning_rate 20</cell><cell>25</cell><cell>30</cell><cell>35</cell></row><row><cell></cell><cell></cell><cell cols="6">(c) LR range test for WMT'14 EN-DE</cell><cell></cell><cell></cell><cell>(d) LR range test for ImageNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>We chose the fraction to be 0.1</cell></row><row><cell cols="5">in our experiments.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">10. https://github.com/nachiket273/One_Cycle_Policy</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 17 :</head><label>17</label><figDesc>Cifar-10 on Resnet-18 full budget training (200 epochs): Training loss and Test accuracy for more learning rate schedules. We report the mean and standard deviation over 7 runs.Figure 4cshows the learning rate range test for WMT'14 EN-DE on the transformer networks. The minima occurs near 1.25e ?3 . For the maximum learning rate, we choose 2.5e ?4 for the default one-cycle policy. For linear, cosine decay schedules we start with a seed</figDesc><table><row><cell>LR Schedule</cell><cell>Test Accuracy</cell><cell>Train Loss</cell></row><row><cell>One-Cycle</cell><cell>94.08 (0.07)</cell><cell>0.0041 (6e-5)</cell></row><row><cell>Cosine Decay</cell><cell>95.23 (0.11)</cell><cell>0.0023 (9e-5)</cell></row><row><cell>Linear Decay</cell><cell>95.18 (0.15)</cell><cell>0.0018 (7e-5)</cell></row><row><cell>Knee schedule</cell><cell>95.26 (0.11)</cell><cell>0.0023 (1e-4)</cell></row><row><cell>A.3 WMT'14 EN-DE</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 18 :</head><label>18</label><figDesc>Cifar-10 on Resnet-18 short budget training (150 epochs): Training loss and Test accuracy for more learning rate schedules. We report the mean and standard deviation over 7 runs.</figDesc><table><row><cell>LR Schedule</cell><cell>Test Accuracy</cell><cell>Train Loss</cell></row><row><cell>One-Cycle</cell><cell cols="2">93.84 (0.082) 0.0052 (7e-5)</cell></row><row><cell>Cosine Decay</cell><cell>95.06 (0.16)</cell><cell>0.0030 (2e-4)</cell></row><row><cell>Linear Decay</cell><cell>95.02 (0.10)</cell><cell>0.0021 (1e-4)</cell></row><row><cell>Knee schedule</cell><cell>95.14 (0.18)</cell><cell>0.0044 (3e-4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 19</head><label>19</label><figDesc></figDesc><table><row><cell cols="4">: ImageNet with ResNet-50 full budget training (90 epochs): Training loss, Test</cell></row><row><cell cols="4">Top-1 and Test Top-5 for more learning rate schedules. We report the mean and standard</cell></row><row><cell>deviation over 3 runs.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR Schedule</cell><cell>Test Top-1</cell><cell>Test Top-5</cell><cell>Train Loss (av)</cell></row><row><cell>One Cycle</cell><cell>75.39 (0.137)</cell><cell>92.56 (0.040)</cell><cell>0.96 (0.003)</cell></row><row><cell>Cosine Decay</cell><cell>76.41 (0.212)</cell><cell>93.28 (0.066)</cell><cell>0.80 (0.002)</cell></row><row><cell>Linear decay</cell><cell>76.54 (0.155)</cell><cell>93.21 (0.051)</cell><cell>0.75 (0.001)</cell></row><row><cell cols="3">Knee schedule 76.71 (0.097) 93.32 (0.031)</cell><cell>0.79 (0.001)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 20</head><label>20</label><figDesc>as used in the standard baselines The training, validation perplexity and BLEU scores for the various schedules are shown in</figDesc><table><row><cell cols="4">: ImageNet with ResNet-50 short budget training (50 epochs): Training loss, Test</cell></row><row><cell cols="4">Top-1 and Test Top-5 for more learning rate schedules. We report the mean and standard</cell></row><row><cell>deviation over 3 runs.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR Schedule</cell><cell>Test Top-1</cell><cell>Test Top-5</cell><cell>Train Loss (av)</cell></row><row><cell>One Cycle</cell><cell cols="2">75.36 (0.096) 92.53 (0.079)</cell><cell>1.033 (0.004)</cell></row><row><cell cols="3">Cosine Decay 75.71 (0.116) 92.81 (0.033)</cell><cell>0.96 (0.002)</cell></row><row><cell>Linear decay</cell><cell cols="2">75.82 (0.080) 92.84 (0.036)</cell><cell>0.91 (0.002)</cell></row><row><cell cols="3">Knee schedule 75.92 (0.11) 92.90 (0.085)</cell><cell>0.90 (0.003)</cell></row><row><cell>learning rate of 3e ?4</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 21 :</head><label>21</label><figDesc> on Transformer networks full budget training (70 epochs): Training, validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report the mean and standard deviation over 3 runs.</figDesc><table><row><cell>LR Schedule</cell><cell>Test BLEU Score</cell><cell>Train ppl</cell><cell>Validation ppl</cell></row><row><cell>One-Cycle</cell><cell>27.19 (0.081)</cell><cell>3.96 (0.014)</cell><cell>4.95 (0.013)</cell></row><row><cell>Cosine Decay</cell><cell>27.35 (0.09)</cell><cell>3.87 (0.011)</cell><cell>4.91 (0.008)</cell></row><row><cell>Linear Decay</cell><cell>27.29 (0.06)</cell><cell>3.87 (0.017)</cell><cell>4.89 (0.02)</cell></row><row><cell>Knee schedule</cell><cell>27.53 (0.12)</cell><cell>3.89 (0.017)</cell><cell>4.87 (0.006)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 22</head><label>22</label><figDesc></figDesc><table><row><cell cols="4">: WMT'14 (EN-DE) on Transformer networks short budget training (30 epochs):</cell></row><row><cell cols="4">Training, validation perplexity and test BLEU scores for more learning rate schedules. The</cell></row><row><cell cols="4">test BLEU scores are computed on the checkpoint with the best validation perplexity. We</cell></row><row><cell cols="3">report the mean and standard deviation over 3 runs.</cell><cell></cell></row><row><cell>LR Schedule</cell><cell>Test BLEU Score</cell><cell>Train ppl</cell><cell>Validation ppl</cell></row><row><cell>One-Cycle</cell><cell>26.80 (0.2)</cell><cell>4.38 (0.017)</cell><cell>5.02 (0.007)</cell></row><row><cell>Cosine Decay</cell><cell>26.95 (0.23)</cell><cell>4.32 (0.013)</cell><cell>4.99 (0.011)</cell></row><row><cell>Linear Decay</cell><cell>26.77 (0.12)</cell><cell>4.36 (0.092)</cell><cell>5.02 (0.01)</cell></row><row><cell>Knee schedule</cell><cell>27.28 (0.17)</cell><cell>4.31 (0.02)</cell><cell>4.92 (0.007)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 23 :</head><label>23</label><figDesc>IWSLT'14 (DE-EN) on Transformer networks full budget training (50 epochs): Training, validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report the mean and standard deviation over 3 runs.</figDesc><table><row><cell>LR Schedule</cell><cell>Test BLEU Score</cell><cell>Train ppl</cell><cell>Validation ppl</cell></row><row><cell>One-Cycle</cell><cell>34.77 (0.064)</cell><cell>3.68 (0.009)</cell><cell>4.97 (0.010)</cell></row><row><cell>Cosine Decay</cell><cell>35.21 (0.063)</cell><cell>3.08 (0.004)</cell><cell>4.88 (0.014)</cell></row><row><cell>Linear Decay</cell><cell>34.97 (0.035)</cell><cell>3.36 (0.001)</cell><cell>4.92 (0.035)</cell></row><row><cell>Knee schedule</cell><cell>35.53 (0.06)</cell><cell>3.00 (0.044)</cell><cell>4.86 (0.02)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 24 :</head><label>24</label><figDesc> on Transformer networks short budget training (35 epochs): Training, validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report the mean and standard deviation over 3 runs.</figDesc><table><row><cell>LR Schedule</cell><cell>Test BLEU Score</cell><cell>Train ppl</cell><cell>Validation ppl</cell></row><row><cell>One-Cycle</cell><cell>34.43 (0.26)</cell><cell>3.98 (0.028)</cell><cell>5.09 (0.017)</cell></row><row><cell>Cosine Decay</cell><cell>34.46 (0.33)</cell><cell>3.86 (0.131)</cell><cell>5.06 (0.106)</cell></row><row><cell>Linear Decay</cell><cell>34.16 (0.28)</cell><cell>4.11 (0.092)</cell><cell>5.14 (0.066)</cell></row><row><cell>Knee schedule</cell><cell>35.08 (0.12)</cell><cell>3.58 (0.063)</cell><cell>4.90 (0.049)</cell></row><row><cell cols="3">A.5 SQuAD-v1.1 finetuning with BERT BASE</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 25 :</head><label>25</label><figDesc>.1 fine-tuning on BERT BASE for more learning rate schedules. We report the average training loss, average test EM, F1 scores over 3 runs. ImageNet on Resnet-50 trained with Momentum. Shown are the training loss, top-1/top-5 test accuracy and learning rate as a function of epochs, for the baseline scheme (orange) vs the Knee schedule scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.</figDesc><table><row><cell>LR Schedule</cell><cell>EM (av)</cell><cell>F1 (av)</cell><cell>Train Loss (av)</cell></row><row><cell>One Cycle</cell><cell>79.9 (0.17)</cell><cell>87.8 (0.091)</cell><cell>1.062 (0.003)</cell></row><row><cell>Cosine Decay</cell><cell>81.31 (0.07)</cell><cell>88.61 (0.040)</cell><cell>0.999 (0.003)</cell></row><row><cell>Linear decay</cell><cell>80.89 (0.15)</cell><cell>88.38 (0.042)</cell><cell>1.0003 (0.004)</cell></row><row><cell cols="3">Knee schedule 81.38 (0.02) 88.66 (0.045)</cell><cell>1.003 (0.002)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 26</head><label>26</label><figDesc>for details.</figDesc><table><row><cell>LR Schedule</cell><cell cols="5">F1 -Trial 1 F1 -Trial 2 F1 -Trial 3 F1 avg. F1 max</cell></row><row><cell>Baseline (short budget)</cell><cell>90.39</cell><cell>90.64</cell><cell>90.53</cell><cell>90.52</cell><cell>90.64</cell></row><row><cell>Knee schedule ( short budget )</cell><cell>91.22</cell><cell>91.29</cell><cell>91.18</cell><cell>91.23</cell><cell>91.29</cell></row><row><cell>Knee schedule ( full budget )</cell><cell>91.45</cell><cell>91.41</cell><cell>91.51</cell><cell>91.46</cell><cell>91.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 26 :</head><label>26</label><figDesc>SQuAD fine-tuning on BERT LARGE . We report F1 scores for 3 different trials as well as the maximum and average values.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. We used the opensource implementation at: https://github.com/cybertronai/imagenet18 old</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. We used the open-source implementation at: https://github.com/kuangliu/pytorch-cifar 4. We used the open-source implementation at: https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>We would like to thank Sanjith Athlur for his help in setting up the VM cluster for large training runs and Harshay Shah for helpful discussions on minima width computation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Stronger generalization bounds for deep nets via a compression approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05296</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Shaping the learning landscape in neural networks around wide flat minima. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Pittorino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.07833" />
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shaping the learning landscape in neural networks around wide flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Pittorino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="170" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">and Ale s Tamchyna. Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W14/W14-3302" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richardh</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihuang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciyou</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1137/0916069</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign, iwslt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation<address><addrLine>Hanoi, Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124018</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sharp minima can generalize for deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Essentially no barriers in neural network energy landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kambis</forename><surname>Veschgini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Salmhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1309" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Topology and geometry of half-rectified network optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01540</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10026</idno>
		<title level="m">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13277</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>matter little near convergence</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Towards understanding generalization in gradient-based meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Guiroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07287</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Stanis Law Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04623</idno>
		<title level="m">Three factors influencing minima in sgd</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the relation between the sharpest directions of DNN loss and the SGD step length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Stanis Law Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amost</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storkey</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkgEaj05t7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping Tak Peter</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04836</idno>
		<title level="m">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Limitations of the empirical fisher approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Kunstner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12558</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards explaining the regularization effect of initial large learning rate in training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04595</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">An empirical model of large-batch training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openai Dota</forename><surname>Team</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06162</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Empirical analysis of the hessian of over-parametrized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>V Ugur G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04454</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">iclr 2018 workshop contribution. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03600</idno>
		<title level="m">Measuring the effects of data parallelism on neural network training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat data augmentation approach for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13818</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piotr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Memming</forename><surname>Sokol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03785</idno>
		<title level="m">Information geometry of orthogonal initializations and training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Identifying generalization properties in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07402</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8279" to="8288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeke</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10941</idno>
		<title level="m">Spectral norm regularization for improving the generalizability of deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
