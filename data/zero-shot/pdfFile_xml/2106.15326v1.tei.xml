<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Source-free Domain Adaptation via Avatar Prototype Generation and Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Pazhou Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Niu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxia</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Big Data and Intelligent Robot</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Pazhou Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Source-free Domain Adaptation via Avatar Prototype Generation and Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study a practical domain adaptation task, called source-free unsupervised domain adaptation (UDA) problem, in which we cannot access source domain data due to data privacy issues but only a pre-trained source model and unlabeled target data are available. This task, however, is very difficult due to one key challenge: the lack of source data and target domain labels makes model adaptation very challenging. To address this, we propose to mine the hidden knowledge in the source model and exploit it to generate source avatar prototypes (i.e., representative features for each source class) as well as target pseudo labels for domain alignment. To this end, we propose a Contrastive Prototype Generation and Adaptation (CPGA) method. Specifically, CPGA consists of two stages: (1) prototype generation: by exploring the classification boundary information of the source model, we train a prototype generator to generate avatar prototypes via contrastive learning. (2) prototype adaptation: based on the generated source prototypes and target pseudo labels, we develop a new robust contrastive prototype adaptation strategy to align each pseudo-labeled target data to the corresponding source prototypes. Extensive experiments on three UDA benchmark datasets demonstrate the effectiveness and superiority of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised domain adaptation (UDA) has achieved remarkable success in many applications, such as image classification and semantic segmentation <ref type="bibr">[Yan et al., 2017;</ref><ref type="bibr">Liang et al., 2019;</ref><ref type="bibr">Zhang et al., 2020]</ref>. The goal of UDA is to leverage a label-rich source domain to improve the model performance on an unlabeled target domain, which bypasses the dependence on laborious target data annotation. Generally, UDA methods can be divided into two categories, i.e., data-level UDA and feature-level UDA. Datalevel methods <ref type="bibr">[Sankaranarayanan et al., 2018;</ref><ref type="bibr">Hoffman et al., 2018]</ref> attempt to mitigate domain shifts by image transformation between domains via generative adversarial networks <ref type="bibr">[Goodfellow et al., 2014]</ref>. By contrast, feature-level methods <ref type="bibr">[Ganin and Lempitsky, 2015;</ref><ref type="bibr">Wei et al., 2016]</ref> focus on alleviating domain discrepancies by learning domaininvariant feature representations. In real-world applications, however, one may only access a source trained model instead of source data due to the law of privacy protection. As a result, many existing UDA methods are incapable due to the lack of source data. Therefore, this paper considers a more practical task, called source-free UDA <ref type="bibr" target="#b12">Li et al., 2020]</ref>, which seeks to adapt a well-trained source model to a target domain without using any source data.</p><p>Due to the absence of source data as well as target domain labels, it is difficult to estimate the source domain distribution and exploit target class information for alleviating domain discrepancy as previous UDA methods do. Such a dilemma makes source-free UDA very challenging. To solve this task, existing source-free UDA methods seek to refine the source model either by generating target-style images (e.g., <ref type="bibr" target="#b12">MA [Li et al., 2020]</ref>) or by pseudo-labeling target data (e.g., ). However, directly generating images from the source model can be very difficult and pseudo-labeling may lead to wrong labels due to domain shifts, both of which compromise the training procedure.</p><p>To handle the absence of source data, our motivation is to mine the hidden knowledge in the source model. By exploring the source model, we seek to generate feature prototypes of each source class and target pseudo labels for domain alignment. To this end, we propose a new Contrastive Prototype Generation and Adaptation (CPGA) method. Specifically, CPGA contains two stages: (1) Prototype generation: by exploring the classification boundary information in the source classifier, we train a prototype generator to generate source prototypes based on contrastive learning. (2) Prototype adaptation: to mitigate domain discrepancies, based on the generated feature prototypes and target pseudo labels, we develop a new contrastive prototype adaptation strategy to align each pseudo-labeled target data to the source prototype with the same class. To alleviate label noise, we enhance the alignment via confidence reweighting and early learning regularization. Meanwhile, we further boost the alignment via feature clustering to make the target features more compact.</p><p>In this way, we are able to well adapt the source-trained model to the unlabeled target domain even without any source data.</p><p>The contributions of this paper are summarized as follows:</p><p>? In CPGA, we propose a contrastive prototype generation strategy for source-free UDA. Such a strategy can generate representative (i.e., intra-class compact and interclass separated) avatar feature prototypes for each class. The generated prototypes can be applied to help conventional UDA methods to handle source-free UDA.</p><p>? In CPGA, we also propose a robust contrastive prototype adaptation strategy for source-free UDA. Such a strategy can align each pseudo-labeled target data to the corresponding source prototype and meanwhile alleviate the issue of pseudo label noise.</p><p>? Extensive experiments on three domain adaptation benchmark datasets demonstrate the effectiveness and superiority of the proposed method.  <ref type="bibr">et al., 2020]</ref> leverages contrastive learning to explicitly minimize intra-class distance and maximize inter-class distance in terms of both intra-domain and inter-domain. However, the source data may be unavailable in practice due to privacy issues, making these methods incapable.</p><p>Source-free Unsupervised Domain Adaptation. Sourcefree UDA <ref type="bibr" target="#b20">[Kim et al., 2020]</ref> aims to adapt the source model to an unlabeled target domain without using the source data. Existing methods seek to refine the source model either by pseudo-labeling (e.g., ) or by generating target-style images (e.g., <ref type="bibr" target="#b12">MA [Li et al., 2020]</ref>). However, due to the domain discrepancy, the pseudo labels can be noisy, which is ignored by SHOT. Besides, directly generating target-style images from the source model can be very difficult due to training difficulties of GANs. Very recently,  proposes to use the source classifier as source anchors and use them for domain alignment. However, BAIT requires dividing target data into certain and uncertain sets via prediction entropy of source classifier, which may lead to wrong division due to domain shifts. Compared with the above methods, we propose to generate source feature prototypes for each class instead of directly generating images. Besides, we alleviate the negative transfer brought by noisy pseudo labels through confidence reweighting and regularization.</p><p>3 Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>We focus on the task of source-free unsupervised domain adaptation (UDA) in this paper, where only a welltrained source model and unlabeled target data are accessible. Specifically, we consider a K-class classification task, where the source and target domains share with the same label space. We assume that the pre-trained source model consists of a feature extractor G e and a classifier C y . Moreover, we denote the unlabeled target domain by D t ={x i } nt i=1 , where n t is the number of target samples.</p><p>The key goal is to adapt the source model to the target domain with access to only unlabeled target data. Such a task, however, is very challenging due to the lack of source domain data and target domain annotations. Hence, conventional UDA methods requiring source data are unable to tackle this task. To address this task, we innovatively propose a Contrastive Prototype Generation and Adaptation (CPGA) method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Scheme</head><p>Inspired by that feature prototypes can represent a group of semantically similar instances <ref type="bibr">[Snell et al., 2017]</ref>, we explore to generate avatar feature prototypes to represent each source class and use them for class-wise domain alignment. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the proposed CPGA consist of two stages: prototype generation and prototype adaptation.</p><p>In the stage one (Section 3.3), inspired by that the classifier of the source model contains class distribution information <ref type="bibr">[Xu et al., 2020]</ref>, we propose to train a class conditional generator G g to learn such class information and generate avatar feature prototypes for each class. Meanwhile, we use the source classifier C y to judge whether G g generates correct feature prototypes w.r.t. classes. By training the generator G g to confuse C y via both cross-entropy L ce and the contrastive loss L p con , we are able to generate intra-class compact and inter-class separated feature prototypes. Meanwhile, to overcome the lack of target domain annotations, we resort to a self pseudo-labeling strategy to generate pseudo labels for each target data (Section 3.4).</p><p>In the stage two (Section 3.5), we adapt the source model to the target by aligning the pseudo-labeled target features to the source prototypes. Specifically, we conduct class-wise alignment through a contrastive loss L w con based on a domain projector C p . Meanwhile, we devise an early learning regularization term L elr to prevent remembering noisy pseudo labels. Lastly, to make the feature more discriminative, we further impose a neighborhood clustering loss L nc .</p><p>The overall training procedure of CPGA can be summarized as follows: min</p><formula xml:id="formula_0">?g L ce (? g ) + L p con (? g ),<label>(1)</label></formula><formula xml:id="formula_1">min {?e,?p} L w con (? e , ? p ) + ?L elr (? e , ? p ) + ?L nc (? e ),<label>(2)</label></formula><p>where ? g , ? e and ? p denotes the parameters of the generator G g , the feature extractor G e and the projector C p , respectively. Moreover, ? and ? are trade-off parameters to balance losses. For simplicity, we set the trade-off parameter to 1 in Eq.</p><p>(1) based on our preliminary studies. Gg is trained to generate avatar feature prototypes via Lce and L p con .</p><p>(2) Prototype adaptation: in each training batch, we use the learned prototype generator to generate one prototype for each class. Based on the generated prototypes and pseudo labels obtained by clustering, we align each pseudo-labeled target feature to the corresponding class prototype by training a domain-invariant feature extractor via L w con , L elr and Lnc. Note that the classifier Cy is fixed during the whole training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contrastive Prototype Generation</head><p>The absence of the source data makes UDA challenging. To handle this, we propose to generate feature prototypes for each class by exploring the class distribution information hidden in the source classifier <ref type="bibr">[Xu et al., 2020]</ref>. To this end, we use the source classifier C y to train the class conditional generator G g . To be specific, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, given a uniform noise z?U (0, 1) and a label y?R K as inputs, the generator G g first generates the feature prototype p=G g (y, z) (More details of the generator and the generation process can be found in Supplementary). Then, the classifier G y judges whether the generated prototype belongs to y and trains the generator via the cross entropy loss:</p><formula xml:id="formula_2">L ce = ?y log C y (p),<label>(3)</label></formula><p>where p is the generated prototype and C y (p) denotes the prediction of the classifier. In this way, the generator is capable of generating feature prototypes for each category.</p><p>However, as shown in <ref type="figure">Figure 3</ref>(a), training the generator with only the cross entropy may make the feature prototypes not well compact and prototypical. As a result, domain alignment with these prototypes may make the adapted model less discriminative, leading to insufficient performance (See <ref type="table">Table 4</ref>). To address this, motivated by InfoNCE [van den <ref type="bibr">Oord et al., 2018;</ref><ref type="bibr">Zhang et al., 2021]</ref>, we further impose a contrastive loss for all generated prototypes to encourage more prototypical prototypes:</p><formula xml:id="formula_3">L p con =? log exp(?(p, k + )/? ) exp(?(p, k + )/? )+ K?1 j=1 exp(?(p, k ? j )/? ) ,<label>(4)</label></formula><p>where p denotes any anchor prototype. For each anchor, we sample the positive pair k + by randomly selecting a gener-  <ref type="figure">Figure 2</ref>: Visualizations of the generated feature prototypes by the generator trained with different losses. Compared to training with only the cross entropy Lce, the contrastive loss L p con encourages the prototypes of the same category to be more compact and those of different categories to be more separated. Better viewed in color. ated prototype with the same category to the anchor p, and sample K?1 negative pairs k ? that have diverse classes with the anchor. Here, in each training batch, we generate at least 2 prototypes for each class in the stage one. Moreover, ?(?, ?) denotes the cosine similarity and ? is a temperature factor.</p><p>As shown in <ref type="figure">Figure 3</ref>(b), by training the generator with L ce +L p con , the generated prototypes are more representative (i.e., intra-class compact and inter-class separated). Interestingly, we empirically observe that the inter-class cosine distance will converge closely to 1 (i.e., cosine similarity close to 0) by training with L ce +L p con (See <ref type="table">Table 4</ref>), if the feature dimensions are larger than the number of classes. That is, the generated prototypes of different categories are approximatively orthometric in the high-dimensional feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pseudo Label Generation for Target Data</head><p>Domain alignment can be conducted based on the generated avatar source prototypes, However, the alignment is nontrivial due to the lack of target annotations, which makes the class-wise alignment difficult <ref type="bibr">[Pei et al., 2018;</ref><ref type="bibr" target="#b4">Kang et al., 2019]</ref>. To address this, we generate pseudo labels based on a self-supervised pseudo-labeling strategy, proposed in . To be specific, let q i =G e (x i ) denote the feature vector and let? k i =C k y (q) be the predicted probability of the classifier regarding the class k. We first attain the initial centroid for each class k by:</p><formula xml:id="formula_4">c k = nt i=1? k i q i nt i=1? k i ,<label>(5)</label></formula><p>where n t is the number of target data. These centroids help to characterize the distribution of different categories . Then, the pseudo label of the i-th target data is obtained via a nearest centroid approach:</p><formula xml:id="formula_5">y i = arg max k ?(q i , c k ),<label>(6)</label></formula><p>where ?(?, ?) denotes the cosine similarity, and the pseudo label? i ?R 1 is a scalar index. During the training process, we update the centroid of each class by c k = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Contrastive Prototype Adaptation</head><p>Based on the generated prototypes and target pseudo labels, we conduct prototype adaptation to alleviate domain shifts. Here, in each training batch, we generate one prototype for each class. However, due to domain discrepancies, the pseudo labels can be quite noisy, making the adaptation difficult. To address this, we propose a new contrastive prototype adaptation strategy, which consists of three key components: (1) weighted contrastive alignment; (2) early learning regularization; (3) target neighborhood clustering. Weighted Contrastive Alignment. Based on the pseudolabeled target data, we then conduct class-wise contrastive learning to align the target data to the corresponding source feature prototype. However, the pseudo labels may be noisy, which degrades contrastive alignment. To address this, we propose to differentiate pseudo-labeled target data and assign higher importance to the reliable ones. Motivated by <ref type="bibr">[Chen et al., 2019]</ref> that reliable samples are generally more close to the class centroid, we compute the confidence weight by:</p><formula xml:id="formula_6">w i = exp(?(q i , c? i )/? ) K k=1 exp(?(q i , c k )/? ) ,<label>(7)</label></formula><p>where the feature with higher similarity to the corresponding centriod will have higher importance. Then, we can conduct weighted contrastive alignment. To this end, inspired by [Chen et al., 2020], we first use a non-linear projector C p to project the target features and source prototypes to a l 2normalized contrastive feature space. Specifically, the target contrastive feature is denoted as u=C p (q), while the prototype contrastive feature is denoted as v=C p (p). Then, for any target feature u i as an anchor, we conduct prototype adaptation via a weighted contrastive loss:</p><formula xml:id="formula_7">L w con = ?w i log exp(u i v + /? ) exp(u i v + /? )+ K?1 j=1 exp(u i v ? j /? ) ,<label>(8)</label></formula><p>where the positive pair v + is the prototype with the same class to the anchor u i , while the negative pairs v ? are the prototypes with different classes. Early Learning Regularization. To further prevent the model from memorizing noise, we propose to regularize the learning process via an early learning regularizer. Since DNNs first memorize the clean samples with correct labels and then the noisy data with wrong labels <ref type="bibr">[Arpit et al., 2017]</ref>, the model in the "early learning" phase can be more predictable to the noisy data. Therefore, we seek to use the early predictions of each sample to regularize learning. To this end, we devise a memory bank H={h 1 , h 2 , ..., h nt } to record nonparametric predictions of each target sample, and update them based on new predictions via a momentum strategy. Formally, for the i-th sample, we predict its non-parametric prediction regarding the k-th prototype by o</p><formula xml:id="formula_8">i,k = exp(u i v k /? ) K j=1 exp(u i vj /? ) , and update the momentum by: h i ? ? ?h i + (1 ? ?)o i , (9) where o i =[o i,1 , ..., o i,K ]</formula><p>, and ? denotes the momentum coefficient. Based on the memory bank, for the i-th data, we further train the model via an early learning regularizer L elr , proposed in <ref type="bibr">[Liu et al., 2020]</ref>:</p><formula xml:id="formula_9">L elr = log(1 ? o i h i ).</formula><p>(10) This regularizer enforces the current prediction to be close to the prediction momentum, which helps to prevent overfitting to label noise. Note that the use of L elr in this paper is different from <ref type="bibr">[Liu et al., 2020]</ref>, which focuses on classification tasks and uses parametric predictions. Target Neighborhood Clustering. To enhance the contrastive alignment, we further resort to feature clustering to make the target features more compact. Inspired by <ref type="bibr">[Saito et al., 2020]</ref> that the intra-class samples in the same domain are generally more close, we propose to close the distance between each target sample and its nearby neighbors. To this end, we maintain a memory bank Q={q 1 , q 2 , ..., q nt } to restore all target features, which are updated when new features are extracted in each iteration. Based on the bank, for the i-th sample's feature q i , we can compute its normalized similarity with any feature q j by s i,j = exp(?(q i ,q j )/? ) n t l=1,l =i exp(?(q i ,q l )/? ) . Motivated by that minimizing the entropy of the normalized similarity helps to learn compact features for similar data <ref type="bibr">[Saito et al., 2020]</ref>, we further train the extractor via a neighborhood clustering loss:</p><formula xml:id="formula_10">L nc = ? nt j=1,j =i s i,j log(s i,j ).<label>(11)</label></formula><p>Note that the entropy minimization here does not use pseudo labels, so the learned compact target features are (to some degree) robust to pseudo label noise. We summarize the overall training scheme of CPGA in Algorithms 1, while the inference is provided in the supplementary. Compute Lce and L p con based on Eqns.</p><p>(3) and (4); 4:</p><p>loss.backward() based on Eqn. (1). 5: end for 6: for m = 1 ? M do 7:</p><p>Generate prototypes p for each class based on fixed Gg; 8:</p><p>Extract target data features Ge(x) based on Ge; 9:</p><p>Obtain target pseudo labels based on Eqn. (6); 10:</p><p>Obtain contrastive features ht based on Cp; 11:</p><p>Compute L w con , L elr , Lnc based on Eqns. (8), (10), (11); 12:</p><p>loss.backward() based on Eqn.</p><p>(2). 13: end for 14: Output: Ge and Cy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We conduct the experiments on three benchmark datasets: <ref type="formula" target="#formula_0">(1)</ref>  Implementation Details. We implement our method based on PyTorch 1 . For a fair comparison, we report the results of all baselines in the corresponding papers. For the network architecture, we adopt a ResNet <ref type="bibr">[He et al., 2016]</ref>, pre-trained on ImageNet, as the backbone of all methods. Following , we replace the original fully connected (FC) layer with a task-specific FC layer followed by a weight normalization layer. The projector consists of three FC layers with hidden feature dimensions of 1024, 512 and 256. We train the source model via label smoothing technique <ref type="bibr">[M?ller et al., 2019]</ref> and train CPGA using SGD optimizer. We set the learning rate and epoch to 0.01 and 40 for VisDA and to 0.001 and 400 for Office-31 and Office-Home. For hyperparameters, we set ?, ?, ? and batch size to 0.05, 0.9, 0.07 and 64, respectively. Besides, we set ?=7 for Office-31 and Office-home while ?=5 for VisDA. Following <ref type="bibr">[Xu et al., 2020]</ref>, the dimension of noise z is 100. We put more implementation details in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with State-of-the-arts</head><p>In this section, we compare our proposed CPGA with the state-of-the-art methods. For Office-31, as shown in <ref type="table" target="#tab_2">Table 1</ref>, the proposed CPGA achieves the best performance compared with source-free UDA methods w.r.t. the average accuracy over 6 transfer tasks. Moreover, our method shows its superiority in the task of A?D and D?A and comparable results on the other tasks. Note that even compared with the state-ofthe-art methods using source data (e.g., SRDC), our CPGA is able to obtain a competitive result as well. Besides, from <ref type="table" target="#tab_4">Table 2</ref>, CPGA outperforms all the state-of-the-art methods w.r.t. the average accuracy (i.e., per-class accuracy) on the more challenging dataset VisDA. Specifically, CPGA gets the best accuracy in the eight categories and obtains comparable results in others. Moreover, our CPGA is able to surpass the baseline methods with source data (e.g., CoSCA), which demonstrates the superiority of our proposed method. For Office-Home, we put the results in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>To evaluate the effectiveness of the proposed two modules (i.e., prototype generation and prototype adaptation) and the sensitivity of hyper-parameters, we conduct a series of ablation studies on VisDA. Effectiveness of Prototype Generation. In this section, we verify the effect of our generated prototypes in the existing domain adaptation methods (e.g., <ref type="bibr">DANN [Ganin and Lempitsky, 2015]</ref>, ADDA <ref type="bibr">[Tzeng et al., 2017]</ref> and <ref type="bibr">DMAN [Zhang et al., 2019a]</ref>), which, previously, cannot solve the domain adaptation problem without source data. To this end, we introduce our prototype generation module to replace their source data-oriented parts. From <ref type="table" target="#tab_5">Table 3</ref>, based on prototypes, the existing methods achieve competitive performance compared with the counterparts using source data, or even perform better in some tasks. It demonstrates the superiority and applicability of our prototype generation scheme.    <ref type="table">Table 4</ref>: Ablation studies on prototype generation in the stage one with different losses. Inter-class distance and intra-class distance is based on cosine distance (range from 0 to 2). We report per-class accuracy (%) after training the model on VisDA for 400 epochs.</p><p>Ablation Studies on Prototype Generation. To study the impact of our contrastive loss L p con , we compare the generated prototype results from models with and without L p con . From <ref type="table">Table 4</ref> 2 , compared with training by cross-entropy loss L ce only, optimizing the generator via L ce +L p con makes the inter-class features separated (i.e., larger inter-class distance) and intra-class features compact (i.e., smaller intra-class distance). The L p con loss also helps to enhance the performance from 85.0% to 86.0%.</p><p>Ablation Studies on Prototype Adaptation. To investigate the losses of prototype adaptation, we show the quantitative results of the models optimized by different losses. As shown in <ref type="table">Table 5</ref>, compared with the conventional contrastive loss L con , our proposed contrastive loss L w con achieves a more promising result on VisDA. Such a result verifies the ability of alleviating pseudo label noise of the confidence weight w. Besides, our model has the ability to further improve the performance when introducing the losses L elr and L nc . When combining all the three losses (i.e., L w con , L elr and L nc ), we obtain the best performance.</p><p>2 <ref type="figure">Figure 2</ref> shows the corresponding visual results of <ref type="table">Table 4</ref>. Backbone L con L w con L elr L nc Per-class (%) 52.4 80.9 82.7 85.4 86.0 <ref type="table">Table 5</ref>: Ablation study for the losses (i.e., L w con , L elr and Lnc) of prototype adaptation. We show the per-class accuracy (%) of the model trained on VisDA for 400 epochs. Lcon denotes L w con without confidence weight w.   <ref type="bibr">1,</ref><ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">7,</ref><ref type="bibr">9]</ref> and ? is chosen from [0.001, 0.005, 0.01, 0.05, 0.1]. In each experiment, the rest of hyper-parameters are fixed.</p><p>Influence of Hyper-parameters. In this section, we evaluate the sensitivity of two hyper-parameters ? and ? on VisDA via an unsupervised reverse validation strategy <ref type="bibr">[Ganin et al., 2016]</ref> based on the source prototypes. For convenience, we set ? = 0.05 when studying ?, and set ? = 5 when studying ?. As shown in <ref type="table" target="#tab_6">Table 6</ref>, the proposed method achieves the best performance when setting ? = 5 and ? = 0.05 on VisDA. The results also demonstrate that our method is non-sensitive for the hyper-parameters. Besides, we put more analysis of hyper-parameters in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper has proposed a prototype generation and adaptation (namely CPGA) method for source-free UDA. Specifically, we overcome the lack of source data by generating avatar feature prototypes for each class via contrastive learning. Based on the generated prototypes, we develop a robust contrastive prototype adaptation strategy to pull the pseudolabeled target data toward the corresponding source prototypes. In this way, CPGA adapts the source model to the target domain without access to any source data. Extensive experiments verify the effectiveness and superiority of CPGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inference Details of CPGA</head><p>In this section, we present the pseudo-code of CPGA during inference. Specifically, when getting a well-trained CPGA, we can obtain the target prediction based on the feature extractor G e and the classifier C y . As shown in Algorithm 2, given an input image x, we first capture the corresponding feature G e (x) and then feed the feature into the classifier C y to generate the target prediction.</p><p>Algorithm 2 Inference of CPGA Require: Target data x, feature extractor Ge and classifier Cy. 1: Extract feature Ge(x) regrading x using Ge; 2: Compute the prediction Cy(Ge(x)) using Cy; 3: Output: Cy(Ge(x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Implementation Details</head><p>Generator Architecture. As shown in <ref type="table">Table 7</ref>, the generator consists of an embedding layer, two FC layers and two deconvolution layers. Similar to <ref type="bibr">ACGAN [Odena et al., 2017]</ref>, given an input noise z?U (0, 1) and a label y?R K , we first map the label into a vector using the embedding layer. After that, we combine the vector with the given noise by a element-wise multiplication and then feed it into the following layers. Since we propose to obtain feature prototypes instead of images, we reshape the output of the generator into a feature vector with the same dimensions as the last FC layer.</p><p>Training. In the stage one, we train the generator by optimizing L ce +L p con . The batchsize is set to 128. We use the SGD optimizer with learning rate = 0.001. In the stage two, to achieve class-wise domain alignment, we generate feature prototypes for K classes in each epoch. Optional Hyper-parameter Selection. Following <ref type="bibr">[Ganin et al., 2016]</ref>, we select the hyper-parameters via an unsupervised reverse validation strategy. Such a strategy consists of two steps: (1) We generate source prototypes for K classes and predicted labels for the target domain via a well-trained CPGA.</p><p>(2) We train another CPGA with pseudo-labeled target data served as the source domain and evaluate the model on the source prototypes. By the end, we obtain the corresponding hyper-parameters based on the best accuracy on source prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Experimental Results</head><p>Comparison with State-of-the-art Methods. We verify the effectiveness of our method on the Office-Home dataset. From <ref type="table">Table 8</ref>, the results show that: (1) CPGA outperforms all the conventional unsupervised domain adaptation methods, which needs to use the source data. (2) CPGA achieve the competitive performance compared with the state-of-theart source-free UDA methods, i.e.,  and . Besides, we also provide our reimplemented results of the published source-free UDA methods on VisDA and Office-31 based on their published source codes (See <ref type="table">Table 9</ref> and <ref type="table" target="#tab_2">Table 11</ref>). Influence of Hyper-parameters. In this section, we provide more results for the hyper-parameters ? and ? on VisDA. As shown in <ref type="table" target="#tab_2">Table 10</ref>, our method achieves the best performance with the setting ?=0.9 and ?=5 on VisDA. Visualization of Optimization Curve. <ref type="figure">Figure 3</ref> shows our method converges well in terms of the total loss and accuracy in the training phase. Also, the curve on the validation set means our method does not suffer from pseudo label noise. Robustness Comparisons with BAIT. As shown in <ref type="bibr">Figure 4, BAIT [Yang et al., 2020b</ref>] may overfit to mistaken divisions of certain and uncertain sets, leading to poor generalization abilities. In contrast, our method is more robust and can conquer the issue of pseudo label noise.    <ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">7]</ref>. In each experiment, the rest of hyper-parameters are fixed to the values mentioned in the main paper. We report the results of the model trained on VisDA for 40 epochs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of CPGA. CPGA contains two stages: (1) Prototype generation: under the guidance of the fixed classifier, a generator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Training with Lce (b) Training with Lce+L p con</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>?i=k) and then update pseudo labels based on Eqn. (6) in each epoch, where I(?) is the indicator function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Office-31 [Saenko et al., 2010]  is a standard domain adaptation dataset that is made up of three distinct domains, i.e., Amazon (A), Webcam (W) and DSLR (D). Three domains share 31 categories and contain 2817, 795 and 498 samples, respectively.(2) VisDA [Peng et al., 2017] is a large-scale challenging dataset that concentrates on the 12class synthesis-to-real object recognition task. The source domain contains 152k synthetic images while the target domain has 55k real object images. (3) Office-Home [Venkateswara et al., 2017] is a medium-sized dataset, which contains four distinct domains, i.e., Artistic images (Ar), Clip Art (Cl), Product images (Pr) and Real-world images (Rw). Each of the four domains has 65 categories. Baselines. We compare CPGA with three types of baselines: (1) source-only: ResNet [He et al., 2016]; (2) unsupervised domain adaptation with source data: MCD [Saito et al., 2018], CDAN [Long et al., 2018], TPN [Pan et al., 2019], SAFN [Xu et al., 2019], SWD [Lee et al., 2019], MDD [Zhang et al., 2019b], CAN [Kang et al., 2019], DMRL [Wu et al., 2020], BDG [Yang et al., 2020a], PAL [Hu et al., 2020], MCC [Jin et al., 2020], SRDC [Tang et al., 2020]; (3) source-free unsupervised domain adaptation: SHOT [Liang et al., 2020], PrDA [Kim et al., 2020], MA [Li et al., 2020] and BAIT [Yang et al., 2020b].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Optimization curves of CPGA on Office-31(A?W). Testing curves of CPGA and BAIT on VisDA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Training of CPGA Require: Unlabeled target data Dt={xi} n t i=1 ; Source model {Ge, Cy}; Training epoch E, M ; Parameters ?, ?, ? , ?. Initialize: Projector Cp; Generator Gg.</figDesc><table><row><cell cols="2">1: for e = 1 ? E do</cell></row><row><cell>2:</cell><cell>Generate prototypes p based on Gg;</cell></row><row><cell>3:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Accuracy (%) on the small-sized Office-31 (ResNet-50).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MethodSource-free plane bicycle bus car horse knife mcycl person plant sktbrd train truck Per-classResNet-101 [He et al., 2016] 55.1 53.3 61.9 59.1 80.6 17.9 79.7 31.2 81.0 26.5 73.5 8.5 52.4 CDAN [Long et al., 2018] 85.2 66.9 83.0 50.8 84.2 74.9 88.1 74.5 83.4 76.0 81.9 38.0 73.9 SAFN [Xu et al., 2019] 93.6 61.3 84.1 70.6 94.1 79.0 91.8 79.6 89.9 55.6 89.0 24.4 76.1 SWD [Lee et al., 2019] 90.8 82.5 81.7 70.5 91.7 69.5 86.3 77.5 87.4 63.6 85.6 29.2 76.4 TPN [Pan et al., 2019] 93.7 85.1 69.2 81.6 93.5 61.9 89.3 81.4 93.5 81.6 84.5 49.9 80.4 PAL [Hu et al., 2020] 90.9 50.5 72.3 82.7 88.3 88.3 90.3 79.8 89.7 79.2 88.1 39.4 78.3 MCC [Jin et al., 2020] 88.7 80.3 80.5 71.5 90.1 93.2 85.0 71.6 89.4 73.8 85.0 36.9 78.8 CoSCA [Dai et al., 2020] 95.7 87.4 85.7 73.5 95.3 72.8 91.5 84.8 94.6 87.9 87.</figDesc><table><row><cell></cell><cell>9 36.8</cell><cell>82.9</cell></row><row><cell>PrDA [Kim et al., 2020]</cell><cell>86.9 81.7 84.6 63.9 93.1 91.4 86.6 71.9 84.5 58.2 74.5 42.7</cell><cell>76.7</cell></row><row><cell>SHOT [Liang et al., 2020]</cell><cell>92.6 81.1 80.1 58.5 89.7 86.1 81.5 77.8 89.5 84.9 84.3 49.3</cell><cell>79.6</cell></row><row><cell>MA [Li et al., 2020]</cell><cell>94.8 73.4 68.8 74.8 93.1 95.4 88.6 84.7 89.1 84.7 83.5 48.1</cell><cell>81.6</cell></row><row><cell>BAIT [Yang et al., 2020b]</cell><cell>93.7 83.2 84.5 65.0 92.9 95.4 88.1 80.8 90.0 89.0 84.0 45.3</cell><cell>82.7</cell></row><row><cell>CPGA (ours, 40 epochs)</cell><cell>94.8 83.6 79.7 65.1 92.5 94.7 90.1 82.4 88.8 88.0 88.9 60.1</cell><cell>84.1</cell></row><row><cell>CPGA (ours, 400 epochs)</cell><cell>95.6 89.0 75.4 64.9 91.7 97.5 89.7 83.8 93.9 93.4 87.7 69.0</cell><cell>86.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracies (%) on the large-scale VisDA dataset (ResNet-101).</figDesc><table><row><cell>Method</cell><cell>A?D A?W D?W W?D D?A W?A Avg.</cell></row><row><cell cols="2">DANN (with source data) 79.7 82.0 96.9 99.1 68.2 67.4 82.2</cell></row><row><cell cols="2">DANN (with prototypes) 83.7 81.1 97.5 99.8 63.4 63.6 81.5</cell></row><row><cell cols="2">DMAN (with source data) 83.3 85.7 97.1 100.0 65.1 64.4 82.6</cell></row><row><cell cols="2">DMAN (with prototypes) 86.3 84.2 97.7 100.0 64.7 64.5 82.9</cell></row><row><cell cols="2">ADDA (with source data) 82.9 79.9 97.4 99.4 64.9 63.6 81.4</cell></row><row><cell cols="2">ADDA (with prototypes) 83.5 81.9 97.2 100.0 63.8 63.0 81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of the existing domain adaptation methods with source data or prototypes on Office-31 (ResNet-50).</figDesc><table><row><cell cols="4">Objective Inter-class distance Intra-class distance Per-class (%)</cell></row><row><cell>L ce</cell><cell>0.7860</cell><cell>3.343 ? e ?4</cell><cell>85.0</cell></row><row><cell>L ce + L p con</cell><cell>1.0034</cell><cell>2.670 ? e ?6</cell><cell>86.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Influence of the trade-off parameter ? and ? in terms of per-class accuracy (%) on VisDA. The value of ? is chosen from [</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 11 :</head><label>11</label><figDesc>Classification accuracies (%) on the Office-31 dataset (ResNet-50). We adopt underline to denote reimplemented results.</figDesc><table><row><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>0.99</cell></row><row><cell>3 5 7</cell><cell>81.2 81.3 79.7</cell><cell>83.0 82.2 81.6</cell><cell>83.9 84.1 83.3</cell><cell>83.0 83.2 83.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Influence of the trade-off parameters ? and ? in terms of per-class accuracy (%) on VisDA. The value of ? is chosen from [0.5, 0.7, 0.9, 0.99] and ? is chosen from [</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code is available: github.com/SCUT-AILab/CPGA.MethodSource-free A?D A?W D?W W?D D?A W?A Avg.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by Key Realm R&amp;D Program of Guangzhou <ref type="formula">(202007030007)</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>References <ref type="bibr">[Arpit et al., 2017]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we provide the algorithm of inference scheme (Section 5), more implementation details (Section 5), and more experimental results (Section 5).  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnet-50</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>He et al.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcd [saito</surname></persName>
		</author>
		<idno>92.2 88.6 98.5 100.0 69.5 69.7 86.5</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cdan [long</surname></persName>
		</author>
		<idno>98.6 100.0 71.0 69.3 87.7</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mdd [zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Can [kang</surname></persName>
		</author>
		<idno>95.0 94.5 99.1 99.6 70.3 66.4 90.6</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dmrl [wu</surname></persName>
		</author>
		<idno>93.4 90.8 99.0 100.0 73.0 71.2 87.9</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bdg [</surname></persName>
		</author>
		<idno>2020a] 93.6 93.6 99.0 100.0 73.2 72.0 88.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcc [jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srdc [tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Prda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno>2020] 92.2 91.1 98.2 99.5 71.0 71.2</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shot [liang</surname></persName>
		</author>
		<idno>93.1 90.9 98.8 99.9 74.5 74.8 88.7</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bait [yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma [li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnet-50</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>He et al.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcd [saito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cdan [long</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mdd [zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bnm [cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bdg [</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srdc [tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Prda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shot [liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shot [liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bait [yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bait [yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">We adopt underline to denote reimplemented results. Method Source-free plane bicycle bus car horse knife mcycl person plant sktbrd train truck Per-class SHOT</title>
	</analytic>
	<monogr>
		<title level="m">Classification accuracies (%) on the Office-Home dataset</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>ResNet-50. Liang et al., 2020</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shot [liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bait [yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bait [yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">We adopt underline to denote reimplemented results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?d A?w D?w W?d D?a W?a</forename><surname>Avg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Classification accuracies (%) on large-scale VisDA dataset</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>ResNet-101</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shot [liang</surname></persName>
		</author>
		<idno>93.1 90.9 98.8 99.9 74.5 74.8 88.7</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shot [liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bait [yang</surname></persName>
		</author>
		<idno>2020b] 92.0 94.6 98.1 100.0 74.6 75.2 89.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bait [yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
