<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizable Human Pose Triangulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristijan</forename><surname>Bartol</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bojani?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomislav</forename><surname>Petkovi?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomislav</forename><surname>Pribani?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalizable Human Pose Triangulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of generalizability for multi-view 3D human pose estimation. The standard approach is to first detect 2D keypoints in images and then apply triangulation from multiple views. Even though the existing methods achieve remarkably accurate 3D pose estimation on public benchmarks, most of them are limited to a single spatial camera arrangement and their number. Several methods address this limitation but demonstrate significantly degraded performance on novel views. We propose a stochastic framework for human pose triangulation and demonstrate a superior generalization across different camera arrangements on two public datasets. In addition, we apply the same approach to the fundamental matrix estimation problem, showing that the proposed method can successfully apply to other computer vision problems. The stochastic framework achieves more than 8.8% improvement on the 3D pose estimation task, compared to the state-of-the-art, and more than 30% improvement for fundamental matrix estimation, compared to a standard algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation is a vision task of detecting the keypoints that represent a standard set of human joints. The area is extremely competitive, especially due to the advances in deep learning. Pose estimation is particularly important for applications such as medicine, fashion industry, anthropometry, and entertainment <ref type="bibr" target="#b0">[1]</ref>. In this work, we focus on 3D human pose estimation from multiple views in a single time frame.</p><p>The common approach to multi-view pose estimation is to (1) detect correspondent 2D keypoints in each view using pretrained pose detector <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b36">35]</ref>, and then (2) triangulate <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b33">32]</ref>. A naive approach takes 2D detections as they are and applies triangulation from all available views. Due to the variety of poses and self-occlusions, some views contain erroneous detections, which should be ignored or their influence mitigated in the triangulation process. One way to ignore the erroneous detections is to apply Different camera configurations D if f e r e n t d a t a s e t s Different camera number <ref type="figure">Figure 1</ref>: We propose a stochastic framework for human pose triangulation from multiple views and demonstrate its successful generalization across different camera arrangements, their number, and different public datasets. The upper two and the lower left image shows different camera arrangements and their number on CMU Panoptic Studio dataset <ref type="bibr" target="#b17">[17]</ref>. The lower right part shows the Human3.6M's 4-camera arrangement <ref type="bibr" target="#b14">[14]</ref>.</p><p>RANSAC <ref type="bibr" target="#b10">[10]</ref>, marking the keypoints whose reprojection errors are above a threshold as outliers <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b13">13]</ref>. The problem with vanilla RANSAC is that it is non-differentiable, so the gradients are not back-propagated, which disables end-to-end learning. Most of the state-of-the-art 3D pose estimation approaches extract 2D image features, such as heatmaps, from multiple views, and combine them for 3D elevation in an end-to-end fashion <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b26">25]</ref>; we refer to those approaches as the learnable triangulation approaches.</p><p>Due to a mostly-fixed set of cameras during training, the learnable triangulation approaches are often limited to a single camera arrangement and their number. Several works attempt to generalize outside the training data <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b35">34]</ref>, but the demonstrated performance on novel views is significantly lower than using the original (base) views.</p><p>Inspired by stochastic learning <ref type="bibr" target="#b28">[27]</ref> and its applications in computer vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, we propose generalizable triangulation of human pose. First, we generate a pool of random hypotheses. A hypothesis is a 3D pose where the points are obtained by triangulating a random subset of views for each joint separately. Each generated hypothesis pass through a scoring neural network. The loss function is an expectation of the triangulation error, i.e. E(h i ) = i e i s i , where e i is the error of the hypothesis h i and s i is the hypothesis score. By minimizing the error expectation, the model learns the distribution of hypotheses. The key idea is to learn to evaluate 3D pose hypotheses without considering the spatial camera arrangement used for triangulation.</p><p>The proposed approach has several practical advantages over the previous methods. First, we demonstrate its consistent generalization performance across different camera arrangement on two public datasets -Human3.6M <ref type="bibr" target="#b14">[14]</ref> and Panoptic Studio <ref type="bibr" target="#b17">[17]</ref> (see <ref type="figure">Fig. 1</ref>). Second, we show that the proposed model learns human pose prior and define a novel metric for pose prior evaluation. Finally, we apply the same stochastic approach to the problem of fundamental matrix estimation from noisy 2D detections and compare it to the standard 8-point algorithm, showing that the proposed framework successfully applies to computer vision problems other than human pose triangulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We distinguish two types of related work. First, we focus on triangulation-based 3D pose estimation methods and methods that attempt to generalize between the different camera arrangements and datasets. Second, we relate to keypoint correspondence methods and point out how our problem differs from the standard correspondence problem.</p><p>Triangulation. Most of the single-person image-based approaches either use robust triangulation (RANSAC) or apply learnable triangulation. Several methods <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b25">24]</ref> based on robust triangulation use RANSAC on many (more than four) views to apply triangulation only on inlier detection candidates to produce pseudo ground truth data. He et al. <ref type="bibr" target="#b13">[13]</ref> exploit epipolar constraints to find the keypoint matches between multiple images and then apply robust triangulation.</p><p>The standard approach for learnable triangulation using deep learning models <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b2">3]</ref> is to first extract 2D pose heatmaps, where each heatmap represents the probability of a keypoint location. Cross-view fusion <ref type="bibr" target="#b26">[25]</ref> builds upon the pictorial structures model <ref type="bibr" target="#b1">[2]</ref> to combine 2D keypoint features from multiple views to estimate a 3D pose. An algebraic triangulation <ref type="bibr" target="#b15">[15]</ref> estimates the confidence for each keypoint detection and applies weighted triangulation. Their volumetric approach combines the multi-view features and builds the volumetric grid, obtaining the current state-of-the-art for single-frame 3D pose. Finally, <ref type="bibr" target="#b27">[26]</ref> fuses the features into a unified latent representation that is less memory intensive than the volumetric grids. Similar to us, they also attempt to disentangle from the specific spatial camera arrangement.</p><p>Keypoint correspondence. The standard keypointbased computer vision approaches, such as structure-frommotion <ref type="bibr" target="#b29">[28]</ref>, rely on sparse keypoint detections to establish initial 3D geometry. The core problem is to determine the correspondences between the extracted keypoint detections across images, under various illumination changes, textureless surfaces, and repetitive structures <ref type="bibr" target="#b11">[11]</ref>. The usual approach is to apply keypoint descriptor such as SIFT <ref type="bibr" target="#b22">[21]</ref> and find inlier correspondences using RANSAC <ref type="bibr" target="#b10">[10]</ref>. Even though this paradigm is successful in practice, it is not differentiable and, therefore, cannot be used in an end-to-end learning fashion.</p><p>Several works have proposed soft and differentiable versions of RANSAC (DSAC) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b40">39]</ref>. The successful soft RANSAC alternative <ref type="bibr" target="#b40">[39]</ref> learns to extract both local features of each data point, as well as retain the global information of the 3D scene. Similar to us, they also demonstrate convincing generalization capabilities to unseen 3D scenes. On the other hand, DSAC and its variants <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> propose a probabilistic learning scheme, i.e. minimizing the error expectation. We follow their approach but also discover that different strategies work better for our problem (see Sec. <ref type="bibr">3 and 4)</ref>.</p><p>In contrast to the standard keypoint matching approaches, we extract keypoints with already known human joint correspondences between the views. However, our correspondent keypoints are noisy, oscillating around the centers of the joints, which potentially leads to erroneous triangulation. Our model demonstrates robustness to erroneous keypoint detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We first describe the generic stochastic framework, and then describe it more specifically for generalizable pose triangulation and fundamental matrix estimation. The framework consists of several steps, shown in  <ref type="figure" target="#fig_0">Figure 2</ref>: An overview of our method. Before stochastic learning, 2D keypoints, y, are extracted. In each frame, the hypothesis pool, h i ? H, is generated, and the poses are passed through the scoring network, f S . The hypothesis? i is selected based on the estimated scores s i . Finally, the total loss, l total , consists of three components (l stoch , l entropy , l est ), and is calculated with respect to the ground truth, h * .</p><p>2. Hypothesis generation, H. As it is possible to generate an extremely large number of hypotheses, only a subset of random hypotheses is created. Following <ref type="bibr" target="#b28">[27]</ref> and <ref type="bibr" target="#b3">[4]</ref>, we model the hypothesis generation step as a stochastic node. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Hypothesis selection,? i . We experiment with several hypothesis selection strategies. The one that works the best for us is the weighted average of all hypotheses:</p><formula xml:id="formula_0">h weight = i s i h i , i s i = 1, h i ? H,<label>(1)</label></formula><p>where the scores s i are used as weights. We also try other strategies, such as the stochastic selection:</p><formula xml:id="formula_1">h stoch = h i , with i ? ? H ,<label>(2)</label></formula><p>where hypothesis h i is selected based on the estimated distribution ? H . As shown in Sec. 4, the stochastic selection performs worse than the weighted, in contrast to <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Loss calculation, l total . The loss function consists of several components:</p><p>(a) Stochastic loss. Following <ref type="bibr" target="#b3">[4]</ref>, we calculate our stochastic loss as an expectation of error for all hypotheses, l stoch = E(e H ) = i e(h i , h * )s i , where e i is the error of the estimated hypothesis with respect to the ground truth, h * , and s i represent the probability that the error is minimal.</p><p>(b) Entropy loss. Score estimations s i tend to quickly converge to zero. To stabilize the estimation values, we follow <ref type="bibr" target="#b4">[5]</ref> and minimize an entropy function, l entropy = ? i s i log(s i ).</p><p>(c) Estimation loss. We define it as the error of the selected hypothesis with respect to the ground 3D pose, l est = e i (? i , h * ). The estimation loss, in the case of generalizable pose triangulation, is most similar to the standard 3D pose estimation loss, used by the competing approaches <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b18">18]</ref>.</p><p>Finally, the total loss is a sum of the three components, l total = ? l stoch + ? l entropy + ? l est , where ?, ?, and ? are fixed hyperparameters that regulate relative values between the components.</p><p>In order for the estimated scores s i to represent the probabilities, their values need to be normalized into [0, 1] range. The standard way to normalize the output values is to apply the softmax function, ?(s i ) = exp si j exp sj . To avoid early convergence, we use the Gumbel-Softmax function <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b23">22]</ref>:</p><formula xml:id="formula_2">? GS (s i ) = exp((log s i + g i )/? ) k j=1 exp((log s j + g j )/? ) ,<label>(3)</label></formula><p>where ? is a temperature parameter, and g i represent samples drawn from Gumbel(0, 1) <ref type="bibr" target="#b24">[23]</ref> distribution. The temperature ? regulates the broadness of the distribution.</p><p>For lower temperatures (? &lt; 1), the influence of lowerscore hypotheses is limited compared to higher-score hypotheses, and vice versa. The purpose of Gumbel(0, 1) is to add noise to each sample while retaining the original distribution(s), which allows the model to be more flexible with the hypothesis selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generalizable Pose Triangulation</head><p>We now describe the stochastic framework specifically for learning human pose triangulation.</p><p>Pose generation. The 3D human pose hypothesis, h i ? H, is generated in the following way. For each joint k, a subset of views, v k , is randomly selected. The detections from the selected views are triangulated to produce a 3D joint.</p><p>Pose normalization. The input to the pose scoring network, f S,pose are 3D pose coordinates, p, normalized in the following way -we select three points: left and right shoulder and the pelvis (between the hips), calculate the rotation between the normal of the plane given by the three points, and the normal of the xy-plane, and apply that rotation to all coordinates. Other than the 3D pose coordinates, we also extract 16 body part lengths, given by all adjacent joints, e.g. left lower arm, left upper arm, left shoulder, etc. Finally, we concatenate both normalized 3D coordinates and the body part lengths into a 1D vector and pass it through the network. The output is a scalar, s i , representing the score of the hypothesis h i .</p><p>Pose estimation error. The pose estimation error, e i (? i , h * ), is a mean per-joint precision error (MPJPE) <ref type="bibr" target="#b14">[14]</ref> between the estimated 3D pose,p i , and the ground truth, p * :</p><formula xml:id="formula_3">e i (? i , h * ) = e i (p i , p * ) = 1 J J k ||p ik ? p * k || 2 ,<label>(4)</label></formula><p>where p ik is the k-th keypoint of the i-th pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fundamental Matrix Estimation</head><p>We describe how to learn fundamental matrix estimation between the pairs of cameras using the proposed stochastic framework. The fundamental matrix describes the relationship between the two views via x ? 2 F x 1 = 0, where x 1 and x 2 are the corresponding 2D points in the first (target) and the second (reference) view. From the fundamental matrix, relative rotation and translation (the relative camera pose) between the views can be obtained <ref type="bibr" target="#b12">[12]</ref>.</p><p>Hypothesis generation. The relative camera pose hypothesis, h i , is generated in a slightly different way than the 3D pose hypothesis. The required number of points to determine the fundamental matrix is 8 when an 8-point algorithm is used <ref type="bibr" target="#b21">[20]</ref>. However, with the presence of noise, the required number of points is usually much higher. Instead of using a single time frame as in pose triangulation, we select the keypoints from M frames, having a total of M * J individual point correspondences. The camera hypothesis h i is obtained using a subset of T &lt; M * J correspondences, passed through an 8-point algorithm. The result of an 8point algorithm are four possible rotations and translations; we select the correct one in a standard way <ref type="bibr" target="#b12">[12]</ref>.</p><p>Input preparation. The input to the camera pose scoring network, f S,cam , are the distances between the corresponding projected rays. The rays are obtained using the reference camera parameters, (R ref , t ref ), and the estimated relative camera pose, (R rel,i , t rel,i ). To achieve the permutation invariance between the line distances on the input, we simply sort the values before passing it through the network.</p><p>Hypothesis selection. The camera pose hypothesis, h weight , is selected as the weighted average of the rotation 1 , i.e. a weighted average of the translation of all hypotheses.</p><p>Estimation error. The hypothesis estimation error, e i , is calculated as:</p><formula xml:id="formula_4">e i (? i , h * ) = e i (X i , X * ) = ||X i ? X * || 2<label>(5)</label></formula><p>where X * are random 3D points (used as ground truth), andX i are 3D points obtained by projecting the points X * to 2D planes, using the estimated parameters, (R i ,t i ), and then projected back to 3D. More specifically, using the estimated, target projection matrix,</p><formula xml:id="formula_5">P i = K i [R i |t i ] and the reference projection matrix, P ref = K ref [R ref |t ref ]</formula><p>, the points X * are first projected to 2D,x i =P i X * , and then triangulated using P ref andP i . The intrinsic matrices K i are assumed to be known for all cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The stochastic framework is evaluated on Human3.6M <ref type="bibr" target="#b14">[14]</ref> and Panoptic Studio <ref type="bibr" target="#b17">[17]</ref> datasets. As most of the previous 3D pose estimation approaches presented their results on Human3.6M, we use it for the quantitative comparison to state-of-the-art. Panoptic Studio contains a relatively large number of cameras <ref type="bibr" target="#b32">(31)</ref> with useful data annotations (camera parameters, 3D and 2D poses). We use the Panoptic Studio dataset to evaluate the generalization performance between different camera arrangements and their number. We also evaluate the generalization between the Panoptic Studio and Human3.6M datasets. As experiments are based on a single-person pose estimation, we use Panoptic Studio sequences that contain single person in the scene, following <ref type="bibr" target="#b37">[36]</ref>.</p><p>Other than the evaluation of our best result (? weight ), we also compare between different hypotheses:</p><p>? Weighted average hypothesis,? weight ,  <ref type="table">Table 2</ref>: The evaluation of generalization performance from CMU Panoptic Studio <ref type="bibr" target="#b17">[17]</ref> to Human3.6M dataset <ref type="bibr" target="#b14">[14]</ref>, compared to the volumetric approach of Iskakov et al. <ref type="bibr" target="#b15">[15]</ref>. The proposed approach achieves 8.8% better performance on H3.6M compared to <ref type="bibr" target="#b15">[15]</ref>, when trained on a 4-camera CMU3 dataset (see <ref type="table" target="#tab_0">Table 1</ref>).   an uniform distribution,</p><p>? Best and worst hypotheses 2 , h best and h worst , with the lowest and the highest errors, e min and e max .</p><p>Additionally, we also compare ourselves with RANSAC as reported in <ref type="bibr" target="#b15">[15]</ref> (see Subsec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generalization Performance</head><p>One of the most important properties of the proposed model is that it generalizes well to different spatial arrangements and number of cameras, and different datasets, which is a major limitation of the previous models. To evaluate the generalization performance across data sets, we select five different camera arrangements: The setup is as follows. Each of the five camera arrangements is first used for training, and then the generalization performance is tested on the remaining four arrangements. The five selected sets differ with respect to the spatial camera arrangement and their number. Additionally, the fifth camera set (H36M) is used to test the transfer learning capabilities between the datasets. All the results in this subsection are obtained using h weight hypothesis. Our Generalization Performance. <ref type="table" target="#tab_0">Table 1</ref> shows consistent performance on each of the five test datasets, regardless of the selected training dataset. In particular, the performance between different test sets on the Panoptic Studio dataset is within 5% difference, which demonstrates robustness to various camera arrangements and their number (intra-dataset). The inter-dataset generalization is also successful, which we further evaluate against the competing methods <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b27">26]</ref>. Note that the demonstrated generalization can be exploited both in training time and in inference.</p><p>Volumetric Triangulation. <ref type="table">Table 2</ref> compares our proposed method to the state-of-the-art 3D pose estimation approach <ref type="bibr" target="#b15">[15]</ref>. Iskakov et al. reported an average 34.0 mm error on Human3.6M test set when they trained on CMU Panoptic Studio (4-camera arrangement). Compared to them, we achieve 31.0 mm on our 4-camera arrangement (CMU3), demonstrating an improvement in inter-dataset generalization (see <ref type="table" target="#tab_0">Table 1</ref> for the comprehensive results).</p><p>Remelli et al. <ref type="table" target="#tab_2">Table 3</ref> compares our method to Remelli et al. <ref type="bibr" target="#b27">[26]</ref>. Similar to us, they explicitly address the generalization to novel views. They demonstrate their intra-dataset generalization performance on Total Capture <ref type="bibr" target="#b34">[33]</ref>, by comparing the test performances on cameras (1, 3, 5, 7) as a base arrangement (TC1) and testing it on cameras <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">8)</ref> as a novel arrangement (TC2). We do not evaluate our model on Total Capture. Instead, to compare with Remelli et al., we measure the performance between the CMU camera test sets and evaluate relative score differences. The performance of our model is consistent across different camera arrangements and their number for intra-dataset configuration. Moreover, our inter-dataset performance from CMU Panoptic Studio to Human3.6M is 15.1%, which is still better than the best result by Remelli et al. Note that the interdataset experiment is the most difficult as it also includes the changes in camera arrangement.</p><p>RANSAC. We outperform RANSAC on Panoptic Studio by a large margin. We can explain this by the fact that CMU does not have a full view of a person in most cameras, leading to strong occlusions and missing parts. As RANSAC takes only reprojection errors of individual 3D joints as an inlier selection criterion, it is unable to evaluate the estimated 3D pose as a whole, in contrast to our model that learns human pose prior (see Sec. 4.3).</p><p>Algebraic Triangulation. The algebraic triangulation <ref type="bibr" target="#b15">[15]</ref> is originally proposed as an improvement over RANSAC, where the weight is estimated for each joint location. The weight-based model indeed outperforms RANSAC both on Human3.6M and Panoptic Studio. However, as the authors point out in <ref type="bibr" target="#b15">[15]</ref>, it has several drawbacks. First, it processes each view independently, and second, it separately triangulates each joint. Therefore, the weight-based algebraic model suffers from the same problem as RANSAC by not taking the whole pose into account. Our model, on the other hand, successfully learns human pose prior, which allows it to select more feasible poses, making it more robust to occlusions and missing body parts. Note that <ref type="bibr" target="#b15">[15]</ref> does not test their weighted model on unseen views. Therefore, <ref type="table" target="#tab_0">Table 1</ref> shows the result of the model w/o weights, as this model is consistent across different camera sets. The actual result of the weighted model might differ, but it is hard to estimate by how much.</p><p>VoxelPose. VoxelPose <ref type="bibr" target="#b35">[34]</ref> reports 25.51mm MPJPE score on their intra-dataset experiment, compared to our 25.42mm. Even though we achieve comparable performances, the significant difference is that we did not pretrain our 2D backbone on the Panoptic Studio dataset, which would most likely further improve our 2D keypoint estimation and, consequently, final 3D pose estimations (Supp.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Base Dataset Performance</head><p>The comparison to state-of-the-art is shown in <ref type="table" target="#tab_5">Table 5</ref>.</p><p>Note that the <ref type="table">Table only</ref> shows the methods that use Hu-man3.6M for training and testing, with no additional training data (therefore, excluding Iskakov et al. <ref type="bibr" target="#b15">[15]</ref>). Compared to the best-performing single-frame method, Epipolar Transformers <ref type="bibr" target="#b13">[13]</ref>, we obtain 2.2 mm worse MPJPE, but outperform most of the other recent methods. <ref type="table" target="#tab_6">Table 6</ref> shows the MPJPE scores of all previously described pose hypotheses on the two datasets, compared to the RANSAC result, as reported in <ref type="bibr" target="#b15">[15]</ref>. Even though our weighted average hypothesis,? weight , is outperformed by the RANSAC approach on Human3.6M, we show a significant improvement on Panoptic Studio. Also, note that RANSAC is competitive against most of the state-of-the-art approaches on Human3.6M that do not use additional train-  ing data.</p><p>Regarding other results, the average hypothesis,? avg performs better than the stochastic,? stoch . The stochastic performs even worse than the random hypothesis on Panoptic Studio. The most probable hypothesis,? most , outperforms the average on Panoptic Studio. Note that the difference between best and worst hypothesis (h best , h worst ) is significantly different on the two datasets. This suggests that the hypotheses generated on Panoptic Studio are more similar to each other and the distribution is less broad. The difference between the most and the least probable hypotheses (? most ,? least ) is reasonable on both datasets, which confirms that our model learned to differentiate between the poses. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the qualitative performance comparison between several hypotheses. The least probable hypothesis, h least , does not have visually plausible pose reconstruction, while the random hypothesis,? random , has some obvious errors in the upper body. The most probable hypothesis,? most has minor reconstruction errors on the right arm and shoulder. The weighted hypothesis,? weight , is visually comparable to ground truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Human Pose Prior</head><p>We demonstrate the successful pose prior learning of the pose scoring network, f S,pose . There are previous works that attempt learning human pose prior <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b1">2]</ref>, but they do not quantitatively evaluate their methods. The idea of learning pose prior is to differentiate between the 3D poses that are more plausible and the poses that are less plausible with respect to several human body properties. The properties that can be extracted from the 3D pose are based on the body part lengths and between-joint angles. In this work, we focus on body part lengths, i.e. left-right body symmetry.</p><p>The body symmetry is measured for six different body left-right part pairs: upper arms, lower arms, shoulders, hips, upper legs, and lower legs. For each pair, l, we calculate the ratio, r il between the left and right part, in each time frame, i. The final pose prior metric is a variance of the ratios over time: where r l is the mean ratio for the pair l, and T is the number of frames. The reason for using ratios instead of the differences between the body parts is that some people are naturally asymmetric, so the idea is only to measure the consistency over time. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the pose prior metrics for the subject 9 of the Human3.6M dataset, for different hypotheses. As expected, the values are generally the lowest for our best performing hypothesis,? weight , followed by the average hypothesis,? avg . The difference between the most probable and the least probable hypothesis (? most ,? least ) suggests that we successfully learned body pose prior, i.e. differentiate between the plausible poses with respect to the body symmetry consistency over time. Note that the best hypothesis, h best , is comparable to? weight . <ref type="table" target="#tab_7">Table 7</ref> shows the fundamental matrix estimation results on all 4-view combinations on Human3.6M. The four metrics are used for the evaluation:</p><formula xml:id="formula_6">S 2 = i (r il ? r l ) 2 T ? 1 ,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Fundamental Matrix Estimation</head><p>? Rotation error, E R = ||quat(R rel ) ? quat(R * rel )|| 2 , where quat() represents the conversion to quaternions,</p><formula xml:id="formula_7">? Translation error (mm), E t = ||t rel ? t * rel || 2 ,</formula><p>? 2D error (pixels), E 2D = ||x i ? x * || 2 , where x * represents random 3D points, X * , projected using the ground truth relative projection matrix, P * , and</p><p>? 3D error (mm), E 3D = e i (? i , h * ), from Eq. 5.</p><p>The obtained results show that the model achieves subpixel error (E 2D ) for two pairs of views <ref type="figure" target="#fig_0">((1, 2) and (2, 3)</ref>), and only few pixels in the worst case, which corresponds to several millimeters when reprojected back to 3D (E 3D ). Note that the adjacent pairs of views have lower errors than the opposite pairs, as expected.</p><p>In <ref type="figure" target="#fig_4">Fig. 5</ref>, we compare our 3D errors (E 3D ) to the vanilla 8-point algorithm, on the (2, 3) camera pair, using different </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The proposed generalizable approach is a promising novel direction for 3D human pose estimation, as well as other related computer vision problems, such as the camera pose estimation. The demonstrated results show convincing generalization capabilities between different camera arrangements and datasets, outperforming previous methods. The model requires relatively little training data, which makes training faster and more convenient for smaller datasets, as further discussed in Supplementary.</p><p>The overall performance is competitive in both human pose triangulation and camera pose estimation tasks. By combining these two steps, it is possible to transfer the performance of the base dataset to any novel multi-camera dataset, in inference. The next reasonable step is to exploit image features in an end-to-end learning fashion, which should further improve the performance and possibly outperform the state-of-the-art even on the base dataset. The current model supports only a single-person pose triangulation. To extend to multi-person, we need to solve the keypoint correspondence problem between the people.</p><p>The main focus of the Supplementary Appendix is to demonstrate the application of the proposed model to novel camera arrangements and datasets that have unknown relative camera poses, i.e. extrinsic parameters</p><formula xml:id="formula_8">E ref, i = [R ref, i |t ref, i ],</formula><p>where ref is the reference camera, and i is each of the relative cameras. The camera poses are estimated based on the fundamental matrix estimation method described in the main paper. We further dissect relative camera pose estimation into the estimation of relative rotation, R ref, i , and relative translation, t ref, i , showing that the unknown translations have more significant impact on the performance than the unknown rotations (Appendix A). Finally, we briefly discuss other works, implementation details, and the limitations of the model in more detail and propose future work, in addition to the main paper (Appendix D). The ethical considerations are addressed in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance with Estimated Camera Poses</head><p>We evaluate the performance of the generalizable human pose triangulation model in case when the camera poses are estimated using the proposed fundamental matrix estimation method, on Human3.6M. In particular, we compare the performances between the test sets with known extrinsics, estimated relative rotations R ref, i , estimated relative translations, t ref, i , and estimated extrinsics (both rotation and translation). Additionally, we compare the performances when Human3.6M is used as the training dataset (base-dataset experiment), and when CMU3, described in the main paper, is used for training (inter-dataset experiment).</p><p>The results are shown in <ref type="table">Table 8</ref>. As expected, the performance on the base dataset is better than the performance on inter-dataset experiment. In overall, the performances on both the base experiment and inter-dataset experiment are satisfactory, taking into account that the rotations, translations, i.e. both, are unknown. Notably, the performance of the model significantly drops for unknown relative translations, while the unknown relative rotations only slightly affect the performance. We assume that the rotations are simply estimated more accurately than translations, hence the difference. To verify this assumption, we analyze 2D and 3D errors, defined in the main paper, for estimated rotations, i.e., translations separately.</p><p>Ablative Analysis of Camera Pose Estimation. <ref type="table" target="#tab_9">Table 9</ref> shows the fundamental matrix estimation errors (E 2D and E 3D , described in the main paper) between the pairs of views, in case when only rotation is estimated and the translation is known, and vice versa. The errors in case of the estimated translations are always higher compared to the case of estimated rotations, therefore, this result might explain the performance drop shown in <ref type="table">Table 8</ref>. The future <ref type="table">Table 8</ref>: The evaluation of the model in case of unknown relative camera poses on Human3.6M <ref type="bibr" target="#b14">[14]</ref>. We evaluate the model in base-dataset (same camera arrangement for training and testing) and inter-dataset (from CMU3 <ref type="bibr" target="#b17">[17]</ref> to Hu-man3.6M). We also dissect the analysis into the cases when rotation, i.e., translation only is unknown. Note that all Rs and ts shown in the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Other Works</head><p>The Epipolar Transformers <ref type="bibr" target="#b13">[13]</ref> outperforms our method on Human3.6M (base dataset). However, note that our model outperforms their lightweight, transformer model on H36M (30.4mm, <ref type="table" target="#tab_6">Table 6</ref> [13], compared to our 29.1mm, <ref type="table" target="#tab_3">Table 4</ref>, main paper). The difference in performances would most likely increase when evaluated on novel views, especially as the authors did not tackle the generalization problem at all. Further, their heavy-weight model might overfit even more on the base camera arrangement of the train dataset(s), so we can expect an increased performance drop on unseen views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>The selected hyperparameters set is shown in Tab. 10. The two hyperparameters used specifically for pose triangulation, i.e., fundamental matrix estimation, are the number of joints in the pose model, J = 17, and the number  <ref type="bibr">16 16</ref> of frames from which the camera hypotheses are sampled, M = 80. The required number of training iterations is relatively small. We obtain our best results using only 500 iterations. In each iteration, we generate 200 hypotheses. This is a great advantage of the approach, especially when only small amount data annotations are required. In particular, 500 iterations correspond to 500 data samples, i.e., 500/16?32 batches (batch size 16, <ref type="table" target="#tab_0">Table 10</ref>), meaning that the gradients were applied ?32 times for the model to be fully trained. It takes about 3 minutes to train the model, but this can be further improved by more efficient implementation of the hypothesis generation on CPU. Moreover, the training time is shorter, which simplifies the optimal hyperparameter search. Finally, the current implementation fits into ?1GB of GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Limitations</head><p>The main limitation of our model is that it strongly depends on the performance of the 2D detector <ref type="bibr" target="#b38">[37]</ref>. This is best seen in <ref type="table" target="#tab_0">Table 11</ref> that shows the difference in the performance on train, validation, and test 3 . The difference between the validation and test performance, in particular, can be explained by the fact that the 2D backbone has been finetuned on the whole training and validation splits, while it has never seen the test data. What this means is that we did not tackle the problem of train-to-test generalization; instead, we improved the between-test-sets generalization, which is a weaker result. The consequence of this train-test difference is that the performance on novel data will suffer mostly from the performance drop of the detector.</p><p>Another limitation is that the current model does not learn end-to-end. The consequence is that the model, at best, learns to differentiate well between the poses. But once the poses are good enough, the network can't differentiate further and will simply assign the same scores, converging into an average of "good-enough" 3D poses 4 . <ref type="bibr" target="#b2">3</ref> Note that, for training, we use subjects 1, 5, 6, 7, for validation, we use subject 8, and the remaining subjects 9 and 11 are used for testing. <ref type="bibr" target="#b3">4</ref> The good poses should be the ones that are symmetric and the ones that have body part ratios consistent with the ratios of an average (training Therefore, future work should definitely address this limitation by exploiting image features to obtain additional information about the keypoints. One way to use image features is through the confidence predictions, similar to previous works <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">8]</ref>.</p><p>Finally, we assume that the intrinsic camera parameters and the scale are known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ethical Considerations</head><p>For all of our experiments, we use two well-known, public datasets -Human3.6M and CMU Panoptic Studio. From the information obtained from the corresponding websites, it is unclear whether the datasets have the IRB approvals. We verified with the authors of the Panoptic Studio that the dataset has the approval. We also contacted the authors of Human3.6M, but did not get the confirmation at the moment of writing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 .</head><label>3</label><figDesc>Hypothesis scoring, f S . Each generated hypothesis h i ? H is scored using a scoring function, f S (h i |y) = s i . The scoring function is a neural network, i.e. a multi-layer perceptron. The network architectures for 3D pose triangulation and fundamental matrix estimation differ and are specified at the end of the Sec. 4. The network is the only learnable part of our model. The estimated scores s i , passed through the Gumbel-Softmax, ? GS (s i ) (Eq. 3), represent the estimated probability distribution of the hypotheses H, ? H .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison between four 3D pose hypotheses compared to ground truth (gt), on Human3.6M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Evaluation of the human pose prior metric for different hypotheses, and six body part pairs (lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The comparison of the E3D errors between the stochastic model and the 8-point algorithm, for different number of input frames (between 10 and 100), using the camera pair (2, 3) of Hu-man3.6M. For every number of frames, the experiments is done 10 times. The lines show mean values, and the fill parts show standard deviations. The values are clipped to 80 mm. number of input frames. Our model consistently outperforms the 8-point algorithm, showing robustness to noise and increased confidence due to lower variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The demonstration of the generalization performance (MPJPE in mm) on five data sets, featuring different spatial camera placements, different number of cameras, and different datasets (CMU Panoptic Studio and Human3.6M). Each row shows the performance on five test sets when the specified train set is used. The maximal difference between the scores for particular test sets is shown in the last column. The last row demonstrates inter-dataset generalization performance, while other rows show intra-dataset performance. H36M 33.4 H36M 31.0 H36M 32.5 H36M 29.1 15.1%</figDesc><table><row><cell>Train</cell><cell>CMU1</cell><cell>CMU2</cell><cell>CMU3</cell><cell>CMU4</cell><cell>H36M</cell><cell>Max diff. ?</cell></row><row><cell></cell><cell cols="5">CMU1 25.8 CMU1 25.8 CMU1 25.6 CMU1 25.2 CMU1 25.6</cell><cell>2.3%</cell></row><row><cell></cell><cell cols="5">CMU2 25.4 CMU2 26.0 CMU2 25.5 CMU2 25.6 CMU2 25.9</cell><cell>2.4%</cell></row><row><cell>Test</cell><cell cols="5">CMU3 24.9 CMU3 26.0 CMU3 25.0 CMU3 25.0 CMU3 25.7</cell><cell>4.4%</cell></row><row><cell></cell><cell cols="5">CMU4 25.1 CMU4 25.6 CMU4 25.3 CMU4 25.1 CMU4 25.5</cell><cell>2.0%</cell></row><row><cell></cell><cell>H36M 33.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Most and least probable hypotheses,? most and? least , the hypotheses with maximal and minimal estimated score, s max and s min ,</figDesc><table><row><cell></cell><cell>Intra-dataset</cell><cell></cell><cell></cell></row><row><cell>Method (train dataset)</cell><cell cols="3">Base test Novel test Diff. ?</cell></row><row><cell cols="2">Remelli et al. [26] (TC1)  ? 27.5 mm</cell><cell>38.2 mm</cell><cell>38.9%</cell></row><row><cell cols="2">Remelli et al. [26] (TC1)  ? 39.3 mm</cell><cell>48.2 mm</cell><cell>22.6%</cell></row><row><cell>Ours (CMU1)</cell><cell>24.9 mm</cell><cell>25.8 mm</cell><cell>3.6%</cell></row><row><cell>Ours (CMU3)</cell><cell>25.0 mm</cell><cell>25.6 mm</cell><cell>2.4%</cell></row><row><cell>Ours (CMU4)</cell><cell>25.0 mm</cell><cell>25.6 mm</cell><cell>2.4%</cell></row><row><cell>Ours (CMU2)</cell><cell>25.6 mm</cell><cell>26.0 mm</cell><cell>1.6%</cell></row><row><cell></cell><cell>Inter-dataset</cell><cell></cell><cell></cell></row><row><cell>Method (train dataset)</cell><cell>H36M</cell><cell>CMU1</cell><cell>Diff. ?</cell></row><row><cell>Ours (H36M)</cell><cell>29.1 mm</cell><cell>33.5 mm</cell><cell>15.1%</cell></row><row><cell cols="4">? Average hypothesis,? avg , obtained as an average of all</cell></row><row><cell>hypotheses,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell></row></table><note>The evaluation of generalization performance compared to Remelli et al. [26] (lower is better). We measure the perfor- mance drop between the base test set and the novel test set for intra-dataset and inter-dataset configurations. Note that we do not compare on the same datasets, so we only measure the rela- tive drop in percentages. Still, our approach demonstrates a sig- nificantly smaller performance drop compared to the competing method in all setups. The ? presents the canonical fusion, and the ? presents the baseline approach in [26].? Stochastic hypothesis,? stoch , selected randomly, based on the estimated distribution ? H , ? Random hypothesis,? random , selected randomly from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The comparison to RANSAC, algebraic triangulation<ref type="bibr" target="#b15">[15]</ref>, and VoxelPose<ref type="bibr" target="#b35">[34]</ref> on Panoptic Studio (intra-dataset)[mm]. The numbers show the performance on novel camera views. Our number is obtained as an average over 12 non-diagonal values ofTable 1.</figDesc><table><row><cell cols="4">Intra-dataset (CMU Panoptic Studio)</cell></row><row><cell cols="4">RANSAC Algebraic VoxelPose Ours</cell></row><row><cell>39.5</cell><cell>33.4</cell><cell>25.5</cell><cell>25.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>No additional training data setup. Overall comparison to the state-of-the-art on Human3.6M dataset. The proposed method outperforms most of the state-of-the-art methods. All values are showing MPJPE scores (mm).</figDesc><table><row><cell>Protocol 1, abs. positions</cell><cell>Dir.</cell><cell>Disc.</cell><cell>Eat</cell><cell>Greet</cell><cell>Phone</cell><cell>Photo</cell><cell>Pose</cell><cell>Purch.</cell><cell>Sit</cell><cell>SitD.</cell><cell>Smoke</cell><cell>Wait</cell><cell>WalkD.</cell><cell>Walk</cell><cell>WalkT.</cell><cell>Avg ?</cell></row><row><cell>Tome et al. [32]</cell><cell>43.3</cell><cell>49.6</cell><cell>42.0</cell><cell>48.8</cell><cell>51.1</cell><cell>64.3</cell><cell>40.3</cell><cell>43.3</cell><cell>66.0</cell><cell>95.2</cell><cell>50.2</cell><cell>52.2</cell><cell>51.1</cell><cell>43.9</cell><cell>45.3</cell><cell>52.8</cell></row><row><cell>Kadkhodamohammadi et al. [18]</cell><cell>39.4</cell><cell>46.9</cell><cell>41.0</cell><cell>42.7</cell><cell>53.6</cell><cell>54.8</cell><cell>41.4</cell><cell>50.0</cell><cell>59.9</cell><cell>78.8</cell><cell>49.8</cell><cell>46.2</cell><cell>51.1</cell><cell>40.5</cell><cell>41.0</cell><cell>49.1</cell></row><row><cell>Cross-view fusion [25]</cell><cell>28.9</cell><cell>32.5</cell><cell>26.6</cell><cell>28.1</cell><cell>28.3</cell><cell>29.3</cell><cell>28.0</cell><cell>36.8</cell><cell>41.0</cell><cell>30.5</cell><cell>35.6</cell><cell>30.0</cell><cell>28.3</cell><cell>30.0</cell><cell>30.5</cell><cell>31.2</cell></row><row><cell>Remelli et al. [26]</cell><cell>27.3</cell><cell>32.1</cell><cell>25.0</cell><cell>26.5</cell><cell>29.3</cell><cell>35.4</cell><cell>28.8</cell><cell>31.6</cell><cell>36.4</cell><cell>31.7</cell><cell>31.2</cell><cell>29.9</cell><cell>26.9</cell><cell>33.7</cell><cell>30.4</cell><cell>30.2</cell></row><row><cell>Epipolar transformers [13]</cell><cell>25.7</cell><cell>27.7</cell><cell>23.7</cell><cell>24.8</cell><cell>26.9</cell><cell>31.4</cell><cell>24.9</cell><cell>26.5</cell><cell>28.8</cell><cell>31.7</cell><cell>28.2</cell><cell>26.4</cell><cell>23.6</cell><cell>28.3</cell><cell>23.5</cell><cell>26.9</cell></row><row><cell>Ours (h weight )</cell><cell>27.5</cell><cell>28.4</cell><cell>29.3</cell><cell>27.5</cell><cell>30.1</cell><cell>28.1</cell><cell>27.9</cell><cell>30.8</cell><cell>32.9</cell><cell>32.5</cell><cell>30.8</cell><cell>29.4</cell><cell>28.5</cell><cell>30.5</cell><cell>30.1</cell><cell>29.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Overall quantitative comparison between the hypotheses. The values are showing MPJPE scores in mm (the lower is better).</figDesc><table><row><cell cols="3">Hypothesis Human3.6M ? Panoptic Studio ?</cell></row><row><cell>h weight</cell><cell>29.1</cell><cell>24.9</cell></row><row><cell>havg</cell><cell>31.2 +2.1</cell><cell>25.9 +1.0</cell></row><row><cell>hmost</cell><cell>41.3 +12.2</cell><cell>25.0 +0.1</cell></row><row><cell>h least</cell><cell>74.5 +45.4</cell><cell>29.8 +3.9</cell></row><row><cell>h stoch</cell><cell>41.3 +12.2</cell><cell>26.5 +1.6</cell></row><row><cell>h random</cell><cell>45.0 +15.9</cell><cell>26.1 +1.2</cell></row><row><cell>h best</cell><cell>22.3 -6.8</cell><cell>24.4 -0.5</cell></row><row><cell>hworst</cell><cell>98.9 +69.8</cell><cell>31.0 +6.1</cell></row><row><cell>RANSAC</cell><cell>27.4 -1.7</cell><cell>39.5 +14.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Evaluation of fundamental matrix estimation for all pairs of views on Human3.6M, based on four error metrics. Note that the camera pairs (1, 3), and (2, 4) are diagonal, while other pairs are adjacent.</figDesc><table><row><cell cols="2">Camera pair ER ?</cell><cell>Et ?</cell><cell>E2D ? E3D ?</cell></row><row><cell>(1, 3)</cell><cell cols="3">1.7e-2 2.3e+1 3.7e+0 2.3e+1</cell></row><row><cell>(2, 4)</cell><cell cols="3">3.2e-2 4.9e+0 1.3e+0 2.3e+1</cell></row><row><cell>(1, 4)</cell><cell cols="3">9.8e-3 2.7e+1 2.3e+0 1.3e+1</cell></row><row><cell>(2, 3)</cell><cell cols="3">2.1e-3 9.9e+0 7.2e-1 1.3e+1</cell></row><row><cell>(3, 4)</cell><cell cols="3">4.7e-3 8.5e+0 1.2e+0 5.5e+0</cell></row><row><cell>(1, 2)</cell><cell cols="3">4.8e-3 4.8e+0 8.1e-1 4.9e+0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>table correspond to R ref, i and r ref, i , but are abbreviated.</figDesc><table><row><cell></cell><cell cols="2">Base dataset (Human3.6M)</cell><cell></cell></row><row><cell cols="4">Known [R|t] Estimated R Estimated t Estimated [R|t]</cell></row><row><cell>29.1 mm</cell><cell>29.4 mm</cell><cell>36.7 mm</cell><cell>37.3 mm</cell></row><row><cell></cell><cell cols="2">Inter-dataset (CMU3 ? Human3.6M)</cell><cell></cell></row><row><cell cols="4">Known [R|t] Estimated R Estimated t Estimated [R|t]</cell></row><row><cell>31.0 mm</cell><cell>33.6 mm</cell><cell>42.2 mm</cell><cell>44.5 mm</cell></row><row><cell cols="4">work should focus on improving translation estimation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Dissecting the evaluation of fundamental matrix estimation on two cases -when the rotations, i.e., the translations are estimated, for all pairs of views on Hu-man3.6M. The 2D errors, E 2D are shown in pixels, and 3D errors, E 3D are shown in millimeters.</figDesc><table><row><cell></cell><cell cols="2">Estimated R</cell><cell cols="2">Estimated t</cell></row><row><cell cols="3">Camera pair E2D E3D</cell><cell cols="2">E2D E3D</cell></row><row><cell>(1, 3)</cell><cell>1.2</cell><cell>10.8</cell><cell>1.8</cell><cell>18.2</cell></row><row><cell>(2, 4)</cell><cell>0.9</cell><cell>9.7</cell><cell>1.6</cell><cell>15.3</cell></row><row><cell>(1, 4)</cell><cell>0.9</cell><cell>6.4</cell><cell>1.2</cell><cell>8.9</cell></row><row><cell>(2, 3)</cell><cell>0.6</cell><cell>3.9</cell><cell>1.0</cell><cell>4.5</cell></row><row><cell>(3, 4)</cell><cell>0.4</cell><cell>1.2</cell><cell>0.7</cell><cell>4.0</cell></row><row><cell>(1, 2)</cell><cell>0.4</cell><cell>1.8</cell><cell>0.7</cell><cell>3.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>The Table of hyperparameters for the two tasks.</figDesc><table><row><cell></cell><cell>3D pose</cell><cell>Camera</cell></row><row><cell>Learning rate</cell><cell>5  *  10e ?4</cell><cell>10e ?5</cell></row><row><cell>?</cell><cell>1.5</cell><cell>1.2</cell></row><row><cell>?, ?, ?</cell><cell cols="2">(1.0, 0.01, 0.02) (1.0, 0.01, 0.0)</cell></row><row><cell>Network layer sizes</cell><cell>(1000, 900 900, 900, 700)</cell><cell>(1000, 900, 900)</cell></row><row><cell># hypotheses in sample</cell><cell>200</cell><cell>100</cell></row><row><cell>Batch size</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>The comparison between train, validation, and test performance on Human3.6M (in case of base-dataset configuration). There is a significant difference in the performance between train (validation) and test.</figDesc><table><row><cell></cell><cell>Human3.6M</cell><cell></cell></row><row><cell>Train</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>13.8 mm</cell><cell>14.2 mm</cell><cell>29.1 mm</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. Pre-training. Prior to stochastic learning, the 2D poses (keypoints) are extracted for all images in the dataset. In all our experiments, we use the keypoints extracted using a baseline model<ref type="bibr" target="#b38">[37]</ref> pretrained on Human3.6M dataset. The input to stochastic model, therefore, consists only of keypoint detections, y. In each frame, JxK keypoints are detected, where J is the number of joints, and K is the number of views.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The rotations are converted to quaternions, for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the best and the worst hypotheses are not available in inference (missing?sign), because they are determined using ground truth.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work has been supported by the Croatian Science Foundation under the project IP-2018-01-8118.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A review of body measurement using 3d scanning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bartol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bojani?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petkovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pribani?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67281" to="67301" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-supervised 3d human pose estimation with multipleview geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouazizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiederer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kressel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<idno>abs/2108.07777</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dsac -differentiable ransac for camera localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning less is more -6d camera localization via 3d surface regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural-guided ransac: Learning where to sample model hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Note that the good poses should have high pose prior scores</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time multi-view 3d human pose estimation using semantic feedback to smart edge sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bultmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems XVII, Virtual Event</title>
		<editor>D. A. Shell, M. Toussaint, and M. A. Hsieh, editors</editor>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981-06-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-View Stereo: A Tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hern?ndez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno>abs/1611.01144</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A generalizable approach for multi-view 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A computer algorithm for reconstructing a scene from two projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Longuet-Higgins</surname></persName>
		</author>
		<idno>1987. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Readings in Computer Vision</title>
		<editor>M. A. Fischler and O. Firschein</editor>
		<meeting><address><addrLine>San Francisco (CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="page" from="61" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
		<idno>abs/1611.00712</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A* sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interhand2.6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/2008.09309</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4341" to="4350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lightweight multi-view 3d pose estimation through cameradisentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6039" to="6048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient estimation using stochastic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptively multi-view and temporal fusing transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2110.05092</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4645" to="4653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">View-invariant probabilistic embedding for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="53" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tom?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 British Machine Vision Conference (BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Voxelpose: Towards multicamera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), 2020</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10957" to="10966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to find good correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2666" to="2674" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
