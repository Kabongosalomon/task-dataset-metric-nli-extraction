<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Learning by Integrating Spatial and Frequency Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Kansas</orgName>
								<address>
									<postCode>66045</postCode>
									<settlement>Lawrence</settlement>
									<region>KS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<postCode>M5B 2K3</postCode>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Learning by Integrating Spatial and Frequency Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Few-shot learning</term>
					<term>discrete cosine transformation</term>
					<term>image classification</term>
					<term>frequency information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human beings can recognize new objects with only a few labeled examples, however, few-shot learning remains a challenging problem for machine learning systems. Most previous algorithms in few-shot learning only utilize spatial information of the images. In this paper, we propose to integrate the frequency information into the learning model to boost the discrimination ability of the system. We employ Discrete Cosine Transformation (DCT) to generate the frequency representation, then, integrate the features from both the spatial domain and frequency domain for classification. The proposed strategy and its effectiveness are validated with different backbones, datasets, and algorithms. Extensive experiments demonstrate that the frequency information is complementary to the spatial representations in few-shot classification. The classification accuracy is boosted significantly by integrating features from both the spatial and frequency domains in different few-shot learning tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the explosion in the amount of data we generate today, deep learning, as a data-driven method, has become the hotspot and achieved significant performance in many directions in computer vision <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. However, there are still many scenarios when we cannot access enough training data, especially in the medical field. For instance, to classify rare diseases, we have only limited data to build models. In addition, collecting a large scale of data could be expensive in some cases. Compared to the data-driven algorithms, human beings can learn a visual concept even using only a single example <ref type="bibr" target="#b6">[7]</ref>. This challenge gives birth to a new topic in computer vision, few-shot learning (FSL).</p><p>Exploiting the successful application with large-scale data, a direct way to improve few-shot learning performance is through data-augmentation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. By feeding the network with more positive and/or negative samples, it may mimic large-scale data tasks and achieve a reasonable performance. Although data augmentation could be an auxiliary method for few-shot learning algorithms, adding more data could not solve the few-shot learning problem essentially since it is impossible to generate "enough" meaningful training data (like thousands of data) to cover as much as possible different distributions of the data and turn it into a large-scale data problem.</p><p>The task of few-shot learning is to recognize novel classes with only a few (e.g. up to 5 <ref type="bibr" target="#b11">[12]</ref>) given labeled images.</p><p>Many researchers started to focus on using the task-level method <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>, meta-learning <ref type="bibr" target="#b15">[16]</ref>, to solve the few-shot learning problem. Meta-learning, or "learn to learn <ref type="bibr" target="#b15">[16]</ref>", treats every classification task as a single task. For example, for k-way n-shot image classification where each task contains images from k classes with n labeled images for each class, the goal is to recognize the test images with only k ? n labeled images for every task. The k-way n-shot image set can be regarded as one classification task, which is also called "episode" <ref type="bibr" target="#b11">[12]</ref> Meta-learning aims to learn transferable knowledge from past experience and the study can be divided as following three categories <ref type="bibr" target="#b12">[13]</ref>, memorybased method <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, metric-based method <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and optimization-based method <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Memorybased methods endeavor to use external memories <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> to memorize past information. Optimization-based methods learn to optimize <ref type="bibr" target="#b12">[13]</ref>. The metric-based methods learn to compare between a few labeled images and the test images <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Nevertheless, all methods mentioned above use only spatial images as the input of the backbone. Inspired by the fact that human vision is more sensitive to low-frequency information <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, we assume that human beings are learning some frequency information when classifying images. Results from the recent research <ref type="bibr" target="#b20">[21]</ref> also demonstrated impressive performance in the frequency domain where the backbone networks are fed with frequency domain information. To exploit the advantages of both the spatial domain and frequency domain information, we propose to integrate the features extracted from the spatial domain and the frequency domain, aiming to increase the performance of few-shot learning. As illustrated in <ref type="figure">Figure 1</ref>, the spatial domain feature module employs CNN to obtain the representation of original images, and the frequency domain feature module utilizes the Discrete Cosine Transformation (DCT) module <ref type="bibr" target="#b21">[22]</ref> to generate the frequency representation of original images and feed them into CNN to generate frequency features. Both the frequency features and spatial features are then fused together, followed by a classifier.</p><p>The main contributions of this study are as follows:</p><p>1) The paper proposes to exploit DCT with static selected channels to few-shot classification and implements the idea on different FSL models. 2) We investigate the influence of DCT filter size on the  <ref type="figure">Figure 1</ref>: The structure of the proposed framework. We concatenate the features from two networks. The upper network, "CNN (s)" denotes a regular image classification network where we draw the input layers before the backbone from the whole CNN and "(s)" means the "spatial domain". In the lower network, "CNN (f)", the images will go through the DCT module first to generate the frequency representation before being supplied to the CNN backbone, and "(f)" represents the "frequency domain". Finally, we concatenate features obtained from both networks to generate the final classification score, which is the whole output network, "CNN (s+f)". few-shot classification and find the relationship between DCT filter size and the classification accuracy.</p><p>3) Extensive experiments demonstrate that integrating the features from both the spatial and frequency domains can significantly increase the classification accuracy in few-shot classification. The source code of the proposed model can be downloaded from https://github.com/xiangyu8/PT-MAP-sf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Few-shot Learning</head><p>Few-shot learning aims to learn to classify query examples from "novel" classes given a few labeled support examples from the "novel" classes and abundant labeled support examples from "base" classes. Recent deep learning based few-shot learning algorithms could be roughly divided into 4 categories: 1) Data augmentation is a direct datalevel method to improve classification accuracy in fewshot learning via mimicking large-scale data algorithms <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, which is usually added to meta-learners as auxiliary. By generating more positive and/or negative data according to the given labeled data, more information could be fed to the deep neural network. 2) Metric-learning based method, or learn to compare, is one type of metalearning based approaches in few-shot learning, which focuses on constructing an appropriate embedding space to yield corresponding features of images and then calculating the similarity between the features of given labeled images and test images. Related researches include <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b18">[19]</ref>. 3) Optimization-based meta-learning methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b26">[27]</ref>, or learn to optimize, usually train another network to get the optimization hyper-parameters to adjust to the few-shot learning scenario that is different from previously fixed hyper-parameters. 4) Another type of metalearning in few-shot learning is to use external memories <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>All approaches mentioned above employ the spatial RGB images as the input of backbones. However, none of them make use of the frequency representations. In our work, we preprocess the RGB images with a DCT module to obtain the frequency representations and then input them to the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning in the Frequency Domain</head><p>Frequency-domain based algorithms are widely used in network compression <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, focusing on modifying the network to yield better efficiency. However, our DCT-based method focuses on increasing the classifying accuracy by reducing the input size with little modification of the model itself. The band-limited algorithm presented in <ref type="bibr" target="#b29">[30]</ref> shows that the model focuses on leveraging lower-frequency components. However, this FFT-based method is more effective on larger kernels instead of the most commonly used smaller filters like 3 ? 3 and 1 ? 1 in most neural networks. <ref type="bibr" target="#b30">[31]</ref> uses DCT coefficients during JPEG encoding which helps to improve the efficiency. <ref type="bibr" target="#b20">[21]</ref> shows that high-frequency channels of DCT could be removed without accuracy loss or even help to increase the classification accuracy in largescale image classification. However, none of the above mentioned frequency-based algorithms try to jointly exploit the features from both the spatial and frequency domains. <ref type="bibr" target="#b31">[32]</ref> proposes to add spatial-spectral convolution blocks in convolution layers to learn more powerful representations, while it requires much revision on the network and extra computations. Moreover, it employs DCT in a different way from ours. Based on frequency analysis, we propose to explore its effect on few-shot learning by integrating features from both the spatial and frequency domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Description on Few-shot Learning</head><p>Considering a problem of few-shot classification, let C source and C target denotes the source classes and target classes, respectively, and these two classes are disjoint. In the source classes C source , we have abundant labeled samples as the training data D source , while only a few labeled data D target are accessible for each class in the target classes C target . For a k-way n-shot learning problem, where we have n labeled support samples for k novel classes in C target , our task is to classify a query sample into one of these k support classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discrete Cosine Transform (DCT)</head><p>Inspired by the study <ref type="bibr" target="#b20">[21]</ref>, we design the DCT pipeline as shown in figure 2. To generate the frequency representation, we perform some pre-processing first, including the standard transformation as illustrated in <ref type="bibr" target="#b32">[33]</ref>, rotating, cropping, and translating, and obtain the image with size S image in step (a), e.g. 448 ? 448. Then, similar to the JPEG compression pipeline, we convert the high-resolution RGB images to YCbCr images, where we follow the 4:2:0 Chroma subsampling as shown in step (b) considering human vision system is more sensitive to brightness (Y) than color (Cr and Cb). After that, we divide each channel into S dct ? S dct patches and perform S dct ? S dct DCT transformation on each patch, where S dct is the size of the DCT filter, which is assigned at (e.g. 8 ? 8). In this way, we obtain an 8 ? 8 frequency representation for each 8?8 patch in each YCbCr channel, with lower frequency in the left top corner and higher frequency in the right bottom corner as illustrated in step (c). Next, for each channel, we reshape these 8 ? 8 frequency patches to cubes by grouping the same frequency to one sub-channel and yield (d). Finally, from (d) to (e), we first select more impactful frequency sub-channels obtained in <ref type="bibr" target="#b20">[21]</ref> from each YCbCr channel, concatenate them into one frequency cube and then upsample CrCb elements to the same size of Y element by interpolation, which would be the input of the following deep neural network. In (e), from left to right, it shows low to high-frequency elements for Y, Cb, and Cr respectively.</p><p>The final input size after DCT module, the size of (f), would be S image /S dct , e.g. 448/8 = 56, and the input size of ResNet is also 56 ? 56. So we could keep the network same size by adjusting S image and/or S dct . In this way, we can handle images with a vast range of size to keep as much as information from the original images (e.g. when S dct = 16 and the input size of the backbone is 56, we could process images with size S image = 56 ? 16 = 896). The DCT transform of an image can be denoted by:</p><formula xml:id="formula_0">D = T M T<label>(1)</label></formula><p>where M is the S dct ? S dct image patch after subtracting 128 for each pixel. And T is the DCT transformation matrix determined by:</p><formula xml:id="formula_1">T i,j = 1 ? N , i = 0 2 N cos (2j+1)i? 2N , i &gt; 0 (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Frequency Channels Selection</head><p>According to the study <ref type="bibr" target="#b20">[21]</ref>, low-frequency elements would be selected more frequently by designing a channel selection module carefully. In our implementation, we choose the following 24 channels obtained in <ref type="bibr" target="#b20">[21]</ref> as shown in <ref type="figure" target="#fig_2">Figure 3</ref>: the top left 4 ? 4 square for Y channel and the top left 2 ? 2 for Cr and Cb channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network</head><p>The framework of the proposed network is illustrated in <ref type="figure">Figure 1</ref>. The upper network, "CNN (s)" denotes a vanilla image classification network where we draw the input layers before the backbone from the whole CNN and"(s)" means the "spatial domain". In the lower branch, "CNN (f)", images will go through the DCT module first to obtain the frequency representation before being fed to the following CNN backbone, and "(f)" represents the "frequency domain". Finally, we integrate the information from the two domains by concatenating the normalized features obtained from both networks to output the final classification score, which is denoted as the whole outer network, "CNN (s+f)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Setup</head><p>Datasets. The proposed framework has been evaluated on three popular few-shot learning datasets: mini-ImageNet, CUB, and CIFAR-FS. 1) mini-ImageNet. This is a popular few-shot learning dataset first proposed in <ref type="bibr" target="#b11">[12]</ref>, which samples 100 classes from the original ILSVRC-12 dataset <ref type="bibr" target="#b33">[34]</ref>. For each class, it contains about 600 images. All images are 84 ? 84 RGB colored. In our experiment, we follow the split in <ref type="bibr" target="#b13">[14]</ref>, with 64 classes for training set, <ref type="bibr" target="#b15">16</ref>     rescaled to 56 ? S dct and the input dimension varies from the number of selected frequency channels. For example, when we use 8?8 DCT filters, the original images will be 448?448. To train (s+f), we train (s) and (f) separately first, and then fine-tune the classifier on the integrated features to get the final classifier for (s+f). This framework can also be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Application to Existing Learning Models</head><p>In this section, we explore the generalization ability of the proposed approach to other few-shot learning frameworks.</p><p>1) Influence of the integrated features: We integrate the proposed strategy to the following few-shot learning frameworks on mini-ImageNet: the Matching Network (MN) <ref type="bibr" target="#b11">[12]</ref> (metric-based algorithm), S2M2 R <ref type="bibr" target="#b24">[25]</ref> (pre-train and  based or metric-learning based methods. Another interesting observation is that when we train S2M2 R (f), the rotation loss decreases to less than 0.1 rapidly within a couple of epochs. This might because the rotation trick can work for spatial input while failed when it comes to frequency input, which might also explain that S2M2 R (s) works better than S2M2 R (f). This set of experiments demonstrates that the integrated features can work for different few-shot classification frameworks.</p><p>2) Comparison with the state-of-the-art: We compare the proposed network (s+f) with the state-of-the-art on the benchmarks. PT+MAP <ref type="bibr" target="#b40">[41]</ref> proposes to leverage the learned features to Gaussian-like distribution and add it to the network S2M2 R <ref type="bibr" target="#b24">[25]</ref>. Since the proposed strategy is designed in a preprocessing way, making it possible to combine it with any networks. We implement our method to PT+MAP and name it by adding "(s+f)" to the models we use. The images are resized to 84 ? 84 and 448 ? 448 for the spatial and frequency input respectively. 8?8 DCT filter with static frequency channel selection is employed.</p><p>The results are shown in <ref type="table" target="#tab_2">Table II</ref>. It is evident that our method can increase the accuracy of the state-of-theart by a large margin for all datasets we tested, including mini-Imagenet, CUB datasets, and CIFAR-FS. In all three datasets, PT+MAP achieves the best performance in terms of accuracy. For mini-Imagenet, our approach increases the best accuracy by 1.89% and 1.8% for 5-way 1-shot and 5way 5-shot, respectively. For the CUB dataset, the accuracy is increased by 3.93% and 2.71% respectively for the two tasks. Please note that for the CIFAR-FS dataset, the image size is small, only has 32?32. However, we still observe 1.81% and 1.48% increases for the two tasks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study 1) Few-shot learning with DCT:</head><p>In this section, we first validate the effectiveness of DCT module and integrated network by comparing baseline++ (s), baseline++ (f) and baseline++ (s+f) w/o data augmentation during training on miniImageNet, where baseline++ <ref type="bibr" target="#b38">[39]</ref> uses all images from base classes to pre-train and then fine-tune with a few support samples from testing set before testing. The only difference between the baseline <ref type="bibr" target="#b39">[40]</ref> and baseline++ is that the baseline uses a linear classifier while baseline++ calculates cosine distance. The backbone we choose for feature extraction is ResNet 34. Both 5-way 1-shot and 5way 5-shot image classification tasks are evaluated. For data augmentation during training, we performed random crop, left-right flip and color jitter as in the paper <ref type="bibr" target="#b38">[39]</ref> The experimental results are shown in <ref type="table" target="#tab_2">Table III</ref>, from which we can see that the accuracy of baseline++ (f) is higher than baseline++ (s) by 4.73% and 1.28% respectively for 5-way 1-shot classification task without and with data augmentation during the first training phase, 4.07% and 2.6% for the 5-way 5-shot task. The baseline++ (s+f) in all cases further increases the classification accuracy by 1.5-3.5 %, showing that baseline++ (f) is not just an improvement of baseline++ (s) but a complementary method of it and learning from both the spatial and frequency domain could increase the classification accuracy.</p><p>2) Influence of original information quantity: For the DCT branch, the network has more flexibility to choose image size with the existence of the DCT module, even take larger images compared with the spatial version. To explore the effect of information quantity the frequency branch takes, we conducted experiments on miniImageNet. The backbone of the frequency branch is ResNet 10 to save time and data augmentation is employed during training. The results are <ref type="table" target="#tab_2">Table III</ref>: Results on miniImageNet with different inputs as shown in <ref type="figure">Figure 1</ref>. The backbone is ResNet34. Top left square 24 channels as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref> are selected for frequency versions. The highest accuracy (%) is highlighted.  From the table, we can see that, when baseline++ (s) and baseline++ (f) utilize the same information from the original images, baseline++ (s) performs better than baseline++ (f) when the image is 84 ? 84 but worse for image size 224 ? 224, which means (f) holds the potential to perform better with fewer parameters than (s) when inputting enough information. When we input baseline++ (f) with more information than baseline (s), 448 instead of 224, the accuracy for (f) gets slightly improved, 0.9 % for 1 shot and 0.06 % for 5 shot. We think the reason is the information quantity is more and more approaching the amount needed by the current frequency backbone. Moreover, in all cases baseline++ (s+f) performs better than both baseline++ (s) and baseline++ (f), which means that the spatial and frequency representation are complementary to each other. To conclude, we can use larger images (if we could access them) to increase the accuracy by using the DCT module, and integrated features can always improve the performance no matter whether the frequency branch could access larger images compared to the spatial branch.</p><p>3) Different DCT filters and selected channels: In this experiment, we explore the effect of different sizes of DCT filters, 2, 4, 6, and 8, and different selected channels, 24 channels and all frequency channels as shown in <ref type="table" target="#tab_6">Table V</ref>. The backbone is ResNet18. For (f) version, the number of channels is 24 and S dct ? S dct ? 3 when we select 24 and all channels respectively, e.g. when the DCT filter size is 4, S dct = 4, the number of all channels will be 4 ? 4 ? 3 = 48, where 4 ? 4 is the size of DTC module and 3 is from Y, Cr and Cb channels. For the spatial branch, we use all 224 as the image size. For the DCT branch, we resize images to 56 ? S dct directly, e.g. if S dct = 4, the input is rescaled to 56 ? 4 = 224. Experiments with and without data augmentation during training are evaluated.</p><p>According to <ref type="table" target="#tab_6">Table V</ref>, all baseline++ (f) methods outperform their corresponding baseline++ (s) version without the DCT module, even when the DCT filter size is as small as 2?2. When all frequency channels are employed, we find the accuracy is increased with the increase of the size of the DCT filter no matter there is a data augmentation or not for the 5-way 1-shot classification. However, the increase is very small, and for 5-way 5-shot task, we do not observe this trend, and the baseline++ (f) with S dct = 2 even outperforms other filter sizes when there is no data augmentation. On the other hand, when only the top left 24 frequency channels are employed, the accuracy increases mildly with that of the filter sizes for both 5-way 1-shot and 5-way 5-shot.</p><p>From these observations, we can see that the filter size has little influence on the few-shot classification when we perform static frequency channel selection. The influence is neglectable in comparison with the influence of data augmentation. In this experiment, DCT filter size 6 with all channels and data augmentation achieves the best performance for the 5-way 1-shot task, and DCT filter size 8 with 24 channels and data augmentation achieves the best performance for the 5-way 5-shot task. However, the increase is not significant. In practice, we can simply choose a small filter size to save the computation cost. 4) Integrated features with different backbone: In this section, to verify the impact of integrated features under different backbones, we implement different versions of baseline++, (f) and (s+f), with the backbone ResNet10, ResNet18, and ResNet34 as shown in <ref type="table" target="#tab_2">Table VI</ref>. Image size for (s) and (f) is 224 and 448 respectively. 8 ? 8 DCT filters and static channel selection are implemented. In all cases, baseline++(f), when we preprocess images with the DCT module, outperforms all baseline++ (s) when there is no DCT module. Furthermore, in both tasks (5-way 1shot and 5-way 5-shot classification) and for all backbones (ResNet10, ResNet18, and ResNet34), the baseline++ (s+f) version achieves the best performance compared with base-line++(f) and baseline++(s), and the accuracy is improved by a margin of 4-6% compared with their vanilla versions, baseline++(s). This further verifies that learning from the frequency domain is a complementary method of learning from the spatial domain and integrating both features could further increase the classification accuracy. The experiment also demonstrates the effectiveness of the proposed approach on different backbone networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Feature Visualization</head><p>To visually understand the feature we learned from the whole framework, the t-SNE visualization <ref type="bibr" target="#b45">[46]</ref> is shown in    and baseline++(s+f). 5 novel categories are selected randomly and 80 samples for each class are employed. It can be observed from <ref type="figure" target="#fig_3">Figure 4</ref> that the clustering results by integrating features from both the spatial and frequency domains are more compact than those from only the spatial domain, which further verify the effectiveness of integrating both domains in improving the clustering ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have proposed to apply the DCT preprocessing technique to generate the frequency information of images and integrate the representations from both the spatial and frequency domains to increase the performance of few-shot classification. Through extensive experiments, we have demonstrated that the frequency information is complementary to feature representation, and integrating the features learned from both the spatial and frequency domains can significantly increase the performance of fewshot learning. The proposed strategy can act as a plug-in module for other few-shot learning models to increase their classification accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>frequency representation (d) reorganized frequency representation (e) final frequency representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The 8 ? 8 frequency index after DCT transformation. Each index represents one frequency component. Low frequencies lie in the top left corner and high frequencies are in the bottom right corner. We choose the top left square frequency elements as selected frequency channels. Specifically, 4 ? 4 square for Y channel and 2 ? 2 for the Cr and Cb channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The features for (a) and (b) are learned from base-line++ with backbone ResNet 10, specifically baseline++(s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>t-SNE feature visualization results. (a): base-line++(s). (b): baseline++(s+f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>classes for validation set, and 20 classes for testing set. Among these, classes from training, validation, and testing set are disjoint.2) CUB. This dataset was introduced in<ref type="bibr" target="#b34">[35]</ref> and it contains 6,033 bird images, 130, 20, and 50 classes for training, validation, and testing, respectively. 3) CIFAR-FS. This dataset<ref type="bibr" target="#b35">[36]</ref> is obtained by randomly splitting 100 classes in CIFAR-100<ref type="bibr" target="#b36">[37]</ref> into 64 training classes, 16 validation classes, and 20 novel classes. All images in this dataset are of the size 32?32.</figDesc><table /><note>Implementation details. For CNN (s) where we input the backbones with images, the input samples are resized to 224 ? 224 for ResNet [1] backbones and 84 ? 84 for WRN-28-10 [38] on mini-ImageNet and CUB, 32 ? 32 for CIFAR-FS. For CNN (f), the frequency version, images are</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table I :</head><label>I</label><figDesc>Improvement after integrating features from both the spatial and frequency domains to existing methods on miniImageNet. The highest accuracy (%) with 95% confidence interval is highlighted. # and * denotes results reported in<ref type="bibr" target="#b38">[39]</ref> and our reproduced results to the published ones respectively.</figDesc><table><row><cell>backbone</cell><cell>method</cell><cell cols="2">accuracy on miniImageNet 1-shot 5-shot</cell></row><row><cell></cell><cell>MN [12] #</cell><cell cols="2">54.49?0.81 68.82?0.65</cell></row><row><cell>ResNet10</cell><cell>MN (s)  *  MN (f)</cell><cell cols="2">52.98?0.21 72.41?0.16 55.98?0.20 74.17?0.16</cell></row><row><cell></cell><cell>MN (s+f)</cell><cell cols="2">57.32?0.21 76.27?0.16</cell></row><row><cell></cell><cell></cell><cell>+4.34</cell><cell>+3.86</cell></row><row><cell>WRN-28-10</cell><cell>S2M2 R [25] S2M2 R (s)  *  S2M2 R (f)</cell><cell cols="2">64.93?0.18 83.18?0.11 63.09?0.17 80.88?0.11 63.03?0.18 80.80?0.11</cell></row><row><cell></cell><cell>S2M2 R (s+f)</cell><cell cols="2">66.88?0.18 84.26?0.10</cell></row><row><cell></cell><cell></cell><cell>+3.79</cell><cell>+3.38</cell></row><row><cell></cell><cell>PT+MAP [41]</cell><cell cols="2">82.92?0.26 88.82?0.13</cell></row><row><cell>WRN-28-10</cell><cell>PT+MAP (s)  *  PT+MAP (f)</cell><cell cols="2">80.73?0.24 87.81?0.13 82.04?0.23 88.68?0.12</cell></row><row><cell></cell><cell cols="3">PT+MAP (s+f) 84.81?0.22 90.62?0.11</cell></row><row><cell></cell><cell></cell><cell>+4.08</cell><cell>+2.81</cell></row><row><cell cols="4">finetune), and PT+MAP [41] (post-process S2M2 R features).</cell></row><row><cell cols="4">The input is 84?84 for the spatial branch to yield a fair</cell></row><row><cell cols="4">comparison with previous results and 448?448 for the</cell></row><row><cell cols="4">frequency domain. All images are pre-processed with data</cell></row><row><cell cols="4">augmentation during training. The results are shown in Table</cell></row></table><note>I. It is evident that the proposed scheme (s+f) promotes the accuracy by about 2.8-4.3% in all cases compared with the original version (s) of the models. This shows that the integrating features from both domains work for different frameworks and its improvement is not limited to fine-tuning</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II :</head><label>II</label><figDesc>Comparison with the state-of-the-art on mini-ImageNet, CUB, and CIFAR-FS. The highest accuracy (%) is highlighted. * means results for miniImageNet and CUB-200-2011 datasets are from<ref type="bibr" target="#b38">[39]</ref>, and results for CIFAR-FS are from<ref type="bibr" target="#b35">[36]</ref>.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>miniImageNet 1-shot 5-shot</cell><cell>CUB-200-2011 1-shot 5-shot</cell><cell cols="2">CIFAR?FS 1-shot 5-shot</cell></row><row><cell>ProtoNet  *  [19]</cell><cell>ConvNet</cell><cell>50.37?0.83 67.33?0.67</cell><cell>66.36?1.00 82.03?0.59</cell><cell>55.5?0.70</cell><cell>72.0?0.60</cell></row><row><cell>MAML  *  [13]</cell><cell>ConvNet</cell><cell>50.96?0.92 66.09?0.71</cell><cell>66.26?1.05 78.82?0.70</cell><cell>58.9?1.9</cell><cell>71.5?1.0</cell></row><row><cell>RelationNet  *  [15]</cell><cell>ConvNet</cell><cell>51.84?0.88 64.55?0.70</cell><cell>64.38?0.94 80.16?0.64</cell><cell>55.0?1.0</cell><cell>72.0?0.60</cell></row><row><cell>S2M2 R [25]</cell><cell cols="2">WRN-28-10 64.93?0.18 83.18?0.11</cell><cell>80.68?0.81 90.85?0.44</cell><cell cols="2">74.81?0.19 87.47?0.13</cell></row><row><cell>AFHN [42]</cell><cell>ResNet18</cell><cell>62.38?0.72 78.16?0.56</cell><cell>70.53?1.01 83.95?0.63</cell><cell>-</cell><cell>-</cell></row><row><cell>DPGN [43]</cell><cell>ResNet12</cell><cell>67.77?0.32 84.60?0.43</cell><cell>75.71?0.47 91.48?0.33</cell><cell cols="2">77.90?0.50 90.20?0.40</cell></row><row><cell>DeepEMD-sampling [44]</cell><cell>ResNet12</cell><cell>68.77?0.29 84.13?0.53</cell><cell>79.27?0.29 89.80?0.51</cell><cell>-</cell><cell>-</cell></row><row><cell>PT+MAP [41]</cell><cell cols="2">WRN-28-10 82.92?0.26 88.82?0.13</cell><cell>91.55?0.19 93.99?0.10</cell><cell cols="2">87.69?0.23 90.68?0.15</cell></row><row><cell>PT+MAP (s+f) (ours)</cell><cell cols="2">WRN-28-10 84.81?0.22 90.62?0.11</cell><cell>95.48?0.13 96.70?0.07</cell><cell cols="2">89.50?0.21 92.16?0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table IV :</head><label>IV</label><figDesc>Results on miniImageNet with different information quantity supplied to the frequency channel as shown inFigure 1. The backbone is ResNet 10 and data augmentation when training is implemented. The highest accuracy (%) is highlighted.</figDesc><table><row><cell>method</cell><cell>image size</cell><cell cols="2">miniImageNet 5-way 1-shot 5-way 5-shot</cell></row><row><cell>baseline++ (s)</cell><cell>84</cell><cell>52.32?0.17</cell><cell>68.24?0.14</cell></row><row><cell>baseline++ (f)</cell><cell>84?448</cell><cell>49.47?0.17</cell><cell>65.64?0.12</cell></row><row><cell>baseline++ (s+f)</cell><cell>N/A</cell><cell>56.32?0.17</cell><cell>75.70?0.13</cell></row><row><cell>baseline++ (s)</cell><cell>224</cell><cell>57.52?0.17</cell><cell>75.56?0.13</cell></row><row><cell>baseline++ (f)</cell><cell>224?448</cell><cell>58.71?0.17</cell><cell>76.55?0.12</cell></row><row><cell>baseline++ (s+f)</cell><cell>N/A</cell><cell>62.23?0.18</cell><cell>80.08?0.12</cell></row><row><cell>baseline++ (s)</cell><cell>224</cell><cell>57.52?0.17</cell><cell>75.56?0.13</cell></row><row><cell>baseline++ (f)</cell><cell>448</cell><cell>59.61?0.18</cell><cell>76.61?0.12</cell></row><row><cell>baseline++ (s+f)</cell><cell>N/A</cell><cell>62.30?0.18</cell><cell>79.93?0.11</cell></row><row><cell cols="4">tabulated in Table IV. For the first two parts in the table,</cell></row><row><cell cols="4">we preprocess images for baseline++ (f) with the same data</cell></row><row><cell cols="4">augmentation method as the baseline (s) to generate 84?84</cell></row><row><cell cols="4">and 224?224 images. Then, we upsample these images to</cell></row><row><cell cols="4">448?448 to exploit the DCT module. For the bottom part</cell></row><row><cell cols="4">in this table, we resize the images directly to 448?448 to</cell></row><row><cell cols="3">include more original information.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table V :</head><label>V</label><figDesc>The effect of different sizes of DCT filters and whether we select frequency channels with backbone ResNet18 on miniImageNet. The number of channels denotes the channels before the backbone with the input layers removed, e.g. for ResNet, the input before the basic backbone is 56 ? 56 ? 64. The highest accuracy (%) is highlighted.</figDesc><table><row><cell>method</cell><cell>trainaug</cell><cell>DCT filter size</cell><cell>channels</cell><cell cols="2">accuracy on miniImageNet 5-way 1-shot 5-way 5-shot</cell></row><row><cell>baseline++ (s)</cell><cell>False</cell><cell>-</cell><cell>64</cell><cell>48.40?0.17</cell><cell>62.94?0.13</cell></row><row><cell>baseline++ (f)</cell><cell>False</cell><cell>2</cell><cell>all (12)</cell><cell>49.05?0.17</cell><cell>65.45?0.13</cell></row><row><cell>baseline++ (f)</cell><cell>False</cell><cell>4</cell><cell>all (48)</cell><cell>49.67?0.18</cell><cell>65.07?0.13</cell></row><row><cell>baseline++ (f)</cell><cell>False</cell><cell>6</cell><cell>all (108)</cell><cell>49.70?0.17</cell><cell>64.51?0.13</cell></row><row><cell>baseline++ (s)</cell><cell>True</cell><cell>-</cell><cell>64</cell><cell>56.48?0.17</cell><cell>74.00?0.13</cell></row><row><cell>baseline++ (f)</cell><cell>True</cell><cell>2</cell><cell>all (12)</cell><cell>57.79?0.17</cell><cell>75.50?0.12</cell></row><row><cell>baseline++ (f)</cell><cell>True</cell><cell>4</cell><cell>all (48)</cell><cell>58.41?0.17</cell><cell>76.01?0.12</cell></row><row><cell>baseline++ (f)</cell><cell>True</cell><cell>6</cell><cell>all (108)</cell><cell>58.98?0.17</cell><cell>75.39?0.12</cell></row><row><cell>baseline++ (f)</cell><cell>False</cell><cell>4</cell><cell>24</cell><cell>50.11?0.17</cell><cell>63.91?0.13</cell></row><row><cell>baseline++ (f)</cell><cell>False</cell><cell>6</cell><cell>24</cell><cell>50.72?0.18</cell><cell>64.86?0.13</cell></row><row><cell>baseline++ (f)</cell><cell>False</cell><cell>8</cell><cell>24</cell><cell>51.04?0.18</cell><cell>65.76?0.13</cell></row><row><cell>baseline++ (f)</cell><cell>True</cell><cell>4</cell><cell>24</cell><cell>58.02?0.18</cell><cell>75.73?0.13</cell></row><row><cell>baseline++ (f)</cell><cell>True</cell><cell>6</cell><cell>24</cell><cell>57.74?0.17</cell><cell>75.66?0.12</cell></row><row><cell>baseline++ (f)</cell><cell>True</cell><cell>8</cell><cell>24</cell><cell>58.25?0.18</cell><cell>76.23?0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table VI :</head><label>VI</label><figDesc>Results on miniImageNet with different backbone networks when we implement different input versions, (s), (f) and (s+f), as shown inFigure 1to baseline++. The highest accuracy (%) is highlighted.</figDesc><table><row><cell>backbone</cell><cell>method</cell><cell cols="2">accuracy on miniImageNet 1-shot 5-shot</cell></row><row><cell></cell><cell>baseline++ (s)</cell><cell cols="2">57.52?0.17 75.56?0.13</cell></row><row><cell>ResNet10</cell><cell>baseline++ (f)</cell><cell cols="2">59.61?0.18 76.61?0.12</cell></row><row><cell></cell><cell cols="3">baseline++ (s+f) 62.30?0.18 79.93?0.12</cell></row><row><cell></cell><cell></cell><cell>+4.78</cell><cell>+4.37</cell></row><row><cell></cell><cell>baseline++ (s)</cell><cell cols="2">56.48?0.17 74.00?0.13</cell></row><row><cell>ResNet18</cell><cell>baseline++ (f)</cell><cell cols="2">58.52?0.18 76.23?0.13</cell></row><row><cell></cell><cell cols="3">baseline++ (s+f) 61.66?0.18 79.70?0.12</cell></row><row><cell></cell><cell></cell><cell>+5.18</cell><cell>+5.70</cell></row><row><cell></cell><cell>baseline++ (s)</cell><cell cols="2">57.94?0.18 73.98?0.13</cell></row><row><cell>ResNet34</cell><cell>baseline++ (f)</cell><cell cols="2">59.22?0.18 76.58?0.13</cell></row><row><cell></cell><cell cols="3">baseline++ (s+f) 62.75?0.18 79.73?0.12</cell></row><row><cell></cell><cell></cell><cell>+4.81</cell><cell>+5.75</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The work was supported in part by The National Aeronautics and Space Administration (NASA) under grant no. 80NSSC20M0160.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep feature augmentation for occluded image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">107737</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Branch-and-Pruning Optimization Towards Global Optimality in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01730</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SOSD-Net: Joint semantic object segmentation and depth estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Location-aware box reasoning for anchor-based single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="129300" to="129309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multi-Resolution Fusion and Multi-scale Input Priors Based Crowd Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sajid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01664</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Meaning and compositionality as statistical induction of categories and constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">560</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Few-shot learning via saliency-guided hallucination of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2770" to="2779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8680" to="8689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lowshot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7278" to="7286" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1199" to="1208" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coloring with limited data: Few-shot colorization via memory augmented networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning of human visual sensitivity in image quality assessment framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1676" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning in the Frequency Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discrete cosine transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="93" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale few-shot learning: Knowledge transfer with class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7212" to="7220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fewshot image recognition with knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="441" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9715" to="9724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On first-order metalearning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Packing convolutional neural networks in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2495" to="2510" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Compressing convolutional neural networks in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1475" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Band-limited training and inference for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dziedzic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paparrizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09287</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster neural networks straight from jpeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gueguen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3933" to="3944" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shifted spatial-spectral convolution for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Asia</title>
		<meeting>the ACM Multimedia Asia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08136</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5822" to="5830" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Leveraging the feature distribution in transfer-based few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03806</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial feature hallucination networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="470" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dpgn: Distribution propagation graph network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="390" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Van</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
