<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Neural Networks for Automatic Meter Reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayson</forename><surname>Laroca</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Laboratory of Vision, Robotics and Imaging</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<addrLine>Av. Coronel Francisco Her?clito dos Santos 100</addrLine>
									<postCode>81530-000</postCode>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Barroso</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Laboratory of Vision, Robotics and Imaging</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<addrLine>Av. Coronel Francisco Her?clito dos Santos 100</addrLine>
									<postCode>81530-000</postCode>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><forename type="middle">A</forename><surname>Diniz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Smart Surveillance Interest Group</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<addrLine>Av. Ant?nio Carlos 6627, Belo Horizonte</addrLine>
									<postCode>31270-010</postCode>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Smart Surveillance Interest Group</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<addrLine>Av. Ant?nio Carlos 6627, Belo Horizonte</addrLine>
									<postCode>31270-010</postCode>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Robson Schwartz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Smart Surveillance Interest Group</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<addrLine>Av. Ant?nio Carlos 6627, Belo Horizonte</addrLine>
									<postCode>31270-010</postCode>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Menotti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Laboratory of Vision, Robotics and Imaging</orgName>
								<orgName type="institution">Federal University of Paran?</orgName>
								<address>
									<addrLine>Av. Coronel Francisco Her?clito dos Santos 100</addrLine>
									<postCode>81530-000</postCode>
									<settlement>Curitiba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Neural Networks for Automatic Meter Reading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1117/1.JEI.28.1.013023)</idno>
					<note>This is an author-prepared version. The official version is available in the SPIE digital library (</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>automatic meter reading</term>
					<term>convolutional neural networks</term>
					<term>deep learning</term>
					<term>public dataset *Rayson Laroca, rblsantos@infufprbr</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we tackle Automatic Meter Reading (AMR) by leveraging the high capability of Convolutional Neural Networks (CNNs). We design a two-stage approach that employs the Fast-YOLO object detector for counter detection and evaluates three different CNN-based approaches for counter recognition. In the AMR literature, most datasets are not available to the research community since the images belong to a service company. In this sense, we introduce a new public dataset, called UFPR-AMR dataset, with 2,000 fully and manually annotated images. This dataset is, to the best of our knowledge, three times larger than the largest public dataset found in the literature and contains a well-defined evaluation protocol to assist the development and evaluation of AMR methods. Furthermore, we propose the use of a data augmentation technique to generate a balanced training set with many more examples to train the CNN models for counter recognition. In the proposed dataset, impressive results were obtained and a detailed speed/accuracy trade-off evaluation of each model was performed. In a public dataset, state-of-the-art results were achieved using less than 200 images for training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic Meter Reading (AMR) refers to automatically record the consumption of electric energy, gas and water for both monitoring and billing. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> Despite the existence of smart readers, <ref type="bibr" target="#b3">4</ref> they are not widespread in many countries, especially in the underdeveloped ones, and the reading is still performed manually on site by an operator who takes a picture as reading proof. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5</ref> Since this operation is prone to errors, another operator needs to check the proof image to confirm the reading. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5</ref> This offline checking is expensive in terms of human effort and time, and has low efficiency. <ref type="bibr" target="#b5">6</ref> Moreover, due to a large number of images to be evaluated, the inspection is usually done by sampling <ref type="bibr" target="#b6">7</ref> and errors might go unnoticed.</p><p>Performing the meter inspection automatically would reduce mistakes introduced by the human factor and save manpower. Furthermore, the reading could also be executed fully automatically arXiv:1902.09600v1 [cs.CV] 25 Feb 2019 using cameras installed in the meter box. <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8</ref> Image-based AMR has advantages such as lower cost and fast installation since it does not require renewal or replacement of existent meters. <ref type="bibr" target="#b8">9</ref> A common AMR approach includes three phases, namely: (i) counter detection, (ii) digit segmentation and (iii) digit recognition. Counter detection is the fundamental stage, as its performance largely determines the overall accuracy and processing speed of the entire AMR system.</p><p>Despite the importance of a robust AMR system and that major advances have been achieved in computer vision using deep learning, <ref type="bibr" target="#b9">10</ref> to the best of our knowledge, only in Ref. <ref type="bibr">11, published</ref> very recently, Convolutional Neural Networks (CNNs) were employed at all AMR stages. Previous works relied, in at least one stage, on handcrafted features that capture certain morphological and color attributes of the meters/counters. These features are easily affected by noise and might not be robust to different types of meters.</p><p>Deep learning approaches are particularly dependent on the availability of large quantities of training data to generalize well and yield high classification accuracy for unseen data. <ref type="bibr" target="#b11">12</ref> Some previous works 2, 6, 11 employed large datasets (e.g., more than 45,000 images) to train and evaluate their systems. However, these datasets were not made public. In the AMR literature, the datasets are usually not publicly available since the images belong to the [electricity, gas, water] company.</p><p>In this sense, we introduce a new public dataset, called UFPR-AMR dataset, with 2,000 fully annotated images to assist the development and evaluation of AMR methods. The proposed dataset is three times larger than the largest public dataset <ref type="bibr" target="#b12">13</ref> found in the literature.</p><p>In this paper, we design a two-stage approach for AMR. We first detect the counter region and then tackle the digit segmentation and recognition stages jointly by leveraging the high capability of CNNs. We employ a smaller version of the YOLO object detector, called Fast-YOLO, <ref type="bibr" target="#b13">14</ref> for counter detection. Afterward, we evaluate three CNN-based approaches, i.e. CR-NET, <ref type="bibr" target="#b14">15</ref> Multi-Task Learning <ref type="bibr" target="#b15">16</ref> and Convolutional Recurrent Neural Network (CRNN), <ref type="bibr" target="#b16">17</ref> for the counter recognition stage (i.e., digit segmentation and recognition). CR-NET is a YOLO-based model proposed for license plate character detection and recognition, while Multi-Task and CRNN are segmentation-free approaches designed respectively for the recognition of license plates and scene text. These approaches were chosen since promising results have been achieved through them in these applications. Finally, we propose the use of a data augmentation process to train the CNN models for counter recognition to explore different types of counter/digit deformations and their influence on the models' performance.</p><p>The experimental evaluation demonstrates the effectiveness of the CNN models for AMR.</p><p>First, all counter regions were correctly located through Fast-YOLO in the proposed dataset and also in two public datasets found for this task. <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13</ref> Second, the CR-NET model yielded promising recognition results, outperforming both Multi-Task and CRNN models in the UFPR-AMR dataset.</p><p>Finally, an impressive recognition rate of 97.30% was achieved using Fast-YOLO and CR-NET in a set of images proposed for end-to-end evaluations of AMR systems, called Meter-Integration subset, <ref type="bibr" target="#b4">5</ref> against 85% and 87% achieved by the baselines. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5</ref> In addition, the CR-NET and Multi-Task models are able to achieve outstanding frames per second (FPS) rates in a high-end GPU, being possible to process respectively 185 and 250 FPS.</p><p>Considering the aforementioned discussion, the main contributions of our work are summarized as follows:</p><p>? A two-stage AMR approach with CNNs being employed for both counter detection and recognition. In the latter, three different types of CNN are evaluated;</p><p>? A public dataset for AMR with 2,000 fully and manually annotated images/meters (i.e., 10,000 digits) with a well-defined evaluation protocol, allowing a fair comparison between different approaches for this task;</p><p>? The CNN-based approaches outperformed all baselines in public datasets and achieved impressive results in both accuracy and computational time in the proposed UFPR-AMR dataset.</p><p>The remainder of this paper is organized as follows. We briefly review related works in Section 2. The UFPR-AMR dataset is introduced in Section 3. The methodology is presented in Section 4. We report and discuss the results in Section 5. Conclusions and future work are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>AMR intersects with other Optical Character Recognition (OCR) applications, such as license plate recognition <ref type="bibr" target="#b17">18</ref> and robust reading, <ref type="bibr" target="#b18">19</ref> as it must reliably extract text information from images taken under different conditions. Although AMR is not as widespread in the literature as these applications, a satisfactory number of works have been produced in recent years. <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20</ref> Here, we briefly survey these works by first describing the approaches employed for each AMR stage.</p><p>Next, we present some papers that address two stages jointly or using the same method. Then, we discuss the deep learning approaches and datasets used so far. Finally, we conclude this section with final remarks.</p><p>Counter Detection: Many pioneering approaches exploited the vertical and horizontal pixel projections histograms for counter detection. <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21</ref> Projection-based methods can be easily affected by the rotation of the counter. Refs. 2, 6, 7, 13, 20, 22 took advantage of prior knowledge such as counter's position and/or its colors (e.g., green background and red decimal digits). A major drawback of these techniques is that they might not work on all meter types and the color information might not be stable when the illumination changes. Other works include the use of template matching 7 and the AdaBoost classifier. <ref type="bibr" target="#b2">3</ref> In the latter, normalized gradient magnitude, Histogram of Oriented Gradients (HOG) and LUV color channels were adopted as low-level feature descriptors.</p><p>Digit Segmentation: Projection and color-based approaches have also been widely employed for digit segmentation. <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23</ref>  Digit Recognition: Template matching <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> along with simple measures of similarity have been widely used for digit recognition. Nevertheless, it is known that if a digit is different from the template due to any font change, rotation or noise, this approach produces incorrect recognition. <ref type="bibr" target="#b17">18</ref> Thus, many authors have employed an SVM classifier for digit recognition. In Refs. <ref type="bibr" target="#b4">5</ref> AMR presents an unusual challenge in OCR: rotating digits. Typically, this is the major cause of errors, even when robust approaches are employed for digit recognition. <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26</ref> In Ref. <ref type="bibr">23, this</ref> problem was addressed using a Hausdorff distance based algorithm, achieving excellent recognition results in real time. Note that all images were extracted from a single meter and, as pointed out by the authors, a controlled environment was required since there were no preprocessing stage and no algorithm for angle correction.  <ref type="bibr" target="#b24">25</ref>. In summary, a watershed algorithm was applied to improve counter detection and Fourier analysis was employed to avoid false positives in digit segmentation. Although better results were attained, only 100 images were used to evaluate their system performance, which may not be representative enough. It should be noted that, to the best of our knowledge, this was the first work to make the images used in the experiments publicly available. Gao et al. <ref type="bibr" target="#b2">3</ref> designed a bidirectional long short-term memory (LSTM) network for counter recognition. In their approach, a feature sequence is first generated by a network that combines convolutional and recurrent layers. Then, an attention decoder predicts, recurrently, one digit at each step according to the feature representation. A promising accuracy rate was reported, with most of the errors appearing in cases of half digits.</p><p>G?mez et al. <ref type="bibr" target="#b10">11</ref> presented a segmentation-free AMR system able to output readings directly without explicit counter detection. A CNN architecture was trained in an end-to-end manner where the initial convolutional layers extract visual features of the whole image and the fully connected layers predict the probabilities for each digit. Even though an impressive overall accuracy was achieved, their approach was evaluated only on a large private dataset which has almost 180k training samples and mostly images with the counter well centered and occupying a good portion of the image. Thus, as pointed out by the authors, small-meter images pose difficulties to their system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>To the best of our knowledge, only Refs. 5, 13 made available the datasets used in their experiments. These datasets are composed of gas meter images with resolution of 640 ? 480 pixels (mostly) and the counter occupying a large portion of the image, which facilitates its detection.</p><p>Additionally, both datasets are small (253 and 640 images, respectively) and the cameras used to capture them were not specified. It is important to note that in the dataset introduced in Ref. <ref type="bibr" target="#b4">5,</ref><ref type="bibr">153</ref> images are divided into different subsets for the evaluation of each stage and only 100 images are for the end-to-end evaluation of the AMR system. Also, there is no split protocol in Ref. <ref type="bibr" target="#b12">13</ref>, which prevents a fair comparison between different approaches.</p><p>Deep Learning: Recently, deep learning approaches have won many machine learning competitions and challenges, even achieving superhuman visual results in some domains. <ref type="bibr" target="#b26">27</ref> Such a fact motivated us to employ deep learning for AMR, since we could find only three works 3, 6, 11 employing CNNs in this context and all of them made use of large private datasets, overlooking the public datasets. This suggests that these models are able to generalize only with many training samples Final Remarks: The approaches developed for AMR are still limited. In addition to the aforementioned points (i.e., private datasets and handcrafted features), many authors do not report the computational time of their approaches, making it difficult an accurate analysis of their speed/accuracy trade-off, as well as their applicability. In this paper, CNNs are used for both counter detection and recognition. We evaluate the CNNs that achieved state-of-the-art results in other applications in both the proposed and public datasets, reporting the accuracy and the computational time to enable fair comparisons in future works.</p><p>3 The UFPR-AMR Dataset <ref type="bibr">Fig</ref> 1 Sample images of the UFPR-AMR dataset (some images were slightly resized for display purposes). Note the diversity of meter types and conditions, as well as the existence of several textual blocks similar to the counter region.</p><p>The proposed dataset contains 2,000 images taken from inside a warehouse of the Energy Company of Paran? <ref type="bibr">(Copel)</ref>, which directly serves more than 4 million consuming units in the Brazilian state of Paran?. <ref type="bibr" target="#b27">28</ref> Therefore, our dataset presents electric meters of different types and in different conditions. The diversity of the dataset is shown in <ref type="figure">Fig. 1</ref>. One can see that (i) the counter occupies a small portion in the image, which makes its location more difficult; (ii) there are several similar textual blocks (e.g., meter specifications and serial number) to the counter region.</p><p>The UFPR-AMR dataset is publicly available to the research community at https://web.inf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ufpr.br/vri/databases/ufpr-amr/.</head><p>Meter images commonly have some artifacts (e.g., blur, reflections, low contrast, broken glass, dirt, among others) due to the meter's conditions and the misuse of the camera by the human operator, which may impair the reading of electric energy consumption. In addition, it is possible that the digits are rotating or in-between positions (e.g., a digit going from 4 to 5) in some types of counters. In such cases, we consider the lowest digit as the ground truth, since this is the protocol adopted at Copel. The exception, to have a reasonable rule, is between digits 9 and 0, where it should be labeled as 9.</p><p>The images were acquired with three different cameras and are available in the JPG format with resolution between 2,340 ? 4,160 and 3,120 ? 4,160 pixels. The cameras used were: LG G3 D855, Samsung Galaxy J7 Prime and iPhone 6s. As the cameras (cell phones) belong to different price ranges, the images presumably have different levels of quality. Additional information can be seen in <ref type="table">Table 1</ref>.</p><p>Every image has the following annotations available in a text file: the camera in which the image was taken, the counter position (x, y, w, h), the reading, as well as the position of each digit. <ref type="table">Table 1</ref> Additional information about the UFPR-AMR dataset: (a) how many images were captured with each camera; (b) dimensions of counters and digits (width ? height in pixels). It is noteworthy the large variation in the sizes of counters and digits.</p><p>(a)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera Images</head><p>LG <ref type="formula">G3</ref>  All counters of the dataset (regardless of meter type) have 5 digits, and thus 10,000 digits were manually annotated.</p><p>Remark that a brand new meter starts with 00000 and the most significant digit positions take longer to be increased. Then, it is natural that the less significant digits (i.e., 0 and 1) have many more instances than the others. Nonetheless, digits 4-9 have a fairly similar number of examples.  The dataset is split into three sets: training (800 images), validation (400 images) and test (800 images). We adopt this protocol (i.e., with a larger test set) since it has already been adopted in other datasets <ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30</ref> and to provide more samples for analysis of statistical significance. It should be noted that this division was made randomly and the sets generated are explicitly available along with the UFPR-AMR dataset. Additionally, experiments carried out by us suggested that dividing the dataset multiple times and then averaging the results is not necessary, as the proposed division is representative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>Meters have many textual blocks that can be confused with the counter's reading. Moreover, the Region of Interest (ROI) (i.e., the counter) usually occupies a small portion of the image and its position varies according to the meter type. Therefore, we propose to first locate the counter region and then perform its recognition in the detected patch. We tackle both stages by leveraging the high capability of state-of-the-art CNNs. It is remarkable that, to the best of our knowledge, this is only the second work in which both stages are addressed using CNNs 11 and the first with the experiments being performed on public datasets.</p><p>In the following sections, we describe the CNN models employed for counter detection and counter recognition. It is worth noting that all parameters (e.g., CNNs input size, number of epochs, among others) specified here are defined based on the validation set and presented in Section 5,</p><p>where the experiments are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Counter Detection</head><p>Recently, great progress has been made in object detection through models inspired by YOLO, <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32</ref> a CNN-based object detection system that (i) reframes object detection as a single regression problem; (ii) achieved outstanding and state-of-the-art results in the PASCAL VOC and COCO detection tasks. <ref type="bibr" target="#b32">33</ref> For that reason, we decided to fine-tune it for counter detection. However, as we want to detect only one class and the computational cost is one of our main concerns, we chose to use a smaller model, called Fast-YOLO, <ref type="bibr" target="#b13">14</ref> which uses fewer convolutional layers than YOLO and fewer filters in those layers. Despite being smaller, Fast-YOLO (architecture shown in <ref type="table" target="#tab_3">Table 2</ref>) yielded outstanding results, i.e. detections with Intersection over Union (IoU) ? 0.8</p><p>with the ground truth, in preliminary experiments. The IoU is often used to assess the quality of predictions in object detection tasks <ref type="bibr" target="#b33">34</ref> and can be expressed by the formula</p><formula xml:id="formula_0">IoU = area(B p ? B gt ) area(B p ? B gt ) ,<label>(1)</label></formula><p>where B p and B gt are the predicted and ground truth bounding boxes, respectively. The closer the IoU is to 1, the better the detection. For this reason, we believe that very deep models are not necessary to handle the detection of a single class of objects.</p><p>For counter detection, we use the weights pre-trained on ImageNet <ref type="bibr" target="#b34">35</ref> and perform two minor changes in the Fast-YOLO model. First, we recalculate the anchor boxes for the UFPR-AMR dataset using the algorithm available in Ref. <ref type="bibr" target="#b35">36</ref>. Anchors are initial shapes that serve as references at multiple scales and aspect ratios. Instead of predicting arbitrary bounding boxes, YOLO only adjusts the size of the nearest anchor to the size of the object. Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn. <ref type="bibr" target="#b32">33</ref> Then, we reduce the number of filters in the last convolutional layer from 125 to 30 to output 1 class instead of 20. The number of filters in the last layer is given by</p><formula xml:id="formula_1">filters = (C + 5) ? A ,<label>(2)</label></formula><p>where A is the number of anchor boxes (we use A = 5) used to predict bounding boxes. Each bounding box has four coordinates (x, y, w, h), a objectness value 37 (i.e., how likely the bounding box contains an object) along with the probability of that object belonging to each of the C classes, in our case C = 1 (i.e., only the counter region). <ref type="bibr" target="#b32">33</ref> Remark that the choice of appropriate anchor boxes is very important, and thus our boxes are similar to counters in size and aspect ratio.</p><p>We employ Fast-YOLO's multi-scale training. <ref type="bibr" target="#b32">33</ref> In short, every 10 batches, the network randomly chooses a new image dimension size from 320 ? 320 to 608 ? 608 pixels (default values).</p><p>These dimensions were chosen considering that the Fast-YOLO model down samples the image by a factor of 32. As pointed out in Ref. <ref type="bibr" target="#b32">33</ref>, this approach forces the network to learn to predict well across a variety of input dimensions. Then, we use 416 ? 416 images as input since the best results (speed/accuracy trade-off in the validation set) were obtained with this dimension as input. It is remarkable that, although YOLO networks have a 1 : 1 input aspect ratio, previous works <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30</ref> have attained excellent object detection results (over 99% recall) in images with different aspect ratios (e.g., 1,920 ? 1,080). All image resizing operations were performed using bilinear interpolation.</p><p>In cases where more than one counter is detected, we consider only the detection with the highest confidence since each image/meter has only one counter. To avoid losing digits in cases where the counter is not very well detected, we add a margin (with size chosen based on the validation set) on the detected patch so that all digits are within it for the recognition stage. A negative recognition result is given in cases where no counter is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Counter Recognition</head><p>We employ three CNN-based approaches for performing counter recognition: CR-NET, <ref type="bibr" target="#b14">15</ref> Multi-Task Learning <ref type="bibr" target="#b15">16</ref> and CRNN. <ref type="bibr" target="#b16">17</ref> These models were chosen because promising results were obtained through them in other OCR applications, such as license plate recognition and scene text recognition. It is noteworthy that, unlike CR-NET, the last two models do not need the coordinates of each digit in the training phase. In other words, Multi-Task Learning and CRNN approaches only need the counter's reading. This is of paramount importance in cases where a large number of images is available for learning (e.g., millions or hundreds of thousands), since manually labeling each digit is very costly and prone to errors.</p><p>The remainder of this section is organized into four parts, one to describe the data augmentation method, which is essential to effectively train the deep models, and one part for each CNN approach employed for counter recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Data Augmentation</head><p>It is well known that unbalanced data is undesirable for neural network classifiers since the learning of some patterns might be biased. For instance, some classifiers may learn to always classify the first digit as 0, but this is not always the case (see <ref type="figure" target="#fig_2">Fig. 2</ref>), although it is by far the most common. To address this issue, we employ the data augmentation technique proposed in Ref. <ref type="bibr" target="#b15">16</ref>.</p><p>Using this technique, we are able to create a new set of images, where each digit class is equally represented in every position. This set consists of permutations of the original images. The order and frequency of the digits in the generated counters are chosen to uniformly distribute the digits along the positions. Note that the location of each digit (i.e., its bounding box) is required to apply this data augmentation technique.</p><p>Some artificially generated images when applying the method in the UFPR-AMR dataset are shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. We also perform random variations of brightness, rotation and crop coordinates to increase even more the robustness of our augmented images, creating new training examples for the CNNs. As can be seen, the data augmentation approach works on different types of meters.</p><p>The adjustment of parameters is of paramount importance for the effectiveness of this technique since the presence of very large variations in brightness, rotation or cropping, for instance, might impair the recognition through the generation of images that do not match real scenarios.</p><p>Therefore, the parameter ranges were empirically determined based on experiments performed on the validation set, i.e., brightness variation of the pixels [0.5; 2], rotation angles between ?5 ? and 5 ? and cropping from ?2% to 8% of the counter size. Once these ranges were established, new counter images were generated using random values within those ranges for each parameter. The CR-NET architecture is shown in <ref type="table" target="#tab_4">Table 3</ref>. As in the counter detection stage, we recalculate the anchors for our data and make adjustments in the number of filters in the last layer. Furthermore, we adapt the input image size taking into account the aspect ratio of the counters, which have a different aspect ratio when compared to license plates in Ref. <ref type="bibr" target="#b14">15</ref>. Then, we use as input an image with resolution of 400 ? 106 pixels since the results obtained when using other sizes (e.g.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>360</head><p>? 95 and 440 ? 116) were worse or similar, but with a higher computational cost.</p><p>We consider only the five digits detected/recognized with highest confidence, since commonly more than five digits are predicted. However, we noticed that the same digit might be detected more than once by the network. Therefore, we first apply a non-maximal suppression algorithm to eliminate redundant detections. Although highly unlikely (i.e., ? 0.1%), it is also possible that less than five digits are detected by the CR-NET, as shown in <ref type="figure" target="#fig_5">Fig 4.</ref> In such cases, we reject the counter's recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Multi-Task Learning</head><p>Multi-Task Learning is another approach for character string recognition developed for license plates. <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b37">38</ref> This method skips the character segmentation stage and directly recognizes the character string of an image (here, the cropped counter). Since there might be multiple characters, each character is modeled as a task on the network.</p><p>For the UFPR-AMR dataset, we use a similar architecture adding the restraint that each character must be a digit, transforming the output space from 36 (their work considers numbers and letters) to 10 for each digit. The architecture holistically segments and recognizes all five characters due to its multi-task output. <ref type="table" target="#tab_5">Table 4</ref> shows the architecture of the model, which is very compact with only 4 convolutional layers followed by a fully connected shared layer and two fully connected layers for each digit, indexed from 1 to 5. Each output represents the classification of one of the digits. Thus, no explicit segmentation is performed in this approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Convolutional Recurrent Neural Network</head><p>CRNN 17 is a model designed for scene text recognition that consists of convolutional layers followed by recurrent layers, in addition to a custom transcription layer to convert the per-frame predictions into a label sequence. Given the counter patch, containing the digits, the convolutional layers act as a feature extractor, which is then transformed into a sequence of feature vectors and fed into an LSTM 39 recurrent layer. This layer handles the input as a sequence labeling problem, predicting a label distribution y = y 1 , y 2 , ..., y t for each feature vector x = x 1 , x 2 , ..., x t from the feature map.</p><p>The Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b39">40</ref> cost function is adopted for sequence decoding. The CTC has a softmax layer with a label more than the original 10 digits. The activation of each feature vector corresponds to a unique label that can be one of the ten digits or a 'blank' (i.e., the absence of digit). Thus, this model is able to predict a variable number of digits, differently from Multi-Task where 5 digits are always predicted. As the classification is done through the whole feature map from the convolutional layers, digit segmentation is not required.</p><p>We evaluate different network architectures with variations in the input size and in the number of filters and convolutional layers. As shown in <ref type="table" target="#tab_6">Table 5</ref>, the input size is 160 ? 40 pixels and there are only one LSTM layer (instead of two, as in Ref. 17) since the best results (considering the speed/accuracy trade-off) in the validation set were obtained with these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we report the experiments carried out to verify the effectiveness of the CNN-based methods in the UFPR-AMR dataset and also in public datasets. All experiments were performed We first assess counter detection since the counter regions used for recognition are from the detection results, rather than cropped directly from the ground truth. This is done to provide a realistic evaluation of the entire AMR system, where well-performed counter detection is essential to achieve outstanding recognition results. Next, each approach for counter recognition is evaluated and a comparison between them is presented.</p><p>Counter detection is evaluated in the UFPR-AMR dataset and also in two public datasets, <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13</ref> while counter recognition is assessed only in the UFPR-AMR dataset. This is because (i) two different sets of images were used to evaluate digit segmentation and recognition in Ref. <ref type="bibr" target="#b4">5</ref>, and thus it is not possible to use these sets in the counter recognition approaches (where these stages are performed jointly); (ii) Ref. <ref type="bibr" target="#b12">13</ref> performed digit recognition on a subset of their dataset which was not made publicly available.</p><p>We will finally evaluate the entire AMR pipeline in a subset of 100 images (640 ? 480) taken from the public dataset introduced by Vanetti et al. <ref type="bibr" target="#b4">5</ref> This subset, called Meter-Integration, was used to perform an overall evaluation of the AMR methods proposed in Refs. 2,5. It should be noted that other subsets of the dataset, containing different images, were used to evaluate each AMR stage independently and the training images (in the overall evaluation) are from these subsets. <ref type="bibr" target="#b4">5</ref> Aiming at a fair comparison, we employ the same protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Counter Detection</head><p>For evaluating counter detection, we employ the bounding box evaluation defined in the PASCAL VOC Challenge, <ref type="bibr" target="#b33">34</ref> where the predicted bounding box is considered to be correct if its IoU with the ground truth is greater than 50% (IoU &gt; 0.5). This metric was also used in previous works, <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25</ref> being interesting once it penalizes both over-and under-estimated objects.</p><p>According to the detection evaluation described above, the network correctly detected 99.75%</p><p>of the counters with an average IoU of 83%, failing to locate the counter in just two images (798/800). However, in these two cases, it is still possible to recognize the digits from the detected counters, since they were actually detected (with IoU ? 0.5) and all digits are within the ROI after adding a margin (as explained in Section 4.1). In the validation set, a margin of 20%</p><p>(of the bounding box size) is required so that all digits are within the ROI. Thus, we applied a 20% margin in the test set as well. <ref type="figure" target="#fig_6">Fig. 5</ref> shows both cases where the counters were detected with IoU ? 0.5 before and after adding this margin. Note that, in this way, all counter digits are within the located region using Fast-YOLO.</p><p>Some detection results achieved by the Fast-YOLO model are shown in <ref type="figure" target="#fig_7">Fig. 6</ref>. As can be seen, well-located predictions were attained on counters of different types and under different conditions.</p><p>In terms of computational speed, the Fast-YOLO model takes about 3.30 ms per image (303  with steps at 25k and 35k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Counter Detection on Public Datasets</head><p>To demonstrate the robustness of Fast-YOLO for counter detection, we employ it on the public datasets found in the literature <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13</ref> and compare the results with those reported in previous works.</p><p>Vanetti et al. <ref type="bibr" target="#b4">5</ref> employed a subset of 153 images of their dataset specially for the evaluation of counter detection, being 102 for training and 51 for testing. In Ref. <ref type="bibr" target="#b12">13</ref>, a larger dataset (with 640 images) was introduced, but no split protocol was defined.</p><p>As the dataset introduced in Ref. 5 has a split protocol, we employed the same division in our experiments. We randomly removed 20 images from the training set and used them as validation.</p><p>For the experiments performed in the dataset introduced in Ref. <ref type="bibr" target="#b12">13</ref>, we perform 5-fold crossvalidation with images assigned to folds randomly in order to achieve a fair comparison. Thus, in each run, we used 384 images (60%) for training and 128 images (20%) for each validation and testing, i.e., a 3/1/1 split protocol.</p><p>As mentioned in the related work section, both datasets are composed of gas meter images.</p><p>Such a fact is relevant since gas meters usually have red decimal digits that should be discarded in the reading process. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13</ref> Therefore, we manually labeled, in each image, a bounding box containing only the significant digits for training Fast-YOLO. These annotations are also publicly available to the research community at https://web.inf.ufpr.br/vri/databases/ ufpr-amr/. Nodari &amp; Gallo <ref type="bibr" target="#b24">25</ref> ? 70.00% <ref type="bibr">Gon?alves 13</ref> 96.09% 88.24% Vanetti et al. <ref type="bibr" target="#b4">5</ref> ? 96.00% Fast-YOLO 100.00% 100.00%</p><p>Fast-YOLO (IoU &gt; 0.7) 98.59% 92.16%</p><p>The Fast-YOLO model correctly detected 100% of the counters in both datasets, outperforming the results obtained in previous works, as shown in <ref type="table" target="#tab_7">Table 6</ref>. It is noteworthy the outstanding IoU values attained: on average 83.39% in the dataset proposed in Ref. <ref type="bibr" target="#b4">5</ref> and 91.28% in the dataset introduced in Ref. <ref type="bibr" target="#b12">13</ref>. We believe that these excellent results are due to the fact that, in these datasets, the counter occupies a large portion of the image and the meters/counters are quite similar when comparing with the UFPR-AMR dataset. <ref type="figure" target="#fig_8">Fig. 7</ref> shows a counter from each dataset detected using Fast-YOLO. Additionally, we reported the result with a higher detection threshold (i.e., IoU &gt; 0.7). It is remarkable that more than 90% of the counters were located with an IoU (with the ground truth) greater than 0.7 in both datasets. We noticed that the detections with a lower IoU occurred mainly in cases where the meter/counter was inclined or tilted, as illustrated in <ref type="figure" target="#fig_9">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Counter Recognition</head><p>For this experiment, we report the mean of 10 runs for both digit and counter recognition accuracy.</p><p>While the former is the number of correctly recognized digits divided by the number of digits in the test set, the latter is defined as the number of correctly recognized counters divided by the test set size, since each image has only a single meter/counter. Additionally, all CNN models were trained with and without data augmentation, so that we can analyze how data augmentation (described in The recognition rates achieved by all models are shown in <ref type="table" target="#tab_8">Table 7</ref>. We performed statistical paired t-tests at a significance level ? = 0.05, which showed that there is a significant difference in the results obtained with different models. As expected, the results are greatly improved when taking advantage of data augmentation. The best results were achieved with the CR-NET model, which correctly recognized 94.13% of the counters with data augmentation against 92.30% and 87.69% through CRNN and Multi-Task Learning, respectively. This suggests that segmentationfree approaches require a lot of training data to achieve promising recognition rates, as in Ref <ref type="formula" target="#formula_0">. 11</ref> where 177,758 images were used for training. It is important to highlight that it was not possible to recognize any counter when training the Multi-Task model without data augmentation. We performed several experiments reducing the size of the Multi-Task network to verify if a smaller network could learn a better discriminant function. However, better results were not achieved. This is because the dataset is biased and so is the recognition. Even though the first digit has the strongest bias (given the large amount of 0 and 1s in that position), the other digits still have a considerable bias due to the low number of training samples. For example, the Multi-Task network may learn to predict the last digit/task as '5' on every occasion it sees a particular combination of the other digits that is present in the training set.</p><p>In other words, the network may learn correlations between the outputs that do not exist in practice (in other applications this may be beneficial, but in this case it is not). Such a fact explains why the segmentation-free approaches had a higher performance gain with data augmentation, which balanced the training set and eliminated the undesired correlation between the outputs.</p><p>To assess the speed/accuracy trade-off of the three CNN models, we list in <ref type="table" target="#tab_9">Table 8</ref> the time required for each approach to perform the recognition stage. We report the FPS rate achieved by each approach considering only the recognition stage and also considering the detection stage (in parenthesis), which takes about 3.30 ms per image using Fast-YOLO. The reported time is the average time spent processing all images, assuming that the network weights are already loaded.</p><p>For completeness, for each network, we also list the number of parameters as well as the number of billion floating-point operations (BFLOP) required for a single forward pass over a single image. It is worth noting that: (i) even though the Multi-Task network has many more parameters than CR-NET and CRNN, it is still the fastest one; (ii) the CRNN model requires a lower number of floating-point operations for a single forward pass than the CR-NET and Multi-Task networks, however, it is still the model that takes more time to process a single image. In this sense, we claim that there are several factors (in addition to those mentioned above) that affect the time it takes for a network to process a frame, e.g., the input size, its specific characteristics and the framework in which it is implemented. For example, two networks may require exactly the same number of floating-point operations (or have the same number of parameters) and still one be much faster than the other. Although much effort was made to ensure fairness in our experiments, the comparison might not be entirely fair since we used different frameworks to implement the networks and there are probably differences in implementation and optimization between them. The CR-NET model was trained using the Darknet framework, <ref type="bibr" target="#b40">41</ref> whereas the CRNN and Multi-Task models were trained using PyTorch <ref type="bibr" target="#b41">42</ref> and Keras, <ref type="bibr" target="#b42">43</ref> respectively. <ref type="figure" target="#fig_11">Fig. 9</ref> illustrates some of the recognition results obtained in the UFPR-AMR dataset when employing the CR-NET model (i.e., the one with the best accuracy). It is noticeable that the model is able to generalize well and correctly recognize counters from meters of different types and in different conditions. Regarding the errors, we noticed that they occurred mainly due to rotating digits and artifacts in the counter region, such as reflections and dirt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Overall Evaluation on the Meter-Integration Subset</head><p>The Meter-Integration subset <ref type="bibr" target="#b4">5</ref>  and not a fixed number of digits, we adopted a 0.5 confidence threshold (we report it for sake of reproducibility) to deal with a variable number of digits, instead of always considering 5 digits per counter. This threshold was chosen based on 12 validation images (i.e., 20%) randomly taken from the training set. <ref type="table" target="#tab_11">Table 9</ref> shows the results obtained in previous works and using the Fast-YOLO and CR-NET networks for counter detection and recognition, respectively. As expected, the recognition rate accomplished by our deep learning approach was considerably better than those obtained in previous works (87% ? 94.50%), which employed methods based on conventional image processing with handcrafted features. It is noteworthy the ability of both Fast-YOLO and CR-NET models to generalize with very few training images in each stage, i.e., 102 for counter detection and 62 for counter recognition.</p><p>The results were improved when using data augmentation, as in the experiments carried out on the UFPR-AMR dataset. The accuracy achieved was 97.30%, significantly outperforming the baselines. It is worth noting that, on average, only 2-3 counters were incorrectly classified and generally the error occurred in the rightmost digit of the counter. Two samples of errors are shown in <ref type="figure" target="#fig_12">Fig. 10</ref>: the last digit 1 was incorrectly labeled as 0 in one of the cases, probably due to some noise in the image, while in the other case the last digit was detected/recognized with confidence lower than 0.5, apparently due to the m 3 text touching the digit (there were no similar examples in the training set). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we presented a two-stage AMR approach with CNNs being employed for both counter detection and recognition. The Fast-YOLO 14 model was employed for counter detection, while three CNN-based approaches (CR-NET, <ref type="bibr" target="#b14">15</ref> Multi-Task Learning <ref type="bibr" target="#b15">16</ref> and CRNN 17 ) were employed for counter recognition. In addition, we proposed the use of data augmentation for training the CNN models for counter recognition, in order to construct a balanced training set with many more examples.</p><p>We also introduced a public dataset that includes 2,000 images (with 10,000 manually labeled digits) from electric meters of different types and in different conditions, i.e., the UFPR-AMR dataset. It is three times larger than the largest dataset found in the literature for this task and contains a well-defined evaluation protocol, allowing a fair comparison of different methods. Furthermore, we labeled the region containing the significant digits in two public datasets <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13</ref> and these annotations are also publicly available to the research community.</p><p>The counter detection stage was successfully tackled using the Fast-YOLO model, which was able to detect the region containing the significant digits in all images of every dataset evaluated in this work. For counter recognition, the CR-NET model yielded the best recognition results in the UFPR-AMR dataset (i.e., 94.13%), outperforming both Multi-Task and CRNN models which achieved 87.69% and 92.30%, respectively. These results were attained by taking advantage of data augmentation, which was essential to accomplishing promising results. In a public dataset, <ref type="bibr" target="#b4">5</ref> outstanding results (i.e., an overall accuracy of 97.30%) were achieved using less than 200 images for training the Fast-YOLO and CR-NET models, significantly outperforming both baselines.</p><p>The CR-NET and Multi-Task models achieved impressive FPS rates on a high-end graphic card. When considering the time spent in the detection stage, it is possible to process 185 and 250 FPS using the CR-NET and Multi-Task models, respectively. Therefore, these approaches can be employed (taking a few seconds) in low-end setups or even in some mobile phones.</p><p>As future work, we intend to create an extension of the UFPR-AMR dataset with more than 10,000 images of meters of different types and under different conditions acquired by the company's employees to perform a more realistic analysis of deep learning techniques in the AMR context. Additionally, we plan to explore the meter's model in the AMR pipeline and investigate in depth the cases where the counter has rotating digits since this is one of the main causes of errors in AMR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Miscellaneous: Nodari &amp; Gallo 25 exploited an ensemble of Multilayer Perceptron (MLP) networks to perform the counter detection and digit segmentation without preprocessing and postprocessing stages. Since low F-measure rates were achieved, extra techniques were added in Ref. 5, an extension of Ref.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(e.g., 177,758 images in the segmentation-free system proposed in Ref.11). Moreover, (i) conventional image processing with handcrafted features was used in at least one stage in Refs. 3, 6, (ii) the images used in Ref. 3 are mostly sharp and very similar, which does not represent realworld conditions, and (iii) the poor digit segmentation accuracy obtained in Ref. 6, i.e. 81%, through a sequence of conventional image processing methods, discourages its use in real-world applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>shows the distribution of the digits in the UFPR-AMR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig 2</head><label>2</label><figDesc>Frequency distribution of digits in the UFPR-AMR dataset. It is worth noting that the first position (i.e., the most significant) consists almost exclusively of 0s and 1s. On the other hand, the frequency of digits in the other positions is very well balanced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 3</head><label>3</label><figDesc>Data augmentation examples, where the images in the upper-left corner of (a) and (b) are the originals, and the others were generated automatically. In (a) and (b), counters of different types and aspect ratios are shown.4.2.2 CR-NETCR-NET is a YOLO-based model proposed for license plate character detection and recognition.<ref type="bibr" target="#b14">15</ref> This model consists of the first eleven layers of YOLO and four other convolutional layers added to improve non-linearity. In Ref.<ref type="bibr" target="#b14">15</ref>, CR-NET (with an input size of 240 ? 80 pixels) was capable of detecting and recognizing license plate characters at 448 FPS. Laroca et al.<ref type="bibr" target="#b29">30</ref> also achieved great results applying CR-NET for this purpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 4 A</head><label>4</label><figDesc>counter where less than 5 digits were detected/recognized by the CR-NET. We could employ leading zeros (e.g., 4063 ? 04063), however, this could result in a large error in the meter reading.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 5</head><label>5</label><figDesc>Bounding boxes predicted by the Fast-YOLO model before (a) and after (b) adding the margin (20% of the bounding box size).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 6</head><label>6</label><figDesc>Samples of counter detection obtained with the Fast-YOLO model in the UFPR-AMR dataset. FPS). The model was trained using the Darknet framework 41 and the following parameters were used for training the network: 60k iterations (max batches) and learning rate = [10 -3 , 10 -4 , 10 -5 ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig 7</head><label>7</label><figDesc>Examples of counter detection obtained with the Fast-YOLO model. Note that the counter region in the images of the Dataset 13 (left) and Dataset 5 (right) is quite larger than in the UFPR-AMR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig 8</head><label>8</label><figDesc>Samples of counters detected with a lower IoU with the ground truth. (a) and (b) show images of the datasets proposed in Refs. 13 and 5, respectively. The predicted position and ground truth are outlined in red and green, respectively. Observe that all digits would be within the ROI with the addition of a small margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Section 4 . 2 . 1 )</head><label>421</label><figDesc>affects the performance of each model. For a fair comparison, we (i) generated 300,000 images and applied them for training all CNNs (more images were not generated due to hardware limitations); (ii) disabled the Darknet's (hence CR-NET's) built-in data augmentation, which creates a number of images with changed colors (hue, saturation, exposure) randomly cropped and resized; and (iii) evaluated different margin values (less than the 20% applied previously) in the predictions obtained by Fast-YOLO, since each approach might work better with different margin values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig 9</head><label>9</label><figDesc>Results obtained by the CR-NET model in the UFPR-AMR dataset. The first three rows show examples of successfully recognized counters, while the last two rows show samples of incorrectly recognized counters. Some images were slightly resized for display purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig 10</head><label>10</label><figDesc>Incorrect readings obtained with the Fast-YOLO &amp; CR-NET approach, where the last digit was incorrectly classified (left), and the last digit was detected/recognized with a confidence value below the threshold (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The use of morphological operations with Connected Components Analysis (CCA) was considered in Refs. 6, 20. However, it presents the drawback of depending largely on the result of binarization as it cannot segment digits correctly if they are connected or broken. In Ref. 8, a binary digit/non-digit Support Vector Machine (SVM) was applied in a sliding window fashion, while Gallo et al. 2 exploited Maximally Stable Extremal Regions (MSER). In Ref. 2, the MSER algorithm failed to segment digits in images with problems such as bluring and perspective distortions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, 8, simple features such as pixel intensity were used in training, while HOG descriptors were adopted as features in Refs. 2,7. Although some promising results have been attained, it should be noted that it is not trivial to find the appropriate hyper-parameters of SVM classifiers as well as the best features to be extracted. The open-source Tesseract OCR Engine 24 was applied in Refs. 5, 6, 25, however, satisfactory results were not obtained in any of them. Cerman et al. 6 achieved a remarkable improvement in digit recognition when using a CNN inspired by the LeNet-5 architecture instead of Tesseract.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Fast-YOLO network used to detect the counter region.</figDesc><table><row><cell></cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell></row><row><cell>0</cell><cell>conv</cell><cell>16</cell><cell cols="3">3 ? 3/1 416 ? 416 ? 3 416 ? 416 ? 16</cell></row><row><cell>1</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 416 ? 416 ? 16 208 ? 208 ? 16</cell></row><row><cell>2</cell><cell>conv</cell><cell>32</cell><cell cols="3">3 ? 3/1 208 ? 208 ? 16 208 ? 208 ? 32</cell></row><row><cell>3</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 208 ? 208 ? 32 104 ? 104 ? 32</cell></row><row><cell>4</cell><cell>conv</cell><cell>64</cell><cell cols="3">3 ? 3/1 104 ? 104 ? 32 104 ? 104 ? 64</cell></row><row><cell>5</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 104 ? 104 ? 64 52 ? 52 ? 64</cell></row><row><cell>6</cell><cell>conv</cell><cell>128</cell><cell cols="2">3 ? 3/1 52 ? 52 ? 64</cell><cell>52 ? 52 ? 128</cell></row><row><cell>7</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 52 ? 52 ? 128 26 ? 26 ? 128</cell></row><row><cell>8</cell><cell>conv</cell><cell>256</cell><cell cols="3">3 ? 3/1 26 ? 26 ? 128 26 ? 26 ? 256</cell></row><row><cell>9</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 26 ? 26 ? 256 13 ? 13 ? 256</cell></row><row><cell>10</cell><cell>conv</cell><cell>512</cell><cell cols="3">3 ? 3/1 13 ? 13 ? 256 13 ? 13 ? 512</cell></row><row><cell>11</cell><cell>max</cell><cell></cell><cell>2 ? 2/</cell><cell></cell></row></table><note>1 13 ? 13 ? 512 13 ? 13 ? 512 12 conv 1024 3 ? 3/1 13 ? 13 ? 512 13 ? 13 ? 1024 13 conv 1024 3 ? 3/1 13 ? 13 ? 1024 13 ? 13 ? 1024 14 conv 30 1 ? 1/1 13 ? 13 ? 1024 13 ? 13 ? 30 15 detection</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>CR-NET with some modifications for counter recognition: input size of 400 ? 106 pixels and 75 filters in the last layer.</figDesc><table><row><cell></cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell></row><row><cell>0</cell><cell>conv</cell><cell>32</cell><cell cols="3">3 ? 3/1 400 ? 106 ? 3 400 ? 106 ? 32</cell></row><row><cell>1</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 400 ? 106 ? 32 200 ? 53 ? 32</cell></row><row><cell>2</cell><cell>conv</cell><cell>64</cell><cell cols="3">3 ? 3/1 200 ? 53 ? 32 200 ? 53 ? 64</cell></row><row><cell>3</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 200 ? 53 ? 64 100 ? 26 ? 64</cell></row><row><cell>4</cell><cell>conv</cell><cell>128</cell><cell cols="3">3 ? 3/1 100 ? 26 ? 64 100 ? 26 ? 128</cell></row><row><cell>5</cell><cell>conv</cell><cell>64</cell><cell cols="3">1 ? 1/1 100 ? 26 ? 128 100 ? 26 ? 64</cell></row><row><cell>6</cell><cell>conv</cell><cell>128</cell><cell cols="3">3 ? 3/1 100 ? 26 ? 64 100 ? 26 ? 128</cell></row><row><cell>7</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 100 ? 26 ? 128 50 ? 13 ? 128</cell></row><row><cell>8</cell><cell>conv</cell><cell>256</cell><cell cols="3">3 ? 3/1 50 ? 13 ? 128 50 ? 13 ? 256</cell></row><row><cell>9</cell><cell>conv</cell><cell>128</cell><cell cols="3">1 ? 1/1 50 ? 13 ? 256 50 ? 13 ? 128</cell></row><row><cell>10</cell><cell>conv</cell><cell>256</cell><cell cols="3">3 ? 3/1 50 ? 13 ? 128 50 ? 13 ? 256</cell></row><row><cell>11</cell><cell>conv</cell><cell>512</cell><cell cols="3">3 ? 3/1 50 ? 13 ? 256 50 ? 13 ? 512</cell></row><row><cell>12</cell><cell>conv</cell><cell>256</cell><cell cols="3">1 ? 1/1 50 ? 13 ? 512 50 ? 13 ? 256</cell></row><row><cell>13</cell><cell>conv</cell><cell>512</cell><cell cols="3">3 ? 3/1 50 ? 13 ? 256 50 ? 13 ? 512</cell></row><row><cell>14</cell><cell>conv</cell><cell>75</cell><cell cols="2">1 ? 1/1 50 ? 13 ? 512</cell><cell>50 ? 13 ? 75</cell></row><row><cell cols="2">15 detection</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Multi-Task layers and hyperparameters.</figDesc><table><row><cell></cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell></row><row><cell>0</cell><cell>conv</cell><cell>128</cell><cell cols="3">5 ? 5/1 220 ? 60 ? 1 220 ? 60 ? 128</cell></row><row><cell>1</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 220 ? 60 ? 128 110 ? 30 ? 128</cell></row><row><cell>2</cell><cell>conv</cell><cell>128</cell><cell cols="3">3 ? 3/1 110 ? 30 ? 128 110 ? 30 ? 128</cell></row><row><cell>3</cell><cell>conv</cell><cell>192</cell><cell cols="3">3 ? 3/1 110 ? 30 ? 128 110 ? 30 ? 192</cell></row><row><cell>4</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 110 ? 30 ? 192 55 ? 15 ? 192</cell></row><row><cell>5</cell><cell>conv</cell><cell>256</cell><cell cols="3">3 ? 3/1 55 ? 15 ? 192 55 ? 15 ? 256</cell></row><row><cell>6</cell><cell>max</cell><cell></cell><cell cols="2">2 ? 2/2 55 ? 15 ? 256</cell><cell>27 ? 7 ? 256</cell></row><row><cell>7</cell><cell>flatten</cell><cell></cell><cell></cell><cell>27 ? 7 ? 256</cell><cell>48384</cell></row><row><cell></cell><cell>Layer</cell><cell cols="2">Neurons</cell><cell>Input</cell><cell>Output</cell></row><row><cell>8</cell><cell>dense</cell><cell cols="2">4096</cell><cell>48384</cell><cell>4096</cell></row><row><cell cols="2">9 dense[1..5]</cell><cell cols="2">512</cell><cell>4096</cell><cell>512</cell></row><row><cell cols="2">10 dense[1..5]</cell><cell></cell><cell>10</cell><cell>512</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>CRNN layers and hyperparameters.</figDesc><table><row><cell></cell><cell>Layer</cell><cell>Filters</cell><cell>Size</cell><cell>Input</cell><cell>Output</cell></row><row><cell>0</cell><cell>conv</cell><cell>64</cell><cell>3 ? 3/1</cell><cell cols="2">160 ? 40 ? 1 160 ? 40 ? 64</cell></row><row><cell>1</cell><cell>max</cell><cell></cell><cell>2 ? 2/2</cell><cell cols="2">160 ? 40 ? 64 80 ? 20 ? 64</cell></row><row><cell>2</cell><cell>conv</cell><cell>128</cell><cell>3 ? 3/1</cell><cell cols="2">80 ? 20 ? 64 80 ? 20 ? 128</cell></row><row><cell>3</cell><cell>max</cell><cell></cell><cell>2 ? 2/2</cell><cell cols="2">80 ? 20 ? 128 40 ? 10 ? 128</cell></row><row><cell>4</cell><cell>conv</cell><cell>256</cell><cell>3 ? 3/1</cell><cell cols="2">40 ? 10 ? 128 40 ? 10 ? 256</cell></row><row><cell>5</cell><cell>conv</cell><cell>256</cell><cell>3 ? 3/1</cell><cell cols="2">40 ? 10 ? 256 40 ? 10 ? 256</cell></row><row><cell>6</cell><cell>max</cell><cell></cell><cell cols="3">2 ? 2/2 ? 1 40 ? 10 ? 256 40 ? 5 ? 256</cell></row><row><cell>7</cell><cell>conv</cell><cell>512</cell><cell>3 ? 3/1</cell><cell>40 ? 5 ? 256</cell><cell>40 ? 5 ? 512</cell></row><row><cell>8</cell><cell>batch</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>conv</cell><cell>512</cell><cell>3 ? 3/1</cell><cell>40 ? 5 ? 512</cell><cell>40 ? 5 ? 512</cell></row><row><cell cols="2">10 batch</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">11 max</cell><cell></cell><cell cols="2">2 ? 2/2 ? 1 40 ? 5 ? 512</cell><cell>40 ? 2 ? 512</cell></row><row><cell cols="2">12 conv</cell><cell>512</cell><cell cols="2">2 ? 2/2 ? 1 40 ? 2 ? 512</cell><cell>40 ? 1 ? 512</cell></row><row><cell></cell><cell>Layer</cell><cell></cell><cell>Input</cell><cell>Hidden Layer</cell><cell>Output</cell></row><row><cell cols="2">13 LSTM</cell><cell cols="2">512 ? 1 ? 40</cell><cell>256</cell><cell>11</cell></row></table><note>on a computer with an AMD Ryzen Threadripper 1920X 3.5GHz CPU, 32 GB of RAM and an NVIDIA Titan Xp GPU (3,840 CUDA cores and 12 GB of RAM).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 F</head><label>6</label><figDesc>-measure values obtained by Fast-YOLO and previous works in the public datasets found in the literature.</figDesc><table><row><cell>Approach</cell><cell>F-measure Dataset 13 Dataset 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>Recognition rates obtained in the UFPR-AMR dataset using Fast-YOLO for counter detection and each of the CNN models for counter recognition.</figDesc><table><row><cell>Approach</cell><cell>Accuracy (%) Digits Counters</cell></row><row><cell>Multi-Task (original training set)</cell><cell>24.64 ? 0.25 00.00 ? 0.00</cell></row><row><cell>CRNN (original training set)</cell><cell>92.85 ? 0.93 77.75 ? 2.39</cell></row><row><cell>CR-NET (original training set)</cell><cell>97.78 ? 0.17 91.95 ? 0.52</cell></row><row><cell cols="2">Multi-Task (with data augmentation) 95.96 ? 0.25 87.69 ? 0.40</cell></row><row><cell>CRNN (with data augmentation)</cell><cell>97.87 ? 0.21 92.30 ? 0.56</cell></row><row><cell cols="2">CR-NET (with data augmentation) 98.30 ? 0.09 94.13 ? 0.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>Results obtained in the UFPR-AMR dataset and the computational time required for each approach to perform counter recognition. In parentheses is shown the FPS rate when considering the detection stage.The CR-NET and Multi-Task approaches achieved impressive FPS rates. Looking atTable 8, the difference between using each one of them is clear. The CR-NET model achieved an accuracy of 94.13% at 475 FPS, while the Multi-Task model was capable of processing 1437 FPS with a recognition rate of 87.69%. When considering the time spent in the detection stage, it is possible to process 185 and 250 FPS using the CR-NET and Multi-Task models, respectively.</figDesc><table><row><cell cols="4">Approach BFLOP Parameters Time (ms)</cell><cell>FPS</cell><cell>Accuracy (%)</cell></row><row><cell>Multi-Task</cell><cell>3.45</cell><cell>209M</cell><cell>0.6956</cell><cell cols="2">1437 (250) 87.69 ? 0.40</cell></row><row><cell>CRNN</cell><cell>2.50</cell><cell>7M</cell><cell>5.1751</cell><cell cols="2">193 (118) 92.30 ? 0.56</cell></row><row><cell>CR-NET</cell><cell>5.37</cell><cell>3M</cell><cell>2.1071</cell><cell cols="2">475 (185) 94.13 ? 0.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>was used to evaluate the AMR methods proposed in Refs.5, 13.    Thus, we decided to perform experiments on this dataset and compare the results with those obtained in both works. As previously mentioned, the training images are from other subsets of the dataset proposed in Ref.<ref type="bibr" target="#b4">5</ref>. Remark that there are only 102 and 62 training images for counter detection and recognition, respectively.We employ only the CR-NET model in this experiment since it outperformed both Multi-Task and CRNN models in the UFPR-AMR dataset. The mean accuracy of 10 runs is reported for both digit and counter recognition accuracy. As the counters in the training set have from 4 to 7 digits</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc>Results obtained in the Meter-Integration subset by previous works and using Fast-YOLO &amp; CR-NET. Fast-YOLO &amp; CR-NET (original training set) 97.94 ? 0.85 94.50 ? 1.72 Fast-YOLO &amp; CR-NET (data augmentation) 99.56 ? 0.34 97.30 ? 1.42</figDesc><table><row><cell>Approach</cell><cell cols="2">Accuracy (%) Digits Counters</cell></row><row><cell>Gallo et al. 2 (original training set)</cell><cell>?</cell><cell>85.00</cell></row><row><cell>Vanetti et al. 5 (original training set)</cell><cell>?</cell><cell>87.00</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by grants from the National Council for Scientific and Technological De- </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Study of the automatic reading of watt meter based on image processing technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Industrial Electronics and Applications</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2214" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust angle invariant GAS meter reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamberletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Noce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic watermeter digit recognition on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internet Multimedia Computing and Service</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on smart metering and smart grid communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kabalci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Renewable and Sustainable Energy Reviews</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="302" to="318" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gas meter reading from real world images using a multi-net system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vanetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nodari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="519" to="526" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A mobile recognition system for analog energy meter scanning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shalunts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<imprint>
			<publisher>Springer International Publisher</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic consumption reading on electromechanical meters using HoG and SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quintanilha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latin American Conference on Networked and Electronic Media</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support vector machine based automatic electric meter reading system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">C P</forename><surname>Edward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computational Intelligence and Computing Research</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic reading of domestic electric meter: an intelligent device based on image processing and ZigBee/Ethernet communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Real-Time Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cutting sayre&apos;s knot: Reading scene text without segmentation. application to utility meters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusi?ol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IAPR International Workshop on Document Analysis Systems (DAS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks and data augmentation for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="279" to="283" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reconhecimento de d?gitos em imagens de medidores de consumo de g?s natural utilizando t?cnicas de vis?o computacional</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gon?alves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Universidade Tecnol?gica Federal do Paran? -UTFPR</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time brazilian license plate detection and recognition using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Montazzolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Conference on Graphics, Patterns and Images (SIBGRAPI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time automatic license plate recognition through deep multi-task networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31th Conference on Graphics, Patterns and Images (SIBGRAPI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic license plate recognition (ALPR): A stateof-the-art review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shehata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="311" to="325" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ICDAR 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Digital electric meter reading recognition based on horizontal and vertical binary pattern</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khaliluzzaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yakub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Electrical Information and Communication Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Research on remote meter automatic reading based on computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE PES Transmission and Distribution Conference and Exposition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic electricity meter reading based on image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Elrefaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bajaber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Natheir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Jordan Conference on Applied Electrical Engineering and Computing Technologies (AEECT)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HD MR: a new algorithm for number recognition in electrical meters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berdugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jabba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Turkish Journal of Elec. Engineering &amp; Comp. Sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="87" to="96" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An overview of the Tesseract OCR Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="629" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multi-neural network approach to image detection and segmentation of gas meter counter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nodari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR Conference on Machine Vision Applications</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="239" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Design and research of digital meter identifier based on image and wireless communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Industrial Mechatronics and Automation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Energy Company Of Paran?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Copel</surname></persName>
		</author>
		<ptr target="http://www.copel.com/hpcopel/english/.Accessed" />
		<imprint>
			<biblScope unit="page" from="2018" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Benchmark for license plate character segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P G</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A robust real-time automatic license plate recognition based on the yolo detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SqueezeDet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="446" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LCDet: Low-complexity fully-convolutional neural networks for object detection in embedded systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="411" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">YOLOv2 and YOLOv3: how to improve object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexeyab</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Holistic recognition of low quality license plates by CNN using track annotated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?pa?hel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jur?nek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intern. Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to forget: continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Darknet: Open source neural networks in C</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<ptr target="http://pjreddie.com/darknet/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Rayson Laroca received his bachelor&apos;s degree in software engineering from the State University of Ponta Grossa, Brazil. Currently, he is a master&apos;s student at the Federal University of Paran?, Brazil. His research interests include machine learning, pattern recognition and computer vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Victor Barroso is an undergraduate student in computer science at the Federal University of</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">His research interests include machine learning, computer vision, pattern recognition and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brazil</forename><surname>Paran?</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Diniz is a master&apos;s student at the Federal University of Minas Gerais, Brazil, where he also received his bachelor&apos;s degree in computer science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matheus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research focuses on deep learning techniques applied to computer vision and surveillance</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Gon?alves is a PhD student at Federal University of Minas Gerais</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gabriel</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>He received a</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
