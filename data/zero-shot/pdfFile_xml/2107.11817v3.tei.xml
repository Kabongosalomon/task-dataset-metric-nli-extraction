<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Go Wider Instead of Deeper</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
							<email>f.xue@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziji</forename><surname>Shi</surname></persName>
							<email>ziji.shi@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Futao</forename><surname>Wei</surname></persName>
							<email>weifutao2019@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Lou</surname></persName>
							<email>yuxuanlou@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<email>liuyong@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
							<email>youy@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Go Wider Instead of Deeper</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>More transformer blocks with residual connections have recently achieved impressive results on various tasks. To achieve better performance with fewer trainable parameters, recent methods are proposed to go shallower by parameter sharing or model compressing along with the depth. However, weak modeling capacity limits their performance. Contrastively, going wider by inducing more trainable matrixes and parameters would produce a huge model requiring advanced parallelism to train and inference. In this paper, we propose a parameter-efficient framework, going wider instead of deeper. Specially, following existing works, we adapt parameter sharing to compress along depth. But, such deployment would limit the performance. To maximize modeling capacity, we scale along model width by replacing feed-forward network (FFN) with mixture-ofexperts (MoE). Across transformer blocks, instead of sharing normalization layers, we propose to use individual layernorms to transform various semantic representations in a more parameter-efficient way. To evaluate our plug-and-run framework, we design WideNet and conduct comprehensive experiments on popular computer vision and natural language processing benchmarks. On ImageNet-1K, our best model outperforms Vision Transformer (ViT) by 1.5% with 0.72? trainable parameters. Using 0.46? and 0.13? parameters, our WideNet can still surpass ViT and ViT-MoE by 0.8% and 2.1%, respectively. On four natural language processing datasets, WideNet outperforms ALBERT by 1.8% on average and surpass BERT using factorized embedding parameterization by 0.8% with fewer parameters. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Transformer-based models have achieved promising results on various tasks (e.g., Q&amp;A <ref type="bibr" target="#b13">(Qu et al. 2019;</ref><ref type="bibr" target="#b29">Yang et al. 2020)</ref>, relation extraction <ref type="bibr">(Xue et al. 2020b,a;</ref><ref type="bibr" target="#b34">Zhou et al. 2020)</ref>). To further improve the effectiveness and efficiency of the transformer, there are two trains of thought to deploy trainable parameters. The first thought is to scale transformer along width to more trainable parameters (e.g., Switch Transformer <ref type="bibr" target="#b6">(Fedus, Zoph, and Shazeer 2021)</ref>, ViT-MoE <ref type="bibr" target="#b18">(Riquelme et al. 2021)</ref>). These sparse models can scale to extremely large models with comparable FLOPs by sparse conditional computation. Another thought is to decrease the trainable parameters for a lite model. To this end, some works propose to reuse the trainable parameters across transformer blocks (e.g., Universal Transformer <ref type="bibr" target="#b2">(Dehghani et al. 2018</ref>) and ALBERT <ref type="bibr" target="#b9">(Lan et al. 2019)</ref>). Model compression <ref type="bibr" target="#b22">Sun et al. 2019)</ref> can also make transformer more parameter efficient.</p><p>The two existing methods both have their own limitations. For huge models, one typical and effective method to scale trainable parameters is replacing part of the feed-forward network (FFN) layer in transformer blocks with MoE layers. In each MoE layer, to refine one single token representation, only a few experts are activated, so the MoE based transformer holds comparable FLOPs with the vanilla transformer. However, during training and inference, we are required to use advanced parallelisms (e.g., tensor <ref type="bibr" target="#b20">(Shoeybi et al. 2019)</ref>, sequence , pipeline <ref type="bibr" target="#b7">(Huang et al. 2018</ref>) and expert parallelism <ref type="bibr" target="#b10">(Lepikhin et al. 2020)</ref>) to hold these models on TPU or GPU. Also, the performance cannot improve linearly during scaling. Another limitation is that the sparseness of MoE based models cannot scale well on relatively small datasets. We will discuss the reason for this phenomenon in the following sections. For small models, although they can reduce trainable parameters significantly by going shallower, the performance of these shallower models is still under the original transformers. These smaller models are constructed by compressing the original model along with depth so all transformer blocks share the same knowledge. Such structure induces the unavoidable loss of model capacity.</p><p>In this paper, we present a parameter deployment framework that deploys trainable parameters more effectively: going wider instead of deeper. We then implement it on the transformer and named it as WideNet. Specially, we first employs parameter sharing along with depth to go shallower. Due to avoidable model capacity loss, we go wider by using the same MoE layer in all transformer blocks. The multihead attention layer is also shared across the blocks. To help the transformer blocks learn different semantics and maximize the modeling capacity from MoE layer, we do not share the normalization layers. Different trainable parameters of the normalization layer enable transformer blocks to be fed by diversified representations. Since the modeling capacity of each transformer block has been enhanced by the MoE layer, it can model diversified semantics effec- tively with the same trainable parameters. Therefore, with one attention layer and one single stronger MoE layer learning complex representations, and independent normalization layers for diversified semantic representations, going wider instead of deeper is a more parameter-efficient and effective framework. Compared with simply scaling along the width, going wider instead of deeper is a more parameter-efficient framework, which makes the models small enough to be adapted to downstream tasks without advanced parallelisms. Second, each expert in WideNet can be trained by more token representations so that it has better generalization performance.</p><p>Compared with the models simply compressed along with the depth, all transformer blocks in WideNet share one same MoE layer instead of one FFN layer. Such structure maximizes the modeling ability of every transformer block. More experts can model more complex token representations with a stronger capacity. Another difference is the independent normalization layers. These layers come with few additional trainable parameters, but they can transform input representations to other semantic domains. In this case, with a strong enough single MoE layer, WideNet can still model semantics from different levels well. Moreover, in every transformer block, each expert only receives a part of token representations that usually correspond to different input tokens.</p><p>Our contributions are summarized as three folds:</p><p>? To improve the parameter efficiency, we propose sharing the MoE layer across transformer blocks. The shared experts can receive diversified token representations in different transformer blocks, which enables each expert to be fully trained.</p><p>? We propose to keep individual normalization layer across transformer blocks. The individual normalization layers can transform input hidden vectors to semantic information by adding few trainable parameters. Then, diversified input can be fed into the same attention layer or stronger MoE layer to model different semantics. ? By combing the two thoughts above, we propose going wider instead of deeper, a more parameter-efficient and effective framework. We then implement this framework as WideNet and evaluate it on both computer vision and natural language processing tasks. Due to the more efficient parameter deployment, WideNet outperforms baselines with less trainable parameters. We expect our WideNet can serve as a next-generation transformer backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixture-of-Experts</head><p>In this paper, we focus on a novel trainable parameter deployment framework and implement this framework on the transformer as WideNet. The overall structure is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We use Vision Transformer as the backbone in this example, which means we normalize the representations before the attention layer or FFN layer. We also extend WideNet to other transformer models (e.g., BERT <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref>) in this paper. In WideNet, we replace the FFN layer with the MoE layer. Parameter sharing across transformer blocks is employed for a more parameter-efficient deployment. Within each MoE layer, we have one router to select K experts to learn more complex representations. Please note the trainable parameters in layer normalization are not shared for more diversified semantic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Computation with MoE</head><p>Our core idea is to deploy more trainable parameters along the width and fewer trainable parameters along with the depth. To this end, we employ MoE to scale transformer along with width. As a typical conditional computation model <ref type="bibr" target="#b0">(Bengio 2013)</ref>, MoE only activates a few experts, i.e., subsets of a network. For each input, we feed only a part of hidden representations required to be processed into the selected experts. Following <ref type="bibr" target="#b19">Shazeer et al. (2017)</ref>, given E trainable experts and input representation x ? R D , the output of MoE model can be formulated as:</p><formula xml:id="formula_0">MoE(x) = E i=1 g(x) i e(x) i<label>(1)</label></formula><p>where e(?) i is a non-linear transformation R D ? R D of i th expert, and g(?) i is i th element of the output of trainable router g(?), a non-linear mapping R D ? R E . Usually, both e(?) and g(?) are parameterized by neural networks. According to the formulation above, when g(?) is a sparse vector, only part of experts would be activated and updated by back-propagation during training. In this paper, for both vanilla MoE and our WideNet, each expert is an FFN layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Routing</head><p>To ensure a sparse routing g(?), we use TopK() to select the top ranked experts. Then, following <ref type="bibr" target="#b18">Riquelme et al. (2021)</ref>, g(?) can be written as:</p><formula xml:id="formula_1">g(x) = TopK(softmax(f (x) + )) (2) where f (?) is routing linear transformation R D ? R E , and ? N (0, 1 E 2 )</formula><p>is a Gaussian noise for exploration of expert routing. We use softmax after f (?) for better performance and more sparse experts <ref type="bibr" target="#b18">(Riquelme et al. 2021;</ref><ref type="bibr" target="#b6">Fedus, Zoph, and Shazeer 2021)</ref>. When K E, most elements of g(x) would be zero so that sparse conditional computation is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Balanced Loading</head><p>In MoE based transformer, we dispatch each token to K experts. During training, if the MoE model has no regularization, most tokens may be dispatched to a small portion of experts. Such an unbalanced assignment would decrease the throughput of the MoE model. In addition, more importantly, most additional trainable parameters would not be fully trained so that the sparse conditional model cannot surpass the corresponding dense model during scaling. Therefore, for balanced loading, we have two things to avoid:</p><p>(1) too many tokens dispatched to one single expert, and (2) too few tokens received by one single expert. To solve the first issue, buffer capacity B is required. That is, for each expert, we only preserve B token at most regardless of how many tokens are dispatched to this expert. If more than B = CKN L tokens are assigned, the left tokens would be dropped. C is the capacity ratio, a pre-defined hyperparameter to control the ratio of tokens preserved for each expert. Usually, C ? [1, 2], and we set C as 1.2 when no special explanation is used. K is the number of selected experts for each token. N is the batch size on each device 2 . L is the sequence length. For computer vision tasks, L denotes the number of patch tokens in each image.</p><p>Buffer capacity B helps us drop redundant tokens for each expert to maximize throughput but it cannot ensure all experts to receive enough token to train. In other words, until now, the routing is still unbalanced. Therefore, we follow <ref type="bibr" target="#b6">Fedus, Zoph, and Shazeer (2021)</ref> to use a differentiable load balance loss instead of separate load-balancing and importance-weighting losses for a balanced loading in the router. For each routing operation, given E experts and N batches with N L tokens, the following auxiliary loss is added to the total model loss during training:</p><formula xml:id="formula_2">l balance = E ? E i=1 m i ? P i (3)</formula><p>where m is vector. i th element is the fraction of tokens dispatched to expert i:</p><formula xml:id="formula_3">m i = 1 L L j=1 h(x j ) i (4) where h(?) is a index vector selected by TopK in Eq. 2. h(x j ) i is i th element of h(x j ). It is noticeable that, different from g(x) i in Eq. 2, m i and h(x j ) i are non-differentiable.</formula><p>However, a differentiable loss function is required to optimize MoE in an end-to-end fashion. Therefore, we define P i in Eq. 3 as:</p><formula xml:id="formula_4">P i = softmax(f (x) + ) i<label>(5)</label></formula><p>We can observe P i is i th element of routing linear transformation after softmax activation function, and P i is differentiable.</p><p>The goal of load balancing loss is to achieve a balanced assignment. When we minimize l balance , we can see both m and P would close to a uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Go wider instead of deeper Sharing MoE across transformer blocks</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, WideNet adopts parameter sharing across transformer blocks to improve parameter efficiency, and MoE layer is used to improve model capacity. In addition, as we use the MoE layer to obtain a stronger modeling ability, to overcome the overfitting from sparse conditional computation, we are required to feed enough tokens to each expert. To this end, WideNet uses the same router and experts in different transformer blocks. Formally, given hidden representations H 1 = {h 1 1 , h 1 2 , . . . , h 1 L } as input of the first transformer block, we can define the parameter sharing as H i+1 = MoE(H i ), which is different from the existing MoE based models H i+1 = MoE i (H i ). Please note that, although we share trainable parameters in the MoE layer including the router, token representations corresponding to the same token are different in every transformer block.</p><p>That is, h j i and h j+1 i may be dispatched to different experts. Therefore, each expert would be trained by more varied tokens for better generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Individual Layer Normalization</head><p>Although existing works <ref type="bibr" target="#b9">(Lan et al. 2019)</ref> show that the activations in different transformer blocks are similar, the cosine distance is still much larger than zero. Therefore, different from existing works <ref type="bibr" target="#b2">(Dehghani et al. 2018;</ref><ref type="bibr" target="#b9">Lan et al. 2019)</ref> sharing all weights across transformer blocks, to encourage more diversified input representations of different blocks, we only share multi-head attention layer and FFN (or MoE) layer, which means trainable parameters of layer normalization are different across blocks.</p><p>In summary, i th transformer block in our framework can be written as:</p><formula xml:id="formula_5">x = LayerNormal att i (x) x = MHA(x ) + x x = LayerNormal moe i (x) x = MoE(x ) + x (6)</formula><p>The normalization layer LayerNormal(?) is:</p><formula xml:id="formula_6">LayerNormal(x) = x ? E[x] Var[x] + * ? + ?<label>(7)</label></formula><p>where ? ? R D and ? ? R D are two trainable vectors. Layer normalization only requires these two small vectors so individual normalization would just add few trainable parameters into our framework. We can find the difference between shared layer normalization and the individual ones is the mean and magnitude of output. For shared layer normalization, the input of MHA and MoE layer are more similar in different transformer blocks. Since we have shared trainable matrixes, we encourage more diversified input to represent various semantics in different transformer blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization</head><p>Although we reuse the trainable parameters of the router in every transformer block, the assignment would be different due to different input representations. Therefore, given T times routing operation with the same trainable parameters, we have the following loss for optimization:</p><formula xml:id="formula_7">loss = l main + ? T t=1 l T balance<label>(8)</label></formula><p>where ? is a hyper-parameter to ensure a balanced assignment, and we set it as a relatively large number, i.e., 0.01 in this work. Similar to existing MoE based models, we found the performance is non-sensitive to ?. l main is the main target of our transformer. For example, on supervised image classification, l main is cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Computer Vision</head><p>Experimental Settings We use ILSVRC-2012 Ima-geNet <ref type="bibr" target="#b3">(Deng et al. 2009</ref>) and Cifar10 (Krizhevsky, Hinton Instead of achieving SoTA performance, the goal of this paper is to show that our parameter deployment framework can improve the transformer backbone with less trainable parameters. Therefore, we employ LAMB instead of AdamW for more general and typical experiments. For MoE based models (i.e., ViT-MoE and WideNet), we set the weight of load balance loss ? as 0.01. Without special instructions, we use 4 experts in total and Top 2 experts selected in each transformer block. The capacity ratio C is set as 1.2 for a trade-off between accuracy and speed. We pretrain our models on 256 TPUv3 cores. According to recent work <ref type="bibr" target="#b32">(Zhai et al. 2021)</ref>, different types of the prediction head have no significant difference on ImageNet's few-shot performance. We also verify this conclusion on training Im-ageNet from scratch. In this work, for ViT, we use the typical token head, which means we insert [CLS] token at the start of patch tokens and use it to classify the image. For MoE based models, to fully use the token representations after the final MoE layer, we employ a global average pooling head instead of the token head.</p><p>During finetuning, we still follow <ref type="bibr" target="#b5">(Dosovitskiy et al. 2020</ref>) and use SGD optimizer with momentum. Compared with pretraining on ImageNet-1K, label smoothing and warm-up are removed. Another observation is, unlike training MoE based models on huge datasets (e.g., JFT-300M <ref type="bibr" target="#b21">(Sun et al. 2017)</ref> and C4 <ref type="bibr" target="#b14">(Raffel et al. 2019)</ref>), MoE cannot benefit ViT on ImageNet-1K, which is 200 times smaller than original ViT-MoE used in pretraining 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural Language Processing</head><p>The main contribution of this work is to design a more parameter-efficient and plug-in framework for various AI applications. Therefore, we further evaluate our work on natural language processing (NLP) after computer vision (CV). The training of experiments on NLP can still be splitted into 2 stages, pretraining and finetuning.</p><p>Experimental Settings Following BERT <ref type="bibr" target="#b4">(Devlin et al. 2019</ref>) and ALBERT <ref type="bibr" target="#b9">(Lan et al. 2019)</ref>, in this section, we pretrain all models by English Wikipedia <ref type="bibr" target="#b4">(Devlin et al. 2019</ref>) and BOOKCORPUS <ref type="bibr" target="#b35">(Zhu et al. 2015)</ref>. Since the goal of this work is to design a parameter-efficient framework, all models including BERT use factorized embedding parameterization. That is, the WordPiece embedding size E is 128. The hyperparameter settings of experiments on NLP can be found in Appendix, which is the same as ALBERT for a fair comparison. Similar to the experiments on vision tasks, we pretrain our models by LAMB on 256 TPUv3 cores. The learning rate is 0.00176, which is the same as ALBERT claimed <ref type="bibr" target="#b30">(You et al. 2019a)</ref>.</p><p>During finetuning, we evaluate our model on the General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b24">(Wang et al. 2018)</ref>, two versions of the Stanford Ques-tion Answering (SQuAD) dataset <ref type="bibr" target="#b16">(Rajpurkar et al. 2016;</ref><ref type="bibr" target="#b15">Rajpurkar, Jia, and Liang 2018)</ref>. For GLUE experiments, we report median over 5 runs on development set because of relatively large variance.</p><p>Downstream Evaluation Different from the experiments on CV, we report the evaluation results on downstream tasks directly in this section. As shown in <ref type="table" target="#tab_1">Table 2</ref>, when we use more experts, our WideNet outperforms ALBERT by a large margin. For instance, WideNet with 4 experts surpasses AL-BERT by 1.2% in average. When we increase the number of experts E to 16 to achieve slightly less trainiable parameters than BERT with factorized embedding parameterization, our WideNet also outperforms it on all four downstream tasks, which shows the parameter-efficiency and effectiveness of going wider instead of deeper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoE Analysis</head><p>To investigate the reason why MoE cannot scale well on smaller datasets like ImageNet-1K, we conduct two sets of experiments on ViT-MoE and WideNet, respectively. Given following hyper-parameters: (1) Number of training images N I ; (2) Number of patch tokens per image N p ; (3) Number of experts in each transformer block E; (4) Capacity ratio C; (5) Number of experts selected in each transformer block K, as we usually use a large ?, we can assume few tokens would be dropped when we are using C slightly larger than 1.0. Then, we can approximate T ? N I NpK E</p><p>. Existing works <ref type="bibr" target="#b18">(Riquelme et al. 2021;</ref><ref type="bibr" target="#b28">Yang et al. 2021</ref>) have shown that decreasing N I , N p , K and C can induce a performance drop. In the first set of experiments of this section, we scale the number of experts in every transformer block E to control the tokens fed into each expert on ImageNet-1K.</p><p>Results are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. We observe that more experts (trainable parameters) lead to overfitting although more experts mean stronger modeling capacity. Training accuracy is lower than testing accuracy because of data augmentation we introduced in the Experimental Settings Section.</p><p>To further verify that each expert requires varied tokens to train, we conduct the second set of experiments on WideNet. We define the transformer blocks using the same routing assignment that belongs to one group. To change the input diversity of each expert, each group includes more than one transformer block. That is, the hidden representations corresponding to the same token would be fed into the same expert within the same group. We set G groups in total and each group includes D G transformer blocks, where D is the number of transformer blocks.   As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, when we use fewer groups, which means we have fewer routing operations, there is an obvious performance drop. We can suggest less diversified tokens are fed to each expert because fewer groups mean less routing and assignments. Therefore, more diversified tokens are required to train MoE based models on smaller datasets. More importantly, such results show the effectiveness and necessity of our design, routing at every transformer block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Norm Analysis</head><p>We are to analyze the reason why individual layer normalization can improve performance in this section. Compared with the vanilla transformer structure, we share trainable matrixes in MHA and FFN (or MoE) layer across transformer blocks. The modeling capacity is compressed due to the same trainable parameters across blocks. Although WideNet uses the MoE layer to replace the FFN layer to improve capacity, different blocks are still using the same trainable parameters. Therefore, in WideNet, we encourage more diversified input to represent various semantics in different transformer blocks. Compared with vanilla ViT, we expect a larger variance of trainable vectors ? and ? across blocks. In this section, we are interested in layer normalization before MoE or FFN.</p><p>Therefore, for i th element of trainable vector ? or ? in j th block, we compute the distance between this element and all other elements of all vectors from other blocks. Taken ? as example, we can formulate the value y we are interested in as:</p><formula xml:id="formula_8">y = 1 M N 2 N j=1 M m=1 N n=1 I (j =n) |? ij ? ? mn |<label>(9)</label></formula><p>where N is the number of transformer blocks, M is the dimension of vector ? or ?. In <ref type="figure">Fig. 4</ref> and <ref type="figure">Fig. 5</ref>, we can observe that both ? and ? in WideNet have larger y than those in ViT, which means MoE receives more diversified input than ViT. Such result proves our assumption that individual normalization layer can help to model various semantics model with shared large trainable matrixes like MoE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study: Contributions of key modifications</head><p>We first conduct the ablation study to investigate the contributions of our three key modifications (i.e., Independent Layer Normalization, scaling width with MoE layer, and compressing depth with parameter sharing). The results are reported in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>We first replace the individual layer normalizations with the shared ones. We can observe there is a performance drop with almost the same trainable parameters. Such observation shows the effectiveness of our design. In addition, we recover the MoE layer to the FFN layer. Without the MoE layer, the training would be extremely difficult with much less trainable parameters. For example, WideNet-B without MoE layer encounters gradient explosion, and there is a significant performance drop. Finally, without parameter sharing across transformer blocks, we can also observe a slight performance drop and significant parameter increment. For WideNet-H without parameter sharing, it encounters out-ofmemory when training on 256 TPUv3 cores.  Ablation Study: Comparison with comparable speed or computation cost As we set the number of selected experts K as 2 and capacity ratio C as 1.2 in WideNet, there is extra computation cost than vanilla ViT. Therefore, we conduct a second set of ablation studies to evaluate our WideNet with comparable speed or computation cost with the baselines.</p><p>As shown in <ref type="table" target="#tab_3">Table 4</ref>, compared with ViT-L, WideNet-L is more computation expensive. We can observe a training time increment. However, when WideNet-L uses fewer transformer blocks (i.e., 12 blocks) than ViT-L, WideNet-L outperforms ViT-L by 0.7% with slightly less training time and 13.1% parameters, and, similarly, there is a larger performance improvement than ViT-L with parameter sharing. We also scale ViT-L using parameter sharing to a wider FFN layer. Then, for each token, ViT-L would have comparable computation with WideNet-L setting K as 2. We can see scaling to more trainable parameters and FLOPs cannot improve the performance of ViT-L, which also shows the effectiveness and necessity of our framework. Although ViT-L has a comparable computation cost with WideNet for each token, WideNet still spends more training time per epoch. According to our experiments, there are two reasons, i.e., routing operation and C &gt; 1.0. We leave optimize this as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose to go wider instead of deeper for more efficient and effective parameter deployment. We implement this plug and play framework as WideNet. Especially, WideNet first compresses trainable parameters along with depth by parameter-sharing across transformer blocks. To maximize the modeling ability of each transformer block, we replace the FFN layer with the MoE layer. Then, individual layer normalization provides a more parameter-efficient way to transform semantic representations across blocks. We show that WideNet achieves the best performance by less trainable parameters on both computer vision and natural language processing backbones. In particular, on ImageNet-1K, our best model achieves 80.1 Top-1 accuracy with only 63M parameters, which outperforms ViT and ViT-MoE by a large margin. On four natural language processing datasets, WideNet outperforms ALBERT by a large margin and surpass BERT with less trainable parameters. Also, the investigation shows the reason why MoE cannot scale well on smaller datasets. That is, each expert requires enough tokens to train. Moreover, we verified that individual normalization can transform hidden representations to other domains for more diversified semantics. In summary, we show that there is a great potential of this framework to train more parameter-efficient models. Results in <ref type="table" target="#tab_4">Table 5</ref> shows that our WideNet can transfer better performance from pretraining to finetuning. WideNet-L, which outperforms all baselines in pretraining, is still the best model in finetuning. The value of hyper-parameters on computer vision experiments is shown in <ref type="table" target="#tab_5">Table 6</ref>. On ImageNet-1K cosine learning rate decay is used after 30 warmup epochs. Please note all models are using the same hyper-parameters of <ref type="table" target="#tab_5">Table 6</ref>, which also shows the robustness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters of experiments on computer vision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters of experiments on natural language processing</head><p>As shown in <ref type="table" target="#tab_6">Table 7</ref>, we use larger capacity ratio C on SQuAD1.1 as limited training data may induce unbalanced token distribution. Therefore, we set ? as 0 and C as 2.0. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overall architecture of the proposed WideNet. Compared with vanilla transformer, we replace FFN layer by MoE layer and share the trainable parameters except the normalization layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Results of scaling the number of experts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Results of scaling the number of groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Divergence of ? with LayerNorm layers. Divergence of ? with LayerNorm layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on ImageNet-1K pretraining.</figDesc><table><row><cell>Model</cell><cell cols="2">Parameters ImageNet-1K</cell></row><row><cell>ViT-B</cell><cell>87M</cell><cell>78.6</cell></row><row><cell>ViT-L</cell><cell>305M</cell><cell>77.5</cell></row><row><cell cols="2">ViT-MoE-B 128M</cell><cell>77.9</cell></row><row><cell cols="2">ViT-MoE-L 406M</cell><cell>77.4</cell></row><row><cell cols="2">WideNet-B 29M</cell><cell>77.5</cell></row><row><cell>WideNet-L</cell><cell>40M</cell><cell>79.5</cell></row><row><cell cols="2">WideNet-H 63M</cell><cell>80.1</cell></row><row><cell cols="3">et al. 2009) as platforms to evaluate our framework. Ima-</cell></row><row><cell cols="3">geNet we used in this work has 1k classes and 1.3M im-</cell></row><row><cell cols="3">ages. We denote it as ImageNet-1K in the following exper-</cell></row><row><cell cols="3">iments. We select ViT (Dosovitskiy et al. 2020) and ViT-</cell></row><row><cell cols="3">MoE (Riquelme et al. 2021) as baselines. We first reimple-</cell></row></table><note>ment ViT by Tensorflow 2.x and tune it to a reasonable per- formance. For all models in this section, we use Inception- style pre-processing, Mixup (Zhang et al. 2017), RandAug- ment (Cubuk et al. 2020) and label smoothing (Szegedy et al. 2016; Yuan et al. 2020) as data augmentation. We also ob- serve that AdamW optimizer (Loshchilov and Hutter 2017) is sensitive to hyper-parameters and learning schedules. LAMB optimizer (You et al. 2019b) can achieve comparable performance but it is more robust to the hyper-parameters. For fair comparison, following Zhai et al. (2021), we evalu- ate WideNet on three scales (i.e., WideNet-Base, WideNet- Large and WideNet-Huge). The attention and FFN dimen- sions of different scales are the same as ViT-MoE except for WideNet-B. For WideNet-B, we use a hidden dimension of FFN as 4096 instead of 3072 for a more stable training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of funetuning on GLUE benchmarks</figDesc><table><row><cell>Model</cell><cell cols="6">#para SQuAD1.1 SQuAD2.0 MNLI SST-2 Avg</cell></row><row><cell>ALBERT</cell><cell>12M</cell><cell>89.3/82.3</cell><cell>80.0/77.1</cell><cell>81.5</cell><cell>90.3</cell><cell>84.0</cell></row><row><cell>BERT</cell><cell>89M</cell><cell>89.9/82.8</cell><cell>80.3/77.3</cell><cell>83.2</cell><cell>91.5</cell><cell>85.0</cell></row><row><cell>WideNet 4 experts</cell><cell>26M</cell><cell>89.6/82.7</cell><cell>80.6/77.4</cell><cell>82.6</cell><cell>91.1</cell><cell>84.7</cell></row><row><cell>WideNet 8 experts</cell><cell>45M</cell><cell>90.0/82.7</cell><cell>80.6/77.7</cell><cell>83.3</cell><cell>91.9</cell><cell>85.2</cell></row><row><cell cols="2">WideNet 16 experts 83M</cell><cell>90.9/83.8</cell><cell>81.0/77.9</cell><cell>84.1</cell><cell>92.2</cell><cell>85.8</cell></row><row><cell cols="3">Comparison with baselines We follow the hyper-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">parameter setting of baselines in pretraining and finetuning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">for a fair comparison. Please see Appendix for details. Such</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">implementation also shows that our model is robust to hyper-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>parameters.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">We report the Top-1 accuracy on ImageNet-1K in Ta-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ble 1 and Cifar10 in Appendix. Observe that WideNet-H</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">achieves the best performance and significantly outperforms</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ViT and ViT-MoE models on ImageNet-1K. Compared with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">the strongest baseline, our WideNet-H outperforms ViT-B</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">by 1.5% with less trainable parameters. Even if we use the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">smallest model, WideNet-B, it still achieves comparable per-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">formance with ViT-L and ViT-MoE-B with over 4? less</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">trainable parameters. When we scale up to WideNet-L, it</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">has surpassed all baselines with half trainable parameters of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-B and 0.13? parameters of ViT-L.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: Results of ablation study on ImageNet-1K to to in-</cell></row><row><cell cols="3">vestigate the contributions of our three key modifications</cell></row><row><cell cols="3">(i.e., Independent Layer Normalization, scaling width with</cell></row><row><cell cols="3">MoE layer and compressing depth with parameter sharing).</cell></row><row><cell>Model</cell><cell cols="2">Top-1 Parameters</cell></row><row><cell>WideNet-B</cell><cell>77.5</cell><cell>29M</cell></row><row><cell cols="2">w/ shared Layer Norm 76.3</cell><cell>29M</cell></row><row><cell>w/o MoE layer</cell><cell>Nan</cell><cell>9M</cell></row><row><cell cols="2">w/o parameter sharing 77.9</cell><cell>128M</cell></row><row><cell>WideNet-L</cell><cell>79.5</cell><cell>40M</cell></row><row><cell cols="2">w/ shared Layer Norm 78.3</cell><cell>40M</cell></row><row><cell>w/o MoE layer</cell><cell>76.9</cell><cell>15M</cell></row><row><cell cols="2">w/o parameter sharing 77.4</cell><cell>406M</cell></row><row><cell>WideNet-H</cell><cell>80.1</cell><cell>63M</cell></row><row><cell cols="2">w/ shared Layer Norm 76.6</cell><cell>63M</cell></row><row><cell>w/o MoE layer</cell><cell>79.0</cell><cell>23M</cell></row><row><cell cols="2">w/o parameter sharing OOM</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of ablation study on ImageNet-1K to evaluate our WideNet with comparable speed or computation cost. #Blocks is the number of transformer blocks. FNN dim means the dimension of FFN layer. Para Sharing is whether we shared parameters across transformer blocks. Time denotes to TPUv3 core days.</figDesc><table><row><cell>Model</cell><cell cols="5">#Blocks FNN dim Para Sharing Top-1 #Para Time</cell></row><row><cell cols="2">ViT-L ViT-L WideNet-L 12 24 24 ViT-L 24 WideNet-L 24</cell><cell>4096 4096 4096 8192 4096</cell><cell>? ? ? ? ?</cell><cell>77.5 76.9 78.2 75.8 79.5</cell><cell>305M 0.08K 15M 0.07K 40M 0.07K 24M 0.09K 40M 0.14K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on Cifar10 finetuning.</figDesc><table><row><cell>Model</cell><cell cols="2">Parameters Cifar10</cell></row><row><cell>ViT-B</cell><cell>85M</cell><cell>98.3</cell></row><row><cell>ViT-L</cell><cell>305M</cell><cell>98.2</cell></row><row><cell cols="2">ViT-MoE-B 126M</cell><cell>98.5</cell></row><row><cell cols="2">ViT-MoE-L 404M</cell><cell>98.5</cell></row><row><cell cols="2">WideNet-B 27M</cell><cell>98.4</cell></row><row><cell>WideNet-L</cell><cell>38M</cell><cell>98.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameters on ImageNet-1K pretraining and Cifar10 finetuning.</figDesc><table><row><cell>Parameter</cell><cell cols="2">ImageNet-1K Cifar10</cell></row><row><cell>Epoch</cell><cell>300</cell><cell>100</cell></row><row><cell cols="2">Warmup Epochs 30</cell><cell>0</cell></row><row><cell>Batch Size</cell><cell>4096</cell><cell>512</cell></row><row><cell>Learning rate</cell><cell>0.01</cell><cell>0.03</cell></row><row><cell>Weight Decay</cell><cell>0.1</cell><cell>0</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell cols="2">Label smoothing 0.1</cell><cell>0</cell></row><row><cell>Mixup prob.</cell><cell>0.5</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameters on downstream NLP tasks.</figDesc><table><row><cell>Parameter</cell><cell cols="3">SQuAD1.1/2.0 MNLI SST2</cell></row><row><cell>Steps</cell><cell>3649/8144</cell><cell cols="2">10000 5234</cell></row><row><cell>Warmup</cell><cell>365/814</cell><cell>1000</cell><cell>314</cell></row><row><cell>Batch Size</cell><cell>48</cell><cell>128</cell><cell>128</cell></row><row><cell>LR</cell><cell>5e-5/3e-5</cell><cell>3e-5</cell><cell>4e-5</cell></row><row><cell>C</cell><cell>2.0/1.2</cell><cell>1.2</cell><cell>1.2</cell></row><row><cell>?</cell><cell>0/0.01</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Dropout</cell><cell>0.1/0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Max Length 384/512</cell><cell>512</cell><cell>512</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We will release our code upon acceptance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For easier using on downstream tasks, we implement our method with only data parallelism.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This dataset is not publicly available.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Finetuning results on computer vision</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning of representations: Looking forward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on statistical language and speech processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for selfsupervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16668</idno>
		<title level="m">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13120</idno>
		<title level="m">Sequence Parallelism: Making 4D Parallelism Possible</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT with history answer embedding for conversational question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1133" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Know What You Don&apos;t Know: Unanswerable Questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Texas</forename><surname>Austin</surname></persName>
		</author>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05974</idno>
		<title level="m">Scaling Vision with Sparse Mixture of Experts</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multibillion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Patient Knowledge Distillation for BERT Model Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4323" to="4332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BERT-of-Theseus: Compressing BERT by Progressive Module Replacing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7859" to="7869" />
		</imprint>
	</monogr>
	<note>Online: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An Embarrassingly Simple Model for Dialogue Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13873</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">GDP-Net: Refining Latent Multi-View Graph for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06780</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15082</idno>
		<title level="m">Exploring Sparse Expert Models and Beyond</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bert representations for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1556" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reducing BERT pre-training time from 3 days to 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00962</idno>
		<idno>arXiv:1904.00962</idno>
	</analytic>
	<monogr>
		<title level="m">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<title level="m">Scaling vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11304</idno>
		<title level="m">Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
