<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Large-scale Unsupervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Yu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Large-scale Unsupervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-large-scale</term>
					<term>unsupervised</term>
					<term>semantic segmentation</term>
					<term>self-supervised</term>
					<term>ImageNet !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Empowered by large datasets, e.g., ImageNet, unsupervised learning on large-scale data has enabled significant advances for classification tasks. However, whether the large-scale unsupervised semantic segmentation can be achieved remains unknown. There are two major challenges: i) we need a large-scale benchmark for assessing algorithms; ii) we need to develop methods to simultaneously learn category and shape representation in an unsupervised manner. In this work, we propose a new problem of large-scale unsupervised semantic segmentation (LUSS) with a newly created benchmark dataset to help the research progress. Building on the ImageNet dataset, we propose the ImageNet-S dataset with 1.2 million training images and 50k high-quality semantic segmentation annotations for evaluation. Our benchmark has a high data diversity and a clear task objective. We also present a simple yet effective method that works surprisingly well for LUSS. In addition, we benchmark related un/weakly/fully supervised methods accordingly, identifying the challenges and possible directions of LUSS. The benchmark and source code is publicly available at https://github.com/LUSSeg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S EMANTIC segmentation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, aiming to label image pixels with category information, has drawn much research attention. Due to the inherent challenges of this task, most efforts focus on semantic segmentation under environments with limited diversity <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and data scale <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. For instance, the PASCAL VOC segmentation dataset only contains about 2k images, while the BDD100K <ref type="bibr" target="#b8">[9]</ref> focuses on road scenes. Numerous approaches have achieved impressive results in these restricted environments <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Significantly scaling up the problem often results in research domain adaptation, e.g., from PASCAL VOC <ref type="bibr" target="#b9">[10]</ref> to ImageNet <ref type="bibr" target="#b20">[21]</ref>. This motivates us to consider a far more challenging problem: is semantic segmentation possible for large-scale realworld environments with a wide diversity?</p><p>However, due to the huge data scale and privacy issues, annotating images with pixel-level human annotations or even image-level labels is extremely expensive. Lacking sufficient benchmark data limits the large-scale semantic segmentation. When trained with millions or even billions of images, e.g., ImageNet <ref type="bibr" target="#b20">[21]</ref>, JFT-300M <ref type="bibr" target="#b21">[22]</ref>, and Instagram-1B <ref type="bibr" target="#b22">[23]</ref>, unsupervised learning of classification model has recently shown a comparable ability to supervised training <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. To facilitate real-world semantic segmentation, we propose a new problem: Large-scale Unsupervised Semantic Segmentation (LUSS). The LUSS task aims to assign labels to pixels from large-scale data without human-annotation supervision, as shown in <ref type="figure">Figure 1</ref>. Many challenges, e.g., simultaneously shape and category representations learning and unsupervised semantic clustering of large amount of data, need to be tackled to achieve this goal. Specifically, we need to extract semantic representations with category and shape features. Category-related representations are required to  <ref type="figure">Fig. 1</ref>. The Large-scale Unsupervised Semantic Segmentation (LUSS) task aims to assign labels from hundreds of categories to pixels from millions of images without the help of human annotation. The model learns to conduct semantic segmentation with Self-Learning.</p><p>distinguish different classes, and shape-related representations, e.g., objectness, boundary, are the essential pixel-level cues for semantic segmentation. The coexistence of two representations is vital to LUSS because conflict representations might cause incorrect semantic segmentation results. Generating categories from large-scale data requires robust and efficient semantic clustering algorithms. Assigning labels to pixels requires the distinction between related and unrelated semantic areas. Solving these challenges for LUSS could also facilitate many related tasks. For example, the learned shape representations from LUSS can be utilized as the pre-training for pixel-level downstream tasks, e.g., semantic segmentation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref> and instance segmentation <ref type="bibr" target="#b26">[27]</ref> under restricted data scale and diversity. Also, fine-tuning LUSS models in the semi-supervised setting facilitates the real-world application where a small part of large-scale data is human-labeled. We propose a benchmark for LUSS task with high-diversity large-scale data, as well as new sufficient evaluation protocols taking into account different perspectives for the LUSS task. Largescale data with sufficient diversity bring challenges to LUSS, but it arXiv:2106.03149v3 [cs.CV] 3 Nov 2022 also provides the source for obtaining extensive representation cues. Due to insufficient data, a few unsupervised segmentation methods <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> mainly deal with small data with limited number of categories and small diversity, thus are not suitable for the LUSS task. We present a large-scale benchmark dataset for the LUSS task, namely ImageNet-S, based on the commonly used ImageNet dataset <ref type="bibr" target="#b20">[21]</ref> in category representation learning works. We remove the unsegmentable categories, e.g., bookshop, and utilize 919 categories with about 1.2 million images in ImageNet for training. Then we annotate 40k images in the validation set of ImageNet with precise pixel-level semantic segmentation masks for LUSS evaluation. We also annotate about 9k images in the training set to allow more comprehensive evaluation protocols and exploration of future applications. Based on the more precise re-annotated image-level labels in <ref type="bibr" target="#b31">[32]</ref>, we enable ImageNet with multiple categories within one image. The ImageNet-S dataset provides large-scale and high-diversity data for fairly LUSS training and sufficient evaluation.</p><p>We then present a new method for the LUSS task, including unsupervised representation learning, label generation, and finetuning steps. For unsupervised representation learning, we propose 1) a non-contrastive pixel-to-pixel representation alignment strategy to enhance the pixel-level shape representation without hurting the instance-level category representation. 2) a deep-to-shallow supervision strategy to enhance the representation quality of the network mid-level features. The learned representation guarantees the coexistence of shape and category information. We propose a pixel-attention scheme to highlight meaningful semantic regions for label generation, facilitating efficient pixel-level label generation and fine-tuning under a large data scale. Based on the proposed method and ImageNet-S dataset, we study the relation between the proposed LUSS task and some related works (i.e., unsupervised learning <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, weakly supervised semantic segmentation <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and transfer learning on downstream tasks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b26">[27]</ref>) and identify the challenges and possible directions of LUSS. In this work, we make two main contributions: <ref type="bibr">?</ref> We propose a new large-scale unsupervised semantic segmentation problem, the ImageNet-S dataset with nearly 50k pixel-level annotated images, 919 categories, and multiple evaluation protocols. <ref type="bibr">?</ref> We present a novel LUSS method containing enhanced representation learning strategies and pixel-attention scheme, and we benchmark related works for LUSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised Segmentation</head><p>Before the recent advances in deep learning, a plethora of approaches have been developed to segment objects with nonparametric methods (e.g., label transfer <ref type="bibr" target="#b38">[39]</ref>, matching <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and distance evaluation <ref type="bibr" target="#b41">[42]</ref>) and handcrafted features (e.g., boundary <ref type="bibr" target="#b42">[43]</ref> and superpixels <ref type="bibr" target="#b43">[44]</ref>). Some unsupervised segmentation (US) methods only focus on segmenting objects but ignore the category, while LUSS cares about object segmentation and classification. Nevertheless, US models can provide prior knowledge to the LUSS model. Numerous data-driven deep learning models have recently been developed for supervised semantic segmentation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Based on pre-trained representations <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, a few unsupervised semantic segmentation (USS) models have been proposed using segment sorting <ref type="bibr" target="#b29">[30]</ref>, mutual information maximization <ref type="bibr" target="#b27">[28]</ref>, region contrastive learning <ref type="bibr" target="#b30">[31]</ref>, and geometric consistency <ref type="bibr" target="#b45">[46]</ref>. As the extension of USS, LUSS differs from USS with its large-scale data and categories. However, several issues limit the applicability of the USS to the LUSS task. 1) Existing methods focus on small datasets <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> and a few (e.g., 20+) easy categories <ref type="bibr" target="#b27">[28]</ref> (e.g., sky and ground). Because of the insufficient data, the advantages of unsupervised learning of rich representations from large-scale data are not explored. The challenges of large-scale data (e.g., huge computational cost) are also ignored. 2) Due to the lack of clear problem definition and standardized evaluation, some methods utilize supervised prior knowledge, e.g., supervised pre-trained network weights <ref type="bibr" target="#b45">[46]</ref>, supervised edge detection <ref type="bibr" target="#b29">[30]</ref>, and supervised saliency detection <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, making it difficult to evaluate these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-supervised Representation Learning</head><p>The LUSS task relies on the semantic features provided by selfsupervised learning (SSL). SSL approaches facilitate models learning semantic features with pretext tasks <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, e.g., colorization <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, jigsaw puzzles <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, inpainting <ref type="bibr" target="#b58">[59]</ref>, adversarial learning <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, context prediction <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, counting <ref type="bibr" target="#b63">[64]</ref>, rotation predictions <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b64">[65]</ref>, cross-domain prediction <ref type="bibr" target="#b65">[66]</ref>, contrastive learning <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, non-contrastive learning <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b69">[70]</ref>, and clustering <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>. We introduce several categories of SSL methods related to the LUSS task.</p><p>Contrastive-based SSL. As the core of unsupervised contrastive learning methods <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref>, instance discrimination with the contrastive loss <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref> considers images from different views <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b68">[69]</ref> or augmentations <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> as pairs. In addition, it forces the model to learn representations by pushing "negative" pairs away and pulling "positive" pairs closer. A memory bank <ref type="bibr" target="#b83">[84]</ref> is introduced to enlarge the available negative samples for contrastive learning. MoCo <ref type="bibr" target="#b24">[25]</ref> stabilizes the training with a momentum encoder. CMC <ref type="bibr" target="#b23">[24]</ref> proposes contrastive learning from multi-views, and SimCLR <ref type="bibr" target="#b25">[26]</ref> explores the effect of different data augmentations.</p><p>Non-contrastive-based SSL. Some non-contrastive approaches <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b85">[86]</ref> maximize the similarity of different types of outputs of the image and avoid negative pairs. BYOL <ref type="bibr" target="#b33">[34]</ref> predicts previous versions of its outputs generated by the momentum encoder to avoid outputs collapse to a constant trivial solution. SimSiam <ref type="bibr" target="#b69">[70]</ref> applies a stop-gradient operation to avoid collapse. Nevertheless, as the concept of the category is not included in both contrastive and non-contrastive methods, they are less effective for category-related tasks, i.e., instances from the same category not necessarily share similar representations.</p><p>Clustering-based SSL. Another line of work introduces a clustering strategy to unsupervised learning <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b92">[93]</ref> that encourages a group of images to have feature representations close to a cluster center. Asano et al. <ref type="bibr" target="#b70">[71]</ref> propose simultaneous clustering and representation learning by optimizing the same objective. Li et al. <ref type="bibr" target="#b71">[72]</ref> maximize the log-likelihood of the observed data via an expectation-maximization framework that iteratively clusters prototypes and performs contrastive learning. SwAV <ref type="bibr" target="#b32">[33]</ref> simultaneously clusters views while enforcing consistency between cluster assignments. Compared to other representation learning methods, the clustering strategy encourages stronger category-related representations with category centroids.</p><p>Pixel-level SSL. Some works uses self-supervised learning on the pixel-level instead of image-level to enhance the transfer learning ability to downstream tasks <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b94">[95]</ref>. PixPro <ref type="bibr" target="#b34">[35]</ref> applies contrastive learning between neighbour/other pixels and proposes pixel-to-propagation consistency to enhance spatial smoothness. SCRL <ref type="bibr" target="#b93">[94]</ref> produces consistent spatial representations of randomly cropped local regions with the matched location. DenseCL <ref type="bibr" target="#b94">[95]</ref> chooses positive pairs by matching the most similar feature vectors in two views. Despite the good performance for transfer learning, these methods ignore the category-related representation ability required by the LUSS task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Weakly Supervised Semantic Segmentation</head><p>Weakly supervised semantic segmentation (WSSS) <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b97">[98]</ref> aims to carry out the task using weak annotations, e.g., image-level labels. WSSS is related to LUSS as both require shape features. However, some modules in typical WSSS methods, e.g., supervised ImageNet 1k pre-trained models <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref>, image-level ground-truth labels <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b101">[102]</ref>, and large network architectures <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, are not applicable to the LUSS tasks. In addition, it is possible to use other alternative WSSS modules e.g., affinity prediction <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b102">[103]</ref>, region separation <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b103">[104]</ref>, boundary refinement <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b104">[105]</ref>, joint learning <ref type="bibr" target="#b105">[106]</ref>, and sub-category exploration <ref type="bibr" target="#b36">[37]</ref>, to improve LUSS models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LARGE-SCALE UNSUPERVISED SEMANTIC SEG-MENTATION BENCHMARK</head><p>The LUSS task aims to learn semantic segmentation from largescale images without direct/indirect human annotations. Given a large set of images, a LUSS model assigns self-learned labels to each pixel of all images. For ease of understanding, we give one of the possible pipelines for LUSS, as shown in Section 4. A LUSS model simultaneously learns category and shape representations from large-scale data without human annotation. The model uses the learned feature representations for label clustering and assignment to get the generated pixel-level labels. Then, the model is fine-tuned on the generated labels to refine the segmentation results. Ideally, label assignment and refining can be implicitly contained in the unsupervised representation learning process.</p><p>LUSS faces multiple challenges, e.g., semantic representation learning, category generation under large-scale data, and unsupervised setting. Moreover, the lack of benchmarks limits the development of the LUSS task. As such, we develop a LUSS benchmark with a clear objective, large-scale training data, and comprehensive evaluation protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Large-scale LUSS Dataset: ImageNet-S</head><p>The LUSS task is very challenging as it uses no human-annotated labels for training and requires large-scale data to learn rich representations. In principle, the scale of training images required by LUSS increases with the growth of image complexity, i.e., large category numbers and complex senses require more training data. Existing segmentation datasets can hardly support LUSS due to the large image complexity but small data scale. Some datasets, e.g., PASCAL VOC <ref type="bibr" target="#b9">[10]</ref> and CityScapes <ref type="bibr" target="#b6">[7]</ref>, contain a limited number of images under a few scenes. Other datasets, e.g., ADE20K <ref type="bibr" target="#b7">[8]</ref>, COCO <ref type="bibr" target="#b106">[107]</ref>, and COCO-Stuff <ref type="bibr" target="#b10">[11]</ref>, have complex images with a limited number of samples for each category, which is hard for LUSS models to learn rich representations of complex senses using limited data.</p><p>To remedy drawbacks in these datasets, common supervised segmentation approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref> fine-tune models pre-trained with the widely used large-scale ImageNet dataset <ref type="bibr" target="#b107">[108]</ref>, <ref type="bibr" target="#b108">[109]</ref>, <ref type="bibr" target="#b109">[110]</ref>, <ref type="bibr" target="#b110">[111]</ref>. However, recent research <ref type="bibr" target="#b111">[112]</ref>, <ref type="bibr" target="#b112">[113]</ref> suggested that performance on the ImageNet and downstream datasets is not always consistent due to the inconstancy of data distribution, data domain, and task objective. For LUSS, fine-tuning pre-trained models on downstream datasets complicates the evaluation and leads to possible unfair and biased comparisons. ImageNet has diverse classes, a large data scale, simple images, and sufficient images for each category, making learning rich representations feasible. Thus, ImageNet is widely used by most unsupervised learning methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>. However, ImageNet has only image-level annotation, and thus cannot be used for pixel-level evaluation of LUSS. To facilitate the LUSS task, we present a large-scale ImageNet-S dataset by collecting data from the ImageNet dataset <ref type="bibr" target="#b20">[21]</ref> and annotating pixel-level labels for LUSS evaluation. We remove the unsegmentable categories, e.g., bookshop, and utilize 919 categories in ImageNet. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the ImageNet-S dataset (see <ref type="figure" target="#fig_1">Figure 2</ref>) is much larger than existing datasets in terms of image amount and category diversity (see <ref type="figure" target="#fig_3">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Image Annotation.</head><p>We annotate the validation/testing sets and parts of the training set in the ImageNet-S dataset for LUSS evaluation. As the ImageNet dataset has incorrect labels and missing multiple categories, we annotate the pixel-level semantic segmentation masks following the relabeled image-level annotations in <ref type="bibr" target="#b31">[32]</ref> and further correct the missing and incorrect annotations. The objects indicating the image-level labels are annotated, and other parts are annotated as the 'other' category. 'Other' means the categories are not frequently appeared in the dataset or the surrounding environment. For validation/testing sets, we annotate all objects within the 919 selected categories. The parts that are difficult to distinguish are marked as 'ignore', which will not be used for evaluation. For the training set, we randomly pick ten images for each category and annotate objects corresponding to that category while other objects belonging to the 919 categories are labeled with 'ignore'.</p><p>Semantic segmentation mask annotation. Given the categories of an image, the annotator is asked to annotate the corresponding regions and assign the correct categories. The selected 919 categories in the ImageNet-S dataset have high diversity. Some instances cannot be easily distinguished even with given categories,     e.g., two breeds of dogs and uncommon things. To reduce the difficulty for annotators to identify categories, we splice four images with the same category into a four-square image. In this case, annotators can easily distinguish the common categories in the four-square image. For images with multiple complex categories, several groups of images containing the required categories are provided to help annotators identify categories. Since the categories in the ImageNet-S dataset follow a tree-like structure (see <ref type="figure" target="#fig_3">Figure 3</ref>), different annotators are given images from different subsets of the Word-Tree to further reduce the annotation difficulty. Images with a resolution below 1,000 ? 1,000 are resized to 1,000 ? 1,000. The annotator draws polygonal masks to the category-related regions with about 400 to 500 points on the contour for each image. Annotating on resized high-resolution images results in precise pixel-level semantic segmentation masks.</p><p>Labeling process. The annotation team for this dataset contains an organizer, four quality inspectors, and 15 annotators. We introduce the labeling process as follows:       Step 1. Annotators were instructed on how to annotate labels. Then annotators were asked to annotate a group of randomly picked images following instructions. Quality inspectors checked these annotated images, and failure cases were corrected and shown as examples to all annotators.</p><p>Step 2. The annotators were divided into several groups, and each group has a group leader. The organizer then assigned images to each group of annotators. After annotating the images, the group leader summarized all annotations and checks the annotation quality. Other annotators checked the annotations from the group leader in the group.</p><p>Step 3. The checked annotations were then given to the quality inspectors. Quality inspectors checked the annotations and give feedback regarding the failure cases. Common failure cases and corresponding explanations are sent to all groups to improve the annotation quality of the following images.</p><p>Step 4. The organizer then sampled images and checked the corresponding annotations to ensure the annotation quality.</p><p>Correct missing/incorrect labels. During the labeling process, we observed that there were still some missing and incorrect imagelevel annotations in <ref type="bibr" target="#b31">[32]</ref> due to the high diversity and large-scale properties of ImageNet. Therefore, we presented several schemes to correct labels as much as possible: 1) We found that some categories are related to each other, e.g., the spider and spider web usually appear in the same image. Based on the initial human-observed missing categories, we double-checked images whose categories are related to other categories. 2) We used supervised image-level classifiers, e.g., Swin transformer <ref type="bibr" target="#b113">[114]</ref> and Res2Net <ref type="bibr" target="#b109">[110]</ref>, to help find missing categories by checking the labels predicted with high confidence but not in the ground-truth. With these schemes, we managed to correct 296 mislabeled images and find 942 images with missing labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Statistics and distribution.</head><p>Image numbers. As shown in <ref type="table" target="#tab_0">Table 1</ref>, after removing the unsegmentable categories in the ImageNet dataset, e.g., bookshop, valley, and library, the ImageNet-S dataset contains 1,183,322 training, 12,419 validation, and 27,423 testing images from 919 categories. Many existing self-supervised representation learning methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b34">[35]</ref> are trained with the ImageNet dataset. For a fair comparison, we use the full ImageNet dataset that contains 1,281,167 training images for unsupervised representation learning and utilize the ImageNet-S training set for other processes in LUSS. We annotate 39,842 validation/testing images <ref type="bibr">and 9,190</ref> training images with precise pixel-level masks, and some visualized annotations are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Our pixel-level labeling enables the ImageNet-S dataset with multiple labels in each image. <ref type="table" target="#tab_2">Table 2</ref> gives the number of categories per image in the ImageNet-S validation/testing sets. A majority of images contain one category, and 8.6% of images have more than one category. ImageNet-S has simpler images and more categories than existing semantic segmentation datasets, which is suitable for the LUSS task considering the difficulty caused by no human annotation and large image and category numbers.</p><p>Category distribution. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, categories in the ImageNet-S dataset show a tree-like structure as they are extracted from the Word-Tree <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure" target="#fig_10">Figure 4</ref> shows the imagelevel and pixel-level number distribution among categories of the ImageNet-S dataset, i.e., the number of images/pixels per class. The training set and validation/testing sets have similar distributions. The number of images for most categories is balanced, while the number of pixels per category presents the long-tail distribution. The imbalanced pixel-level category distribution may introduce new challenges that are not considered in the image-level representation learning. Compared to the original ImageNet dataset with a similar number of images for each category in the validation set, the relabeled ImageNet-S validation/testing sets presents a more unbalanced number of images over categories.</p><p>Object size. As it is more difficult to segment smaller objects, we divide objects into groups, i.e., small (0%-5%), medium-small (5%-25%), medium-large (25%-50%), and large object size (50%-100%), according to the ratio of object size to the image size. The object size distribution shown in <ref type="figure" target="#fig_13">Figure 5</ref> indicates that most objects are relatively small.</p><p>Position distribution. We superimpose segmentation masks from the validation and testing sets to analyze the position distribution of semantic objects in the dataset, as shown in <ref type="figure" target="#fig_9">Figure 6</ref> (top). The objects in ImageNet-S tend to be in the centre of the image, which explains the effectiveness of the central crops strategy of existing self-supervised methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. We also superimpose the boundary of objects, as shown in <ref type="figure" target="#fig_9">Figure 6</ref> (down). It shows that objects cover almost all areas instead of only the central area of images. In addition, we compare the distributions of our dataset with COCO <ref type="bibr" target="#b106">[107]</ref> and Open Images <ref type="bibr" target="#b114">[115]</ref> datasets in <ref type="figure" target="#fig_9">Figure 6</ref>.    Our dataset and the other two datasets have similar distributions. The centre-skewed distribution is observed for all datasets, and we assume that humans might tend to record more centre-biased images. Interestingly, the distribution map of ImageNet-S is almost identical to the Open Images dataset, famous for its real-life property.</p><p>Annotation consistency. We validate the annotation quality by evaluating the annotation consistency of different people. We have asked four annotators to annotate the same 100 randomly picked images, respectively. Based on four sets of samples, we evaluate the average metrics between each pair using mask mIoU and boundary mIoU, as shown in <ref type="table" target="#tab_3">Table 3</ref>. The mask mIoU is 98.7%, showing a high annotation consistency. With a small d of 2%, the boundary mIoU still has 92.4%, indicating high constant boundary annotations. We visually observe that the main annotation differences are in the boundary regions. By comparing objects of different sizes, smaller objects have lower annotation consistency since the boundary regions occupy a larger proportion of the annotation mask in smaller objects.</p><p>ImageNet-S-50/300 under a limited budget. To facilitate the research under a low computational budget, we develop two subsets containing 50 and 300 categories, namely ImageNet-S 50 and ImageNet-S 300 . Considering the difficulty of the LUSS task, we choose 50 distinguishable categories in daily life for ImageNet-S 50 . The ImageNet-S 300 is composed of ImageNet-S 50 and 250</p><p>ImageNet-S Open Images COCO randomly sampled categories. The number of images in ImageNet-S 50 and ImageNet-S 300 are shown in <ref type="table" target="#tab_0">Table 1</ref>. Even the ImageNet-S 50 subset has more images than most semantic segmentation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Evaluation Protocols</head><p>Due to the lack of ground-truth (GT) labels during training, LUSS models cannot be directly evaluated as in the supervised setting. We present three evaluation protocols for LUSS, including the fully unsupervised evaluation, semi-supervised evaluation, and distance matching evaluation.</p><p>Fully unsupervised protocol. The fully unsupervised evaluation protocol requires no human-annotated labels during training and only needs the validation/testing set for evaluation. Unlike the supervised tasks, categories are generated by the model in the LUSS task, which needs to be matched with GT categories during evaluation. We present the default image-level matching scheme, while a more effective matching scheme should improve LUSS evaluation performance. Suppose the set for matching (normally validation set) has N images and C categories. The number of categories is implicitly contained in the training dataset as the dataset has C major categories. We assume the unsupervised model should learn to generate more than C categories from the dataset during training. The default image-level matching scheme only matches C generated categories with C ground-truth categories.</p><formula xml:id="formula_0">Given the image set D = {D k , k ? [1, N ]} with GT labels G = {G k , k ? [1, N ]} and predicted labels P = {P k , k ? [1, N ]},</formula><p>where G k and P k are the GT and predicted category sets of the image D k . We calculate the matching matrix S ? R C?C between generated and GT categories, in which S ij , representing the matching degree between the i-th generated category and the j-th GT category, is larger when two categories are more likely to be the same category:</p><formula xml:id="formula_1">S ij = N k=1 I{(i, j) ? P k ? G k },<label>(1)</label></formula><p>where P k ? G k is the Cartesian product of P k and G k , and the indicator I equals 1 when (i, j) belongs to P k ? G k .</p><p>With the matching matrix S ? R C?C , we find the bijection f : i ? j between generated and GT categories using the Hungarian algorithm <ref type="bibr" target="#b116">[117]</ref> to maximize C i=1 S i,f (i) . We observe that there are failed matching cases and some generated categories are not in the ground-truth categories, indicating the limit of our baseline matching method. We expect future works to propose more effective matching methods to solve this problem.</p><p>Semi-supervised protocol. We can conduct semi-supervised finetuning to evaluate LUSS models as we annotate about 1% of training images with pixel-level labels. The semi-supervised evaluation protocol requires fine-tuning the trained LUSS models with human-labeled training images. Therefore, this protocol does not need matching generated and GT category. Also, this protocol is suitable for real-world applications where a small part of images are labeled and many images are unlabeled.</p><p>Distance matching protocol. In the distance matching evaluation protocol, we directly get the embeddings of GT categories with the pixel-level labeled training images and match them with embeddings in validation/testing sets to assign labels. Specifically, for pixels with the same category in an image (including the 'other' category), we get the averaged embeddings and the corresponding label in the training set. Then we infer the segmentation masks of the validation/testing sets using the k-NN classifier <ref type="bibr" target="#b83">[84]</ref>. For each pixel embedding in the validation/testing sets, we find the top-k similar embeddings in the training set and the corresponding labels. The assigned label of each pixel is determined by the voting among these k labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Evaluation metrics</head><p>We use the mean intersection over union (mIoU), boundary mIoU (b-mIoU), image-level accuracy (Img-Acc), and F-measure (F ? ) as the evaluation metrics for the LUSS task. During the evaluation, all images are evaluated with the original image resolution. mIoU and b-mIoU are comprehensive evaluation metrics, while Img-Acc and F ? explain the performance from category and shape aspects. We give the implementation details of these evaluation metrics in the supplementary.</p><p>Mean IOU. Similar to the supervised semantic segmentation task <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, we utilize the mIoU metric to evaluate the segmentation mask quality. Apart from the major categories, the 'other' category is also considered to get mIoU.</p><p>Boundary mean IoU. Unlike the mask mIoU above that measures all object regions, the boundary mean IoU (b-mIoU) <ref type="bibr" target="#b115">[116]</ref> focuses on the boundary regions. We use the boundary mIoU to measure the semantic segmentation quality of boundary regions. According to the segmentation consistency analysis in Section 3.1.2, we use d = 3% for boundary IoU <ref type="bibr" target="#b115">[116]</ref>.</p><p>Image-level accuracy. The Img-Acc can evaluate the category representation ability of models. As many images contain multiple labels, we follow <ref type="bibr" target="#b31">[32]</ref> and treat the predicted label as correct if the predicted category with the largest area is within the GT label list.</p><p>F-measure. In addition to category-related representation, we utilize F ? to evaluate the shape quality <ref type="bibr" target="#b117">[118]</ref>, which ignores the semantic categories. We treat major categories as the foreground category and the 'other' category as the background category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seg. head</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large-scale data Supervision</head><p>Self-supervision</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering Pseudo Labels</head><p>Fine-tuning Semantic seg. <ref type="figure" target="#fig_7">Fig. 7</ref>. The pipeline of our method for the LUSS task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A LUSS METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>We summarize the main challenges of the 5) The large-scale training data helps to learn rich representations in an unsupervised learning manner but inevitably causes a large amount of training cost, which requires improving the training efficiency.</p><p>Considering the above challenges, we propose a new method for LUSS, namely PASS, (see <ref type="figure" target="#fig_7">Figure 7</ref>), containing four steps. 1) A randomly initialized model is trained with self-supervision of pretext tasks to learn shape and category representations. After representation learning, we obtain the features set for all training images. 2) We then apply a pixel-attention-based clustering scheme to obtain pseudo categories and assign generated categories to each image pixel. 3) We fine-tune the pre-trained model with the generated pseudo labels to improve the segmentation quality. 4) During inference, the LUSS model assigns generated labels to each pixel of images, same to the supervised model. Noted that the pipeline in our method is not the only option, and other pipelines are also encouraged for the LUSS task. We now give a detailed introduction of each step as follows. Some frequently used symbols are listed in <ref type="table" target="#tab_5">Table 4</ref> for easier understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised Representation Learning</head><p>For the first step in our LUSS method, a randomly initialized model, e.g., ResNet, is trained with self-supervision of pretext tasks to learn semantic representations. The LUSS task requires category-related representation to distinguish scenes from different classes and shape-related representation to form the shape of objects. Prior works have made many efforts to learn image-level category-related representation or pixel-level representation <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b94">[95]</ref>. However, the image-level methods often ignore shaperelated features. The pixel-level methods focus on the transfer learning performance on supervised downstream tasks. As observed by <ref type="bibr" target="#b118">[119]</ref>, the performance of most downstream tasks relies on lowlevel feature from the early stage of the network. Thus, pixel-level methods that perform well on downstream tasks may not learn high-level semantic features with category and shape information.</p><p>To obtain a powerful representation for LUSS, we present two self-supervised learning strategies to enhance both category and shape representation, including 1) a non-contrastive pixel-to-pixel  representation alignment strategy to enhance the pixel-level shaperelated representation without hurting the instance-level category representation. 2) a deep-to-shallow supervision strategy to enhance the representation quality of mid-level features of the network.</p><p>Non-contrastive pixel-to-pixel representation alignment. Pixellevel shape-related representation aims to enhance the feature discrimination ability in pixel-level, i.e., pixels within the same category or from the same image position of different views should have consistent representations, and vice versa. We observe that most existing pixel-level methods perform worse than image-level methods on the LUSS task. We argue existing pixel-level methods focus too much on the pixel-level distinction, thus resulting in semantic variation among pixels within the same instance. To avoid the side-effect of pixel-level representation to instance-level category representation, we propose a non-contrastive pixel-to-pixel representation alignment strategy that aligns the features from the same image position of different views but avoids pushing features from other positions away. As shown in <ref type="figure" target="#fig_15">Figure 8</ref>, given the feature pair predicted from two views of the image, we extract features (z 1 ,z 2 ) of the overlapped pixels and get the pixel-level embedding pairs (? 1 ,? 2 ) by the projection? = M p (z), where M p is the pixel-level multi-layer projection (MLP) layer composed of the two conv1?1 layers and activation layers. We show in Section 5.3.1 that the projection M p (z) ensures less interference of pixel-level representation to the category representation. We utilize the pixel-to-pixel alignment to align the overlapped pixel-level embeddings from two views using the asymmetric loss:</p><formula xml:id="formula_2">L P 2P = L s (P(? 1 ), G(? 2 )) + L s ( G(? 1 ), P(? 2 )),<label>(2)</label></formula><p>where the projection P is a pixel-level MLP predictor, G is the stop gradient operation to avoid the collapse of predictor <ref type="bibr" target="#b69">[70]</ref>, and L s is a cosine similarity loss. The proposed non-contrastive pixel-to-pixel alignment forms a robust pixel-level representation across different views and maintains the category representation ability.</p><p>Deep-to-shallow supervision. The quality of low/mid-level representation, i.e., representation in early network layers, is proven critical to vision tasks <ref type="bibr" target="#b112">[113]</ref>, <ref type="bibr" target="#b119">[120]</ref>. Islam et al. <ref type="bibr" target="#b119">[120]</ref> reveal representations with rich low/mid-level semantics in early layers result in quick adaptation to a new task. Similarly, Kotar et al. <ref type="bibr" target="#b112">[113]</ref> show the benefit of high-quality low-level features learned with contrastive-based methods. Most existing works optimize midlevel representation by the indirect gradient back-propagation from the high-level of the network <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b71">[72]</ref>. We observe that directly applying low/mid-level features for representation learning leads to sub-optimal performance as these features lack semantics. Therefore, we propose a deep-to-shallow supervision strategy to enhance the representation of low/mid-level features with the guidance of high-quality high-level features.</p><p>As shown in <ref type="figure">Figure 9</ref>, given two views augmented from one image, we obtain the feature pairs (z 2 ) from the s stage of the network. We mainly explore the effect of deep-to-shallow supervision on image-level for simplicity. Given a network with four stages, the image-level embeddings used for deep-to-shallow supervision are obtained as follows:</p><formula xml:id="formula_3">u (s) i = M s I (P(z (s) i )) s = 4; M s I (P(M s K (z (s) i ))) s &lt; 4,<label>(3)</label></formula><p>where P is the global average pooling operation in the spatial dimension, M s I and M s K are the image-level/pixel-level MLP layers of the stage s, respectively. We observe that directly pooling the mid-level features causes the representation collapse, and adding M s K avoids this problem. In the deep-to-shallow supervision strategy, the embeddings from the last stage of one view are used to supervise embeddings of another view from all stages:</p><formula xml:id="formula_4">L D2S = 1 |S| j?S j L I (u (4) 1 , u (j) 2 ) + 1 |S| j?S j L I (u (4) 2 , u (j) 1 ),<label>(4)</label></formula><p>where S is a set containing stages used for deep-to-shallow supervision, and L I is the image-level loss. L I can be multiple definitions, and we use the clustering loss <ref type="bibr" target="#b32">[33]</ref> as L I in our work.</p><p>Training loss for representation learning. Our proposed pixel-topixel alignment and deep-to-shallow supervision can cooperate with existing methods to improve representation quality. The summarized loss for the unsupervised representation learning step is written as:</p><formula xml:id="formula_5">L sum = L P 2P + L D2S + L e ,<label>(5)</label></formula><p>where L e is the loss of existing methods, e.g., SwAV <ref type="bibr" target="#b32">[33]</ref> and PixelPro <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pixel-label Generation with Pixel-Attention</head><p>After representation learning, we obtain the features set Z = {z k ? R L?H?W , k ? [1, N ]} for all training images, where N is the number of images, L, H, and W are the number of dimensions, height, and width of the output features. We cluster Z to obtain C generated categories and assign generated categories to each pixel. A straightforward way for label generation is to cluster embeddings of all pixels in the training set, which is too costly due to the large-scale data in LUSS, e.g., clustering training images of ImageNet-S at pixel-level with 7 ? 7 resolution requires about 114 hours. An alternative is to use image-level features pooled on the spatial dimension to save clustering costs. However, many irrelevant embeddings are included in the pooled features, hurting the clustering quality. We observe that the learned features tend to focus on the regions with more semantic meanings, i.e., pixels with more useful semantic information contribute more to the convergence of unsupervised representation learning. Based on this observation, we propose a pixel-attention scheme to highlight meaningful semantic regions, facilitating the pixel-level label generation with imagelevel features. Specifically, we add a pixel-attention head at the output of models and fine-tune it with representation learning losses to filter out the less semantic meaningful regions. Filtering features with pixel-attention reduces noise in the pooled imagelevel embeddings, improving the clustering quality. Also, the pixelattention separates the semantic regions with less meaningful regions, generating more accurate object shapes during pixellevel label generation. We give the implementation detail of pixelattention in fine-tuning and label generation steps.</p><p>Fine-tuning pixel-attention. Given the feature z of one image predicted by the model, representation learning methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b32">[33]</ref>, calculate losses with the pooled feature embeddings M I (P(z)), where M I is the image-level MLP layer. The pooling operation treats all pixels equally, inevitably introducing noises to image-level embeddings as not all pixels represent meaningful semantics. Our pixel-attention is defined as: where M A is the pixel-level MLP layer, ? ? R L are learnable parameters initialized with zero, ? is a sigmoid function to restrict output attention value, and z is the L2 normalization operation applied on the channel dimension of feature z. Each channel of z has a corresponding pixel-attention map. We multiply the pixelattention to feature z and obtain the pixel-attention enhanced imagelevel embeddingv = M I (P(c(z) ? z )). During fine-tuning, we detach gradients to the network and fine-tune the pixel-attention module by optimizing representation loss calculated fromv. We observe that fine-tuning pixel-attention module with clustering loss <ref type="bibr" target="#b32">[33]</ref> results in decent shape-related pixel-attention results (see <ref type="figure" target="#fig_18">Figure 10</ref>).</p><formula xml:id="formula_6">c(z) = ?(M A ( z ) + ?),<label>(6)</label></formula><p>Label generation with pixel-attention. Based on pixel-attention c(z), we obtain the pixel-attention enhanced image-level featur?</p><formula xml:id="formula_7">Z = {? k ? R L , k ? [1, N ]}, where? k = P(c(z) k ? z k ).</formula><p>We conduct the k-means clustering over? to generate the cluster center K ? R L?C of C categories. With the generated categories, we need to assign pixel-level pseudo labels Q = {q k ? R (C+1)?H?W , k ? [1, N ]} to images. We show in <ref type="figure" target="#fig_18">Figure 10</ref> the fine-tuned pixel-attention highlights semantic regions of images. Therefore, we extract regions with rich semantic information based on pixel-attention:</p><formula xml:id="formula_8">d(z) = 0 1 L 0?i&lt;L i c(z) i &lt; ? ; 1 1 L 0?i&lt;L i c(z) i ? ?,<label>(7)</label></formula><p>where ? is a pre-defined threshold between major categories and the 'other' category, and regions with pixel-attention below ? are traded as the 'other' category. For each pixel in the region of major categories, we assign one category in the cluster center K that has the minimal distance to the feature embedding of the pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Fine-tuning and Inference</head><p>During the fine-tuning step, we load the weights pre-trained with representation learning and add a conv 1?1 layer with L ? (C + 1) channels as the segmentation head. The output features Y = {y k ? R (C+1)?H?W , k ? [1, N ]} from this head are supervised with Q to fine-tune the model using cross-entropy loss. During inference, the LUSS model acts like a fully supervised semantic segmentation model. For each pixel embedding w ? R C+1 in y k , we get the segmentation labels as follow:</p><formula xml:id="formula_9">w = arg max i?[1,C+1] (w i ).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>Training details on the representation learning step. We use ResNet-18 network for the ImageNet-S 50 dataset and utilize ResNet-50 for ImageNet-S 300 and ImageNet-S datasets. For a fair comparison, all networks are trained with a mini-batch size of 256 for 200 epochs in ImageNet-S 50 and 100 epochs in ImageNet-S 300 /ImageNet-S. Our method cooperates with the image-level method SwAV <ref type="bibr" target="#b32">[33]</ref> and the pixel-level method PixelPro <ref type="bibr" target="#b34">[35]</ref>. Following SwAV <ref type="bibr" target="#b32">[33]</ref>, a LARS optimizer is used to update the network with a weight decay of 1e-6 and a momentum of 0.9. The initial learning rate is 0.6 and gradually decays to 6e-6 with the cosine learning rate schedule. For ImageNet-S 300 and ImageNet-S datasets, to make a fair comparison with other methods, we only use two crops with the size of 224?224 for training, and the multi-crop training strategy <ref type="bibr" target="#b32">[33]</ref> is not applied. Same as SwAV, a queue with the length of 3,840 is used beginning from 15 epochs, and the prototypes for clustering are frozen before 5,005 iterations. When training on ImageNet-S 50 , the queue is set to 2048, and prototypes are frozen before 1,001 iterations to ease the convergence. We use the multicrop training strategy containing 6 crops with the size of 96?96 and two crops with the size of 224?224 for training on ImageNet-S 50 . When cooperating with PixelPro <ref type="bibr" target="#b34">[35]</ref>, the training schemes are consistent with the official setting. We train the network with the initial learning rate of 1.0 using a LARS optimizer. After five warm-up epochs, the learning rate gradually decays to 1e-6 with the cosine learning rate schedule.</p><p>Training details on the fine-tuning step. To generate pixel-level labels, we first fine-tune the pixel-attention module for 20 epochs while fixing the model parameters trained in the unsupervised representation learning step. We apply the clustering loss <ref type="bibr" target="#b32">[33]</ref> to fine-tune the pixel-attention module by default. The training strategy remains the same as that in the representation learning step.</p><p>The representation learning losses are removed in the finetuning step, and an extra cross-entropy loss is added to supervise the segmentation head. We load the model weights pre-trained on the representation learning step and fine-tune them for 20 epochs. We train the network using a LARS optimizer with a weight decay of 1e-6, a mini-batch size of 256, and a momentum of 0.9. The initial learning rate is 0.6 and gradually decays to 6e-6 with the cosine learning rate schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with USS Methods</head><p>In this section, we evaluate the performance of the proposed LUSS method on the ImageNet-S dataset using the fully unsupervised evaluation protocol. <ref type="table">Table 5</ref> shows our method achieves reasonable performance on the large-scale data. The visualization shown in <ref type="figure" target="#fig_19">Figure 11</ref> indicates that unsupervised semantic segmentation with the large-scale dataset is achievable.</p><p>Comparison with unsupervised semantic segmentation methods. Existing unsupervised semantic segmentation (USS) methods are designed for relatively small scale data, thus cannot be directly used on the full-scale ImageNet-S dataset due to the training time limit. Therefore, we compare our LUSS method with existing USS methods on the ImageNet-S 50 subset, as shown <ref type="bibr">TABLE 5</ref> Comparison of our proposed LUSS method and existing USS methods on the ImageNet-S dataset using the fully unsupervised evaluation protocol.</p><p>Test mIoU under different object sizes are provided. ? means train model for 200 epochs from scratch. PASS s/p denotes using SwAV <ref type="bibr" target="#b32">[33]</ref> and PixelPro <ref type="bibr" target="#b34">[35]</ref> as the Le in (5), respectively. S means the method use saliency maps. I means initialization with supervised ImageNet 1k pre-training.</p><p>By default, the 'other' category is used to calculate mIoU and b-mIoU. We also give the performance without using the 'other' category in the supplementary. in <ref type="table">Table 5</ref>. All methods trained on ImageNet-S 50 utilize the ResNet-18 network for a fair comparison. The comparison is not strictly fair because some existing USS methods are not trained in fully unsupervised settings. For example, MDC <ref type="bibr" target="#b87">[88]</ref> and PiCIE <ref type="bibr" target="#b45">[46]</ref> initialize models with supervised ImageNet 1k pre-trained weights. These two methods suffer from large performance drops when using MoCo <ref type="bibr" target="#b24">[25]</ref> pre-trained weights, indicating that supervised pre-training is a vital step. MaskContrast <ref type="bibr" target="#b30">[31]</ref> is initialized with the MoCo pre-trained weights and trained with extra saliency maps as supervision. There is a large performance loss when this model is trained from scratch. In contrast, our LUSS method is trained from scratch with no direct/indirect human supervision. Our method includes the proposed representation learning strategy, label generation approach, and fine-tuning scheme. To validate the generalizability of our method, we implement our method based on two representation learning methods, i.e., SwAV <ref type="bibr" target="#b32">[33]</ref> and PixelPro <ref type="bibr" target="#b34">[35]</ref>. Our method outperforms existing USS methods with a clear margin in mIoU. Benefiting from extra saliency maps, MaskContrast has a higher F ? than our method. Using the same saliency maps, our saliency-enhanced method clearly outperforms MaskContrast in F ? and achieves much higher mIoU. Note that saliency maps in <ref type="bibr" target="#b30">[31]</ref> are not strictly unsupervised version because the supervised ImageNet pretraining weights are used. When using saliency maps from the fully unsupervised method RC <ref type="bibr" target="#b117">[118]</ref>, our method still achieves competitive performance. We also implement other USS methods, e.g., IIC <ref type="bibr" target="#b89">[90]</ref>. However, as these methods are designed for semantic segmentation under several categories, they fail to converge on the ImageNet-S 50 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LUSS</head><p>Performance of different object sizes. As introduced in Section 3.1.2, the ImageNet-S dataset is divided into multiple groups by object size. We evaluate the test mIoU under different object sizes, as shown in <ref type="table">Table 5</ref>. The performance of small objects is worse than large objects in mask and boundary mIoU, indicating that small objects need a model with a more precise pixel-level representation and segmentation ability. Note that performance in boundary mIoU has smaller gaps of different object sizes than mask mIoU since the boundary mIoU is more robust to object size changes.</p><p>Difference between different data scales. As shown in <ref type="table">Table 5</ref>, we train our method on ImageNet-S 50 , ImageNet-S 300 , and ImageNet-S datasets. The performance scores drop with the growth of the data scale, showing the great challenge of the large-scale data to unsupervised semantic segmentation. We observe that PixelPro based method outperforms the SwAV based method on the ImageNet-S 50 dataset, but the SwAV based method achieves better performance on ImageNet-S 300 and ImageNet-S datasets. We conclude that different data scales prefer different representation learning strategies.</p><p>To evaluate the performance gap between large and small datasets, we train models on large sets and evaluate models on small sets, as shown in <ref type="table" target="#tab_7">Table 6</ref>. Models trained on large sets are inferior to models trained on small sets. Testing on the ImageNet-S 50 set, the model trained on ImageNet-S 50 achieves the best performance for all metrics, while the model trained with ImageNet-S 919 has the worst scores. A similar trend is also observed when evaluating on ImageNet-S 300 set. These results indicate that training unsupervised models on larger datasets is harder than on small datasets, showing the great challenge of largescale data. However, the performance gaps are relatively small, which may be closed by stronger future methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Representation Learning</head><p>In this section, we benchmark our proposed and some existing unsupervised representation learning methods on the LUSS task. We conduct experiments on the ImageNet-S 300 dataset to save computational cost unless otherwise stated. To avoid the influence of fine-tuning step in LUSS, we apply the distance matching protocol for LUSS evaluation as introduced in Section 3.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation of the proposed representation learning strategy.</head><p>We implement the proposed non-negative pixel-to-pixel (P2P) alignment and deep-to-shallow (D2S) supervision on top of SwAV <ref type="bibr" target="#b32">[33]</ref> and PixelPro <ref type="bibr" target="#b34">[35]</ref>. <ref type="table" target="#tab_8">Table 7</ref>(a) shows that PixelPro performs much worse than SwAV due to the missing category-related representation ability needed by LUSS. Therefore, we add the clustering loss <ref type="bibr" target="#b32">[33]</ref> to PixelPro to form a reasonable baseline model for LUSS. As shown in <ref type="table" target="#tab_8">Table 7</ref>(a), our method improves the SwAV and PixelPro with 2.6% and 7.6% in test mIoU on the ImageNet-S 300 set, respectively. Specifically, the P2P alignment has a gain of 2.2% in test mIoU compared to the image-level method SwAV, and it also improves the clustering-loss enhanced PixelPro by 0.5% in test mIoU. The D2S supervision brings the further gain of 0.4% and 1.4% over SwAV and PixelPro based baselines, respectively. In summary, the P2P alignment effectively enhances the pixellevel representation of image-level methods, and D2S supervision enriches the instance-level category representation of pixel-level methods. The P2P alignment and D2S supervision still improve the methods designed for pixel-level and image-level representation, respectively, showing the robustness of the proposed strategies. As shown in <ref type="table" target="#tab_11">Table 9</ref>, our proposed representation learning strategy also outperforms baselines on the ImageNet-S dataset.</p><p>Non-negative pixel-to-pixel alignment. We utilize the nonnegative P2P alignment to enhance the pixel-level representation without hurting the instance-level category representation. We also compare different pixel-level alignment strategies, including clustering, contrastive, and non-contrastive types. We set pixels at the same position of two views as positive pairs and other pixels as negative pixels. As shown in <ref type="table" target="#tab_9">Table 8</ref>(a), both three pixel-level alignment strategies have higher F ? compared to the baseline, showing improved shape representation quality. However, due to the semantic variation among pixels in the same object, clustering and contrastive losses suffer from the performance drop on mIoU and Img-Acc. In contrast, the proposed non-negative P2P alignment has performance gains over the baseline in mIoU and Img-Acc, due to maintaining the representation constancy of pixels belonging to the same semantic instance. We also analyze the effectiveness of the projection M p (z) in  supervision outperforms the same-stage supervision in mIoU and Img-Acc. By default, we use the deep features from one view to supervising shallow features from another view. In <ref type="table" target="#tab_9">Table 8</ref>(d), we study the effects of applying D2S supervision on features belonging to the same view. Cross-view supervision is slightly better than same-view supervision. We observe that the training loss of same-view supervision is lower than cross-view supervision. We conclude that the same-view supervision over-fits to a sub-optimal solution, hurting the evaluation performance. The D2S supervises multiple features from different early stages of the network. As shown in <ref type="table" target="#tab_8">Table 7</ref>(a), we study the effects of supervising different stages on SwAV and PixelPro based methods. We observe that different methods require different stages to get optimal results, e.g., supervising stage 3-2 is worse than supervising stage 3 in SwAV, but PixelPro benefits from more supervision to the early stages. We choose the stages for D2S supervision by ablation study.</p><p>Benchmarking unsupervised learning methods. To analyze the representation ability of unsupervised learning methods on the LUSS task, we categorize and benchmark some representative methods, including contrastive, non-contrastive, clustering, and pixel-level methods. As shown in <ref type="table" target="#tab_11">Table 9</ref>, image-level methods have a clear advantage over pixel-level methods on mIoU, imagelevel accuracy, and F ? . Pixel-level methods focus too much on the pixel-level distinction, resulting in semantic variation among pixels within the same instance, i.e., one instance contains multiple categories. In comparison, image-level methods provide constant instance-level category-related representation as these Performance comparison of unsupervised representation learning methods and our proposed representation learning enhancement methods using distance matching evaluation protocol. PASS s/p denotes using SwAV <ref type="bibr" target="#b32">[33]</ref> and PixelPro <ref type="bibr" target="#b34">[35]</ref> as the Le in (5), respectively. All models are trained with <ref type="bibr" target="#b99">100</ref>  losses encourage distinguishing among images. However, pixellevel representation is vital to the LUSS task as our proposed non-contrastive P2P alignment method has a considerable gain over the image-level method SwAV. We observe that the clustering methods outperform the contrastive and non-contrastive methods in image-level accuracy but have worse performance on shape-related F ? . The clustering strategy encourages stronger category-related representations with category centroids than contrastive and noncontrastive methods. But due to all pixels of one image in clustering methods being close to category centroids, the representation difference between major and other categories is weakened. The image-level supervised method has better category centroids than the clustering method, and it also has worse F ? than clustering methods. These results explain why clustering methods have worse F ? .</p><p>What role does the category play in the LUSS task? To answer this question, we use the models trained with image-level supervision as the baseline. As shown in <ref type="table" target="#tab_11">Table 9</ref>, the supervised model performs better than the unsupervised models in mIoU. In addition, it outperforms unsupervised models in terms of imagelevel accuracy by a large margin. In contrast, the performance in shape-related metric, i.e., F ? , is worse than most unsupervised methods. These results show that category features indeed facilitate the LUSS task. However, shape features cannot be learned solely </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Label generation and Fine-tuning</head><p>We evaluate the effectiveness of the proposed pixel-attentionbased label generation and fine-tuning scheme using the fully unsupervised evaluation protocol as described in Section 3.2.1. We conduct ablation on the ImageNet-S 50 set unless otherwise stated.</p><p>Effect of pixel-label generation. We compare our proposed pixelattention-based pixel-label generation method with image-level and pixel-level label generation methods. We briefly introduce the label generation and fine-tuning process using image-level and pixellevel label generation methods, respectively. The image-level label generation method clusters C categories over the pooled imagelevel embeddings and assign image-level labels to each image. During fine-tuning, a fully connected (FC) layer is supervised with image-level labels. The FC layer is replaced with a 1?1 conv layer to obtain pixel-level segmentation masks during inference. Due to lacking the 'other' category, we apply the class activation mapping (CAM) based mask generation method that is widely used by WSSS methods to generate the final segmentation masks. The implementation details are introduced in the supplementary.</p><p>Clustering on pixel-level embeddings is too costly on the largescale ImageNet-S dataset. Instead, we implement the pixel-level method on the ImageNet-S 50 set for comparison. We cluster C + 1 categories using pixel-level embeddings and fine-tune them with the pixel-level labels. As shown in <ref type="table" target="#tab_0">Table 10</ref>(a), the proposed pixel-attention-based label generation method outperforms imagelevel and pixel-level methods with a considerable margin. The image-level method has better F ? than our method. We apply this inference strategy in our method, and the F ? is also significantly improved while the mIoU has negligible change.</p><p>Clustering time comparison. We compare the clustering time of pixel-attention-based label generation with the other two label generation methods in <ref type="table" target="#tab_0">Table 10</ref>(b). Our method has the same clustering time as the image-level method as they both use image-level embeddings. Using output feature maps with the lowresolution of 7?7, the pixel-level method is much slower than our method due to the huge number of pixels in the training set. When clustering on the full ImageNet-S dataset, the time of the pixel-level method is about 114 hours, which is unacceptable for real usage.</p><p>Shared/unshared pixel-attention for output features. By default, we generate a unique pixel-attention map for each channel of output features. We also study the effect of using one shared pixel-attention map for all channels. The results in <ref type="table" target="#tab_0">Table 10</ref>(c) indicate that using an unshared pixel-attention map for each channel results in better performance. We visualize the pixel-attention maps of different channels in <ref type="figure" target="#fig_18">Figure 10</ref>. Most channels focus on the semantic regions, while a few channels highlight the background regions. Also, the focus of each pixel-attention map is not identical, explaining the effectiveness of unshared pixel-attention.</p><p>Effect of fine-tuning. Our pixel-attention-based label generation method directly generates pixel-level segmentation masks, i.e., pixel-level labels. We compare the performance before/after the fine-tuning step to validate the effect of fine-tuning step in our LUSS method. As shown in <ref type="table" target="#tab_0">Table 10</ref>(d), fine-tuning improves the test mIoU by 3.3%, indicating that the generated pixel-level labels are still noisy and fine-tuning further improves the semantic segmentation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Transfer Learning to Other Tasks</head><p>Before the proposed LUSS task, unsupervised representation learning methods mostly served as pre-training schemes for transfer learning on downstream tasks <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b34">[35]</ref>. The LUSS task requires shape-related and category-related representations from self-supervised representation learning. In this section, we study if the representation learned for the LUSS task benefits the pixel-level downstream tasks, e.g., semantic segmentation, instance segmentation, and object detection. We also compare the effects of representation learning methods on LUSS and downstream tasks. For fair comparisons, the ResNet-50 <ref type="bibr" target="#b107">[108]</ref> network is pre-trained on the ImageNet-S 300 or ImageNet-S datasets with 100 epochs using different representation learning methods unless otherwise stated.</p><p>Instance segmentation and object detection. We utilize MaskR-CNN <ref type="bibr" target="#b26">[27]</ref> as the detector for instance segmentation and object detection tasks. Models are trained on the COCO17 <ref type="bibr" target="#b106">[107]</ref> training set and evaluated on the validation set. Following common settings <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b34">[35]</ref>, we load the weights of ResNet-50 pretrained on different representation learning methods and apply the 1? training schedule. As shown in <ref type="table" target="#tab_0">Table 11</ref>, we validate our proposed learning strategies, i.e., non-contrastive P2P alignment and D2S supervision based on the SwAV <ref type="bibr" target="#b32">[33]</ref> and PixelPro <ref type="bibr" target="#b34">[35]</ref>. We first compare models pre-trained on the ImageNet-S 300 dataset. On instance segmentation, our method improves the SwAV and PixelPro by 1.4% and 1.0% in mAP, respectively. Similarly, the learning methods pre-trained on ImageNet-S 300 and ImageNet-S datasets. All models are trained with 100 epochs. PASS s/p denotes using SwAV <ref type="bibr" target="#b32">[33]</ref> and PixelPro <ref type="bibr" target="#b34">[35]</ref> as the Le in (5), respectively.</p><p>Supervised means initializing the model with image-level supervised pre-training. performance gain of our method over SwAV and PixelPro are 1.7% and 1.2% in mAP on object detection, respectively. These results prove that our representation learning method for the LUSS task has constant performance gains over different baselines on instance segmentation and object segmentation tasks. The pixel-level method PixelPro outperforms other image-level methods, e.g., SwAV, AdCo, and SimSiam, proving that pixel-level methods have a stronger transferring ability to these two pixel-level downstream tasks. When pre-trained on the full ImageNet-S dataset, our method still outperforms baselines, e.g., PixelPro based method has gains of 0.6% and 0.7% in mAP on instance segmentation and object detection tasks, respectively.</p><p>Semantic segmentation. We also transfer pre-trained models to the semantic segmentation task on the PASCAL VOC dataset <ref type="bibr" target="#b123">[124]</ref>, using ResNet-50 based Deeplab V3+ <ref type="bibr" target="#b11">[12]</ref> network. The network is trained on the Pascal VOC SBD training set <ref type="bibr" target="#b124">[125]</ref> and evaluated on the validation set. Following the training setting of <ref type="bibr" target="#b125">[126]</ref>, we train the network for 20k iterations with a batch size of 16. The images are scaled with a ratio of 0.5 to 2.0 and then cropped to 512 for training. When pre-trained on the ImageNet-S 300 dataset, our method outperforms SwAV and PixelPro baselines by 1.9% and 2.3% in mIoU, respectively. The performance gains over baselines are 2.3% and 2.2% in mIoU using the ImageNet-S pre-trained models. Pixel-level method PixelPro has a clear advantage over other image-level methods, showing the pixel-level representation is crucial for semantic segmentation. The contrast-based methods are better than clustering and non-contrastive methods for semantic segmentation though they are both image-level methods.</p><p>Relation between LUSS and transfer learning. We compare representation learning methods on the LUSS and downstream tasks in <ref type="table" target="#tab_11">Table 9</ref> and <ref type="table" target="#tab_0">Table 11</ref>, respectively. Compared among image-level methods, the clustering method SwAV has better performance on the LUSS task due to the high category accuracy. On downstream tasks, SwAV is inferior to many methods that achieve worse performance on the LUSS task. For example, on the downstream instance segmentation task, the contrastive method MoCov2 has a gain of 1.3% in mAP over SwAV but has a 10% gap in mIoU on the LUSS task. This observation is constant with the finding et al. <ref type="bibr" target="#b112">[113]</ref> that contrastive methods learn better lowlevel features to benefit pixel-level downstream tasks. Compared to image-level methods, the pixel-level method PixelPro has a clear advantage over image-level methods. But its performance is worse than many image-level methods on the LUSS task. The pixellevel methods learn distinguishable pixel-level representations for downstream tasks but lack enough category-related representations for the LUSS task. Comparing methods within one category, most of the well-performed methods on the LUSS task achieve better performance on downstream tasks. Therefore, the LUSS and downstream tasks require different representations but all benefit from high-quality representations. We also demonstrate the effectiveness of our proposed P2P alignment and D2S supervision on the LUSS task ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">LUSS vs. WSSS</head><p>Weakly supervised semantic segmentation (WSSS) with imagelevel labels learn to segment semantic objects with only imagelevel labels. We analyze the influence of typical settings in WSSS methods on the ImageNet-S 50 dataset, e.g., supervised ImageNet 1k pre-trained models <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b102">[103]</ref>, image-level GT labels <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b101">[102]</ref>, and large network architectures <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b101">[102]</ref>, and we show that these typical settings hinder shifting WSSS methods to the LUSS task. Unless otherwise stated, the settings of WSSS methods are kept the same as official settings.</p><p>In unsupervised settings where GT labels are not available, the self-generated image-level pseudo labels are utilized to replace the GT labels in WSSS methods.</p><p>Pre-trained models. One of the main challenges in LUSS is to learn effective representations without supervision. However, the effect of representation learning, i.e., using weights pre-trained with different approaches, is less explored in the WSSS methods. Existing WSSS methods mostly utilize supervised ImageNet 1k pretrained models and fine-tune models on the semantic segmentation dataset <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b102">[103]</ref>, e.g., PASCAL VOC <ref type="bibr" target="#b9">[10]</ref>.</p><p>To understand the importance of pre-training, we use different pretrained models for SEAM <ref type="bibr" target="#b35">[36]</ref>, SC-CAM <ref type="bibr" target="#b36">[37]</ref>, and AdvCAM <ref type="bibr" target="#b37">[38]</ref>, as shown in <ref type="table" target="#tab_0">Table 12</ref>. We observe that replacing the supervised ImageNet 1k with the supervised ImageNet 50 dataset in SEAM <ref type="bibr" target="#b35">[36]</ref> reduces the test mIoU from 44.5% to 35.8%. Replacing the supervised models with unsupervised models, i.e., MoCo and SwAV, further reduces the test mIoU to 19.1% and 22.3%, respectively. Both SC-CAM and AdvCAM suffered from the same issue, indicating WSSS methods rely heavily on supervised pre-training. The lack of supervised pre-training makes the representation learning crucial to the LUSS task. And our ImageNet-S dataset provides a basis for fairly evaluating the representation quality of pre-trained models.</p><p>Image-level GT labels. One essential difference between WSSS and LUSS tasks is that WSSS requires image-level GT labels. Class activation maps <ref type="bibr" target="#b126">[127]</ref>, <ref type="bibr" target="#b127">[128]</ref>, commonly treated as the initial segment regions, usually cover the most discriminative small area of objects. Numerous WSSS methods heavily rely on GT labels to extend the CAM region to the whole object and remove the wrong region <ref type="bibr" target="#b128">[129]</ref> by image erasing <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b130">[131]</ref>, <ref type="bibr" target="#b131">[132]</ref>, regions growing <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b132">[133]</ref>, <ref type="bibr" target="#b133">[134]</ref>, <ref type="bibr" target="#b134">[135]</ref>, <ref type="bibr" target="#b135">[136]</ref>, stochastic feature selection <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b137">[138]</ref>, gradients manipulation <ref type="bibr" target="#b37">[38]</ref>, or dataset level information <ref type="bibr" target="#b138">[139]</ref>. To analyze the effect of GT labels on WSSS methods, we apply the recent work AdvCAM <ref type="bibr" target="#b37">[38]</ref> to SEAM <ref type="bibr" target="#b35">[36]</ref>. AdvCAM anti-adversarially refines the CAM results by perturbing the images along pixel gradients according to GT labels. <ref type="table" target="#tab_0">Table 12</ref> shows AdvCAM using GT labels improves the baseline of ImageNet 1k and ImageNet 50 pre-trained models with 1.7% and 1.8% in test mIoU. However, when using generated pseudo labels and MoCo pre-trained model, the performance gain is only 0.4%. Using the SwAV pre-trained model with better imagelevel accuracy, AdvCAM improves the model performance by 1.0%. Similarly, the SEAM and SC-CAM with GT labels outperform the unsupervised settings with a large margin. Thus, the GT label reliance makes WSSS methods unable to be directly shipped to the LUSS task due to the absence of the image-level GT label.</p><p>Network architectures. Numerous network architectures have been developed to improve WSSS, including multi-scale enhancement <ref type="bibr" target="#b139">[140]</ref> and affinity prediction <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b102">[103]</ref>. Due to the small size of the PASCAL VOC dataset, many state-of-the-art WSSS methods improve the performance using large models with extensive parameters and computational cost, e.g., wide ResNet-38 <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b122">[123]</ref> and ResNet with small output strides <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b104">[105]</ref>. As the proposed ImageNet-S datasets are 44 to 800 times larger than PASCAL VOC, the computational cost of training LUSS models with large models used by WSSS methods is prohibitively high. To analyze the effect of model architectures, we change the network in SEAM <ref type="bibr" target="#b35">[36]</ref> (see <ref type="table" target="#tab_0">Table 12</ref>). We remove the Deeplab re-training step used in WSSS methods for fair comparisons. When replacing the ResNet-38 <ref type="bibr" target="#b122">[123]</ref> with a standard ResNet-18 <ref type="bibr" target="#b107">[108]</ref>, the test mIoU drops from 49.6% to 44.5%. Large models benefit the performance, but the high computational cost makes the unsupervised training of LUSS models impracticable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Applications of the ImageNet-S dataset</head><p>The proposed ImageNet-S dataset has pixel-level annotations, thus has more applications apart from the LUSS task. This section presents the ImageNet-S dataset for the large-scale semi-supervised semantic segmentation, evaluation of image-level supervised backbone models, and salient object detection with a subset. Large-scale semi-supervised semantic segmentation. Semisupervised semantic segmentation requires training with a small part of labeled data and many unlabeled data. Fine-tuning trained LUSS models on the 1% labeled training images of the ImageNet-S dataset achieves the semi-supervised semantic segmentation, which is the semi-supervised evaluation protocol of LUSS as introduced in Section 3.2.1. We follow the training scheme of fine-tuning step in Section 5.1, except that models are trained with 30 epochs using GT labels. The semi-supervised semantic segmentation results are shown in <ref type="table" target="#tab_0">Table 13</ref>. Our proposed method outperforms the SwAV and PixelPro baselines with considerable margins on ImageNet-S 300 and ImageNet-S datasets, respectively. Our PixelPro based method even suppresses the image-level supervised model on the ImageNet-S 300 dataset. In the semisupervised setting, PixelPro has a similar performance to SwAV, but SwAV has a large advantage over PixelPro in distance matching evaluation results (see <ref type="table" target="#tab_11">Table 9</ref>). We conclude that fine-tuning models with pixel-level GT labels make models require less selflearned category-related representation ability.</p><p>Evaluate supervised backbone models. Apart from the LUSS task, the ImageNet-S dataset can also evaluate the shape and category representation ability of backbone models trained with image-level supervision. We benchmark the mIoU of backbone models on the ImageNet-S testing set using distance matching evaluation protocol, as shown in <ref type="table" target="#tab_0">Table 14</ref>. As a reference, we also obtain the top-1 classification accuracy of these models on the ImageNet-S dataset. We observe that image-level top-1 accuracy is not always constant with the mIoU, indicating that models with good category representation might not be good at shape representation. To observe how much the ImageNet-S dataset can benefit from a good backbone model, we test the recent proposed RF-ConvNeXt <ref type="bibr" target="#b144">[145]</ref> that enhances the ConvNeXt <ref type="bibr" target="#b143">[144]</ref> with more suitable receptive fields. RF-ConvNeXt achieves high semantic segmentation performance, indicating a good backbone network is needed for the ImageNet-S semantic segmentation.</p><p>Salient object detection with ImageNet-S subset. Salient object detection (SOD) aims at segmenting salient objects regardless of categories <ref type="bibr" target="#b117">[118]</ref>. Due to the category insensitive property of SOD <ref type="bibr" target="#b47">[48]</ref>, an unsupervised SOD model can provide shape prior knowledge to LUSS models. To facilitate the SOD task under largescale data, we construct a SOD dataset, namely ImageNet-Sal, by selecting images with salient objects from the ImageNet-S dataset. For pixel-level labeled images in train/val/test sets of ImageNet-S, we manually select images with salient objects and remove the no-salient annotations. For unlabeled images in the training set, we pick salient images with the help of several pre-trained SOD models. As the picked images might not contain salient objects, we encourage future SOD methods to self-identify training images with salient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>This work proposes a new problem of large-scale unsupervised semantic segmentation to facilitate semantic segmentation in realworld environments with a large diversity and large-scale data. We present a benchmark for LUSS to provide large-scale data with high diversity, a clear task objective, and sufficient evaluation. We present one new method of LUSS to assign labels to pixels with category and shape representations learned from the large-scale data without human-annotation supervision. The LUSS method contains enhanced representation learning and pixel-attention assisted pixellevel label generation strategy. We evaluate our method with multiple evaluation protocols and reveal the potential of LUSS to pixel-level downstream tasks, e.g., semantic segmentation. In addition, we benchmark unsupervised representation learning methods and weakly supervised semantic segmentation methods, and we summarize the challenges and possible directions of LUSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">SUPPLEMENTARY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Evaluation metrics</head><p>We use the mean intersection over union (mIoU), image-level accuracy (Img-Acc), and F-measure (F ? ) as the evaluation metrics for the LUSS task. During the evaluation, all images are evaluated with the original image resolution. We give the implementation details of these evaluation metrics.</p><p>Mean IOU. Similar to the supervised semantic segmentation task <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, we utilize the mIoU metric to evaluate the segmentation mask quality. Suppose the images of the validation/testing sets have N P pixels. Pixels have GT labels G P = {G P k , k ? [1, N P ]} and predicted labels P P = {G P k , k ? [1, N P ]}, where G P k and P P k are the GT and predicted label of the k-th pixel in the validation/testing sets. We use t ij to represent the number of pixels that belong to GT category i and are predicted as category j, which is written as:</p><formula xml:id="formula_10">t ij = N P k=1 I{G P k = i &amp; P P k = j}.<label>(9)</label></formula><p>Then, the mIoU is calculated as:</p><formula xml:id="formula_11">mIoU = 1 C + 1 C i=0 t ii C j=0 t ij + C j=0 t ji ? t ii ,<label>(10)</label></formula><p>where 0-th category is the 'other' category, which is also considered to get mIoU.</p><p>Image-level accuracy. The Img-Acc can evaluate the category representation ability of models. As many images contain multiple labels, we follow <ref type="bibr" target="#b31">[32]</ref> and treat the predicted label as correct if the predicted category with the largest area is within the GT label list. Suppose the validation/testing sets have N images and C categories. We present the image set as D = {D k , k ? [1, N ]} with groundtruth (GT) labels G = {G k , k ? [1, N ]} and predicted labels P = {P k , k ? [1, N ]}, where G k and P k are the GT and predicted category sets of the image D k . The image-level accuracy is obtained as follows:</p><formula xml:id="formula_12">Acc = 1 N N k=1 I{ largest(P k ) ? G k },<label>(11)</label></formula><p>where largest(P k ) indicates the category with the largest area of the image, and I is one when this category belongs to G k .</p><p>F-measure. In addition to category-related representation, we utilize F ? to evaluate the shape quality, which ignores the semantic categories. Specifically, we treat pixels of major categories as the foreground category and pixels in the 'other' category as the background category. Suppose the image i has N i pixels, and the corresponding foreground GT mask is denoted as G F i = {G F ik , k ? [1, N i ]} where the G F ik equals 1 when the label of the pixel does not belong to the 'other' category. The predicted foreground mask of image i, denoted by P F i = {P F ik , k ? [1, N i ]}, is obtained in the same way. The precision i of image i is calculated as follows:</p><formula xml:id="formula_13">precision i = Ni k=1 I{G F ik ? P F ik = 1} Ni k=1 I{P F ik = 1} .<label>(12)</label></formula><p>Then the recall i of image i is calculated as follows:</p><formula xml:id="formula_14">recall i = Ni k=1 I{G F ik ? P F ik = 1} Ni k=1 I{G F ik = 1} .<label>(13)</label></formula><p>With recall and precision for each image, we get F ? of the dataset as follows:</p><formula xml:id="formula_15">F ? = 1 N N i=1 (1 + ? 2 ) ? precision i ? recall i ? 2 ? precision i + recall i ,<label>(14)</label></formula><p>where N is the number of images in the validation/testing set, and we follow common settings to set ? 2 = 0.3 <ref type="bibr" target="#b117">[118]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">CAM-based Inference</head><p>We introduce the CAM-based segmentation mask inference strategy used in this work. For each image, we have A c ? R H?W as the CAM for the predicted category c. Then, the CAM is normalized as follows:</p><formula xml:id="formula_16">? c = A c ? min A c max A c ? min A c .<label>(15)</label></formula><p>Then, we assign label c to regions based on the activation values. Specifically, the assigned label at the position (x, y) is f (x,y) = 0? k (x, y) &lt; ? ; k? k (x, y) ? ?.</p><p>where ? indicates a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Performance Without 'other' Category</head><p>Normally, predicting the 'other' category is easier than the rest since the 'other' category covers a larger area of images. Therefore, it's possible that including the 'other' category makes it easier for the model to get higher mIoU. We evaluate the performance without the 'other' category in <ref type="table" target="#tab_0">Table 15</ref>. We observe that the mIoU differences with/without the 'other' category are mostly less than 0.1% on mIoU for the ImageNet-S 300/919 sets. On the ImageNet-S 50 set, there is about a 1% gap in mIoU. Since the mIoU is obtained by averaging IoU on all categories, the 'other' category has a small influence on the mIoU. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Visualization of the ImageNet-S dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>geological</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>/worldcoffeeresearch.org/work/sensory-lexicon/ Category structure tree of the ImageNet-S dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Number of images per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Number of images per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>?10 7 (</head><label>7</label><figDesc>c) Number of pixels per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>?10 6 (</head><label>6</label><figDesc>d) Number of pixels per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 .</head><label>4</label><figDesc>Instance-level/pixel-level number distribution among categories of the ImageNet-S dataset, i.e., the number of images/pixels per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Distribution of validation and testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Distribution of the annotated training images set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 5 .</head><label>5</label><figDesc>Distribution of object size in the ImageNet-S dataset. The object size is defined as the ratio of object size to the image size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 6 .</head><label>6</label><figDesc>Position distribution comparison among datasets: (top) the position distribution for segmentation masks, (down) the position distribution for mask boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 8 .</head><label>8</label><figDesc>Illustration of non-contrastive pixel-to-pixel representation alignment. Mp is the projection layer that ensures less interference of pixellevel representation to the category representation. P is a pixel-level predictor for the asymmetric loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>1 Fig. 9 .</head><label>19</label><figDesc>Illustration of deep-to-shallow supervision. Purple lines denote the supervisions using loss L I . M s I and M s K are omitted for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 10 .</head><label>10</label><figDesc>Visualization of pixel-attention maps of different channels. Most pixel-attention maps highlight the semantic regions, while a few channels highlight the background regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 11 .</head><label>11</label><figDesc>Visualization of unsupervised semantic segmentation results. Last three rows are trained with saliency prior information during label generation, showing better shape quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Categories and number of images comparison between the ImageNet-S dataset and existing semantic segmentation datasets.</figDesc><table><row><cell>Dataset</cell><cell>category</cell><cell>train</cell><cell>val</cell><cell>test</cell></row><row><cell>PASCAL VOC 2012 [10]</cell><cell>20</cell><cell>1,464</cell><cell>1,449</cell><cell>1,456</cell></row><row><cell>CityScapes [7]</cell><cell>19</cell><cell>2,975</cell><cell>500</cell><cell>1,525</cell></row><row><cell>ADE20K [8]</cell><cell>150</cell><cell>20,210</cell><cell>2,000</cell><cell>3,000</cell></row><row><cell>ImageNet-S 50</cell><cell>50</cell><cell>64,431</cell><cell>752</cell><cell>1,682</cell></row><row><cell>ImageNet-S 300</cell><cell>300</cell><cell>384,862</cell><cell>4,097</cell><cell>9,088</cell></row><row><cell>ImageNet-S</cell><cell>919</cell><cell cols="3">1,183,322 12,419 27,423</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>The number of categories in each image in the ImageNet-S dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Number of images</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>val set</cell><cell></cell><cell cols="2">test set</cell><cell></cell></row><row><cell>Categories in each image</cell><cell>1</cell><cell>2</cell><cell>&gt;2</cell><cell>1</cell><cell>2</cell><cell>&gt;2</cell></row><row><cell>ImageNet-S 50</cell><cell>745</cell><cell>7</cell><cell>0</cell><cell>1,676</cell><cell>6</cell><cell>0</cell></row><row><cell>ImageNet-S 300</cell><cell cols="2">3,971 118</cell><cell>8</cell><cell>8,815</cell><cell>264</cell><cell>9</cell></row><row><cell>ImageNet-S</cell><cell cols="6">11,294 954 171 25,133 1,938 352</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 The</head><label>3</label><figDesc></figDesc><table><row><cell>Metrics</cell><cell>d</cell><cell>All</cell><cell>S.</cell><cell>M.S. M.L.</cell><cell>L.</cell></row></table><note>consistency among four annotators using 100 randomly picked images. The d indicates the pixel distance as in [116].Boundary mIoU [116] 2% 92.4 91.0 91.5 92.7 93.4 3% 94.8 92.6 93.9 95.1 95.5 4% 95.9 93.2 95.1 96.2 96.5 Mask mIoU - 98.7 93.4 97.1 99.0 99.3</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Definition of frequently used symbols. ? W output feature of the k-th image. q k (C + 1) ? H ? W pixel-level pseudo labels of the k-th image. y k (C + 1) ? H ? W pixel-level GT labels of the k-th image.</figDesc><table><row><cell cols="3">Symbols Dimensions/Type Meaning</cell></row><row><cell>z</cell><cell>L ? H ? W</cell><cell>output feature of one image.</cell></row><row><cell>z k</cell><cell>L ? H</cell><cell></cell></row></table><note>C scalar number of major categories.L scalar number of dimensions of output feature.H scalar height of output feature.W scalar width of output feature.N scalar number of images.P operation global average pooling over spatial dimensions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Training models on the large set and evaluating models on the small subsets of ImageNet-S with fully unsupervised protocol using PASSs method.</figDesc><table><row><cell>Training set</cell><cell cols="2">mIoU val test</cell><cell cols="2">Img-Acc val test</cell><cell>val</cell><cell>F ?</cell><cell>test</cell></row><row><cell></cell><cell cols="4">Testing on ImageNet-S 50</cell><cell></cell><cell></cell></row><row><cell>ImageNet-S 50</cell><cell>29.2</cell><cell>29.3</cell><cell>66.2</cell><cell>65.5</cell><cell>49.0</cell><cell></cell><cell>49.0</cell></row><row><cell>ImageNet-S 300</cell><cell>27.8</cell><cell>27.4</cell><cell>65.2</cell><cell>63.3</cell><cell>38.6</cell><cell></cell><cell>36.0</cell></row><row><cell>ImageNet-S</cell><cell>24.1</cell><cell>23.0</cell><cell>61.3</cell><cell>57.8</cell><cell>31.3</cell><cell></cell><cell>28.9</cell></row><row><cell></cell><cell cols="4">Testing on ImageNet-S 300</cell><cell></cell><cell></cell></row><row><cell>ImageNet-S 300</cell><cell>18.0</cell><cell>18.1</cell><cell>43.9</cell><cell>42.6</cell><cell>47.6</cell><cell></cell><cell>47.5</cell></row><row><cell>ImageNet-S</cell><cell>16.4</cell><cell>16.6</cell><cell>39.3</cell><cell>37.2</cell><cell>36.8</cell><cell></cell><cell>36.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>Ablation of the proposed P2P alignment and D2S supervision representation learning strategy. All models are trained with 100 epochs.D2S3 and D2S32 mean supervising stage 3 and stage 3-2 of the network, respectively.(a) Ablation on LUSS using distance matching evaluation protocol.</figDesc><table><row><cell>ImageNet-S 300</cell><cell>val</cell><cell cols="2">mIoU test</cell><cell cols="2">Img-Acc val test</cell><cell></cell><cell>val</cell><cell>F ?</cell><cell>test</cell></row><row><cell>SwAV [33]</cell><cell cols="2">22.4</cell><cell>22.6</cell><cell>57.4</cell><cell>57.5</cell><cell cols="2">63.5</cell><cell>63.7</cell></row><row><cell>+P2P</cell><cell cols="2">24.8</cell><cell>24.8</cell><cell>58.4</cell><cell>58.5</cell><cell cols="2">64.5</cell><cell>64.8</cell></row><row><cell>+P2P-D2S3</cell><cell cols="2">25.1</cell><cell>25.2</cell><cell>57.3</cell><cell>57.5</cell><cell cols="2">65.0</cell><cell>65.2</cell></row><row><cell>+P2P-D2S32</cell><cell cols="2">24.8</cell><cell>24.9</cell><cell>56.8</cell><cell>56.6</cell><cell cols="2">65.7</cell><cell>66.0</cell></row><row><cell>PixelPro [35]</cell><cell cols="2">15.5</cell><cell>15.8</cell><cell>44.0</cell><cell>44.3</cell><cell cols="2">62.4</cell><cell>62.6</cell></row><row><cell>+Clustering Loss</cell><cell cols="2">20.8</cell><cell>21.3</cell><cell>52.0</cell><cell>52.1</cell><cell cols="2">61.5</cell><cell>62.1</cell></row><row><cell>+P2P</cell><cell cols="2">21.3</cell><cell>22.0</cell><cell>52.2</cell><cell>52.8</cell><cell cols="2">61.5</cell><cell>62.1</cell></row><row><cell>+P2P-D2S3</cell><cell cols="2">22.2</cell><cell>22.8</cell><cell>53.2</cell><cell>53.1</cell><cell cols="2">62.2</cell><cell>62.9</cell></row><row><cell>+P2P-D2S32</cell><cell cols="2">23.0</cell><cell>23.4</cell><cell>53.3</cell><cell>54.3</cell><cell cols="2">62.4</cell><cell>63.1</cell></row><row><cell cols="8">(b) Ablation of transfer learning on downstream tasks.</cell></row><row><cell>ImageNet-S 300</cell><cell cols="6">COCO SEG AP AP50 AP75 AP AP50 AP75 COCO DET</cell><cell>VOC SEG mIoU</cell></row><row><cell>SwAV [33]</cell><cell cols="5">32.4 52.1 34.6 35.5 54.9 38.6</cell><cell></cell><cell>68.9</cell></row><row><cell>+P2P</cell><cell cols="5">32.8 52.5 34.9 36.0 55.4 39.1</cell><cell></cell><cell>70.4</cell></row><row><cell>+P2P-D2S3</cell><cell cols="5">33.5 53.4 35.8 36.7 56.4 39.4</cell><cell></cell><cell>70.8</cell></row><row><cell>+P2P-D2S32</cell><cell cols="5">33.8 53.7 36.2 37.2 56.6 40.6</cell><cell></cell><cell>70.8</cell></row><row><cell>PixelPro [35]</cell><cell cols="5">34.7 54.8 37.2 38.2 57.5 41.7</cell><cell></cell><cell>72.8</cell></row><row><cell cols="6">+Clustering Loss 34.9 55.2 37.3 38.4 58.1 41.9</cell><cell></cell><cell>73.3</cell></row><row><cell>+P2P</cell><cell cols="5">35.3 55.9 37.9 38.9 58.6 42.4</cell><cell></cell><cell>72.3</cell></row><row><cell>+P2P-D2S3</cell><cell cols="5">35.3 55.9 37.6 38.8 58.6 42.3</cell><cell></cell><cell>73.9</cell></row><row><cell>+P2P-D2S32</cell><cell cols="5">35.7 56.6 38.3 39.4 59.1 43.1</cell><cell></cell><cell>75.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>(b). The P2P</cell></row></table><note>p (z) ensures less interference of pixel-level representation to the category-related representation. Deep-to-shallow supervision. The D2S supervision utilizes high- quality features from the last stage to supervise early-stage features. Table 8(c) compares using features from the same or last stages as supervision to shallow layers. We observe that both settings have improvements over the baseline, and the deep-to-shallow</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8</head><label>8</label><figDesc>Ablation about the P2P alignment and D2S supervision strategy on the ImageNet-S 300 testing set using distance matching evaluation protocols.(a) Different loss types for P2P alignment.</figDesc><table><row><cell>ImageNet-S300</cell><cell>mIoU</cell><cell>Img-Acc</cell><cell>F ?</cell></row><row><cell>SwAV baseline</cell><cell>22.6</cell><cell>57.5</cell><cell>63.7</cell></row><row><cell>+Clustering P2P</cell><cell>21.2</cell><cell>51.8</cell><cell>66.4</cell></row><row><cell>+Contrastive P2P</cell><cell>18.0</cell><cell>46.4</cell><cell>64.6</cell></row><row><cell>+Non-contrastive P2P</cell><cell>24.8</cell><cell>58.5</cell><cell>64.8</cell></row><row><cell cols="3">(b) Effect of projections M in P2P alignment.</cell><cell></cell></row><row><cell>ImageNet-S300</cell><cell>mIoU</cell><cell>Img-Acc</cell><cell>F ?</cell></row><row><cell>SwAV baseline</cell><cell>22.6</cell><cell>57.5</cell><cell>63.7</cell></row><row><cell>P2P without Mp(z)</cell><cell>24.6</cell><cell>57.1</cell><cell>64.9</cell></row><row><cell>P2P with Mp(z)</cell><cell>24.8</cell><cell>58.5</cell><cell>64.8</cell></row><row><cell cols="4">(c) Deep-to-shallow versus same-stage supervisions in D2S supervision.</cell></row><row><cell>ImageNet-S300</cell><cell>mIoU</cell><cell>Img-Acc</cell><cell>F ?</cell></row><row><cell>PixelPro+P2P (baseline)</cell><cell>22.0</cell><cell>52.8</cell><cell>62.1</cell></row><row><cell>+same-stage sup.</cell><cell>22.6</cell><cell>52.9</cell><cell>63.1</cell></row><row><cell>+deep-to-shallow sup.</cell><cell>23.4</cell><cell>54.3</cell><cell>63.1</cell></row><row><cell cols="4">(d) Same-view versus cross-view supervisions in D2S supervision.</cell></row><row><cell>ImageNet-S300</cell><cell>mIoU</cell><cell>Img-Acc</cell><cell>F ?</cell></row><row><cell>PixelPro+P2P (baseline)</cell><cell>22.0</cell><cell>52.8</cell><cell>62.1</cell></row><row><cell>+same-view sup.</cell><cell>23.1</cell><cell>53.9</cell><cell>63.2</cell></row><row><cell>+cross-view sup.</cell><cell>23.4</cell><cell>54.3</cell><cell>63.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9</head><label>9</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>epochs. Supervised means initializing the model with image-level supervised pre-training.</figDesc><table><row><cell>LUSS</cell><cell cols="2">mIoU val test</cell><cell>Img-Acc val test</cell><cell>val</cell><cell>F ?</cell><cell>test</cell></row><row><cell></cell><cell cols="3">ImageNet-S300</cell><cell></cell><cell></cell></row><row><cell>Supervised</cell><cell cols="6">33.8 33.9 80.4 81.5 60.0 60.0</cell></row><row><cell>Contrastive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR [26]</cell><cell cols="6">12.5 12.6 37.7 38.4 63.7 64.0</cell></row><row><cell cols="7">MoCov2 [25], [121] 12.4 12.4 40.3 40.3 64.1 64.4</cell></row><row><cell>AdCo [122]</cell><cell cols="6">21.1 21.5 55.1 54.8 64.9 65.5</cell></row><row><cell>Non-contrastive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BYOL [34]</cell><cell cols="6">13.4 13.4 38.3 38.0 64.0 64.4</cell></row><row><cell>SimSiam [70]</cell><cell cols="6">20.1 20.3 56.9 57.5 65.5 66.0</cell></row><row><cell>Clustering</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCL [93]</cell><cell cols="6">17.4 17.9 48.4 48.0 63.0 63.3</cell></row><row><cell>SwAV [33]</cell><cell cols="6">22.4 22.6 57.4 57.5 63.5 63.7</cell></row><row><cell>PASSs</cell><cell cols="6">25.1 25.2 57.3 57.5 65.0 65.2</cell></row><row><cell>Pixel-level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DenseCL [95]</cell><cell cols="6">13.9 13.8 36.4 36.8 63.7 63.7</cell></row><row><cell>PixelPro [35]</cell><cell cols="6">15.5 15.8 44.0 44.3 62.4 62.6</cell></row><row><cell>PASSp</cell><cell cols="6">23.0 23.4 53.3 54.3 62.4 63.1</cell></row><row><cell></cell><cell cols="3">ImageNet-S</cell><cell></cell><cell></cell></row><row><cell>Supervised</cell><cell cols="6">30.0 29.8 75.9 76.6 58.7 58.7</cell></row><row><cell>PixelPro [35]</cell><cell>7.7</cell><cell>7.5</cell><cell cols="4">26.9 26.5 61.8 61.8</cell></row><row><cell>PASSp</cell><cell>9.8</cell><cell>9.8</cell><cell cols="4">29.4 29.6 61.1 61.3</cell></row><row><cell>SwAV [33]</cell><cell cols="6">15.1 15.1 43.5 43.3 64.2 64.3</cell></row><row><cell>PASSs</cell><cell cols="6">15.6 15.6 43.1 42.9 64.3 64.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 10</head><label>10</label><figDesc>Ablation about pixel-label generation and fine-tuning steps on the ImageNet-S 50 testing set using fully unsupervised evaluation protocol.(a) Comparison with different label generation methods. ? means using the inference strategy of the image-level method.</figDesc><table><row><cell>ImageNet-S50</cell><cell></cell><cell>mIoU</cell><cell>Img-Acc</cell><cell>F ?</cell></row><row><cell>Image-level</cell><cell></cell><cell>26.9</cell><cell>57.6</cell><cell>53.0</cell></row><row><cell>Pixel-level</cell><cell></cell><cell>12.7</cell><cell>37.4</cell><cell>32.9</cell></row><row><cell>Pixel-attention</cell><cell></cell><cell>29.3</cell><cell>65.5</cell><cell>49.0</cell></row><row><cell>Pixel-attention ?</cell><cell></cell><cell>29.2</cell><cell>61.7</cell><cell>52.3</cell></row><row><cell cols="5">(b) Clustering time (second) of different label generation methods.</cell></row><row><cell cols="5">ImageNet-S50 ImageNet-S300 ImageNet-S</cell></row><row><cell>Image-level</cell><cell cols="2">2.8 ? 10 0</cell><cell>8.9 ? 10 1</cell><cell>7.5 ? 10 2</cell></row><row><cell>Pixel-level</cell><cell cols="2">3.2 ? 10 2</cell><cell>4.6 ? 10 4</cell><cell>4.1 ? 10 5</cell></row><row><cell>Pixel-attention</cell><cell cols="2">2.8 ? 10 0</cell><cell>8.9 ? 10 1</cell><cell>7.5 ? 10 2</cell></row><row><cell cols="5">(c) Shared/unshared pixel-attention maps for the output features.</cell></row><row><cell>ImageNet-S50</cell><cell></cell><cell>mIoU</cell><cell>Img-Acc</cell><cell>F ?</cell></row><row><cell>Shared</cell><cell></cell><cell>28.4</cell><cell>64.3</cell><cell>48.8</cell></row><row><cell>Unshared</cell><cell></cell><cell>29.3</cell><cell>65.5</cell><cell>49.0</cell></row><row><cell cols="5">(d) Effectiveness of the fine-tuning step in our LUSS method.</cell></row><row><cell>ImageNet-S50</cell><cell></cell><cell>mIoU</cell><cell>Img-Acc</cell><cell>F ?</cell></row><row><cell cols="2">Before fine-tuning</cell><cell>26.0</cell><cell>63.8</cell><cell>44.7</cell></row><row><cell>After fine-tuning</cell><cell></cell><cell>29.3</cell><cell>65.5</cell><cell>49.0</cell></row><row><cell cols="3">by category representation learning.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 11 Transfer</head><label>11</label><figDesc></figDesc><table><row><cell>learning comparison among unsupervised representation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>(a)) and downstream tasks (Table 7(b)).</cell></row><row><cell>Both proposed strategies improve the performance on LUSS and</cell></row><row><cell>downstream tasks, showing the generalizability of the proposed</cell></row><row><cell>representation learning method.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 12</head><label>12</label><figDesc>Ablation of shipping WSSS methods to the LUSS task. Properties in WSSS, i.e., supervised pre-trained models, image-level GT labels, and large networks, that are not applicable in LUSS, make WSSS methods have a large performance drop in the LUSS task.</figDesc><table><row><cell>ImageNet-S 50</cell><cell cols="2">Arch.</cell><cell></cell><cell cols="2">Param./MACC</cell><cell></cell><cell></cell><cell>Pre-train</cell><cell>Labels</cell><cell>val</cell><cell cols="2">mIoU</cell><cell>test</cell><cell>Img-Acc val test</cell><cell>val</cell><cell>F ?</cell><cell>test</cell></row><row><cell></cell><cell cols="3">ResNet-38 [123]</cell><cell cols="2">105.5M/100.4G</cell><cell></cell><cell cols="2">Sup. ImageNet 1k</cell><cell>GT</cell><cell cols="2">49.7</cell><cell>49.6</cell><cell>96.6</cell><cell>95.7</cell><cell>61.5</cell><cell>60.9</cell></row><row><cell></cell><cell cols="3">ResNet-18 [108]</cell><cell cols="2">11.3M/1.9G</cell><cell></cell><cell cols="2">Sup. ImageNet 1k</cell><cell>GT</cell><cell cols="2">45.2</cell><cell>44.5</cell><cell>90.9</cell><cell>90.4</cell><cell>55.9</cell><cell>54.5</cell></row><row><cell>SEAM [36]</cell><cell cols="3">ResNet-18 [108] ResNet-18 [108]</cell><cell cols="2">11.3M/1.9G 11.3M/1.9G</cell><cell cols="3">Sup. ImageNet-50 MoCo. ImageNet-S 50</cell><cell>GT -</cell><cell cols="2">35.1 19.0</cell><cell>35.8 19.1</cell><cell>81.2 45.1</cell><cell>81.5 46.7</cell><cell>46.3 45.1</cell><cell>46.5 45.3</cell></row><row><cell></cell><cell cols="3">ResNet-18 [108]</cell><cell cols="2">11.3M/1.9G</cell><cell cols="3">SwAV. ImageNet-S 50</cell><cell>-</cell><cell cols="2">22.1</cell><cell>22.3</cell><cell>54.6</cell><cell>53.5</cell><cell>41.1</cell><cell>41.1</cell></row><row><cell></cell><cell cols="3">ResNet-18 [108]</cell><cell cols="2">11.5M/1.8G</cell><cell></cell><cell cols="2">Sup. ImageNet 1k</cell><cell>GT</cell><cell cols="2">38.5</cell><cell>39.3</cell><cell>81.9</cell><cell>83.8</cell><cell>49.4</cell><cell>49.6</cell></row><row><cell>SC-CAM [37]</cell><cell cols="3">ResNet-18 [108] ResNet-18 [108]</cell><cell cols="2">11.5M/1.8G 11.5M/1.8G</cell><cell cols="3">Sup. ImageNet 50 MoCo. ImageNet-S 50</cell><cell>GT -</cell><cell cols="2">31.3 17.7</cell><cell>32.1 18.1</cell><cell>70.2 43.7</cell><cell>71.0 45.7</cell><cell>44.1 39.7</cell><cell>44.4 40.0</cell></row><row><cell></cell><cell cols="3">ResNet-18 [108]</cell><cell cols="2">11.5M/1.8G</cell><cell cols="3">SwAV. ImageNet-S 50</cell><cell>-</cell><cell cols="2">19.0</cell><cell>19.7</cell><cell>50.0</cell><cell>49.1</cell><cell>38.6</cell><cell>40.8</cell></row><row><cell></cell><cell cols="3">ResNet-18 [108]</cell><cell cols="2">11.3M/1.9G</cell><cell></cell><cell cols="2">Sup. ImageNet 1k</cell><cell>GT</cell><cell cols="2">46.9</cell><cell>46.2</cell><cell>90.9</cell><cell>90.4</cell><cell>58.4</cell><cell>57.5</cell></row><row><cell>SEAM [36]</cell><cell cols="3">ResNet-18 [108]</cell><cell cols="2">11.3M/1.9G</cell><cell></cell><cell cols="2">Sup. ImageNet 50</cell><cell>GT</cell><cell cols="2">36.9</cell><cell>37.6</cell><cell>81.2</cell><cell>81.5</cell><cell>49.2</cell><cell>49.6</cell></row><row><cell>+AdvCAM [38]</cell><cell cols="3">ResNet-18 [108]</cell><cell cols="2">11.3M/1.9G</cell><cell cols="3">MoCo. ImageNet-S 50</cell><cell>-</cell><cell cols="2">19.2</cell><cell>19.5</cell><cell>45.1</cell><cell>46.7</cell><cell>46.8</cell><cell>47.3</cell></row><row><cell></cell><cell cols="3">ResNet-18 [108]</cell><cell cols="2">11.3M/1.9G</cell><cell cols="3">SwAV. ImageNet-S 50</cell><cell>-</cell><cell cols="2">23.7</cell><cell>23.3</cell><cell>54.6</cell><cell>53.5</cell><cell>44.2</cell><cell>43.9</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE 13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Semi-supervised semantic segmentation (semi-supervised evaluation</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">protocol) using the ImageNet-S 50 /ImageNet-S datasets. PASS s/p</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">denotes using SwAV [33] and PixelPro [35] as the Le in (5), respectively.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Supervised means initializing the model with image-level supervised</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">pre-training.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Semi-supervised</cell><cell>val</cell><cell cols="2">mIoU test</cell><cell cols="2">Img-Acc val test</cell><cell>val</cell><cell>F ?</cell><cell>test</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">ImageNet-S 300</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Supervised</cell><cell cols="2">27.7</cell><cell>27.5</cell><cell>61.1</cell><cell>62.3</cell><cell>64.3</cell><cell></cell><cell>64.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR [26]</cell><cell cols="2">12.7</cell><cell>12.6</cell><cell>34.4</cell><cell>34.8</cell><cell>59.1</cell><cell></cell><cell>59.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BYOL [34]</cell><cell cols="2">10.5</cell><cell>10.6</cell><cell>30.1</cell><cell>30.5</cell><cell>58.5</cell><cell></cell><cell>59.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCov2 [25], [121]</cell><cell cols="2">12.6</cell><cell>12.3</cell><cell>33.0</cell><cell>32.5</cell><cell>59.2</cell><cell></cell><cell>59.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DenseCL [95]</cell><cell cols="2">16.2</cell><cell>16.0</cell><cell>34.9</cell><cell>35.7</cell><cell>61.0</cell><cell></cell><cell>60.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdCo [122]</cell><cell cols="2">19.6</cell><cell>19.6</cell><cell>45.4</cell><cell>45.4</cell><cell>63.8</cell><cell></cell><cell>63.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCL [93]</cell><cell cols="2">17.3</cell><cell>17.4</cell><cell>41.7</cell><cell>41.8</cell><cell>61.7</cell><cell></cell><cell>61.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwAV [33]</cell><cell cols="2">23.0</cell><cell>23.3</cell><cell>51.2</cell><cell>51.5</cell><cell>64.0</cell><cell></cell><cell>64.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PASSs</cell><cell cols="2">25.7</cell><cell>25.7</cell><cell>52.3</cell><cell>52.8</cell><cell>65.5</cell><cell></cell><cell>66.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PixelPro [35]</cell><cell cols="2">23.3</cell><cell>23.4</cell><cell>49.0</cell><cell>48.9</cell><cell>66.0</cell><cell></cell><cell>66.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PASSp</cell><cell cols="2">29.7</cell><cell>29.8</cell><cell>56.9</cell><cell>56.9</cell><cell>68.1</cell><cell></cell><cell>68.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">ImageNet-S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Supervised</cell><cell cols="2">25.7</cell><cell>25.0</cell><cell>57.3</cell><cell>57.4</cell><cell>66.3</cell><cell></cell><cell>66.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PixelPro [35]</cell><cell cols="2">16.0</cell><cell>15.6</cell><cell>36.0</cell><cell>36.2</cell><cell>66.2</cell><cell></cell><cell>66.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PASSp</cell><cell cols="2">18.9</cell><cell>18.6</cell><cell>40.9</cell><cell>41.3</cell><cell>68.0</cell><cell></cell><cell>68.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwAV [33]</cell><cell cols="2">18.2</cell><cell>17.9</cell><cell>42.8</cell><cell>43.2</cell><cell>66.0</cell><cell></cell><cell>66.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PASSs</cell><cell cols="2">19.4</cell><cell>19.2</cell><cell>43.3</cell><cell>43.4</cell><cell>66.6</cell><cell></cell><cell>66.9</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 14 mIoU</head><label>14</label><figDesc>results of supervised backbone models using distance matching evaluation protocol on the ImageNet-S testing set. Top-1 Acc. is the classification accuracy on the ImageNet-S testing set. * indicates models are finetuned with the ImageNet-S semi-supervised segmentation training set.</figDesc><table><row><cell>Supervised</cell><cell>Top-1 Acc.</cell><cell>mIoU</cell></row><row><cell>ImageNet-S</cell><cell></cell><cell></cell></row><row><cell>ResNet-50 [108]</cell><cell>83.6</cell><cell>29.8</cell></row><row><cell>ResNet-101 [108]</cell><cell>84.3</cell><cell>31.4</cell></row><row><cell>DenseNet-161 [141]</cell><cell>84.3</cell><cell>29.8</cell></row><row><cell>Inception V3 [142]</cell><cell>77.7</cell><cell>29.9</cell></row><row><cell>ResNeXt-50 [109]</cell><cell>84.4</cell><cell>32.6</cell></row><row><cell>ResNeXt-101 [109]</cell><cell>85.5</cell><cell>34.8</cell></row><row><cell>EfficientNet-B3 [143]</cell><cell>85.3</cell><cell>32.3</cell></row><row><cell>Res2Net-50 [110]</cell><cell>84.8</cell><cell>35.7</cell></row><row><cell>Res2Net-101 [110]</cell><cell>85.6</cell><cell>37.2</cell></row><row><cell>Swin-S [114]</cell><cell>87.8</cell><cell>38.6</cell></row><row><cell>Swin-B [114]</cell><cell>88.0</cell><cell>38.2</cell></row><row><cell>ConvNeXt-T  *  [144]</cell><cell>-</cell><cell>45.1</cell></row><row><cell>RF-ConvNeXt-T  *  (SingleRF) [145]</cell><cell>-</cell><cell>46.2</cell></row><row><cell>RF-ConvNeXt-T  *  (MultipleRF) [145]</cell><cell>-</cell><cell>47.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 15</head><label>15</label><figDesc>Comparison of our proposed LUSS method and existing USS methods on the ImageNet-S dataset using fully unsupervised evaluation protocol. The 'other' category is not considered during the evaluation.Zhong-Yu Li is a Ph.D. student from the college of computer science, Nankai university. He is supervised via Prof. Ming-Ming cheng. His research interests include deep learning, machine learning and computer vision. Ming-Hsuan Yang is a professor in Electrical Engineering and Computer Science at University of California, Merced. He received the PhD degree in Computer Science from the University of Illinois at Urbana-Champaign in 2000. Yang has served as an associate editor of the IEEE TPAMI, IJCV, CVIU, etc. He received the NSF CAREER award in 2012 and the Google Faculty Award in 2009. Ming-Ming Cheng received his PhD degree from Tsinghua University in 2012, and then worked with Prof. Philip Torr in Oxford for 2 years. He is now a professor at Nankai University, leading the Media Computing Lab. His research interests includes computer vision and computer graphics. He received awards including ACM China Rising Star Award, IBM Global SUR Award, etc. He is a senior member of the IEEE and on the editorial boards of IEEE TPAMI and IEEE TIP. Junwei Han is currently a Full Professor with Northwestern Polytechnical University, Xi'an, China. His research interests include computer vision, multimedia processing, and brain imaging analysis. He is an Associate Editor of IEEE Trans. on Human-Machine Systems, Neurocomputing, Multidimensional Systems and Signal Processing, and Machine Vision and Applications. Torr received the PhD degree from Oxford University. After working for another three years at Oxford, he worked for six years for Microsoft Research, first in Redmond, then in Cambridge, founding the vision side of the Machine Learning and Perception Group. He is now a professor at Oxford University. He has won awards from top vision conferences, including ICCV, CVPR, ECCV, NIPS and BMVC. He is a senior member of the IEEE and Fellow of the Royal Society.</figDesc><table><row><cell>LUSS</cell><cell>Pior</cell><cell cols="2">mIoU val test</cell><cell cols="2">b-mIoU val test</cell><cell>Img-Acc val test</cell><cell>val</cell><cell>F ?</cell><cell>test</cell><cell>S.</cell><cell cols="2">mIoU M.S. M.L.</cell><cell>L.</cell><cell>S.</cell><cell>b-mIoU M.S. M.L.</cell><cell>L.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ImageNet-S 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MDC [46], [88]</cell><cell>-</cell><cell>4.0</cell><cell>3.7</cell><cell>1.4</cell><cell>1.2</cell><cell cols="4">14.9 13.4 31.6 31.3</cell><cell>0.4</cell><cell>2.6</cell><cell>3.8</cell><cell>5.0</cell><cell>0.2</cell><cell>1.1</cell><cell>1.4</cell><cell>1.5</cell></row><row><cell>MDC [46], [88]</cell><cell>I</cell><cell cols="2">14.9 14.6</cell><cell>3.2</cell><cell>3.1</cell><cell cols="4">44.8 40.8 33.2 32.6</cell><cell>2.7</cell><cell>11.1</cell><cell cols="2">14.93 19.51</cell><cell>0.9</cell><cell>2.2</cell><cell>3.2</cell><cell>4.8</cell></row><row><cell>PiCIE [46]</cell><cell>-</cell><cell>5.0</cell><cell>4.6</cell><cell>1.8</cell><cell>1.6</cell><cell cols="4">15.8 14.0 14.6 32.2</cell><cell>0.2</cell><cell>3.1</cell><cell>5.1</cell><cell>5.4</cell><cell>0.1</cell><cell>1.2</cell><cell>1.7</cell><cell>1.9</cell></row><row><cell>PiCIE [46]</cell><cell>I</cell><cell cols="2">18.1 17.9</cell><cell>3.7</cell><cell>4.0</cell><cell cols="4">45.0 44.0 32.1 31.6</cell><cell>4.4</cell><cell>13.3</cell><cell>20.4</cell><cell>23.5</cell><cell>0.9</cell><cell>2.7</cell><cell>4.4</cell><cell>5.8</cell></row><row><cell>MaskCon [31]</cell><cell>S</cell><cell cols="9">23.5 23.1 14.8 14.3 47.9 47.6 65.7 66.2 10.2</cell><cell>24.4</cell><cell>23.7</cell><cell>19.7</cell><cell>8.9</cell><cell>16.1</cell><cell>13.7</cell><cell>9.9</cell></row><row><cell>MaskCon [31] ?</cell><cell>S</cell><cell>12.7</cell><cell>9.2</cell><cell>7.6</cell><cell>5.3</cell><cell cols="4">30.2 22.4 62.6 62.3</cell><cell>1.5</cell><cell>8.1</cell><cell>9.9</cell><cell>9.7</cell><cell>1.2</cell><cell>5.3</cell><cell>5.6</cell><cell>4.8</cell></row><row><cell>PASSs</cell><cell>-</cell><cell cols="2">28.4 28.6</cell><cell>6.9</cell><cell>6.7</cell><cell cols="4">66.2 65.5 49.0 49.0</cell><cell>4.6</cell><cell>24.1</cell><cell>32.5</cell><cell>32.4</cell><cell>1.9</cell><cell>5.4</cell><cell>7.4</cell><cell>8.9</cell></row><row><cell>PASSp</cell><cell>-</cell><cell cols="2">31.9 31.5</cell><cell>6.6</cell><cell>6.6</cell><cell cols="4">62.9 64.1 48.7 47.9</cell><cell>8.5</cell><cell>25.6</cell><cell>36.0</cell><cell>40.3</cell><cell>4.1</cell><cell>5.1</cell><cell>7.2</cell><cell>9.8</cell></row><row><cell>PASSp+ RC</cell><cell>S</cell><cell cols="9">42.0 41.5 16.8 17.1 58.8 61.8 62.1 61.3 15.8</cell><cell>37.9</cell><cell>44.8</cell><cell>43.2</cell><cell>10.3</cell><cell>16.5</cell><cell>18.3</cell><cell>16.4</cell></row><row><cell>PASSp+ Sal</cell><cell>S</cell><cell cols="9">42.5 41.5 19.7 19.5 64.6 65.2 70.0 69.9 17.2</cell><cell>40.8</cell><cell>44.4</cell><cell>37.8</cell><cell>13.6</cell><cell>21.8</cell><cell>19.9</cell><cell>14.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ImageNet-S 300</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PASSp</cell><cell>-</cell><cell cols="2">16.6 16.0</cell><cell>4.4</cell><cell>4.2</cell><cell cols="4">34.7 32.8 34.4 34.3</cell><cell>2.8</cell><cell>12.0</cell><cell>16.5</cell><cell>21.8</cell><cell>1.4</cell><cell>3.2</cell><cell>3.9</cell><cell>6.4</cell></row><row><cell>PASSs</cell><cell>-</cell><cell cols="2">17.9 18.0</cell><cell>5.1</cell><cell>5.1</cell><cell cols="4">43.9 42.6 47.6 47.5</cell><cell>4.0</cell><cell>13.4</cell><cell>19.4</cell><cell>23.5</cell><cell>1.9</cell><cell>4.1</cell><cell>5.4</cell><cell>7.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ImageNet-S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PASSp</cell><cell>-</cell><cell>7.3</cell><cell>6.6</cell><cell>2.4</cell><cell>2.1</cell><cell cols="4">19.9 18.0 34.8 34.6</cell><cell>1.3</cell><cell>4.6</cell><cell>7.1</cell><cell>8.4</cell><cell>0.6</cell><cell>1.5</cell><cell>2.1</cell><cell>2.8</cell></row><row><cell>PASSs</cell><cell>-</cell><cell cols="2">11.5 11.0</cell><cell>3.8</cell><cell>3.5</cell><cell cols="4">24.0 22.3 37.1 36.9</cell><cell>2.4</cell><cell>8.3</cell><cell>11.9</cell><cell>13.4</cell><cell>1.2</cell><cell>3.0</cell><cell>3.7</cell><cell>4.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work is funded by the National Key Research and Development Program of China Grant No.2018AAA0100400, NSFC (62225604), and the Fundamental Research Funds for the Central Universities (Nankai University, NO. 63223050). Thanks for part of the pixel-level annotation from the Learning from Imperfect Data Challenge <ref type="bibr" target="#b145">[146]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint semantic segmentation and boundary detection using iterative pyramid contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2636" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rethinking bisenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-dataset collaborative learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning statistical texture for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Region mutual information loss for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficientfcn: Holisticallyguided decoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via decoupled body and edge supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to predict contextadaptive convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation transformer: Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Autoregressive unsupervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mix-and-match tuning for self-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7334" to="7344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised semantic segmentation by contrasting object mask proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet?</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jean-Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Florent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Corentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gheshlaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Anti-adversarially manipulated attributions for weakly and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nonparametric scene parsing via label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2368" to="2382" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Segmenting scenes by matching image composites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recognition by association via learning per-exemplar distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatial pyramid based graph reasoning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Picie: Unsupervised semantic segmentation using invariance and equivariance in clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Highly efficient salient object detection with 100k parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A highly efficient model to study the semantics of salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning by learning to spot artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2733" to="2742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Aet vs. aed: Unsupervised representation learning by auto-encoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2547" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Selfsupervised learning from a multi-view perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Let there be color! joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Boosting selfsupervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9359" to="9367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretextinvariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9339" to="9348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5898" to="5906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Cross-domain self-supervised multi-task feature learning using synthetic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="762" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Ym</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Parametric instance classification for unsupervised visual feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Debiased contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<editor>Adv. Neural Inform. Process. Syst., H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8765" to="8775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by invariance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Self-supervised relational reasoning for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Contrastive learning with hard negative samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Understanding self-supervised learning dynamics without contrastive pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2959" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6509" to="6518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Spatially consistent representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9215" to="9223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Cian: Cross-image affinity net for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5208" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with imagelevel supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with boundary exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="347" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Joint learning of saliency detection and weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Representative batch normalization with feature calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">When does contrastive visual representation learning work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05837</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Contrasting contrastive self-supervised representation learning pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9949" to="9959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Boundary IoU: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">What makes instance discrimination good for transfer learning?&quot; in Int</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">A broad study on the transferability of visual representations with contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="8845" to="8855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Fixing localization errors to improve image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Online attention accumulation for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">L2g: A simple local-toglobal knowledge transfer framework for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semisupervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Leveraging instance-, image-and dataset-level information for weakly supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1415" to="1428" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Rf-next: Efficient receptive field search for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Learning from imperfect data (lid) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://lidchallenge.github.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
				<title level="m">Shanghua Gao is a Ph.D. candidate in Media Computing Lab at Nankai University. He is supervised via Prof. Ming-Ming Cheng. His research interests include computer vision and representation learning</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
