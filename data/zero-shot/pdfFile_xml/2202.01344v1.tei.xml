<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Formal Mathematics Statement Curriculum Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">Michael</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunhao</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Baksys</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
						</author>
						<title level="a" type="main">Formal Mathematics Statement Curriculum Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has enjoyed spectacular success in many domains, including language <ref type="bibr" target="#b10">Devlin et al., 2018;</ref><ref type="bibr" target="#b35">Wu et al., 2016)</ref>, vision <ref type="bibr" target="#b31">Tan &amp; Le, 2019)</ref>, and image generation <ref type="bibr" target="#b17">Karras et al., 2019)</ref>. One domain where deep learning has not yet enjoyed a comparable success is in tasks that require extensive planning and symbolic reasoning, with the exception of two-player games <ref type="bibr" target="#b29">(Silver et al., 2016;</ref><ref type="bibr" target="#b2">Berner et al., 2019;</ref><ref type="bibr" target="#b33">Vinyals et al., 2019)</ref>. In such games, deep learning systems exhibit a considerable degree of reasoning, especially when trained with self-play combined with a search procedure such as Monte Carlo Tree Search (MCTS) <ref type="bibr" target="#b4">(Browne et al., 2012)</ref>. But the resulting reasoning abilities achieved are limited due to the relatively narrow scope of games.</p><p>As such, theorem proving in interactive proof assistants, or formal mathematics, appears as an interesting game-like domain to tackle due to its increased scope. Like games, formal mathematics has an automated way of determining <ref type="bibr">Preprint. Under review.</ref> whether a trajectory (i.e. a proof) is successful (i.e. formally correct). But the vast scope of formal mathematics means that any strong reasoning result obtained in it will be more meaningful than comparable results in games (e.g. finding proofs to mathematical conjectures), and could even be applicable to important practical problems (e.g. software verification).</p><p>However, tackling formal mathematics involves two main challenges that we must address in order to continue making progress:</p><p>Infinite action space Not only does formal mathematics have an extremely large search space (like Go for example), it also has an infinite action space. At each step of proof search, the model must choose not from a well-behaved finite set of actions, but a complex and infinite set of tactics, potentially involving exogenous mathematical terms that have to be generated (e.g., generating a mathematical statement to be used as a witness, an object used steps such as "there exists an x ...", or a cut, the introduction and the chaining of a lemma in the middle of a proof).</p><p>No direct self-play setup In formal mathematics, a prover is not playing against an opponent but against a set of statements to prove. When faced with a statement that is just too hard, there is no obvious reframing of the formal mathematics setup that will let the prover generate intermediary easier statements to tackle first. This asymmetry prevents naive application of the symmetric self-play algorithms commonly used in 2-player games.</p><p>These two differences make a naive application of reinforcement learning to formal mathematics unlikely to succeed. Past work proposed to address the infinite action space problem by sampling from a language model <ref type="bibr" target="#b23">(Polu &amp; Sutskever, 2020)</ref>. This paper focuses on this second problem and our basis for addressing it is the observation that the key role of self-play is to provide an unsupervised curriculum. We propose instead to supply auxiliary sets of problem statements (without requiring proofs) of varying difficulty. We empirically show that, when the difficulty of these auxiliary problems is varied enough, a simple expert iteration procedure is able to solve a curriculum of increasingly difficult problems, eventually generalizing to our target distribution. We show that this works with both automaticallygenerated and manually-curated auxiliary distributions of arXiv:2202.01344v1 <ref type="bibr">[cs.</ref>LG] 3 Feb 2022 problems and leverage this to achieve state-of-the-art on the miniF2F benchmark. Our results suggest that continuous self-improvement in formal mathematics can potentially be reduced to the problem of generating such sets of formal statements, which we have done in part manually in this work, but could eventually be scaled in the future with more automation (such as more domain-specific statements generator or even informal to formal machine translation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">miniF2F benchmark</head><p>In this work, we target the miniF2F <ref type="bibr" target="#b38">(Zheng et al., 2021)</ref> benchmark, which consists of 244 validation and 244 test formalized statements of mathematical problems from various competitions. We believe it to be a better measure of mathematical reasoning compared to a formal libraryderived split. Also, the extreme scarcity in formal libraries of this type of problems makes it an ideal test-bed for the expert iteration methodology studied in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contribution</head><p>Our contributions are the following: we present lean-gym, a simple REPL interface for interacting with the Lean theorem prover; we propose an expert iteration methodology for GPT-f <ref type="bibr" target="#b23">(Polu &amp; Sutskever, 2020)</ref> which uses proofs generated by our models as training data to iteratively improve their performance; we demonstrate that, at fixed compute budget, expert iteration outperforms proof search only; we present a synthetic inequality generator and study how expert iteration finds and solves a curriculum of increasingly difficult problems from a set of generated statements of various difficulty; finally, we present a manually curated set of formalized problem statements and leverage it to achieve state-of-the-art performance on the miniF2F benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work strongly relies on, and can be seen as a natural continuation of the work presented in the original GPT-f paper <ref type="bibr" target="#b23">(Polu &amp; Sutskever, 2020)</ref> which studies the use of language models to generate tactics, the PACT paper  which applies GPT-f to Lean and studies the benefits from co-training on self-supervised objectives, and the miniF2F benchmark <ref type="bibr" target="#b38">(Zheng et al., 2021)</ref>.</p><p>We present additional related work in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Formal Environment</head><p>We choose <ref type="bibr">Lean (de Moura et al., 2015;</ref><ref type="bibr">lea)</ref> as our formal environment. Unlike Metamath <ref type="bibr" target="#b22">(Megill &amp; Wheeler, 2019)</ref>, which has been studied in the original GPT-f paper <ref type="bibr" target="#b23">(Polu &amp; Sutskever, 2020)</ref>, Lean benefits from high-level tactics which were shown to be beneficial in the context of the miniF2F benchmark <ref type="bibr" target="#b38">(Zheng et al., 2021</ref>)-Lean proofs are typically 10x shorter than Metamath's. Also, Lean has recently received a lot of attention from the mathematical community, thanks to projects such as the Perfectoid Spaces <ref type="bibr">(Buzzard et al., 2019)</ref> and the Liquid Tensor experiment <ref type="bibr" target="#b27">(Scholze, 2020)</ref>, and benefits from a vibrant community of hundreds of contributors to its main mathematical library called mathlib. We refer to the PACT paper's Background section  for a detailed introduction to Lean in the context of neural theorem proving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">lean-gym</head><p>In the PACT paper , proof search is performed by the Lean runtime using the LEANSTEP environment, with a generic backend interface to models. While easy to use-one just needs to plug in their model-this approach makes it difficult to alter and iterate on the search procedure because it is programmed in Lean (which is not designed or intended for cluster-wide parallelised I/O intensive tasks), and the coupling of the search procedure with the Lean runtime introduces challenges when scaling to a large number of parallel workers.</p><p>To solve these issues we implemented lean-gym 1 -a simple REPL interface over the standard input/output implemented in Lean directly. We present lean-gym's API and discuss some of its advantages and limitations in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proof datasets extraction</head><p>We rely on the proof extraction methodology presented in the PACT paper  to extract human tactic proof steps from mathlib (the tactic dataset) as well as the various other proof artifacts (mix1 and mix2 datasets). We also extract mathlib-{train,valid,test}, the set of statements from mathlib along the split proposed in <ref type="bibr" target="#b12">Han et al. (2021)</ref> (the validation and test splits of tactic, mix1, mix2 being aligned with mathlib-{valid, test} as the splits are determined by declaration name hashes (across all data sources including proof-term mining) as opposed to individual proof steps or data-points).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Expert Iteration</head><p>Expert iteration was introduced in <ref type="bibr" target="#b30">Silver et al. (2017)</ref> and broadly consists in iteratively training models on their previously sampled trajectories, to achieve continuous improvement. In this section we present our expert iteration methodology, including the models and pre-training strategies we rely on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model</head><p>We use decoder-only Transformers similar to GPT-3 . Throughout this paper we focus on a model with 36 layers and 774 million trainable parameters (referred to as the 700m model in the GPT-f paper <ref type="bibr" target="#b23">(Polu &amp; Sutskever, 2020)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pre-Training</head><p>We pre-train our models successively on GPT-3's postprocessed version of CommonCrawl (for 300B tokens) and an updated version of WebMath <ref type="bibr" target="#b23">(Polu &amp; Sutskever, 2020)</ref> (for 72B tokens) whose mix is presented in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training objectives</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Proofstep objective</head><p>The proofstep objective, introduced in <ref type="bibr" target="#b23">Polu &amp; Sutskever (2020)</ref>, consists in generating a PROOFSTEP (a Lean tactic) given a GOAL (a Lean tactic state). We also condition this objective on the current DECLARATION (a Lean theorem name), which remains the same throughout a proof search: DECL &lt;DECLARATION&gt; GOAL &lt;GOAL&gt; PROOFSTEP &lt;PROOFSTEP&gt;.</p><p>The rationale for conditioning on the declaration name is to hint our models on the position of the current declaration in the mathlib library. It can be considered as a weak proxy signal for the large amount of information not shown to the model (the full environment consisting of the available imports and currently open declarations such as module names, notations, declared instances, ...). The declaration name lets models at least in principle memorize and then retrieve some of that information, knowing that lean-gym errors if a theorem or definition that is not available in the environment associated with the current declaration is used by tactics generated by our models. Also note that conversely to <ref type="bibr" target="#b23">Polu &amp; Sutskever (2020)</ref> and like <ref type="bibr" target="#b12">Han et al. (2021)</ref> &lt;GOAL&gt; is not necessarily a single goal but a Lean tactic state, which possibly comprises multiple goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Proofsize objective</head><p>We depart from <ref type="bibr" target="#b23">Polu &amp; Sutskever (2020)</ref> and use a proofsize objective to guide our proof searches, which consists in generating one token that represents a proof size estimate bucket for the current goal (Lean tactic state): DECL &lt;DECLARATION&gt; GOAL &lt;GOAL&gt; PROOFSIZE &lt;PROOFSIZE_BUCKET_TOKEN&gt; For a given goal g, either the goal was proved as part of the proof search and we denote its proof size (the number of tactic applications (compounded Lean tactics counting as one)) as ps(g), or the goal was not proved in which case we assign the goal to a bucket that virtually represents "infinite" proof sizes.</p><p>We use 11 buckets B = 0...10 and compute the proofsize bucket b(g) for a goal g by assigning infinite proof sizes to bucket 0, all proof sizes over 20 to bucket 1 and linearly projecting proof sizes lower than 20 on the remaining buckets 2, ..., 10 (10 being the bucket for the shortest proof sizes). In practice, when training and sampling from the model, we map B to the tokens 'A'...'K'.</p><p>To value goals as we run proof searches, we sample the proofsize bucket token and record the logits p b (g) for each viable bucket and use them to get a weighted average with the following formula: v(g) = 1 #B b?B p b (g).b. As an example, if the model assigns p 0 = 1 (hence p b =0 = 0) then v(g) = 0. Conversely if the model assigns p 10 = 1 (10 being the bucket for the shortest proof sizes) then v(g) = 1.</p><p>The rationale for using this proofsize objective instead of the outcome objective described in <ref type="bibr" target="#b23">Polu &amp; Sutskever (2020)</ref> is that (i) it achieves better performance compared to the outcome objective (see table 1), and (ii) it prioritizes goals that potentially lead to shorter proofs during proof search, creating an intrinsic incentive for the system to converge towards shorter proofs. Similarly to <ref type="bibr" target="#b23">Polu &amp; Sutskever (2020)</ref> we favor this token-based approach to the introduction of a separate value head to keep the overall architecture simple. This way the proofsize objective can be implemented by simply augmenting the training dataset and without any architectural change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Bootstrapping</head><p>Bootstrapping consists in the steps required to train an initial model on both the proofstep objective and the proofsize objective.</p><p>Given a pre-trained model on WebMath, we fine-tune it on the tactic dataset extracted from mathlib as well as the proof artifacts dataset mix1 as described in <ref type="bibr" target="#b12">Han et al. (2021)</ref>. This initial model, which we denote ? 0 is solely trained on the proofstep objective. We use the validation splits of the tactic and m1 datasets to early-stop training. Note that this is our only use of mathlib-valid to influence the training process throughout this paper.</p><p>To generate data for the proofsize objective, we use ? 0 to sample proofs for statements from mathlib-train. For each statement from mathlib-train (25k) we attempt a = 1 proof searches using the cumulative logprob priority search described in Polu &amp; Sutskever (2020) (which does not require a trained value function) using d = 512 expansions and e = 8 samples per expansion. We denote the set of successful proof searches created in this process as S 0 .</p><p>Using S 0 we generate dataset D 0 by concatenating: (i) the <ref type="table">Table 1</ref>. Performance of ?0 and ?1 on mathlib-valid and miniF2Fvalid compared to PACT Lean GPT-f as reported in <ref type="bibr" target="#b12">Han et al. (2021)</ref>; <ref type="bibr" target="#b38">Zheng et al. (2021)</ref>. All models have the same architecture. ?0 is sampled using cumulative logprob priority best-first search. ?1 is sampled using best-first search based on the proofsize objective. We report our setup (d = 512 expansions and e = 8 tactic samples per expansions) as well as the setups used in <ref type="bibr" target="#b12">Han et al. (2021)</ref>; <ref type="bibr" target="#b38">Zheng et al. (2021)</ref> to control for compute. We also report the performance of ?1 on mathlib-valid when trained using the outcome objective from <ref type="bibr" target="#b23">Polu &amp; Sutskever (2020)</ref> as an ablation of our proposed proofsize objective. initial tactic dataset (proofstep objective), (ii) a deduplicated set of proofsteps extracted from the proofs in S 0 (proofstep objective) and (iii) a deduplicated set of proofsize tuples (goals and proofsize) extracted from the full proof searches in S 0 (proofsize objective).</p><p>Note that the full proof searches in S 0 include goals that are visited but eventually remain unproved, which provides useful negative examples for the trained value function (even if these negatives may include provable goals that simply were not prioritized by the search). Also note that S 0 doesn't include failed proof searches (which would contain only negative examples and no proofstep objective data).</p><p>We fine-tune ? 0 on D 0 for exactly one epoch (no use of validation data for early-stopping) to obtain our initial model ? 1 trained on both the proofstep objective and the proofsize objective. ? 0 is used in our expert iteration setup as base model to fine-tune from at each iteration, and ? 1 is our first iterated model or mathlib bootstrapped model trained on both objectives.</p><p>We report in <ref type="table">Table 1</ref> the pass rates of ? 0 and ? 1 on mathlibvalid and miniF2F-valid and compare with previously reported pass rates for equivalent amounts of compute. As reported in <ref type="bibr" target="#b23">Polu &amp; Sutskever (2020)</ref>, training a value function to guide search greatly improves the pass rates of ? 1 on mathlib-valid (see <ref type="bibr" target="#b23">Polu &amp; Sutskever (2020)</ref> for an ablation of the value function). Interestingly, the gap between ? 0 and ? 1 on miniF2F-valid is not as significant, demon-strating that training a value function on proofs sampled from mathlib-train has limited transfer to miniF2F-valid. The main differences with <ref type="bibr" target="#b38">Zheng et al. (2021)</ref>, potentially explaining the gap on minif2f-valid (27.6% vs 23.9%), consists in the new pre-training described in section 4.2 as well as the use of a more recent mathlib checkpoint for the mix1, mix2 and tactic datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Iterated sampling and training</head><p>Our expert iteration process takes as input: (i) a set of formal statements St, (ii) a function a : St ? ? N indicating the number of proof search attempts to run per statement at each iteration, (iii) a base model ? 0 to fine-tune from at each iteration, and (iv) a mathlib bootstrapped model ? 1 trained on both objectives.</p><p>Each iteration k consists in sampling proof searches for statements in St using ? k , filtering successful proof searches S k to extract a new dataset D k , and fine-tuning ? 0 on it to obtain ? k+1 , on which we can iterate. To sample proof searches from St we use the best-first search described in <ref type="bibr" target="#b23">Polu &amp; Sutskever (2020)</ref> with the value function described in section 4.3.2. We attempt a(s ? St) proof searches for each statement s with d = 512 expansions and e = 8 samples per expansion. We denote the set of successful proof searches for iteration k as S k .</p><p>Using S k we generate datasets D k by concatenating: (i) the initial tactic dataset (proofstep objective), (ii) a deduplicated set of proofsteps extracted from the proofs in 1?i?k S k (proofstep objective), and (iii) a deduplicated set of proofsize tuples (goals and proofsize) extracted from the full proof searches in 1?i?k S k (proofsize objective).</p><p>Note that we use a global deduplication across iterations for both proofsteps and proofsize tuples which we found to be important to maintain the stability of the expert iteration procedure. This global deduplication is somewhat equivalent for each statement to growing a unique proof tree by aggregating all the proof searches that have been run for it across iterations. This virtual proof tree accumulates a growing number of positive proof paths as well as a growing number of visited goals that remain unproven. We use these goals as negative examples for the proofsize objective, labeling them with an infinite proofsize. Positive goals are deduplicated keeping the minimum proof sizes across proof searches.</p><p>Finally ? k is obtained by fine-tuning ? 0 for exactly one epoch on D k . Note that the initial tactic dataset is included in each D k , despite ? 0 being already trained on it (along with mix1). We found this repetition to be beneficial overall (as it adds the mathlib extracted proofsteps to our deduplicated per statements virtual proof trees) despite it leading to a slight overfit on the tactic dataset in terms of validation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Expert iteration on mathlib-train</head><p>In this section we propose to set St to the statements in mathlib-train, run our expert iteration process with it and report performance on both mathlib-valid and miniF2F-valid. Performance is reported in terms of pass rate (percentage of successful proof searches) as a function of the number of attempts per statement, noted pass@k where k is the number of attempts per statement at test time. To reduce noise in these metrics we generally run more than k attempts at test time (generally 32 to compute pass@1 and pass@8), averaging across attempts as needed to obtain a smoother pass@k value.</p><p>Given the large number of statements in mathlib-train (25k) we uniformly set a = 1 and use ? 0 and ? 1 as described in section 4.4 and report pass@1 and pass@8 across 8 iterations in figure 1. The pass@1 on mathlib-valid goes from 56.3% for ? 1 to 62.6% for ? 9 . The performance steadily improves and follows a clear logarithmic scaling law on mathlib-valid. It is also notable that, initially, transfer to outof-distribution minif2f-valid appears limited but eventually kicks in as we reach better performance on mathlib-valid. This demonstrates that the expert iteration process does not just overfit to mathlib but also leads to improved performance on out-of-distribution statements.  We define the cumulative pass rate at iteration k as the pass rate consisting of all proof searches up to iteration k (necessarily monotonic in k). Since we set a = 16 for evaluation on mathlib-valid and minif2f-valid at each iteration, the cumulative pass rate at iteration k can be seen as a noisy ensembled pass@16k (multiple models (? k ), no averaging).</p><p>In figure 2, we report this cumulative pass rate for two iteration loops, our normal one and a sampling-only loop where we skip re-training the model between iterations and solely sample from ? 1 . This directly compares test-time compute scaling (scaling proof search attempts) to expert iteration scaling (interleaved training on new data sampled from mathlib-train) and provides a very clear visualization of the gains of expert iteration. For a fair comparison, we also report an adjusted compute line which approximates the test-time performance we would get at each iteration if we were to focus all the additional compute used by expert iteration (sampling proofs from mathlib-train as well as re-training models at each iteration) towards solely running proof searches against mathlib-valid.</p><p>As shown by figure 2, the scaling exponent of expert iteration is substantially higher than the scaling exponent associated with solely scaling test-time compute (running more proof searches), demonstrating the clear benefit of expert iteration. We'll denote the fully iterated model from this section as ? mathlib 9 .</p><p>Even in the presence of ground-truth proofs for each of the statements in mathlib-train (tactic dataset), expert iteration generates data that further improves the performance of the model. The number of statements proved in mathlib-train goes from 17390 (67.8%) at iteration 1 to 19476 (76.0%) at iteration 9, while the average proof length of these statements goes from 4.8 to 4.0. We hypothesize that this continuously improving performance through expert iteration stems from two effects: (i) the model finding new original proofs for the same statements and (ii) the model closing marginally harder statements at each iteration -which in turn provides more useful training data for the next iteration. By iteration 9, the model is trained on more than 90% generated data. We present in Appendix E a few examples of original proofs found by our models on mathlib-train compared with their ground-truth versions.</p><p>To verify our hypothesis that expert iteration is capable of closing a curriculum of increasingly difficult problems out of a set of problem statements, and that this capability is independent of having access to ground-truth proofs, we propose in the next section to study expert iteration applied to a synthetically generated set of problems for which we have fine-grained control on the difficulty of each statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Statement curriculum learning</head><p>In this section we focus on running expert iteration on synthetic statements generated by an inequality generator. The use of synthetic statements enables us to control the difficulty of each statement to present evidence that expert iteration can hill-climb the intrinsic difficulty gradient of the resulting set of statements. In particular, we show that, at fixed compute budget, expert iteration eventually closes proofs of hard statements that remain completely out of reach of simply sampling proof searches without interleaved training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic inequality generator</head><p>We designed a synthetic inequality statement generator for Lean in the spirit of the INT  generator. The generator consists in generating inequalities from well known inequality theorems (AM-GM, Trivial inequality, Cauchy-Schwarz, Bernoulli, Young, H?lder) and composing them. It is driven by two difficulty parameters: N D which controls depth of composition of inequalities and N S which controls the complexity of the input expressions to the composed inequalities. We provide details on its implementation in Appendix D.</p><p>Using this generator we generate a curriculum of 5600 inequality statements (for which we don't have proofs), 100 for each values of 0 ? N S ? 7 and 0 ? N D ? 6. We denote this set of statements as synth-ineq.</p><p>To bootstrap our models capabilities on this specific task, we also generate 100 statements of low difficulty (N D = 1 and N S = 5) and formalize a proof for each of these statements.</p><p>We refer to this dataset as synth-ineq-train. In the rest of this paper we adjunct this training dataset to the tactic dataset used to train our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Expert iteration on synthetic inequality statements</head><p>In this section we propose to set St to the union of the statements in mathlib-train and synth-ineq. Again, we uniformly set a = 1 and use ? 0 and ? 1 as described in section 4.4, except that they are now also trained on synth-ineq-train.</p><p>Similarly to the previous section, we report in <ref type="figure" target="#fig_2">figure 3</ref> the cumulative pass rate for two loops, our standard expert iteration loop, and a proof search only loop where we don't interleave training between iterations. The pass rates are reported split by values of N D (pooling together 0 ? N S ? 7) which we found to be the main driver for difficulty. Despite the challenging nature of these synthetic inequalities, figure 3 demonstrates that expert iteration is capable of learning the intrinsic curriculum induced by synth-ineq. In particular, expert iteration is capable of closing 6 problems of difficulty N D = 6 without having been provided with any seed ground-truth proof for this difficulty level. Note that difficulty N D = 6 remains completely out of reach of simply scaling the number of attempts per statements (the sample only loop remaining stuck at 0 for N D = 6).</p><p>This confirms on our synthetic statements dataset synth-ineq that not only expert iteration is capable of learning the curricula occurring in a set of statements, but this process also enables the emergence of new capabilities without the need for ground-truth proofs (ability to close, highly challenging, deeply composed inequalities).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Targeting miniF2F</head><p>Motivated by the results from Section 5, we curated and manually formalized a set of math exercises to target miniF2F. miniF2F statements being quite out of distribution compared to the distribution of statements present in mathlib (which typically includes generic theorems and lemmas but very few exercises), we hypothesized that if the difficulty of this set of statements was made varied enough, expert iteration could potentially leverage it to effectively shift our models' distribution closer to miniF2F's, and in turn, improve their eventual performance on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Formalization effort</head><p>We manually formalized 327 statements 2 drawn from the following sources: 302 examples and exercises from <ref type="bibr">Lehoczky &amp; Rusczyk (a;b)</ref>. The books are classic problem solving textbooks for students in grades 7-12 preparing for contests such as AMCs and AIMEs. 25 problems from the MATH dataset <ref type="bibr" target="#b13">(Hendrycks et al., 2021)</ref>. All problems were drawn from the train split of the dataset, focusing on difficulty 5 problems (miniF2F only contains problems from the test split).</p><p>We refer to <ref type="bibr" target="#b38">Zheng et al. (2021)</ref> for more details on the formalization procedure and the typical time needed for it as these problems were formalized in similar conditions. We denote this set of statements as miniF2F-curriculum and verified (based on problem provenance and manual inspection of statements) that it had an empty intersection with miniF2F-{test,valid}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Transfer to miniF2F</head><p>In this section we propose to set St to the union of the statements in mathlib-train, synth-ineq and miniF2F-curriculum. We uniformly set a = 1 on mathlib-train and synth-ineq and a = 8 on miniF2F-curriculum and use ? 0 and ? 1 as described in section 5.</p><p>Similarly to previous sections, we report in figure 4 (left) the cumulative pass rate on miniF2F-valid of our full curriculum expert iteration loop and compare them with the mathlib-train only expert iteration from section 4.6. Since more compute is deployed in our full-curriculum loop (more statements) we also report a mathlib-train only loop taking a = 2. At the end of the expert iteration, 100 out of the 327 statements from miniF2F-curriculum end up being closed, suggesting a lack of density in our manually formalized set of statement.</p><p>We also report in figure 4 (right) the pass@1 and pass@8 for our full curriculum expert iteration loop. The steady improvement on miniF2F-valid shows that the expert iteration procedure we propose does not overfit on the statements that compose the curriculum it uses. Despite the potential inefficiency of our curriculum, the improved performance associated with its use demonstrates, as hypothesized, an 2 https://github.com/openai/miniF2F/tree/ statement_curriculum_learning/lean/src/ statement_curriculum_learning  <ref type="figure">Figure 4</ref>. Left: cumulative pass rate on miniF2F-valid for our expert iteration loop using our full curriculum (mathlib-train, synthineq and miniF2F-curriculum) compared to the expert iteration loop from section 4.6. The total number of attempts per iteration in our full loop is 25k + 5.6k + 8 * 327 ? 33.2k, which means the total compute deployed is higher than in the mathlib-train only loop (25k). We therefore also report in dotted a mathlib-train only loop, taking a = 2, whose total number of attempts per iteration is ? 50k. Right: pass@1 (plain) and pass@8 (dotted) for our expert iteration loop using our full curriculum (mathlib-train, synth-ineq and miniF2F-curriculum) compared to the expert iteration loop from section 4.6.</p><p>effective transfer between miniF2F-curriculum, synth-ineq and miniF2F-valid through expert iteration. We'll denote the fully iterated model from this section as ? full 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head><p>We report in table 2 the pass rates on mathlib-{valid, test} and miniF2F-{valid, test} for the models trained in previous sections, namely ? 1 , ? mathlib 9 , and ? full 9 . We achieve a 47.3% pass rate (using a = 64 attempts) on miniF2F-valid and a 36.6% pass rate on miniF2F-test, substantially improving from the previous state-of-the-art <ref type="bibr" target="#b38">(Zheng et al., 2021)</ref>.</p><p>These results include the resolution of 26 AMC12 problems, 6 AIME problems and 2 problems adapted from the IMOs. Out of these statements, 4 AMC12 problems (amc12b_2020_p5, amc12a_2009_p9, amc12a_2003_p24, amc12b_2003_p17), 2 AIME problems (aime_1984_p1, aime_1990_p4), and 2 IMO-adapted problems (imo_1961_p1 3 , imo_1964_p2) are uniquely solved by expert iterated models, the two IMO-adapted and the two AIME problems being uniquely solved by ? full 9 . We provide a selection of the proofs found by our models for these statements as well as a qualitative analysis of them in Appendix F.</p><p>Also, we achieve a higher than 75% pass rate (using a = 64 attempts) on mathlib-{valid, test} (a new state-of-the-art as well) suggesting that our models have the potential to be effectively leveraged as proof assistants in the formalization efforts associated with mathlib.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Model Size</head><p>Throughout this paper, we used a single model size (774m trainable parameters). We briefly experimented with different model sizes (not reported in this paper) and found that model size scaling is not as straightforward as in the case of unsupervised learning . We found that bigger models are better, in the sense that they consistently exhibit higher pass@1. But, they are also much more expensive to sample from. And despite their pass@1 being higher, it is often the case that for a fixed amount of compute, sampling more attempts from a smaller model leads to a better final performance.</p><p>For the compute budget we had available, we estimated the model size we used to be a compelling trade-off. We leave as future work a more thorough study of these dynamics to better understand the different compute frontiers involved.</p><p>Indicatively, with our 774m parameters model, running a full expert iteration to train ? full 9 required about 2000 A100 days of compute. Running one full proof search (a = 1 d = 512 e = 8) when properly parallelised, requires on average about 0.1 A100 hour of compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Limitations</head><p>Despite our models' capability, as discussed in Appendix F.1, to generate cuts and witnesses, we believe that their current main limitation lies in their inability (under our proposed search procedure) to chain more than 2 or 3 non-trivial steps of mathematical reasoning, preventing them from consistently (instead of exceptionally) solving challenging olympiad problems. We've been repeatedly impressed by the complexity of some of the proofsteps generated by our models. But, proofs requiring many of such reasoning steps remain beyond our current compute horizon. Even if we solved a selection of challenging olympiad problems, our models are still very far from being competitive with the brightest students in these competitions.</p><p>While our models have demonstrated some capabilities to generate cuts, the cuts they generate are often shallow (they involve only a few proofsteps and don't necessarily deeply change the structure of the proof-we refer the reader to the Cut-Elimination theorem and <ref type="bibr" target="#b6">Carbone &amp; Semmes (1996)</ref> for a discussion of the influence of cuts on proof size). We believe that studying language models' ability to generate cuts, and designing search procedures that leverage that capability (related ideas can be found in <ref type="bibr">Czechowski et al. (2021)</ref>), are interesting avenues of research to alleviate this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper we presented an expert iteration procedure for GPT-f <ref type="bibr" target="#b23">(Polu &amp; Sutskever, 2020)</ref>, demonstrating that it is capable of solving a curriculum of increasingly difficult problems out of a set of formal statements of sufficiently varied difficulty. Our results suggest that the lack of self-play in the formal mathematics setup can be effectively compensated for by automatically as well as manually curated sets of formal statements, which are much cheaper to formalize than full proofs. Finally, we hope that the statement curriculum learning methodology we presented in this work will help accelerate progress in automated reasoning, especially if scaled with automated generation and curation of formal statements in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>Deep learning applied to premise selection and proof guidance Early applications of deep learning to formal mathematics focused primarily on premise selection and proof guidance. DeepMath <ref type="bibr" target="#b15">(Irving et al., 2016)</ref> explored the use of CNNs and RNNs to predict whether a premise is useful to demonstrate a given conjecture. Their results were later improved with FormulaNet <ref type="bibr" target="#b34">(Wang et al., 2017)</ref> by the use of graph neural networks, reminiscent of NeuroSAT <ref type="bibr" target="#b28">(Selsam et al., 2018)</ref>. Proof guidance consists in selecting the next clause to process inside an automated theorem prover. <ref type="bibr" target="#b21">Loos et al. (2017)</ref> investigated the use of models similar to DeepMath's for proof guidance and demonstrated a significant uplift on the Mizar library. More recently <ref type="bibr" target="#b11">Firoiu et al. (2021)</ref> demonstrated the potential of deep learning techniques to be competitive with E prover's heuristics when applied to resolution calculus while training on fully synthetic data.</p><p>Deep learning applied to automated theorem-proving HOList <ref type="figure" target="#fig_1">(Bansal et al., 2019a)</ref> proposes a formal environment based on HOL Light. They achieve their best performance <ref type="bibr" target="#b1">(Bansal et al., 2019b</ref>) with a GNN model designed for premise selection and the use of exploration. The same team studied the use of a skip-tree objective with Transformers on formal statements <ref type="bibr" target="#b24">(Rabe et al., 2020)</ref>, demonstrating, along with GPT-f <ref type="bibr" target="#b23">(Polu &amp; Sutskever, 2020)</ref>, the potential of leveraging Transformers for formal reasoning. GamePad <ref type="bibr" target="#b14">(Huang et al., 2018)</ref> and CoqGymn/ASTactic <ref type="bibr" target="#b37">(Yang &amp; Deng, 2019)</ref> introduce environments based on the Coq theorem prover. ASTactic generates tactics as programs by sequentially expanding a partial abstract syntax tree. <ref type="bibr" target="#b32">Urban &amp; Jakub?v (2020)</ref> studied the capability of GPT-2 to produce useful conjectures for the Mizar library and IsarStep <ref type="bibr" target="#b20">(Li et al., 2020)</ref> explored the synthesis of intermediate propositions in declarative proofs for Isabelle/HOL using Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lean-gym</head><p>lean-gym presents the following API:</p><p>? init-search: declaration ? tactic_state. Takes a declaration name (a theorem name from the loaded library) and initializes a search while setting the run-time environment at that particular declaration. It returns the initial tactic state along with a fresh search_id and tactic_state_id.</p><p>? run_tac: (tactic_state, tactic) ? tactic_state. Takes a search_id and a tactic_state_id to identify a tactic state, as well as a tactic string to apply to it. It returns a new tactic state and its associated tactic_state_id.</p><p>Below is an example in-terminal trace demonstrating the use of lean-gym's REPL interface:</p><p>$ lean --run src/repl.lean ["init_search", ["int.prime.dvd_mul", ""]] { "error":null, "search_id":"0", "tactic_state":" ? {m n : Z} {p : N}, nat.prime p ? ?p | m * n ? p | m.nat_abs ? p | n.nat_abs", "tactic_state_id":"0" } ...</p><p>["run_tac",["1","1","apply (nat.prime.dvd_mul hp).mp"]] { "error":null, "search_id":"1", "tactic_state":"m n : Z, p : N, hp : nat.prime p, h : ?p | m * n p | m.nat_abs * n.nat_abs", "tactic_state_id":"2" } ...</p><p>Using lean-gym is virtually equivalent to opening a Lean editor at a specific theorem, deleting its proof and interacting with Lean to reconstruct it. Providing a REPL interface over the standard input/output makes it very easy to integrate lean-gym from any programming language. Writing a wrapper in Python, as an example, only takes a few dozen lines of code. Since lean-gym is a Lean program, managing the loaded libraries is done directly using Lean's own infrastructure (using leanpkg.toml), making it quite straightforward to have access to both mathlib and miniF2F statements from the same lean-gym instance.</p><p>Note that lean-gym is stateful, meaning that distributing proof searches on multiple lean-gym instances requires to track which instance is associated with which proof search. In practice, we were able to scale the use of lean-gym to thousands of cores running thousands of proof searches in parallel. Finally, lean-gym's REPL interface is blocking, preventing inner-proof search parallelization, though this limitation can probably be removed in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. WebMath</head><p>Our updated WebMath pre-training dataset consists in the mix presented in table 3.</p><p>As demonstrated in table 3, we empirically up-weighted (compared to their token size) parts of WebMath with high-quality mathematical content while making sure they don't overfit (despite running &gt;1 epochs for some of them). We also included PACT mix2 directly in the WebMath pre-training to avoid having to sequence more than two pre-training phases to prepare Lean models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Synthetic inequalities D.1. Design</head><p>The generator consists of three phases:</p><p>Seed expressions generation The first phase consists in generating seed expressions for which we track the sign. We start by initializing an expression set E composed of tuples of expressions and sign constraints, by generating n v variable names (letters) assumed strictly positive as well as n n integers (for which we know the sign). For N S rounds, we compose elements of E using unary (log(?), log(1/?), sqrt(?)) or binary operations (+, ?, ?, /, ?, max, min) for which we can deduce the sign based on the sign condition of the input expression(s) and re-inject the resulting expression and sign constraint in E. This produces a set E of signed seed expressions of size n v + n n + N S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inequality composition</head><p>The second phase consists in generating inequalities from well known inequality theorems (AM-GM, Trivial inequality, Cauchy-Schwarz, Bernoulli, Young, H?lder) taking as input to these theorems expressions from E based on the sign constraints required for each theorem. We finally compose these inequalities N D times using compositions theorems detailed in D.2. The resulting inequality is a composed inequality of depth N D based on n v + n n + N S seed expressions.</p><p>Simplification We finally post-process these inequalities so that they are parsable by Lean and run them through Lean's simp tactic for a final simplification.</p><p>N D and N S together control for the difficulty of the resulting inequality. N D controls depth of composition, while N S controls for obfuscation as it increases the complexity of the input expressions to the composed inequalities. When sampling inequalities, we n n = 4 and randomly sample 2 ? n v ? 8 at each generation. We report below examples of generated inequalities for various values of N D and N S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. List of inequality composition theorems</head><p>Below is the list of theorem names from mathlib that we use to compose inequalities together. One third of the time, we only transform the current composed inequality with one of the following theorems:</p><formula xml:id="formula_0">? neg_le_neg ? inv_le_inv ? mul_self_le_mul_self ? div_le_one_of_le</formula><p>We otherwise compose the current composed inequality with a newly generated inequality using the following theorems:</p><p>? mul_le_mul </p><formula xml:id="formula_1">? add_le_add ? div_le_div ? mul_le_mul_of_nonneg ? le_mul_of_ratio D.3. Examples N D = 0 N S = 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Example proofs from mathlib-train</head><p>We present in this section original proofs found by our models from mathlib-train, compared with their ground-truth version. We present in this section proofs found by our models from minif2f-{test,valid,curriculum}, demonstrating some of the capabilities emerging from our training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Qualitative analysis of proofs</head><p>We provide qualitative insights in the nature of the proofs found by our models, which we believe are useful to build a better intuition of their capabilities beyond pass rate numbers. Throughout this section, we refer to statements and solutions found by our models that are presented in Appendix F along with comments describing the specificity of each proof.</p><p>First, we observe that a large number of olympiad problems that are designed to be computationally challenging for humans are rendered trivial for our models through the use of Lean tactics. As an example, mathd_numbertheory_447 which is not necessarily considered straightforward for humans, can be closed in Lean by a simple refl (proof found by our models).</p><p>In recent years, Lean's mathlib community has developed high-powered tactics such as linarith/nlinarith (solves (non)linear inequalities), norm_num (normalizes numerical expressions), simp (simplifies goals and hypotheses) and ring (normalizes expressions in a ring). These tactics can be used with arguments to guide their underlying search procedure. As mentioned in <ref type="bibr" target="#b38">Zheng et al. (2021)</ref>, we confirm here that our models acquire advanced capabilities to leverage these high-level tactics by providing exogenous arguments which are not present in the current tactic state. The generation of these exogenous arguments through language modeling seems to require a non-trivial amount of mathematical intuition. imo_1964_p2, imo_1961_p1 and aime_1990_p15 are good examples of such uses.</p><p>We have also observed a number of proofs that require multiple non-trivial reasoning steps through the use of lowerlevel tactics such as use, have, or by_cases that generally involve producing a witness or chaining implications, requiring the generation of context specific exogenous terms. These interesting reasoning steps are structurally different from simple normalization, simplification and rewriting of hypotheses or goals because they heavily rely on our models ability to generate meaningful cuts or witnesses. This capability is, in our opinion, the most exciting stepping stone towards solving more challenging mathematical problems. See, aopsbook_v2_c8_ex1, amc12b_2020_p6 and mathd_train_algebra_217 for examples of such proofs.</p><p>More generally, we also observe that proofs generated by our models have a distinctive style compared to proofs formalized by humans. This stems in part from the model's capability to leverage high-level tactics in a way that is challenging for humans as discussed in this section (e.g. one-liners such as nlinarith [sq_nonneg (x -y), sq_nonneg (y -z)] where humans would generally decompose the problem in a less machine-like way). Additionally, as a result of our search procedure and despite the bias towards shorter proofs introduced by our value function, extraneous proofsteps (such as reversion/introduction of hypotheses, or no-op rewrites) are often interleaved with useful ones, which rarely happens in human formalizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>imo_1961_p1</head><p>Natural language</p><p>Solve the system of equations:</p><formula xml:id="formula_2">x + y + z = a x 2 + y 2 + z 2 = b 2 xy = z 2</formula><p>where a and b are constants. Give the conditions that a and b must satisfy so that x, y, z (the solutions of the system) are distinct positive numbers. Note: the formalized statement in miniF2F is a weaker problem as it focuses on the second part of the question, providing the actual conditions, and asking for a proof that the requirement entails them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comments</head><p>The model is able to close this problem by splitting into cases, contraposing for the last case and using nlinarith. It must be noted that the arguments for the first two nlinarith uses are not necessary, however the [sq_nonneg (x -y), sq_nonneg (y -z)] argument provided on the last line is crucial to close the goal and are completely exogenous (present in no form in the tactic state before). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comments</head><p>The model is able to close a challenging AIME problem by providing crucial exogenous arguments sq_nonneg (x -y) and sq_nonneg (x + y -(2 : R)) to nlinarith, which are required to close the goal (while the other two can be removed). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comments</head><p>The model is able to close the goal by contraposing, supplying a witness by the use of use (0 : R) and finally leveraging the simp and norm_num. This example demonstrates the model's ability to chain multiple non-trivial steps of reasoning including the generation of witnesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formal Mathematics Statement Curriculum Learning amc12b_2020_p6</head><p>Natural language For all integers n ? 9, the value of (n + 2)! ? (n + 1)! n! is always which of the following? (A) a multiple of 4 (B) a multiple of 10 (C) a prime number (D) a perfect square (E) a perfect cube.</p><p>Model proof theorem amc12b_2020_p6 (n : N) (h0 : 9 ? n) : ? x : N, (x:R)^2 = (nat.factorial (n + 2)nat.factorial (n + 1)) / nat.factorial n := begin revert_all, intros n hn, use n + 1, field_simp [nat.factorial_ne_zero, pow_succ'], ring_exp end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comments</head><p>The model exhibits here its ability to implicitly interpret factorial expressions by guessing the correct answer with the use of use n + 1 and straightforwardly proving its claim right after.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>pass@1 (plain) and pass@8 (dotted) for mathlib-valid and minif2f-valid when running 8 expert iterations with St set to be the statements in mathlib-train. The x-axis is log-scaled. It corresponds to the indices of the ? k models and serves as a good proxy to compute (the amount of test-time and train-time compute per iteration being fixed). The y-axis is scaled linearly and simply shifted between the two graphs (spans an equal range).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Cumulative pass rate for our expert iteration loop as well as a sample only loop where we skip re-training the model between iterations. Pass rates are reported for each value of ND (pooling together 0 ? NS ? 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>R) / (10:R)) * b^(10:R) ? ? * a^(10:R) ? ? ? (8:R) / (10:R) * (67:R) + (10:R) ? ? * a + b * (10:R) ? ? := sorry N D = 0 N S = R) + (99:R) * c + (a / f / (6:R) + a * (a / f) / ((d^2 + a^2 / f^2) * (real.log ((59:R) + f)^2 + c^2))) ? ((a / f)^((3:R) / (2:R)) / ((3:R) / (2:R)) + a^3 / (3:R)) / (real.log ((59:R) + f) * d + a / f * c)^2 * (c / (c / (70:R))) + a / f + (c + (1:R))^99 := sorry</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>b, c are the sides of a triangle. Prove that a 2 (b + c ? a) + b 2 (c + a ? b) + c 2 (a + b ? c) b + c -a) + b^2 * (c + a -b) + c^2 * (a + b -c) ? 3 * a * b * c := begin nlinarith [sq_nonneg (b -a), sq_nonneg (c -b), sq_nonneg (a -c), sq_nonneg (c -a)] end Comments The model is able to close an IMO problem in one-line. It correctly provides exogenous arguments to nlinarith, which are necessary to close the goal. Note that either one of the last two arguments in the sequence [sq_nonneg (b -a), sq_nonneg (c -b), sq_nonneg (a -c), sq_nonneg (c -a)] can be omitted. aime_1990_p15 Natural language Find ax 5 + by 5 if the real numbers a, b, x, and y satisfy the equations ax + by = 3, ax 2 + by 2 = 7, ax 3 + by 3 = 16, ax 4 + by 4 = 42. Note: the formalized statement in miniF2F provides the answer and asks for a proof of it. Model proof theorem aime_1990_p15 (a b x y : R) (h0 : a * x + b * y = 3) (h1 : a * x^2 + b * y^2 = 7) (h2 : a * x^3 + b * y^3 = 16) (h3 : a * x^4 + b * y^4 = 42) : a * x^5 + b * y^5 = 20 := begin revert_all, intros a b x y h3 h1 h2 h4, ring_nf at h1 h2, rw ? sub_eq_zero at h1, nlinarith [sq_nonneg (x -y), sq_nonneg (a + b -2), sq_nonneg (x + y -(2:R)), sq_nonneg (a -b -2)] end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>f (x) = Ax + B and g(x)= Bx + A, where A = B. If f (g(x)) ? g(f (x)) = B ? A, what is A + B?Note: the formalized statement in our curriculum provides the answer and asks for a proof of it.Model proof theorem mathd_train_algebra_217 (a b : R) (f g : R ? R) (h0 : ? x, f x = a * x + b) (h1 : ? x, f x = b * x + a) (h2 : a = b) (h3 : ? x, f (g x) -g (f x) = b -a?h1,h2??, use (0 : R), simp only [sub_eq_iff_eq_add, h0, mul_zero] at * , simp only [ * , zero_add], norm_num at h0 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Cumulative pass rate for our expert iteration loop as well as a sample only loop where we skip re-training the model between iterations. The adjusted compute line is computed by fitting the sample only curve and shifting it to approximate a setup where we would focus all the additional compute used by expert iteration (sampling training data from mathlib-train as well as re-training models at each iteration) towards running proof searches against mathlib-valid.</figDesc><table><row><cell>78</cell><cell cols="2">mathlib-valid</cell><cell></cell><cell></cell><cell>46</cell><cell cols="2">minif2f-valid</cell><cell></cell><cell></cell></row><row><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>42</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38</cell><cell></cell><cell cols="2">Expert iteration</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sample only</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Adjusted compute</cell></row><row><cell>68</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>36</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell></row><row><cell>Figure 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of ?1 (value-function based search), ? mathlib 9 (expert iterated on mathlib-train) and ? full 9 (expert iterated on our full curriculum) on mathlib-{valid, test} and miniF2F-{valid, test}. All proof searches are run with d = 512 and e = 8.</figDesc><table><row><cell>Model</cell><cell cols="3">pass@1 pass@8 pass@64</cell></row><row><cell>mathlib-valid</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PACT (Han et al., 2021)</cell><cell>48.4%</cell><cell>-</cell><cell>-</cell></row><row><cell>?1</cell><cell>56.3%</cell><cell>66.3%</cell><cell>72.0%</cell></row><row><cell>? mathlib 9</cell><cell>62.6%</cell><cell>70.7%</cell><cell>75.8%</cell></row><row><cell>? full 9</cell><cell>61.7%</cell><cell>69.8%</cell><cell>75.3%</cell></row><row><cell>mathlib-test</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?1</cell><cell>56.5%</cell><cell>66.9%</cell><cell>73.7%</cell></row><row><cell>? mathlib 9</cell><cell>63.0%</cell><cell>71.5%</cell><cell>77.1%</cell></row><row><cell>? full 9</cell><cell>62.9%</cell><cell>71.6%</cell><cell>76.3%</cell></row><row><cell>miniF2F-valid</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PACT (Zheng et al., 2021)</cell><cell>23.9%</cell><cell>29.3%</cell><cell>-</cell></row><row><cell>?1</cell><cell>28.5%</cell><cell>35.5%</cell><cell>41.2%</cell></row><row><cell>? mathlib 9</cell><cell>31.3%</cell><cell>38.3%</cell><cell>44.1%</cell></row><row><cell>? full 9</cell><cell>33.6%</cell><cell>41.2%</cell><cell>47.3%</cell></row><row><cell>miniF2F-test</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PACT (Zheng et al., 2021)</cell><cell>24.6%</cell><cell>29.2%</cell><cell>-</cell></row><row><cell>?1</cell><cell>25.9%</cell><cell>31.1%</cell><cell>33.6%</cell></row><row><cell>? mathlib 9</cell><cell>27.2%</cell><cell>33.0%</cell><cell>35.2%</cell></row><row><cell>? full 9</cell><cell>29.6%</cell><cell>34.5%</cell><cell>36.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Mix and source of data involved in the updated WebMath pre-training.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Size Mix</cell></row><row><cell>Github Python</cell><cell cols="2">179 GB 25%</cell></row><row><cell>arXiv Math</cell><cell cols="2">10 GB 25%</cell></row><row><cell>Math StackExchange</cell><cell cols="2">2 GB 25%</cell></row><row><cell>PACT mix2</cell><cell cols="2">28 GB 17%</cell></row><row><cell>Math Overflow</cell><cell>200 M</cell><cell>5%</cell></row><row><cell>ProofWiki</cell><cell>30 M</cell><cell>2%</cell></row><row><cell>PlanetMath</cell><cell>25 M</cell><cell>1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Example proofs from minif2f-{test,valid,curriculum}</figDesc><table><row><cell cols="2">sum_range_sub_sum_range prod_inv_distrib F.</cell></row><row><cell>comap_eq_of_inverse Statement Statement</cell><cell>k in range m, f k -lemma prod_inv_distrib : ( lemma sum_range_sub_sum_range {? : Type * } [add_comm_group ?] k in range n, f k = x in s, (f x) ? ?) = {f : N ? ?} {n m : N} (hnm : n ? m) : ( x in s, f x) ? ? :=</cell></row><row><cell></cell><cell>k in (range m).filter (? k, n ? k), f k := begin</cell></row><row><cell>Statement</cell><cell>classical, lemma comap_eq_of_inverse {f : filter ?} {g : filter ?} begin by_cases h : ? x ? s, f x = 0, {? : ? ? ?} (? : ? ? ?) (eq : ? ? ? = id) rw [? sum_sdiff (@filter_subset _ (? k, n ? k) _ { simpa [prod_eq_zero_iff.mpr h, prod_eq_zero_iff] (h? : tendsto ? f g) (h? : tendsto ? g f) : (range m)), sub_eq_iff_eq_add, using h }, comap ? g = f := ? eq_sub_iff_add_eq, add_sub_cancel'], { push_neg at h,</cell></row><row><cell>Ground-truth Ground-truth Ground-truth</cell><cell>begin refine ((comap_mono $ refine finset.sum_congr (finset.ext $ ? a, ?? h, by simp at * ; finish, have h' := prod_ne_zero_iff.mpr h, have hf : ? x ? s, (f x) ? ? * f x = 1 := ? x hx, ? h, have ham : a &lt; m := inv_mul_cancel (h x hx), map_le_iff_le_comap.1 h?).trans _).antisymm lt_of_lt_of_le (mem_range.1 h) hnm, apply mul_right_cancel' h', (map_le_iff_le_comap.1 h?), by simp * at * ?) simp [h, h', ? finset.prod_mul_distrib, rw [comap_comap, eq, comap_id], (? _ _, rfl) prod_congr rfl hf] } exact le_rfl end end end</cell></row><row><cell>Model proof Model proof Model proof</cell><cell>begin begin begin rw [? sum_Ico_eq_sub _ hnm], classical; induction s using refine le_antisymm _ (filter.map_le_iff_le_comap.1 h?), congr, finset.induction_on with a s has ih, refine ? s hs, _, apply finset.ext, { simp, }, rw mem_comap, simp [Ico.mem, * ], simp only [has, prod_insert has, mul_inv_rev'], use [? ? ?' s, h? hs], tauto finish rw [? preimage_comp, eq, preimage_id] end end end</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">OpenAI 2 ?cole Polytechnique 3 University of Cambridge. Correspondence to: Stanislas Polu &lt;spolu@openai.com&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/openai/lean-gym</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that this IMO-adapted statement from miniF2F-valid is a much weaker version than the original problem (see Appendix F for more context)</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural language</head><p>The expression 24x 2 ? 19x ? 35 can be written as (Ax ? 5)(2Bx + C), where A, B, and C are positive numbers. Find AB ? 3C. Note: the formalized statement in miniF2F provides the answer and asks for a proof of it.</p><p>Model proof theorem mathd_algebra_140 (a b c : R) (h0 : 0 &lt; a ? 0 &lt; b ? 0 &lt; c) (h1 : ? x, 24 * x^2 -19 * x -35 = (((a * x) -5) * ((2 * (b * x)) + c))) : a * b -3 * c = -9 := begin revert_all, rintro a b c h1, rw mul_comm, rw ?sub_eq_zero, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comments</head><p>The model mostly "struggles" to make progress up to the last line. There, it presents us with the idea to specialize the statement given in h1 (which became h0 in the process of the proof) at three consecutive natural numbers 1, 2, 3 which closes the goal with nlinarith. This proof is interesting as it demonstrates the model's ability to evaluate symbolic expressions implicitly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comments</head><p>An interesting example of a generated have statement, denoted by h0, which is introduced as a cut in order to simplify the expression containing divisions by using field_simp[h0, norm_neg] at a later step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>mathd_numbertheory_447</head><p>Natural language What is the sum of the units digits of all the multiples of 3 between 0 and 50? Note: the formalized statement in miniF2F provides the answer and asks for a proof of it.</p><p>Model proof theorem mathd_numbertheory_447 : k in finset.filter (? x, 3|x) (finset.erase (finset.range 50) 0), (k % 10) = 78 := begin refl end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comments</head><p>Because the predicate ? x, 3|x is registered as decidable over N, we can state the problem by using finset.filter, which is computable. Hence, refl is able to close the goal.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An environment for machine learning of higher order logic theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to reason in large theories without imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10501</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dota 2 with large scale deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>D?biak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dennison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hashme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06680</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of monte carlo tree search methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rohlfshagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tavener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in games</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Lean perfectoid spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Buzzard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Commelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Massot</surname></persName>
		</author>
		<ptr target="https://leanprover-community" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Making proofs without modus ponens: An introduction to the combinatorics and complexity of cut elimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Semmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="131" to="159" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Subgoal search for complex reasoning tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Odrzyg??d?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zbysi?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zawalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Olejnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kucinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mi?o?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Avigad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Doorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raumer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>The lean theorem prover (system description</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">International Conference on Automated Deduction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="378" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Training a first-order theorem prover from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aygun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Orseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mourad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03798</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Proof artifact co-training for theorem proving with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Polu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Measuring mathematical problem solving with the math dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gamepad: A learning environment for theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00608</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepmath-deep sequence models for premise selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>E?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2235" to="2243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lehoczky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rusczyk</surname></persName>
		</author>
		<idno>978-0-9773045-6-1</idno>
	</analytic>
	<monogr>
		<title level="j">The Art of Problem Solving</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Art of Problem Solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lehoczky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rusczyk</surname></persName>
		</author>
		<idno>978-0-9773045-8-5</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Modelling highlevel mathematical reasoning in mechanised declarative proofs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09265</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep network guided proof search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaliszyk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06972</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Megill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metamath</surname></persName>
		</author>
		<ptr target="http://us.metamath.org/downloads/metamath.pdf" />
		<title level="m">A Computer Language for Pure Mathematics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generative language modeling for automated theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Polu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03393</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mathematical reasoning via self-supervised skip-tree training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot textto-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
	</analytic>
	<monogr>
		<title level="m">Formal Mathematics Statement Curriculum Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Liquid tensor experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scholze</surname></persName>
		</author>
		<ptr target="https://xenaproject.wordpress.com/2020/12/05/liquid-tensor-experiment/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>B?nz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03685</idno>
		<title level="m">Learning a sat solver from single-bit supervision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01815</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakub?v</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14664</idno>
		<title level="m">First neural conjecturing datasets and experiments</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grandmaster level in starcraft ii using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="issue">7782</biblScope>
			<biblScope unit="page" from="350" to="354" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Premise selection for theorem proving by deep graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2786" to="2796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Int: An inequality benchmark for evaluating generalization in theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02924</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning to prove theorems via interacting with proof assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09381</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Polu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00110</idno>
		<title level="m">Minif2f: a cross-system benchmark for formal olympiad-level mathematics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
