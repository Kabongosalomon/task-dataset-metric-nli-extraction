<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdaFace: Quality Adaptive Margin for Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minchul</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
							<email>jain@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AdaFace: Quality Adaptive Margin for Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognition in low quality face datasets is challenging because facial attributes are obscured and degraded. Advances in margin-based loss functions have resulted in enhanced discriminability of faces in the embedding space. Further, previous studies have studied the effect of adaptive losses to assign more importance to misclassified (hard) examples. In this work, we introduce another aspect of adaptiveness in the loss function, namely the image quality. We argue that the strategy to emphasize misclassified samples should be adjusted according to their image quality. Specifically, the relative importance of easy or hard samples should be based on the sample's image quality. We propose a new loss function that emphasizes samples of different difficulties based on their image quality. Our method achieves this in the form of an adaptive margin function by approximating the image quality with feature norms. Extensive experiments show that our method, AdaFace, improves the face recognition performance over the state-ofthe-art (SoTA) on four datasets (IJB-B, IJB-C, IJB-S and TinyFace). Code and models are released in Supp.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image quality is a combination of attributes that indicates how faithfully an image captures the original scene <ref type="bibr" target="#b30">[32]</ref>. Factors that affect the image quality include brightness, contrast, sharpness, noise, color constancy, resolution, tone reproduction, etc. Face images, the focus of this paper, can be captured under a variety of settings for lighting, pose and facial expression, and sometimes under extreme visual changes such as a subject's age or make-up. These parameter settings make the recognition task difficult for learned face recognition (FR) models. Still, the task is achievable in the sense that humans or models can often recognize faces under these difficult settings <ref type="bibr" target="#b35">[37]</ref>. However, when a face image is of low quality, depending on the degree, the recognition task becomes infeasible. <ref type="figure">Fig. 1</ref> shows examples of both high quality and low quality face images. It is not possible to recognize the subjects in the last column of <ref type="figure">Fig. 1</ref>. <ref type="figure">Figure 1</ref>. Examples of face images with different qualities and recognizabilities. Both high and low quality images contain variations in pose, occlusion and resolution that sometimes make the recognition task difficult, yet achievable. Depending on the degree of degradation, some images may become impossible to recognize. By studying the different impacts these images have in training, this work aims to design a novel loss function that is adaptive to a sample's recognizability, driven by its image quality.</p><p>Low quality images like the bottom row of <ref type="figure">Fig. 1</ref> are increasingly becoming an important part of face recognition datasets because they are encountered in surveillance videos and drone footage. Given that SoTA FR methods <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b5">8,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b18">21]</ref> are able to obtain over 98% verification accuracy in relatively high quality datasets such as LFW or CFP-FP <ref type="bibr" target="#b11">[14,</ref><ref type="bibr" target="#b29">31]</ref>, recent FR challenges have moved to lower quality datasets such as IJB-B, IJB-C and IJB-S <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b39">41]</ref>. Although the challenge is to attain high accuracy on low quality datasets, most popular training datasets still remain comprised of high quality images <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b8">11]</ref>. Since only a small portion of training data is low quality, it is important to properly leverage it during training.</p><p>One problem with low quality face images is that they tend to be unrecognizable. When the image degradation is too large, the relevant identity information vanishes from the image, resulting in unidentifiable images. These unidentifiable images are detrimental to the training procedure since a model will try to exploit other visual characteristics, such as clothing color or image resolution, to lower the training loss. If these images are dominant in the distribution of low quality images, the model is likely to perform poorly on low quality datasets during testing.  <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b37">39]</ref>. (b) Proposed adaptive margin function (AdaFace) that is adjusted based on the image quality indicator. If the image quality is indicated to be low, the loss function emphasizes easy samples (thereby avoiding unidentifiable images). Otherwise, the loss emphasizes hard samples.</p><p>Motivated by the presence of unidentifiable facial images, we would like to design a loss function which assigns different importance to samples of different difficulties according to the image quality. We aim to emphasize hard samples for the high quality images and easy samples for low quality images. Typically, assigning different importance to different difficulties of samples is done by looking at the training progression (curriculum learning) <ref type="bibr" target="#b1">[4,</ref><ref type="bibr" target="#b13">16]</ref>. Yet, we show that the sample importance should be adjusted by looking at both the difficulty and the image quality.</p><p>The reason why importance should be set differently according to the image quality is that naively emphasizing hard samples always puts a strong emphasis on unidentifiable images. This is because one can only make a random guess about unidentifiable images and thus, they are always in the hard sample group. There are challenges in introducing image quality into the objective. This is because image quality is a term that is hard to quantify due to its broad definition and scaling samples based on the difficulty often introduces ad-hoc procedures that are heuristic in nature.</p><p>In this work, we present a loss function to achieve the above goal in a seamless way. We find that 1) feature norm can be a good proxy for the image quality, and 2) various margin functions amount to assigning different importance to different difficulties of samples. These two findings are combined in a unified loss function, AdaFace, that adaptively changes the margin function to assign different importance to different difficulties of samples, based on the image quality (see <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>In summary, the contributions of this paper include:</p><p>? We propose a loss function, AdaFace, that assigns differ- ? We verify the efficacy of the proposed method by extensive evaluations on 9 datasets (LFW, CFP-FP, CPLFW, AgeDB, CALFW, IJB-B, IJB-C, IJB-S and TinyFace) of various qualities. We show that the recognition performance on low quality datasets can be hugely increased while maintaining performance on high quality datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Margin Based Loss Function. The margin based softmax loss function is widely used for training face recognition (FR) models <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b37">39]</ref>. Margin is added to the softmax loss because without the margin, learned features are not sufficiently discriminative. SphereFace <ref type="bibr" target="#b21">[24]</ref>, CosFace <ref type="bibr" target="#b37">[39]</ref> and ArcFace <ref type="bibr" target="#b4">[7]</ref> introduce different forms of margin functions. Specifically, it can be written as,</p><formula xml:id="formula_0">L = ? log exp(f (? yi , m)) exp(f (? yi , m)) + n j? =yi exp(s cos ? j ) ,<label>(1)</label></formula><p>where ? j is the angle between the feature vector and the j th classifier weight vector, y i is the index of the ground truth (GT) label, and m is the margin, which is a scalar hyperparameter. f is a margin function, where</p><formula xml:id="formula_1">f (? j , m) SphereFace = s cos(m? j ) j = y i s cos ? j j ? = y i , (2) f (? j , m) CosFace = s(cos ? j ? m) j = y i s cos ? j j ? = y i ,<label>(3)</label></formula><formula xml:id="formula_2">f (? j , m) ArcFace = s cos(? j + m) j = y i s cos ? j j ? = y i .<label>(4)</label></formula><p>Sometimes, ArcFace is referred to as an angular margin and CosFace is referred to as an additive margin. Here, s is a hyper-parameter for scaling. P2SGrad <ref type="bibr" target="#b44">[46]</ref> notes that m and s are sensitive hyper-parameters and proposes to directly modify the gradient to be free of m and s.</p><p>Our approach aims to model the margin m as a function of the image quality because f (? yi , m) has an impact on which samples contribute more gradient (i.e. learning signal) during training. Adaptive Loss Functions. Many studies have introduced an element of adaptiveness in the training objective for either hard sample mining <ref type="bibr" target="#b19">[22,</ref><ref type="bibr" target="#b38">40]</ref>, scheduling difficulty during training <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b33">35]</ref>, or finding optimal hyperparameters <ref type="bibr" target="#b43">[45]</ref>. For example, CurricularFace <ref type="bibr" target="#b13">[16]</ref> brings the idea of curriculum learning into the loss function. During the initial stages of training, the margin for cos ? j (negative cosine similarity) is set to be small so that easy samples can be learned and in the later stages, the margin is increased so that hard samples are learned. Specifically, it is written as</p><formula xml:id="formula_3">f (?j , m) Curricular = s cos(?j + m) j = yi N (t, cos ?j ) j ? = yi ,<label>(5)</label></formula><p>where N (t, cos ?j ) = cos(?j ) s cos(?y i + m) ? cos ?j cos(?j )(t + cos ?j ) s cos(?y i + m) &lt; cos ?j , <ref type="bibr" target="#b3">(6)</ref> and t is a parameter that increases as the training progresses. Therefore, in CurricularFace, the adaptiveness in the margin is based on the training progression (curriculum).</p><p>On the contrary, we argue that the adaptiveness in the margin should be based on the image quality. We believe that among high quality images, if a sample is hard (with respect to a model), the network should learn to exploit the information in the image, but in low quality images, if a sample is hard, it is more likely to be devoid of proper identity clues and the network should not try hard to fit on it.</p><p>MagFace <ref type="bibr" target="#b25">[27]</ref> explores the idea of applying different margins based on recognizability. It applies large angular margins to high norm features on the premise that high norm features are easily recognizable. Large margin pushes features of high norm closer to class centers. Yet, it fails to emphasize hard training samples, which is important for learning discriminative features. A detailed contrast with MagFace can be found in the supplementary B.1. It is also worth mentioning that DDL <ref type="bibr" target="#b12">[15]</ref> uses the distillation loss to minimize the gap between easy and hard sample features. Face Recognition with Low Quality Images. Recent FR models have achieved high performance on datasets where facial attributes are discernable, e.g., LFW <ref type="bibr" target="#b11">[14]</ref>, CFP-FP <ref type="bibr" target="#b29">[31]</ref>, CPLFW <ref type="bibr" target="#b45">[47]</ref>, AgeDB <ref type="bibr" target="#b27">[29]</ref> and CALFW <ref type="bibr" target="#b46">[48]</ref>. Good performance on these datasets can be achieved when the FR model learns discriminative features invariant to lighting, age or pose variations. However, FR in unconstrained scenarios such as in surveillance or low quality videos <ref type="bibr" target="#b40">[42]</ref> brings more problems to the table. Examples of datasets in this setting are IJB-B <ref type="bibr" target="#b39">[41]</ref>, IJB-C <ref type="bibr" target="#b23">[26]</ref> and IJB-S <ref type="bibr" target="#b14">[17]</ref>, where most of the images are of low quality, and some do not contain sufficient identity information, even for human examiners. The key to good performance involves both 1) learning discriminative features for low quality images and 2) learning to discard images that contain few identity cues. The latter is sometimes referred to as quality aware fusion.</p><p>To perform quality aware fusion, probabilistic approaches have been proposed to predict uncertainty in FR representation <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b31">33]</ref>. It is assumed that the features are distributions where the variance can be used to calculate the certainty in prediction. However, due to the instability in the training objective, probabilistic approaches resort to learning mean and variance separately, which is not simple during training and suboptimal as the variance is optimized with a fixed mean. Our work, however, is a modification to the conventional softmax loss, making the framework easy to use. Further, we use the feature norm as a proxy for the predicted quality during quality aware fusion.</p><p>Synthetic data or data augmentations can be used to mimic low quality data. <ref type="bibr" target="#b32">[34]</ref> adopts 3D face reconstruction <ref type="bibr" target="#b7">[10]</ref> to rotate faces and trains a facial attribute labeler to generate pseudo labels of training data. These auxiliary steps complicate the training procedure and make it hard to generalize to other datasets or domains. Our approach only involves simple crop, blur and photometric augmentations, which are also applicable to other datasets and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>The cross entropy softmax loss of a sample x i can be formulated as follows,</p><formula xml:id="formula_4">L CE (x i ) = ? log exp(W yi z i + b yi ) C j=1 exp(W j z j + b j ) ,<label>(7)</label></formula><p>where z i ? R d is the x i 's feature embedding, and x i belongs to the y i th class. W j refers to the jth column of the last FC layer weight matrix, W ? R d?C , and b j refers to the corresponding bias term. C refers to the number of classes. During test time, for an arbitrary pair of images, x p and x q , the cosine similarity metric, zp?zq ?zp??zq? is used to find the closest matching identities. To make the training objective directly optimize the cosine distance, <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b36">38]</ref> use normalized softmax where the bias term is set to zero and the feature z i is normalized and rescaled with s during training. This modification results in</p><formula xml:id="formula_5">L CE (x i ) = ? log exp(s ? cos ? yi ) C j=1 exp(s cos ? j ) ,<label>(8)</label></formula><p>where ? j corresponds to the angle between z i and W j . Follow-up works <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b37">39]</ref> take this formulation and introduces a margin to reduce the intra-class variations. Generally, it can be written as Eq. 1 where margin functions are defined in Eqs. 2, 3 and 4 correspondingly.  <ref type="figure">Figure 3</ref>. Illustration of different margin functions and their gradient scaling terms on the feature space. B0 and B1 show the decision boundary with and without margin m, respectively. The yellow arrow indicates the shift in the boundary due to margin m. In the arc, a well-classified sample will be close to (in angle) the ground truth class weight vector, Wy i . A misclassified sample will be close to Wj, the negative class weight vector. The color within the arc indicates the magnitude of the gradient scaling term g (Eq. 12). Samples in the dark red region will contribute more to learning. Note that additive margin shifts the boundary toward Wy i , without changing the gradient scaling term. However, positive angular margin not only shifts the boundary, but also makes the gradient scale high near the boundary and low away from the boundary. This behavior de-emphasizes very hard samples, and likewise MagFace has similar behavior. On the other hand, negative angular margin induces an opposite behavior. CurricularFace adapts the boundary based on the training stage. Our work adaptively changes the margin functions based on the norm. With high norm, we emphasize samples away from the boundary and with low norm we emphasize samples near the boundary. Circles and triangles in the arc show example scenarios in the right most plot (AdaFace).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Margin Form and the Gradient</head><p>Previous works on margin based softmax focused on how the margin shifts the decision boundaries and what their geometric interpretations are <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b37">39]</ref>. In this section, we show that during backpropagation, the gradient change due to the margin has the effect of scaling the importance of a sample relative to the others. In other words, angular margin can introduce an additional term in the gradient equation that scales the signal according to the sample's difficulty. To show this, we will look at how the gradient equation changes with the margin function f (? yi , m).</p><p>Let P (i) j be the probability output at class j after softmax operation on an input x i . By deriving the gradient equations for L CE w.r.t. W j and x i , we obtain the following,</p><formula xml:id="formula_6">P (i) j = exp(f (cos ? yi )) exp(f (cos ? yi )) + n j? =yi exp(s cos ? j ) ,<label>(9)</label></formula><formula xml:id="formula_7">?L CE ?W j = P (i) j ? 1(y i = j) ?f (cos ? j ) ? cos ? j ? cos ? j ?W j ,<label>(10)</label></formula><formula xml:id="formula_8">?L CE ?x i = C k=1 P (i) k ?1(y i = k) ?f (cos ? k ) ? cos ? k ? cos ? k ?x i .<label>(11)</label></formula><p>In Eqs. 10 and 11, the first two terms, P</p><formula xml:id="formula_9">(i) j ? 1(y i = j)</formula><p>and ?f (cos ?j )</p><p>? cos ?j are scalars. Also, these two are the only terms affected by parameter m through f (cos ? yi ). As the direction term, ? cos ?j ?Wj is free of m, we can think of the first two scalar terms as a gradient scaling term (GST) and denote,</p><formula xml:id="formula_10">g := P (i) j ? 1(y i = j) ?f (cos ? j ) ? cos ? j .<label>(12)</label></formula><p>For the purpose of the GST analysis, we will consider the class index j = y i , since all negative class indices j ? = y i do not have a margin in Eqs. 2, 3, and 4. The GST for the normalized softmax loss is</p><formula xml:id="formula_11">g softmax = (P (i) yi ? 1)s,<label>(13)</label></formula><p>since f (cos ? yi ) = s ? cos ? yi and ?f (cos ?y i ) ? cos ?y i = s. The GST for the CosFace <ref type="bibr" target="#b37">[39]</ref> is also</p><formula xml:id="formula_12">g CosFace = (P (i) yi ? 1)s,<label>(14)</label></formula><p>as f (cos ? yi ) = s(cos ? yi ? m) and ?f (cos ?y i ) ? cos ?y i = s. Yet, the GST for ArcFace <ref type="bibr" target="#b4">[7]</ref> turns out to be</p><formula xml:id="formula_13">g ArcFace = (P (i) j ? 1)s cos(m)+ cos ? yi sin(m) 1? cos 2 ? yi .<label>(15)</label></formula><p>The derivation can be found in the supplementary. Since the GST is a function of ? yi and m as in Eq. 15, it is possible to use it to control the emphasis on samples based on the difficulty, i.e., ? yi during training.</p><p>To understand the effect of GST, we visualize GST w.r.t. the features. <ref type="figure">Fig. 3</ref> shows the GST as the color in the feature space. Note that for the angular margin, the GST peaks at the decision boundary but slowly decreases as it moves away towards W j and harder samples receive less emphasis. If we change the sign of the angular margin, we see an opposite effect. Note that, in the 6th column, MagFace <ref type="bibr" target="#b25">[27]</ref> is an extension of ArcFace (positive angular margin) with larger margin assigned to high norm feature. Both ArcFace and MagFace fail to put high emphasis on hard samples (green area near W j ). We combine all margin functions (positive and negative angular margins and additive margins) to emphasize hard samples when necessary.</p><p>Note that this adaptiveness is also different from approaches that use the training stage to change the relative importance of different difficulties of samples <ref type="bibr" target="#b13">[16]</ref>. <ref type="figure">Fig. 3</ref> shows CurricularFace where the decision boundary and the GST g change depending on the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Norm and Image quality</head><p>Image quality is a comprehensive term that covers characteristics such as brightness, contrast and sharpness. Image quality assessment (IQA) is widely studied in computer vision <ref type="bibr" target="#b41">[43]</ref>. SER-FIQ <ref type="bibr" target="#b34">[36]</ref> is an unsupervised DL method for face IQA. BRISQUE <ref type="bibr" target="#b26">[28]</ref> is a popular algorithm for blind/no-reference IQA. However, such methods are computationally expensive to use during training. In this work, we refrain from introducing an additional module that calculates the image quality. Instead, we use the feature norm as a proxy for the image quality. We observe that, in models trained with a margin-based softmax loss, the feature norm exhibits a trend that is correlated with the image quality.</p><p>In <ref type="figure" target="#fig_3">Fig. 4</ref> (a) we show a correlation plot between the feature norm and the image quality (IQ) score calculated with (1-BRISQUE) as a green curve. We randomly sampled 1, 534 images from the training dataset (MS1MV2 <ref type="bibr" target="#b4">[7]</ref> with augmentations described in Sec. 4.1) and calculate the feature norm using a pretrained model. At the final epoch, the correlation score between the feature norm and IQ score reaches 0.5235 (out of ?1 and 1). The corresponding scatter plot is shown in <ref type="figure" target="#fig_3">Fig. 4 (b)</ref>. This high correlation between the feature norm and the IQ score supports our use of feature norm as the proxy of image quality.</p><p>In <ref type="figure" target="#fig_3">Fig. 4</ref> (a) we also show a correlation plot between the probability output P yi and the IQ score as an orange curve. Note that the correlation is always higher for the feature norm than for P yi . Furthermore, the correlation between the feature norm and IQ score is visible from an early stage of training. This is a useful property for using the feature norm as the proxy of image quality because we can rely on the proxy from the early stage of training. Also, in <ref type="figure" target="#fig_3">Fig. 4</ref> (c), we show a scatter plot between P yi and IQ score. Notice that there is a non-linear relationship between P yi and the image quality. One way to describe a sample's difficulty is with 1? P yi , and the plot shows that the distribution of the difficulty of samples is different based on image quality. Therefore, it makes sense to consider the image quality when adjusting the sample importance according to the difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">AdaFace: Adaptive Margin based on Norm</head><p>To address the problem caused by the unidentifiable images, we propose to adapt the margin function based on the feature norm. In Sec. 3.1, we have shown that using different margin functions can emphasize different difficulties of samples. Also, in Sec. 3.2, we have observed that the feature norm can be a good way to find low quality images. We  will merge the two findings and propose a new loss for FR. Image Quality Indicator.</p><p>As the feature norm, ?z i ? is a model dependent quantity, we normalize it using batch statistics ? z and ? z . Specifically, we let</p><formula xml:id="formula_14">?z i ? = ?z i ? ? ? z ? z /h 1 ?1 ,<label>(16)</label></formula><p>where ? z and ? z are the mean and standard deviation of all ?z i ? within a batch. And ??? refers to clipping the value between ?1 and 1 and stopping the gradient from flowing.</p><formula xml:id="formula_15">Since ?zi???z ?z/h</formula><p>makes the batch distribution of ?z i ? as approximately unit Gaussian, we clip the value to be within ?1 and 1 for better handling. It is known that approximately 68% of the unit Gaussian distribution falls between ?1 and 1, so we introduce the term h to control the concentration.</p><p>We set h such that most of the values ?zi???z ?z/h fall between ?1 and 1. A good value to achieve this would be h = 0.33. Later in Sec. 4.2, we ablate and validate this claim. We stop the gradient from flowing during backpropagation because we do not want features to be optimized to have low norms.</p><p>If the batch size is small, the batch statistics ? z and ? z can be unstable. Thus we use the exponential moving average (EMA) of ? z and ? z across multiple steps to stabilize the batch statistics. Specifically, let ? (k) and ? (k) be the k-th step batch statistics of ?z i ?. Then</p><formula xml:id="formula_16">? z = ?? (k) z + (1 ? ?)? (k?1) z ,<label>(17)</label></formula><p>and ? is a momentum set to 0.99. The same is true for ? z . Adaptive Margin Function. We design a margin function such that 1) if image quality is high, we emphasize hard samples, and 2) if image quality is low, we de-emphasize hard samples. We achieve this with two adaptive terms g angle and g add , referring to angular and additive margins, respectively. Specifically, we let where g angle and g add are the functions of ?z i ?. We define</p><formula xml:id="formula_17">f (? j , m) AdaFace = s cos(? j +g angle )?g add j = y i s cos ? j j ? = y i ,<label>(18)</label></formula><formula xml:id="formula_18">g angle = ?m ? ?z i ?, g add = m ? ?z i ? + m.<label>(19)</label></formula><p>Note that when ?z i ? = ?1, the proposed function becomes ArcFace. When ?z i ? = 0, it becomes CosFace. When ?z i ? = 1, it becomes a negative angular margin with a shift. <ref type="figure">Fig. 3</ref> shows the effect of the adaptive function on the gradient. The high norm features will receive a higher gradient scale, far away from the decision boundary, whereas the low norm features will receive higher gradient scale near the decision boundary. For low norm features, the harder samples away from the boundary are de-emphasized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Implementation Details</head><p>Datasets. We use MS1MV2 <ref type="bibr" target="#b4">[7]</ref>, MS1MV3 <ref type="bibr" target="#b6">[9]</ref> and Web-Face4M <ref type="bibr" target="#b47">[49]</ref> as our training datasets. Each dataset contains 5.8M, 5.1M and 4.2M facial images, respectively. We test on 9 datasets of varying qualities. Following the protocol of <ref type="bibr" target="#b32">[34]</ref>, we categorize the test datasets into 3 types according to the visual quality (examples shown in <ref type="figure" target="#fig_4">Fig. 5</ref>).</p><p>? High Quality: LFW <ref type="bibr" target="#b11">[14]</ref>, CFP-FP <ref type="bibr" target="#b29">[31]</ref>, CPLFW <ref type="bibr" target="#b45">[47]</ref> AgeDB <ref type="bibr" target="#b27">[29]</ref> and CALFW <ref type="bibr" target="#b46">[48]</ref> are popular benchmarks for FR in the well controlled setting. While the images show variations in lighting, pose, or age, they are of sufficiently good quality for face recognition.</p><p>? Mixed Quality: IJB-B and IJB-C <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b39">41]</ref> are datasets collected for the purpose of introducing low quality images in the validation protocol. They contain both high quality images and low quality videos of celebrities.</p><p>? Low Quality: IJB-S <ref type="bibr" target="#b14">[17]</ref> and TinyFace <ref type="bibr" target="#b3">[6]</ref> are datasets with low quality images and/or videos. IJB-S is a surveillance video dataset, with test protocols such as Surveillance-to-Single, Surveillance-to-Booking and Surveillance-to-Surveillance. The first/second word in the protocol refers to the probe/gallery image source.</p><p>Surveillance refers to the surveillance video, Single refers to a high quality enrollment image and Booking refers to multiple enrollment images taken from different viewpoints. TinyFace consists only of low quality images. Training Settings. We preprocess the dataset by cropping and aligning faces with five landmarks, as in <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b42">44]</ref>, resulting in 112 ? 112 images. For the backbone, we adopt ResNet <ref type="bibr" target="#b9">[12]</ref> as modified in <ref type="bibr" target="#b4">[7]</ref>. We use the same optimizer and a learning rate schedule as in <ref type="bibr" target="#b13">[16]</ref>, and train for 24 epochs. The model is trained with SGD with the initial learning rate of 0.1 and step scheduling at 10, 18 and 22 epochs. If the dataset contains augmentations, we add 2 more epochs for convergence. For the scale parameter s, we set it to 64, following the suggestion of <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b37">39]</ref>.</p><p>Augmentations. Since our proposed method is designed to train better in the presence of unidentifiable images in the training data, we introduce three on-the-fly augmentations that are widely used in image classification tasks <ref type="bibr" target="#b10">[13]</ref>, i.e., cropping, rescaling and photometric jittering. These augmentations will create more data but also introduce more unidentifiable images. It is a trade-off that has to be balanced. In FR, these augmentations are not used because they generally do not bring benefit to the performance (as shown in Sec. 4.2). We show that our loss function is capable of reaping the benefit of augmentations because it can adapt to ignore unidentifiable images.</p><p>Cropping defines a random rectangular area (patch) and makes the region outside the area to be 0. We do not cut and resize the image as the alignment of the face is important. Photometric augmentation randomly scales hue, saturation and brightness. Rescaling involves resizing an image to a smaller scale and back, resulting in blurriness. These operations are applied randomly with a probability of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation and Analysis</head><p>For hyperparameter m and h ablation, we adopt a ResNet18 backbone and use 1/6th of the randomly sampled MS1MV2. We use two performance metrics. For High Quality Datasets (HQ), we use an average of 1:1 verification accuracy in LFW, CFP-FP, CPLFW, AgeDB and CALFW. For Low Quality Datasets (LQ), we use an average of the closed-set rank-1 retrieval and the open-set TPIR@FIPR=1% for all 3 protocols of IJB-S. Unless otherwise stated, we augment the data as described in Sec. 4.1.</p><p>Effect of Image Quality Indicator Concentration h. In Sec. 3.3, we claim that h = 0.33 is a good value. To validate this claim, we show in Tab. 1 the performance when varying h. When h = 0.33, the model performs the best. For h = 0.22 or h = 0.66, the performance is still higher than CurricularFace. As long as h is set such that ?z i ? has some variation, h is not very sensitive. We set h = 0.33.</p><p>Effect of Hyperparameter m. The margin m corresponds to both the maximum range of the angular margin and the magnitude of the additive margin. Tab. 1 shows that the performance is best for HQ datasets when m = 0.4 and for LQ datasets when m = 0.75. Large m results in large angular margin variation based on the image quality, resulting in more adaptivity. In subsequent experiments, we choose m = 0.4 since it achieves good performance for LQ datasets without sacrificing performance on HQ datasets.  Effect of Proxy Choice. In Tab. 1, to show the effectiveness of using the feature norm as a proxy for image quality, we switch the feature norm with other quantities such as (1-BRISQUE) or P yi . The performance using the feature norm is superior to using others. The BRISQUE score is precomputed for the training dataset, so it is not as effective in capturing the image quality when training with augmentation. We include P yi to show that the adaptiveness in feature norm is different from adaptiveness in difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Augmentation.</head><p>We introduce on-the-fly augmentations in our training data. Our proposed loss can effectively handle the unidentifiable images, which are generated occasionally during augmentations. We experiment with a larger model ResNet50 on the full MS1MV2 dataset. Tab. 2 shows that indeed the augmentation brings performance gains for AdaFace. The performance on HQ datasets stays the same, whereas LQ datasets enjoy a significant performance gain. Note that the augmentation hurts the performance of CurricularFace, which is in line with our assumption that augmentation is a tradeoff between a positive effect from getting more data and a negative effect from unidentifiable images. Prior works on margin-based softmax do not include on-the-fly augmentations as the performance could be worse. AdaFace avoids overfitting on unidentifiable images, therefore it can exploit the augmentation better. approximately a middle point of the transition from low to high norm samples. The bottom plot shows that many of the probability trajectories of low norm samples never get high probability till the end. It is in line with our claim that low norm features are more likely to be unidentifiable images. It justifies our motivation to put less emphasis on these cases, although they are "hard" cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with SoTA methods</head><p>To compare with SoTA methods, we evaluate ResNet100 trained with AdaFace loss on 9 datasets as listed in Sec. 4.1. For the high quality datasets, Tab. 3 (a) shows that AdaFace performs on par with competitive methods such as Broad-Face <ref type="bibr" target="#b17">[20]</ref>, SCF-ArcFace <ref type="bibr" target="#b18">[21]</ref> and VPL-ArcFace <ref type="bibr" target="#b5">[8]</ref>. This strong performance in high quality datasets is due to the hard sample emphasis on high quality cases during training. Note that some performances in high quality datasets are saturated, making the gain less pronounced. Thus, choosing one model over the others is somewhat difficult based solely on the numbers. Unlike SCF-ArcFace, our method does not use additional learnable layers, nor requires 2-stage training. It is a revamp of the loss function, which makes it easier to apply our method to new tasks or backbones.</p><p>For mixed quality datasets, Tab. 3 (a) clearly shows the improvement of AdaFace. On IJB-B and IJB-C, AdaFace reduces the errors of the second best relatively by 11% and 9% respectively. This shows the efficacy of using feature norms as an image quality proxy to treat samples differently.</p><p>For low quality datasets, Tab. 3 (b) shows that AdaFace substantially outperforms all baselines. Compared to the second best, our averaged performance gain over 4 Rank-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Venue</head><p>Train Data High Quality Mixed Quality LFW <ref type="bibr" target="#b11">[14]</ref> CFP-FP <ref type="bibr" target="#b29">[31]</ref> CPLFW <ref type="bibr" target="#b45">[47]</ref> AgeDB <ref type="bibr" target="#b27">[29]</ref> CALFW <ref type="bibr" target="#b46">[48]</ref> AVG IJB-B <ref type="bibr">[</ref>  1 metrics is 3.5%, and over 3 TPIR@=FPIR=1% metrics is 2.4%. These results show that AdaFace is effective in learning a good representation for the low quality settings as it prevents the model from fitting on unidentifiable images. We further train on a refined dataset, MS1MV3 <ref type="bibr" target="#b6">[9]</ref> for a fair comparison with a recent work VPL-ArcFace <ref type="bibr" target="#b5">[8]</ref>. The performance using MS1MV3 is higher than MS1MV2 due to less noise in MS1MV3. We also train on newly released WebFace4M <ref type="bibr" target="#b47">[49]</ref> dataset. While one method might shine on one type of data, it is remarkable to see that collectively Adaface achieves SOTA performance on test data with a wide range of image quality, and on various training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we address the problem arising from unidentifiable face images in the training dataset. Data collection processes or data augmentations introduce these images in the training data. Motivated by the difference in recognizability based on image quality, we tackle the problem by 1) using a feature norm as a proxy for the image quality and 2) changing the margin function adaptively based on the feature norm to control the gradient scale assigned to different quality of images. We evaluate the efficacy of the proposed adaptive loss on various qualities of datasets and achieve SoTA for mixed and low quality face datasets.</p><p>Limitations. This work addresses the existence of unidentifiable images in the training data. However, a noisy label is also one of the prominent characteristics of large-scale facial training datasets. Our loss function does not give special treatment to mislabeled samples. Since our adaptive loss assigns large importance to difficult samples of high quality, high quality mislabeled images can be wrongly emphasized. We believe future works may adaptively handle both unidentifiability and label noise at the same time. Potential Societal Impacts. We believe that the Computer Vision community as a whole should strive to minimize the negative societal impact. Our experiments use the training dataset MS1MV*, which is a by-product of MS-Celeb <ref type="bibr" target="#b22">[25]</ref>, a dataset withdrawn by its creator. Our usage of MS1MV* is necessary to compare our result with SoTA methods on a fair basis. However, we believe the community should move to new datasets, so we include results on newly released WebFace4M <ref type="bibr" target="#b47">[49]</ref>, to facilitate future research. In the scientific community, collecting human data requires IRB approval to ensure informed consent. While IRB status is typically not provided by dataset creators, we assume that most FR datasets (with the exceptions of IJB-S) do not have IRB, due to the nature of collection procedures. One direction of the FR community is to collect large datasets with informed consent, fostering R&amp;D without societal concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gradient Scaling Term</head><p>In Sec. 3.1 of the main paper, the gradient scaling term (GST), g is introduced. Specifically, it is derived from the gradient equation for the margin-based softmax loss and defined as</p><formula xml:id="formula_19">g := P (i) j ? 1(y i = j) ?f (cos ? j ) ? cos ? j ,<label>(1)</label></formula><p>where P (i) j = exp(f (cos ? yi )) exp(f (cos ? yi )) + n j? =yi exp(s cos ? j )</p><p>.</p><p>(2)</p><p>This scalar term, g affects the magnitude of the gradient during backpropagation from the margin-based softmax loss. The form of g depends on the form of the margin function f (cos ? j ). In Tab. 1, we summarize the margin function f (cos ? j ) and the corresponding GST when j = y i , the ground truth index.   <ref type="bibr" target="#b25">[27]</ref>. However, unlike other works, MagFace is treating m(?zi?) as a term to optimize (i.e. ?zi? is a function of cos ?j), as oppose to treating it as a constant. In this table, we treat ?zi? as a constant to highlight the effect of the margin. The exact form of g for MagFace will be different. In <ref type="figure">Fig. 3</ref> of the main paper, Adaptive Angular Margin is visualized using the equation from this table.</p><p>Note that P yi is also affected by the choice of the margin function f (cos ? yi ) as in Eqn. 2. So, g is a function of m, except for Softmax, and g is affected by m through f (cos ? yi ) in P yi . For Angular Margin, m appears in the equation for g directly. We derive g for Angular Margin below. The term g for the Adaptive Angular Margin and CurricularFace <ref type="bibr" target="#b13">[16]</ref> can be obtained using the g from the Angular Margin. The GST term for AdaFace can be obtained by using g for the Angular Margin and the Additive Margin, and replacing m with adaptive terms g angle and g add . This is possible because ?z i ? is treated as a constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Derivation of Angular Margin</head><p>We can rewrite f (cos ? yi ) as f (cos ? yi ) = s ? (cos(? yi + m)) = s ? (cos ? yi cos m ? sin ? yi sin m) = s ? cos ? yi cos m ? 1 ? cos 2 ? yi sin m ,</p><p>by the laws of trignometry. Therefore, ?f (cos ? yi ) ? cos ? yi = s cos(m) + cos ? yi sin(m) 1 ? cos 2 ? yi .</p><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Interpretation of g</head><p>For Softmax and Additive Margin, we see that g = (P (i) yi ? 1)s. Since the softmax operation in P</p><formula xml:id="formula_21">(i)</formula><p>yi has a tendency to scale the result to be close to either 0 or 1, the first term in g, (P (i) j ? 1) tends to be close to 1 or 0 far away from the decision boundary. In the equation for P yi , there is also s which is a scaling hyper-parameter, and is often set to s = 64 <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b37">39]</ref>. This high s makes the softmax operation even steeper near the decision boundary. This results in almost equal GST for samples away from the decision boundary, regardless of how far they are from the decision boundary. This is evident in <ref type="figure">Fig. 1</ref>, where the blue curve is flat except near the decision boundary when s is high. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Norm Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Correlation between Norm and BRISQUE during Training</head><p>In the Sec. 3.2 of the main paper, we introduce the idea of using the feature norm as a proxy of the image quality. We observe that in models trained with a margin-based softmax loss, the feature norm exhibits a trend that is correlated with the image quality. Here, we show for ArcFace and AdaFace, both loss functions exhibit this trend, in <ref type="figure">Fig. 3</ref>. Regardless of the form of the margin function, the correlation between the feature norm and the image quality is quite similar (green plot in 1st and 2nd columns). We leverage this behavior to design the proxy for the image quality.  <ref type="figure">Figure 3</ref>. Comparison between ArcFace and AdaFace on the correlation between the feature norm and the image quality. We randomly sampled 1, 534 images from the training dataset (MS1MV2 <ref type="bibr" target="#b4">[7]</ref>) to show this plot.</p><p>We use three concepts (image quality, feature norm and sample difficulty) to describe a sample, as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. We leverage the correlation between the feature norm and the image quality to apply different emphasis to different difficulty of samples. In contrast, MagFace learns a representation that aligns the feature norm with recognizability. The term, image quality in MagFace paper <ref type="bibr" target="#b25">[27]</ref> refers to the face recognizability, which is closer in meaning to the sample difficulty than the term, image quality, we use in our paper. Please refer to the <ref type="figure">Fig. 1 (a)</ref> and the first contribution claim of the MagFace paper <ref type="bibr" target="#b25">[27]</ref>. Also note the difference in gradient flow through the feature norm, ?z i ?. MagFace relies on learning the feature that has ?z i ? aligned with the recognizability of the sample, requiring the gradient to flow through ?z i ? during backpropagation. The loss function has the incentive to reduce the margin by reducing ?z i ?. However, our objective is to adaptively change the loss function, itself, so we treat ?z i ? as a constant. Finally, from Tab. 3 of our main paper, AdaFace substantially outperforms MagFace, e.g. reducing the errors of MagFace on IJB-B and IJB-C relatively by 21% and 23% respectively.</p><p>2. Image Quality (e.g. BRISQUE)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sample Difficulty</head><p>(1 ? !! )  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Training Sample Visualization</head><p>Easy Hard We show some visualization of the actual training images. From the randomly sampled 1, 534 images from the training dataset (MS1MV2 <ref type="bibr" target="#b4">[7]</ref>), we divide the samples into 6 different zones. We plot the samples by cos ? yi (decreasing) as the xaxis and the feature norm ?z i ? as y-axis in <ref type="figure" target="#fig_4">Fig. 5</ref>. We divide the plot into 6 zones and sample a few images from each group. Clearly, there are not many samples in the zones highlighted by the gray area (top right and bottom left). This indicates that the sample difficulty distribution is different for each level of feature norm. Furthermore, the samples in the dark green area are mostly unrecognizable images. AdaFace de-emphasizes these samples. Also, the samples in the bright pink area are more difficult samples than the dark pink area. AdaFace puts more emphasis on the harder samples when the feature norm is high. We would like to reminde the readers thatthis figure may serve as an empirical validation of the two-dimensional face image categorization we made in <ref type="figure">Fig. 1 of the</ref>    In <ref type="figure" target="#fig_5">Fig. 6 (a)</ref>, we plot the actual GST term for AdaFace. We use the same 1, 534 images from the training dataset (MS1MV2 <ref type="bibr" target="#b4">[7]</ref>) as in <ref type="figure" target="#fig_4">Fig. 5</ref>. The color of points indicates the magnitude of the GST term. The purple points on the left side of the scatter plot are samples past the decision boundary. Therefore the magnitude of GST term is low. The effective difference in GST term for samples outside the decision boundary can be seen by the color change from green to yellow. Note that AdaFace de-emphasizes samples of low feature norm and high difficulty. This is shown in the lower right region of the plot. In <ref type="figure" target="#fig_5">Fig. 6 (b)</ref>, we warp the plot into the angular space to make a correspondence with the <ref type="figure">Fig. 3</ref> of the main paper, where we illustrate the GST term for AdaFace. We illustrate how actual training samples are distributed in this angular space. In <ref type="figure" target="#fig_5">Fig. 6 (b)</ref> and (c), we visualize two groups of images where one is from the low feature norm area (triangle) and the other is from the high feature norm area (star). AdaFace exploits images that are hard yet recognizable, as indicated by the yellow star regions, and lowers the learning signal from the unrecognizable images, as indicated by the green triangle regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Train Samples' Gradient Scaling Term Comparison with ArcFace</head><p>In <ref type="figure" target="#fig_13">Fig. 7</ref>, we compare the GST term placed on training samples. We have two groups of images. One group is comprised of unrecognizable images, shown under the red bar. Another group is comprised of hard yet recognizable images, shown under the green bar. Each bar corresponds to one training sample, and the height of the bar indicates the magnitude of the gradient scaling term (GST). For ArcFace shown on the left, the same level of GST is placed on all samples. However, in AdaFace, unrecognizable samples are less emphasized relative to the recognizable samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization of Success and Failed Test Images</head><p>We show samples from IJB-C <ref type="bibr" target="#b23">[26]</ref> dataset to show which samples are correctly classified in AdaFace, compared to ArcFace <ref type="bibr" target="#b4">[7]</ref>. In each pair of probe and gallery images, we write the rank and the similarity score for both ArcFace and AdaFace. Rank= 1 is the correct match and a high similarity score is desired. Note that the majority of the cases where AdaFace successfully matches the hard samples for ArcFace are comprised of low quality samples. This shows that indeed AdaFace works well on low quality images. <ref type="figure">Figure 8</ref>. Examples from IJB-C <ref type="bibr" target="#b23">[26]</ref> dataset, where ArcFace fails to identify the subject whereas AdaFace successfully finds the correct match between the probe and the gallery. On the left is the set of probe images and on the right is the set of gallery images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with General Image-Quality Aware Learning Method</head><p>We compare our method with QualNet <ref type="bibr" target="#b16">[19]</ref> (CVPR21) as a comparison with general image-quality aware learning method. The scope of general image-quality aware learning methods is not limited to face recognition, but the idea is applicable. In Tab. 2, we show the comparison with QualNet with models trained on CASIA-WebFace. AdaFace outperforms QualNet on the TinyFace test set. QualNet aligns the low quality (LQ) image feature distribution to the high quality (HQ) features' distribution via a fixed pretrained decoder. In contrast, AdaFace prevents LQ images from degrading the overall recognition performance by de-emphasizing heavily degraded LQ images. Since LQ facial images can often be devoid of identity, it helps to avoid overfitting on unidentifiable LQ images and learn to exploit the identifiable LQ images. This improves generalization across HQ and LQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training Set Test set Rank1 Rank5</p><p>QualNet <ref type="bibr" target="#b16">[19]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effect of Batch Size</head><p>Our image quality proxy ?z i ? does not depend on the batch size due to the exponential moving average in Eq.17 of the main paper (rewritten below).</p><formula xml:id="formula_22">?z i ? = ?z i ? ? ? z ? z /h 1 ?1 ,<label>(5)</label></formula><formula xml:id="formula_23">? z = ?? (k) z + (1 ? ?)? (k?1) z .<label>(6)</label></formula><p>To empirically show this, we train R50 model on MS1MV2 with the batch size of 128, 256 and 512 and report their performance on IJB-B TAR@FAR=0.01%. As shown in Tab. 3, the difference due to the batch size is minimal.  <ref type="table" target="#tab_5">Table 3</ref>. Performance comparison by varying the batch size. This shows that AdaFace performance not subject to different batch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation Details and Code</head><p>The code is released at https://github.com/mk-minchul/AdaFace. For preprocessing the training data MS1MV2 <ref type="bibr" target="#b4">[7]</ref>, we reference InsightFace [1] and InsightFacePytorch <ref type="bibr">[2]</ref>, for the backbone model definition, TFace <ref type="bibr" target="#b0">[3]</ref> and for evaluation of LFW <ref type="bibr" target="#b11">[14]</ref>, CFP-FP <ref type="bibr" target="#b29">[31]</ref>, CPLFW <ref type="bibr" target="#b45">[47]</ref>, AgeDB <ref type="bibr" target="#b27">[29]</ref>, CALFW <ref type="bibr" target="#b46">[48]</ref>, IJB-B <ref type="bibr" target="#b39">[41]</ref>, and IJB-C <ref type="bibr" target="#b23">[26]</ref>, we use InsightFace [1]. For preprocessing IJB-S <ref type="bibr" target="#b14">[17]</ref> and TinyFace <ref type="bibr" target="#b3">[6]</ref>, we use MTCNN <ref type="bibr" target="#b42">[44]</ref> to align faces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Conventional margin based softmax loss vs our AdaFace. (a) A FR training pipeline with a margin based softmax loss. The loss function takes the margin function to induce smaller intra-class variations. Some examples are SphereFace, CosFace and ArcFace</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a) Correlation for all epochs b) Feature norm vs img. qual. c) Prob. output vs img. qual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(a) A plot of Pearson correlation with image quality score (1-BRISQUE) over training epochs. The green and orange curves correspond to the correlation plot using the feature norm ?zi? and the probability output for the ground truth index Py i , respectively. (b) and (c) Corresponding scatter plots for the last epoch. The blue line on the scatter plot and the corresponding equation shows the least square line fitted to the data points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Examples of three categories of test datasets in our study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Analysis.Figure 6 .</head><label>6</label><figDesc>To show how the feature norm ?z i ? and the difficulty of training samples change during training, we plot the sample trajectory inFig. 6. A total of 1, 536 samples are randomly sampled from the training data. Each column in the heatmap represents a sample, and the x-axis is sorted according to the norm of the last epoch. Sample #600 is A plot of training samples' trajectories of feature norm ?zi? and the probability output for the ground truth index Py i . We randomly select 1, 536 samples from the training data with augmentations, and show 8 images evenly sampled from them. The features with low norm have a different probability trajectory than others and the corresponding images are hard to identify.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Methodsf (cos ?j), j ? = yi f (cos ?j), j = yi g when j = yi Softmax s ? cos ?y i s ? cos ?y i P (i)y i ? 1 s Additive Margin (CosFace [39]) s ? cos ?y i s(cos ?y i ? m) P (i) y i ? 1 s Angular Margin (ArcFace [7]) s ? cos ?y i s ? cos(?y i + m) P (i) y i ? 1 s cos(m) + cos ?y i sin(m) ? 1?cos 2 ?y i Adaptive Angular Margin s ? cos ?y i s ? cos(?y i + m(?zi?)) P (i) y i ? 1 s cos(m(?zi?)) +cos ?y i sin(m(?z i ?)) ? 1?cos 2 ?y i m(?zi?) = a monotonically inc. function of ?zi?. In this table, g is derived with ?zi? as a constant. CurricularFace [16] N (t, cos ?j) s ? cos(?y i + m) P (i) y i ? 1 s cos(m) + cos ?y i sin(m) ? 1?cos 2 ?y i N (t, cos ?j) = cos(?j)(t + cos ?j) if s cos(?y i + m) &lt; cos ?j else cos(?j) AdaFace (ours) s ? cos ?y i s ? cos(?y i + gangle) ? gadd P (i) y i ? 1 s cos(gangle) + cos ?y i sin(g angle ) ? 1?cos 2 ?y i gangle = ?m ? ?zi?, gadd = m ? ?zi? + m ?zi? = ?z i ???z ?z /h 1 ?1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Plot of Py i for different values of s. In this figure, Py i is calculated with f (cos ?j) from Softamx (i.e. m = 0). For Softmax and Additive Margin, ?f (cos ?y i ) ? cos ?y i = s. This term is different for Angular Margin due to ?f (cos ?y i ) ? cos ?y i being a function of cos ? yi . The exact form of ?f (cos ?y i ) ? cos ?y ifor Angular Margin is found in Eqn. 4. As shown inFig. 2, Eqn. 4 is monotonically increasing with respect to cos ? yi when m &gt; 0 and vice versa. Note that cos ? yi is how close the sample is to the ground truth weight vector, and it is closely related to the difficulty of the sample during training. Therefore, this partial derivative term from the angular margin,?f (cos ?y i ) ? cos ?y i , can be viewed as scaling the importance of sample based on the difficulty. Plot of ?f (cos ?y i ) ? cos ?y i for different value of m when the margin function is Angluar Margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>An illustration of different components to describe a sample and their usage in previous works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 .</head><label>5</label><figDesc>Actual training data examples corresponding to 6 zones. A pretrained AdaFace model is used as a feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Scatter Plot between cos ! ! and " (b) Scatter Plot in Angular Space (c) Selected Samples Visualization form the scatter plot</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 .</head><label>6</label><figDesc>(a) Scatter plot of samples from Fig. 5 with the color as the GST term. (b): Scatter plot of the same 1, 534 points in angular space. For each feature, the angle from Wy i is calculated from cos ?y i and the distance from the origin is calculated from ?zi?. Both terms are normalized for visualization. (c): Sample image visualization from the low norm and high norm regions of similar cos ?y i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 .</head><label>7</label><figDesc>Comparison of the magnitude of GST term between ArcFace and AdaFace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation of our margin function parameters h and m, and the image quality proxy choice on the ResNet18 backbone. The performance metrics are as described in Sec. 4.2.</figDesc><table><row><cell>Method</cell><cell>h</cell><cell>m</cell><cell></cell><cell>Proxy</cell><cell cols="2">HQ Datasets LQ Datasets</cell></row><row><cell>CurricularFace [16]</cell><cell>-</cell><cell>0.50</cell><cell></cell><cell></cell><cell>93.43</cell><cell>32.92</cell></row><row><cell>aaaAdaFaceaaa</cell><cell>0.22</cell><cell></cell><cell></cell><cell></cell><cell>93.67</cell><cell>34.92</cell></row><row><cell>AdaFace</cell><cell>0.33</cell><cell>0.40</cell><cell></cell><cell>Norm</cell><cell>93.74</cell><cell>35.40</cell></row><row><cell>AdaFace</cell><cell>0.66</cell><cell></cell><cell></cell><cell></cell><cell>93.70</cell><cell>35.29</cell></row><row><cell>aaaAdaFaceaaa</cell><cell></cell><cell>0.40</cell><cell></cell><cell></cell><cell>93.74</cell><cell>35.40</cell></row><row><cell>AdaFace</cell><cell>0.33</cell><cell>0.50</cell><cell></cell><cell>Norm</cell><cell>93.56</cell><cell>35.23</cell></row><row><cell>AdaFace</cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell>93.37</cell><cell>35.69</cell></row><row><cell>aaaAdaFaceaaa</cell><cell></cell><cell></cell><cell></cell><cell>Norm</cell><cell>93.74</cell><cell>35.40</cell></row><row><cell>-</cell><cell cols="2">0.33 0.40</cell><cell cols="2">1?BRISQUE</cell><cell>93.43</cell><cell>34.55</cell></row><row><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>Py i</cell><cell>93.46</cell><cell>35.17</cell></row><row><cell cols="2">Method</cell><cell>p</cell><cell></cell><cell cols="2">HQ Datasets LQ Datasets</cell></row><row><cell cols="4">CurricularFace [16] 0.0</cell><cell>96.85</cell><cell>41.00</cell></row><row><cell cols="4">CurricularFace [16] 0.2</cell><cell>96.75</cell><cell>40.84</cell></row><row><cell cols="4">CurricularFace [16] 0.3</cell><cell>96.59</cell><cell>40.58</cell></row><row><cell cols="2">AdaFace</cell><cell cols="2">0.0</cell><cell>96.72</cell><cell>40.95</cell></row><row><cell cols="2">AdaFace</cell><cell cols="2">0.2</cell><cell>96.88</cell><cell>41.82</cell></row><row><cell cols="2">AdaFace</cell><cell cols="2">0.3</cell><cell>96.78</cell><cell>41.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Ablation of augmentation probability p, on the ResNet50 backbone. The metrics are the same as Tab. 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The percentage of samples with augmentations is higher for the low norm features than for the high norm features. For samples number #0 to #600, about 62.0% are with at least one type of augmentation. For the samples #600 or higher, the percentage is about 38.5%.</figDesc><table /><note>Time Complexity. Compared to classic margin-based loss functions, our method adds a negligible amount of compu- tation in training. With the same setting, ArcFace [7] takes 0.3193s per iteration while AdaFace takes 0.3229s (+1%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Comparison on benchmark datasets, with the ResNet100 backbone. For high quality and mixed quality datasets, 1:1 verification accuracy and TAR@FAR=0.01% are reported respectively. For IJB-S, open-set TPIR@FPIR=1% and closed-set rank retrieval (Rank-1 and Rank-5) are reported. Rank retrieval is also used for TinyFace. [KEYS: Best, Second best, *=our evaluation of the released model]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>Table of margin functions and their gradient scale terms. The concept of Adaptive Angular Margin is explored in MagFace</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>main paper.B.3. Training Samples' Gradient Scaling Term for AdaFace</figDesc><table><row><cell>Hard yet Recognizable Image</cell></row><row><cell>Unrecognizable Images</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>CASIA-Webface TinyFace 35.54 44.45 AdaFace 44.39 47.23 Table 2. Closed set identification performance (ranked match rate) on TinyFace. For a fair comparison, we adopt the train/test setting of QualNet. QualNet results are directly taken from the CVPR21 paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Method Batch size 128 Batch size 256 Batch size 512</figDesc><table><row><cell>AdaFace</cell><cell>94.32</cell><cell>94.42</cell><cell>94.35</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tface</surname></persName>
		</author>
		<ptr target="https://github.com/Tencent/TFace.git" />
		<imprint>
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data uncertainty learning in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5710" to="5719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lowresolution face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ArcFace: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alexandros Lattas, and Stefanos Zafeiriou. Variational prototype learning for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11906" to="11915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lightweight face recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangju</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="534" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MS-Celeb-1M: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Labeled Faces in the Wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Faces in&apos;Real-Life&apos;Images: Detection, Alignment, and Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving face recognition from hard samples via distribution distillation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="138" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CurricularFace: adaptive curriculum learning loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5901" to="5910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">IJB-S: IARPA Janus Surveillance Video Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Nathan D Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">O</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleb</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attentional feature-pair relation networks for accurate face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bong-Nam</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongjin</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daijin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5472" to="5481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quality-agnostic image recognition via invertible decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Insoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Won</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Jin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Joon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12257" to="12266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Broad-Face: Looking at tens of thousands of people at once for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongju</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="536" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spherical confidence learning for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="15629" to="15637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DAM: Discrepancy alignment metric for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3814" to="3823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SphereFace: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grother</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">IARPA Janus Benchmark-C: Face dataset and protocol</title>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MagFace: A universal representation for face recognition and quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhida</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AGEDB: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving face recognition with a qualitybased probabilistic framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Necmiye</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><forename type="middle">W</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>eeding of IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic face embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="6902" to="6911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards universal representation learning for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="6817" to="6826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ser-fiq: unsupervised estimation of face image quality based on stochastic embedding robustness. in 2020 ieee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Terh?rst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Niklas</forename><surname>Kolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVF Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Disentangled representation learning GAN for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>eeding of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">NormFace: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Multimedia</title>
		<meeting>the 25th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CosFace: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mis-classified vector guided softmax loss for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12241" to="12248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">IARPA Janus Benchmark-B face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FAN: Feature adaptation network for surveillance face recognition and normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="301" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Perceptual image quality assessment: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongkuo</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">211301</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adacos: Adaptively scaling cosine logits for effectively learning deep face representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10823" to="10832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">P2sGrad: Refined gradients for optimizing deep face models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengya</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9906" to="9914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cross-Pose LFW: A database for studying cross-pose face recognition in unconstrained environments. Beijing University of Posts and Telecommunications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Cross-Age LFW: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<idno>abs/1708.08197</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">WebFace260M: A benchmark unveiling the power of million-scale deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="10492" to="10502" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
