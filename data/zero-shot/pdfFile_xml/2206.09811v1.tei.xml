<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
							<email>h-xiao20@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
							<email>zhengzhu@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<email>lujiwen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a Shapley value based method to evaluate operation contribution (Shapley-NAS) for neural architecture search. Differentiable architecture search (DARTS) acquires the optimal architectures by optimizing the architecture parameters with gradient descent, which significantly reduces the search cost. However, the magnitude of architecture parameters updated by gradient descent fails to reveal the actual operation importance to the task performance and therefore harms the effectiveness of obtained architectures. By contrast, we propose to evaluate the direct influence of operations on validation accuracy. To deal with the complex relationships between supernet components, we leverage Shapley value to quantify their marginal contributions by considering all possible combinations. Specifically, we iteratively optimize the supernet weights and update the architecture parameters by evaluating operation contributions via Shapley value, so that the optimal architectures are derived by selecting the operations that contribute significantly to the tasks. Since the exact computation of Shapley value is NP-hard, the Monte-Carlo sampling based algorithm with early truncation is employed for efficient approximation, and the momentum update mechanism is adopted to alleviate fluctuation of the sampling process. Extensive experiments on various datasets and various search spaces show that our Shapley-NAS outperforms the state-of-the-art methods by a considerable margin with light search cost. The code is available at https://github.com/Euphoria16/Shapley-NAS.git. Sep conv 3x3 Dilated 5x5 Max Pooling Identity Input Output Forward Update Backward (a) Differentiable NAS S D I M D M Val S D M Val S</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural architecture search (NAS) has attracted great interest in deep learning since it discovers the optimal architecture from a large search space of network components according to task performance and hardware config-* Corresponding author urations. Pioneering works applied reinforcement learning <ref type="bibr" target="#b47">[48]</ref>, evolutionary algorithms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>, and Bayesian optimization <ref type="bibr" target="#b21">[22]</ref> for the architecture search, but the large computational overhead prohibits practical deployment of NAS algorithms. Therefore, it is desirable to design highly efficient search strategies without performance degradation.</p><p>To reduce the search cost of architecture search, several efficient search strategies have been presented including one-shot NAS <ref type="bibr" target="#b28">[29]</ref>, network transformation <ref type="bibr" target="#b2">[3]</ref>, and architecture optimization <ref type="bibr" target="#b25">[26]</ref>. Among these approaches, one-shot NAS preserves the optimal sub-networks from the over-parameterized supernet with weight sharing, which prevents the time-consuming exhaustive training for model evaluation. In particular, DARTS <ref type="bibr" target="#b22">[23]</ref> converted the discrete operation selection into continuous mixing weights learning and iteratively optimized the architecture parameters and supernet weights by gradient descent with significantly reduced search cost. However, the magnitude of architecture parameters in DARTS cannot reflect the actual operation importance in general <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>. That is, the operation with the largest parameter magnitude does not necessarily result in the highest validation accuracy, which degrades the performance of derived architectures.</p><p>In this paper, we present a Shapley-NAS method to evaluate the operation contribution via the Shapley value of supernet components for neural architecture search. Instead of relying on the magnitude of architecture parameters updated by gradient descent, we consider their practical influences on task performance and propose to directly evaluate their contributions to the validation accuracy. Moreover, we observe that the operations in the supernet are related to each other: combinations of operations might have different joint influences on performance compared with their separate ones. In order to deal with such complex relationships, we leverage Shapley value <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, an important solution to attribute contributions to players in cooperative game theory. <ref type="figure">Fig. 1</ref> shows the differences between our Shapley-NAS and existing DARTS methods. Shapley value directly measures the contributions of operations according to the vali- <ref type="figure">Figure 1</ref>. The comparison between DARTS and our Shapley-NAS. (a) DARTS constructs a weight-sharing supernet that consists of all candidate operations. The architecture parameters are optimized by gradient descent, which fails to reflect the importance of operations <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>. (b) The proposed Shapley-NAS method directly evaluates the marginal contribution of operations to the task performance, according to the validation accuracy difference of all possible operation subsets and their counterparts without the given operation. dation accuracy difference. Meanwhile, it considers all possible combinations and quantifies the average marginal contribution to handle complex relationships between individual elements. Benefiting from these, Shapley value is effective for obtaining operation importance that is highly correlated with task performance. Since computing the exact Shapley value is NP-hard, we employ the Monte-Carlo sampling with early truncation for operation permutation set sampling to approximate it efficiently. Finally, we optimize the supernet weights and update the architecture parameters iteratively, where the momentum update mechanism is adopted to alleviate the fluctuation caused by the sampling process. We empirically demonstrate that the obtained Shapley value has a higher correlation with task performance compared with DARTS. We conducted extensive experiments on different datasets across various search spaces, where our Shapley-NAS outperforms the state-of-the-art architecture search methods. We achieve an error rate of 2.43% on CIFAR-10 <ref type="bibr" target="#b18">[19]</ref> on the search space of DARTS and obtain the top-1 accuracy of 23.9% on ImageNet <ref type="bibr" target="#b8">[9]</ref> under the mobile setting. Furthermore, our Shapley-NAS acquires the optimal architectures on CIFAR-10 and CIFAR-100 and near-optimal solutions on ImageNet-16-120 of the NAS-Bench-201 benchmark <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Differentiable NAS: Differentiable architecture search (DARTS) was first proposed by Liu et al. <ref type="bibr" target="#b22">[23]</ref> with the goal of reducing the heavy search cost in NAS. They apply a continuous relaxation to the graphical architecture representation, thus enabling efficient gradient descent to solve the bi-level optimization objective for architecture search. PC-DARTS <ref type="bibr" target="#b39">[40]</ref> further proposed to only search the partiallyconnected operations by leveraging the redundancy in net-work space to further reduce the memory overhead. Despite the computation efficiency of DARTS, several works have challenged its generalizability <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> and stability <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. In order to reduce the bias of DARTS for operation selection, SNAS <ref type="bibr" target="#b38">[39]</ref> and GDAS <ref type="bibr" target="#b9">[10]</ref> introduced stochasticity into the supernet training and adopted the differentiable Gumbel-Softmax trick <ref type="bibr" target="#b16">[17]</ref> for gradient estimation. SGAS <ref type="bibr" target="#b19">[20]</ref> greedily chose and pruned the candidate operations based on edge importance, selection certainty, and selection stability to alleviate the performance gap between the search and evaluation phase. RobustDARTS <ref type="bibr" target="#b42">[43]</ref> found that the stability of DARTS is highly correlated with dominant eigenvalue of the Hessian of validation loss with respect to the architecture parameters. Therefore, they performed early stop regularization according to the largest eigenvalue to avoid poor generation. SmoothDARTS <ref type="bibr" target="#b5">[6]</ref> further smoothed the loss landscape via perturbation-based regularization. However, recent studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref> have demonstrated the magnitude of architecture parameters in the DARTS framework fails to reveal the actual operation importance, which greatly degrades the performance of architectures derived from the search phase.</p><p>Shapley value: Shapley value has been well studied in the cooperative game theory as a fair contribution distribution method <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Recently, Shapley value has been adopted in explainable machine learning to discover the importance of different elements, which can be divided into three groups: explaining feature importance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>, model component importance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref>, and data importance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41]</ref>. For the first regard, Ancona et al. <ref type="bibr" target="#b0">[1]</ref> conducted an axiomatic comparison to show the advantage of the Shapley value over the attribution methods for feature map explanation in deep networks. SHAP <ref type="bibr" target="#b24">[25]</ref> presented the additive feature attribution based on the Shapley value of features to acquire higher consistency with hu-man intuition. For model component importance explanation, ShapNets <ref type="bibr" target="#b36">[37]</ref> leveraged the Shapley transform that transforms the input into Shapley representations so that the network prediction can be explained during the forward pass. Neuron Shapley <ref type="bibr" target="#b12">[13]</ref> identified the most important filters in neural networks and demonstrated potential applications to improve the accuracy, fairness, and robustness of the model prediction. Moreover, Ghorbani et al. <ref type="bibr" target="#b11">[12]</ref> quantified the contribution of individual data points which effectively identified the outliers and corrupted data. Since computing the exact Shapley value is NP-hard, Monte-Carlo sampling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, perturbation-based approximation <ref type="bibr" target="#b0">[1]</ref>, influence function, and many others were presented for efficient estimation of Shapley value. In this paper, we extend the Shapley value to operation importance evaluation in the DARTS framework, so that the optimal architectures are derived by selecting the operations that contribute significantly to the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we first briefly introduce differentiable architecture search (DARTS), which suffers from degenerate architectures due to the mismatch between the architecture parameters and operation importance. Then we propose to directly evaluate the influence of operations on the task performance and introduce Shapley value to quantify their relative contributions at the presence of complex relationships between different operations. We also present the Monte-Carlo sampling algorithm with early truncation for efficient approximation of Shapley value. Finally, we propose Shapley-based architecture search (Shapley-NAS) which can effectively identify the optimal architectures with the most important operations from the large search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>The differentiable architecture search (DARTS) is one of the most popular solutions to identify effective architectures, as it largely reduces the search cost by relaxing the architecture search to continuous mixture weights learning. Following prior works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref>, DARTS searches for the best cell structure and constructs the supernet by repetitions of normal and reduction cells. Each cell is represented by a directed acyclic graph (DAG) with N nodes and E edges, where each node x (i) defines a latent representation and each edge (i, j) is associated with an operation o (i,j) . The core idea of DARTS is to apply continuous relaxation to the search space to perform the gradient-based search. Concretely, the intermediate node is computed as a softmax mixture of candidate operations:</p><formula xml:id="formula_0">o (i,j) (x (i) ) = o?O exp(? (i,j) o ) o ? ?O exp(? (i,j) o ? ) o(x (i) ),<label>(1)</label></formula><p>where O is the set of all candidate operations and ? (i,j) o denotes the mixing weight of operation o (i,j) to construct the supernet. With such relaxation, the architecture search can be performed by jointly optimizing the network weight w and architecture parameters ? in a differentiable manner with the following bi-level objective:</p><formula xml:id="formula_1">min ? L val (w * , ?) s.t. w * = arg min w L train (w, ?).</formula><p>(2) During the search stage, the weight-sharing supernet containing all these candidate operations is optimized by gradient descent. At the end of the search stage, the final architecture is derived by selecting the operation with the largest architecture parameter ? on every edge across all operation choices, o (i,j) = arg max o?O ?</p><formula xml:id="formula_2">(i,j) o .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Operation Importance Evaluation</head><p>The magnitude-based architecture selection process in DARTS relies on an important assumption that the magnitude of architecture parameters represents the operation importance. In other words, it supposes that operations with low magnitude of ? result in weak feature representations and thus have little contribution to the network performance. However, recent studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref> have shown that the value of architecture parameters does not necessarily reflect the actual operation contribution. In many cases, the operation with the largest ? does not result in the highest validation accuracy. Therefore, selecting the best operations based on values of ? may lead to significant performance degradation at the evaluation phase.</p><p>To solve this problem, we propose to perform the architecture search by identifying operations that contribute the most to the validation accuracy. <ref type="bibr" target="#b35">[36]</ref> performs a similar evaluation of operation contribution by removing the target operation from the supernet while keeping all other operations to obtain the performance drop. However, we observe that the operations in the supernet are not independent of each other. To demonstrate the underlying relationships between operations on different edges, we remove only one operation separately on the 4th edge and the 5th edge from the supernet pretrained on the NAS-Bench-201 space, and re-evaluate the supernet accuracy. As shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>, removing the skip connect operation on the 4th edge and the conv 3x3 on the 5th edge leads to the most dramatic performance drop. However, we find the impacts of combinations of the two edges differ from the simple accumulation of their separate influence. We remove one operation for both edges simultaneously and enumerate all candidate operation combinations to show the results. As <ref type="figure" target="#fig_1">Fig. 2b</ref> illustrates, removing conv 3x3 on the 4th edge and conv 1x1 on the 5th edge results in the most significant degradation, while removing the combination of skip connect and conv 3x3 only lead to 3.88% performance drop.   This observation reveals the complex relationships between different operations on different edges: some operations can collaborate with each other and thus have a significant joint contribution to the supernet performance.</p><p>To deal with such relationships, we leverage Shapley value <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, an important solution from cooperative game theory, to evaluate the individual contribution. Specifically, the differentiable architecture search process can be uniquely mapped into a cooperative game, providing a practical scheme to quantify the operation contribution based on Shapley value. In a cooperative game, N players associate with each other, and a value function V maps each subset of players S ? N to a real value V (S), which represents the expected payoff the players can obtain by cooperation. In differentiable NAS, the supernet is composed of several layers with identical cell structures, and each cell has |E| edges each with |O| operations. Therefore, a set of individual operations, N = O ?E = {o (i,j) } o?O,(i,j)?E , can be modeled as players in the cooperative game, where all players work together towards the supernet's performance V (N ). Shapley value is utilized to distribute the total performance gains V (N ) to each player in N . In our problem, for operation</p><formula xml:id="formula_3">o (i,j) , its Shapley value ? (i,j) o</formula><p>can be computed as:</p><formula xml:id="formula_4">? (i,j) o (V ) = 1 |N | S?N \{o (i,j) } V (S ? {o (i,j) }) ? V (S) |N |?1 |S|</formula><p>(3) The Shapley value represents the average marginal contribution of the operation to the network performance, which is obtained by evaluating the performance difference between all operation subsets and their counterparts without the given operation. Here we use the validation accuracy as the value function V to measure the network performance. It has been proved that the formulation of Shapley value in Eq. (3) makes it the only method to quantify individual contributions that uniquely satisfies the following properties <ref type="bibr" target="#b31">[32]</ref>, which we interpret according to our problem:</p><p>Efficiency The performance of the entire supernet is the sum of contributions of individual operations, i.e.</p><formula xml:id="formula_5">o (i,j) ?N ? (i,j) o = V (N ).</formula><p>Null Player If the operation has no impact on the performance when added to or removed from any subsets of the supernet, then its contribution is zero.</p><formula xml:id="formula_6">That is, if V (S) = V (S ? {o (i,j) }) for any operation subset S ? N \ {o (i,j) }, we can derive ? (i,j) o = 0.</formula><p>For example, the zero operation in the search space of DARTS has no impact on the final performance and thus has zero contribution.</p><p>Symmetry If two different operations could be exchanged without affecting the performance, they should be assigned with equal contributions. For any operation subset</p><formula xml:id="formula_7">S ? N \{o (i,j) , o ?(k,l) }, V (S?{o (i,j) }) = V (S?{o ?(k,l) }), then we have ? (i,j) o = ? (k,l) o ? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Shapley Value Approximation</head><p>Although Shapley value can be considered as a desirable attribution metric for quantifying the contribution of operations, directly computing Shapley value from Eq. (3) requires 2 |O|?|E| network evaluations caused by enumerating all possible subsets. Therefore, the exact computation of Shapley value becomes expensive since |O| ? |E| in the common search space is usually large. To efficiently estimate the Shapley value, we present an approximate method based on Monte-Carlo sampling <ref type="bibr" target="#b4">[5]</ref>. Specifically, the Shapley value of operation o (i,j) (denoted as o for simplicity) is equivalent to estimating the mean of a random variable, which can be written as:</p><formula xml:id="formula_8">? o (V ) = 1 N ! R??(N ) [V (P re o (R) ? {o}) ? V (P re o (R))]</formula><p>(4) where ?(N ) denotes the set of permutations of all elements in N , and P re o (R) is the set of predecessors of o in a given permutation R ? ?(N ). Based on Eq. (4), we can get an unbiased approximation of every operation's Shapley value by sampling permutations of operation set N . Notably, the Monte-Carlo estimation reduces the exponential calculation complexity to polynomial-time M ? (|O| ? |E|), where M is the number of samples. Although this sampling-based estimation requires repetitions of accuracy evaluation on the validation set, it only includes the forward process through the supernet with no need for back-propagation, thus enabling efficient approximation of Shapley value.</p><p>Moreover, we find when the number of operations in P re o (R) becomes too small, the task performance degrades dramatically and yields unstable sampling results. Therefore, to reduce the fluctuation of Shapley value estimation, we utilize the early truncation technique during the Monte-Carlo sampling procedure. Specifically, when the masked out operations lead to an extreme performance drop exceeding a pre-defined threshold ?, we break off the current sam-pling. This early truncation technique also reduces nearly half of computation cost, which makes the overall computational overheads comparable with gradient-based architecture parameter optimization in DARTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Shapley-based Architecture Search</head><p>We leverage the Shapley value of operations to guide the architecture search to find the best solutions as it reveals the actual operation contribution to performance. <ref type="figure">Fig. 1</ref> shows the difference between our Shapley-NAS and conventional differential NAS. Instead of updating the architecture parameters by gradient descent, we utilize the Shapley value to represent the relative strength of operations. Specifically, the search objective should be modified as follows:</p><formula xml:id="formula_9">? ? ?(L val (w * , ?))) s.t. w * = arg min w L train (w, ?).</formula><p>(5) Since solving the above problem exactly is impractical, we optimize this objective via an approximate way. We update ? according to the Shapley value estimated by the algorithm presented in Sec. 3.3:</p><formula xml:id="formula_10">? t = ? t?1 + ? ? s t ||s t || 2<label>(6)</label></formula><p>where ? t means the architecture parameter at the t-th step during the optimization, s t represents the accumulated Shapley value in the t th step, || ? || 2 is the L 2 norm and ? is the step size. We iteratively optimize w t by descending ?L train (w t?1 , ? t?1 ) and update architecture parameters ? until convergence. To reduce undesired fluctuation in updating caused by random sampling, we introduce the momentum into the iteration to stabilize the optimization:</p><formula xml:id="formula_11">s t = ? ? s t?1 + (1 ? ?) ? ?(Acc val (w t?1 , ? t?1 )) ||?(Acc val (w t?1 , ? t?1 ))|| 2<label>(7)</label></formula><p>where ? is the momentum coefficient that balances the accumulated Shapley value and the current sampling result, Acc val is the validation accuracy used as value function, and w t?1 is the supernet weights at (t ? 1) step. After the search stage, we derive the final architecture by selecting the operation with the largest contribution on each edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this part, we conducted extensive experiments to evaluate our method on the DARTS search space with CIFAR-10 <ref type="bibr" target="#b18">[19]</ref> and ImageNet <ref type="bibr" target="#b8">[9]</ref> for image classification, as well as on a widely used NAS benchmark dataset, NAS-Bench-201 <ref type="bibr" target="#b10">[11]</ref>. We first introduce the datasets and implementation details of our Shapley-NAS. In the following ablation study, we analyzed the effectiveness of the proposed Shapley value evaluation, as well as the influence of hyperparameters on task performance and search cost. We compare our Shapley-NAS with the state-of-the-art methods with respect to the accuracy, model complexity, and search cost. Finally, we empirically demonstrated the effectiveness of Monte-Carlo sampling estimation, as well as the high correlation between obtained Shapley value and task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Implementation Details</head><p>CIFAR-10: For the CNN search space on CIFAR-10, we employed the same operation space O as DARTS and set the initial channel number as 16. We utilized the partial connection strategy in PC-DARTS <ref type="bibr" target="#b39">[40]</ref> to reduce memory overhead and increase batch size. We trained the supernet for 50 epochs (the first 15 epochs for warm-up) with a batch size of 256. The training set of CIFAR-10 was divided into two parts with equal size, one for optimizing network weights and the other for evaluating Shapley value. We set the number of samples M to be 10 in the Monte-Carlo sampling and the early truncation threshold ? to be 0.5. The momentum coefficient ? and step size ? were assigned to 0.8 and 0.1 respectively. At the evaluation phase, We simply followed the DARTS experimental settings for fair comparison and retrained the network from scratch for 600 epochs.</p><p>ImageNet: ImageNet contains about 1.2 million training and 50K validation images from 1000 categories, which is much more challenging than CIFAR-10. We randomly sampled 10% and 2.5% images from the entire 1.3M training set of ImageNet for training network weights and estimating Shapley value respectively. The supernet was trained for 50 epochs with batch size 1024 and the architecture parameters remained frozen in the first 25 epochs. The other hyper-parameters were the same with CIFAR-10. At the evaluation stage, we trained the network from scratch for 250 epochs by an SGD optimizer with a linearly decayed learning rate initialized as 0.5, a momentum of 0.9, and a weight decay of 3 ? 10 ?5 .</p><p>NAS-Bench-201: NAS-Bench-201 is a popular benchmark to analyze NAS algorithms, as it provides performance of all candidate architectures which can be directly obtained by querying. In the search space of NAS-Bench-201, the operation set O has 5 elements and each cell contains 4 nodes, leading to a total search space of 15,625 architectures. NAS-Bench-201 supports three datasets, CIFAR-10, CIFAR-100, and ImageNet-16-120, and more details about the datasets can be found in their paper <ref type="bibr" target="#b10">[11]</ref>. Specifically, we acquired the task-specific performance by directly searching on the evaluation dataset, and obtained the mean and standard deviation for the best architecture from 4 independent runs with different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Effectiveness of Shapley value evaluation: To verify the effectiveness of Shapley-NAS, we conducted experiments on 4 simplified search spaces S1-S4 proposed by Influence of samples times M and early truncation threshold ?: We also explored the influence of sampling times M and early truncation threshold ? in the Monte-Carlo sampling algorithm. The values of sampling times M and early truncation threshold ? are significant for accurate Shapley value estimation, which also affect the overall search cost. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the test error (%) and search cost (GPU days) on CIFAR-10 with various M and ?. Reducing the number of samples results in lower search cost while degrading the performance since the sampling is not enough to make an accurate estimation. However, the estimation accuracy with samples larger than 10 is not sensitive to the number of samples, and we choose M = 10 for search efficiency. Meanwhile, medium ? also achieves the best accuracy-complexity trade-off as it mitigates the fluctuation of sampling and reduces the search cost.</p><p>Impact of momentum coefficient ? and step size ?: To investigate the influence of momentum coefficient ? and step size ? on test accuracy, we implemented the architecture parameter assignment with different ? and ?. The test error range and model parameter cost are demonstrated in Tab. 2, where medium ? outperforms other values. Small step sizes fail to achieve the optimal distribution when reaching the maximum update iterations, and large step sizes make the supernet optimization hard to converge.  With the increase of ?, the training stabilization becomes enforced, where ? with 0.8 achieves the best accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State-of-the-art NAS Methods</head><p>Tab. 3 shows the performance of Shapley-NAS on CIFAR-10 compared with the state-of-the-art NAS methods. Our Shapley-NAS achieves an average test error of 2.47% while only using 0.3 GPU days, significantly surpassing the DARTS baseline in both search cost and accuracy. The test error of the best single run in our experiments is 2.43%, ranking top amongst popular NAS methods. Although ProxylessNAS <ref type="bibr" target="#b3">[4]</ref> achieves a lower test error of 2.08%, it performs architecture search on a different space with heavy search cost. The low variance of the experimental results also demonstrates the stability of the proposed search method.</p><p>The comparison results on ImageNet with other methods are demonstrated in Tab. 4. We follow the mobile setting in <ref type="bibr" target="#b22">[23]</ref> for ImageNet, where the number of multiply-add operations ("?+") is restricted to be less than 600M. We trained the best-found architecture on CIFAR-10 to evaluate its transferability to ImageNet and obtained a competitive result with 24.3%/7.3% top-1/5 test error, verifying the generalization ability of our Shapley-NAS. We also evaluated the optimal architecture directly searched on ImageNet and obtain a top-1/5 test error of 23.9%/7.2%, which outperforms all other NAS methods with light search cost. Notably, despite the outstanding performance of DrNAS, it has a number of multiply-add operations much over 600M. By contrast, out Shapley-NAS never violates the mobile setting while achieving competitive results.</p><p>For NAS-Bench-201, our Shapley-NAS achieves outstanding performance with 94.37%, 73.51%, and 46.85% test accuracy on CIFAR-10, CIFAR-100, and ImageNet-16-120 respectively, as shown in Tab. 5. Notably, we obtain the global optimal architectures on CIFAR-10 and CIFAR-100, which indicates that the proposed method can identify im-   also acquire a near-optimal solution, which outperforms the state-of-the-art algorithms, again verifying the effectiveness of our Shapley-NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Performance Analysis</head><p>Shapley value estimation by Monte-Carlo Sampling: To verify the effectiveness of Monte-Carlo Sampling for Shapley value approximation, we plot the architecture parameters evolution on the first edge of normal and reduction cells in <ref type="figure" target="#fig_4">Fig. 4</ref>. Note that the curves of the first 15 epochs for warm-up are not presented. As <ref type="figure" target="#fig_4">Fig. 4a</ref> shows, although the max pool 3x3 operation is larger than all other operations at the start, the operation sep conv 5x5 finally becomes the strongest operation since it has the most contribution to the supernet along with the training process. While in <ref type="figure" target="#fig_4">Fig. 4b</ref>, the operation sep conv 3x3 becomes dominant after several epochs, while other operations gradually converge to be very weak. The supernet gradually converges to the final derived architecture using the proposed estimation. Moreover, the architecture parameters are differentiated to make the arg max selection more reliable.</p><p>Correlation between Shapley value and task performance: We investigate the correlation between Shapley value of operations and real task performance on NAS-Bench-201. After the search phase, we sample 200 discrete architectures from the search space and compute their corresponding operation strength by averaging the magnitude of architecture parameters. Then we plot the test accuracy obtained by directly querying along with the computed operation strength of DARTS and our Shapley-NAS. We use the Kendall Tau coefficient to measure the correlation, and the results on CIFAR-10, CIFAR-100, and  ImageNet-16-120 are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. The Shapley value of operations has a higher correlation with the test accuracy (? = 0.526, 0.474, 0.357 respectively), while the magnitude of ? in DARTS is almost entirely uncorrelated with the final task performance. It indicates the effectiveness of Shapley value to help us discover optimal architectures with superior performance during the evaluation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Discussion</head><p>In this paper, we have presented Shapley-NAS, a Shapley value based operation contribution evaluation method for neural architecture search. Since the architecture parameters updated by gradient descent in DARTS cannot reveal the actual operation importance in general, we propose to directly evaluate the marginal contribution of operations on accuracy via Shapley value. Specifically, the Shapley value of operations can be efficiently approximated by Monte-Carlo sampling based algorithm with early truncation, thus enabling the optimization of the supernet whose architecture parameters are directly updated with the operation contribution. Shapley-NAS achieves state-of-theart performance on CIFAR-10, ImageNet, and NAS-Bench-201 benchmarks, which proves its effectiveness to identify the optimal architectures with the most important operations in neural architecture search.</p><p>Limitations: The exact computation of Shapley value is expensive on common search space since it needs exponential times of evaluations, and the resulting heavy search burdens would limit the practical applications for taskspecific network deployment. Therefore, to evaluate operation contribution during the architecture search, we utilize the Monte-Carlo sampling method which gives an unbiased approximation of Shapley value. Despite being computationally efficient, it might not be as accurate as the exact computation via enumerating all possible subsets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>sk</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The performance drop caused by (a) removing the target operation on the 4th and 5th edge separately (b) removing one operation on both edges at the same time and enumerating all combinations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Varying early truncation threshold ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The test error (%) and search cost (GPU days) of the proposed method on CIFAR-10 with (a) different number of samples and (b) various thresholds of early truncation in the Monte-Carlo sampling for Shapley value estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The evolution of architecture parameters ? by estimating Shapley value based on Monte-Carlo Sampling. (a) The curves of ? on the first edge of the normal cell. (b) The curves of ? on the first edge of the reduction cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Correlation between operation strength and test accuracy of 200 sampled architectures using DARTS and our Shapley NAS on NAS-Bench-201. The average operation strength is obtained by the magnitude of corresponding architecture parameters in the supernet, and ? is the Kendall Tau coefficient that measures the correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The test error(%) of different search algorithms on S1-S4. DARTS+Shapley denotes the combination of DARTS and Shapley value evaluation, and * means freezing ? during the search.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S4</cell></row><row><cell></cell><cell>DARTS</cell><cell>3.84</cell><cell>4.85</cell><cell>3.34</cell><cell>7.20</cell></row><row><cell>C10</cell><cell>DARTS+Shapley DARTS+Shapley  *</cell><cell>3.11 2.95</cell><cell>2.92 2.84</cell><cell>2.58 2.67</cell><cell>3.45 2.94</cell></row><row><cell></cell><cell>Shapley-NAS</cell><cell>2.82</cell><cell>2.55</cell><cell>2.42</cell><cell>2.63</cell></row><row><cell></cell><cell>DARTS</cell><cell cols="4">29.46 26.05 28.90 22.85</cell></row><row><cell>C100</cell><cell cols="5">DARTS+Shapley 28.21 24.51 23.67 22.78 DARTS+Shapley  *  25.24 24.66 22.39 22.15</cell></row><row><cell></cell><cell>Shapley-NAS</cell><cell cols="4">23.60 22.77 21.92 21.53</cell></row><row><cell></cell><cell>DARTS</cell><cell>4.58</cell><cell>3.53</cell><cell>3.41</cell><cell>3.05</cell></row><row><cell>SVHN</cell><cell>DARTS+Shapley DARTS+Shapley  *</cell><cell>2.59 2.88</cell><cell>2.72 2.64</cell><cell>2.83 2.49</cell><cell>2.65 2.58</cell></row><row><cell></cell><cell>Shapley-NAS</cell><cell>2.36</cell><cell>2.43</cell><cell>2.34</cell><cell>2.41</cell></row><row><cell cols="6">[43] on CIFAR-10, CIFAR100, and SVHN. We first com-</cell></row><row><cell cols="6">bined the proposed Shapley value evaluation method with</cell></row><row><cell cols="6">DARTS (denoted as DARTS+Shapley in Tab. 1, by only ap-</cell></row><row><cell cols="6">plying Shapley value evaluation at the final discretization</cell></row><row><cell>step, i.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>e. selecting operations based on their Shapley val- ues instead of ?. Moreover, we also tested the performance under the same setting but keeping ? frozen, denoted as DARTS+Shapley* . As shown in Tab. 1, DARTS achieves competitive results with the proposed Shapley evaluation method, even when ? is not optimized in the training. No- tably, our Shapley-NAS still outperforms DARTS+Shapley and DARTS+Shapley* , since taking Shapley value into the supernet optimization can further alleviate the problem caused by gradient-based NAS methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The test error (%) and parameter storage cost (M) of the final architectures w.r.t. different values of momentum coefficient ? and different assignments of step size ?.</figDesc><table><row><cell>step size ?</cell><cell cols="8">? = 0.2 Test Error(%) Params(M) Test Error(%) Params(M) Test Error(%) Params(M) Test Error(%) Params(M) ? = 0.5 ? = 0.8 ? = 0.9</cell></row><row><cell>0.01</cell><cell>2.89 ? 0.21</cell><cell>4.0</cell><cell>2.87 ? 0.16</cell><cell>3.7</cell><cell>2.67 ? 0.06</cell><cell>3.5</cell><cell>2.74 ? 0.11</cell><cell>3.8</cell></row><row><cell>0.05</cell><cell>2.85 ? 0.18</cell><cell>3.6</cell><cell>2.79 ? 0.12</cell><cell>3.4</cell><cell>2.55 ? 0.07</cell><cell>3.2</cell><cell>2.68 ? 0.07</cell><cell>3.5</cell></row><row><cell>0.1</cell><cell>2.82 ? 0.11</cell><cell>3.7</cell><cell>2.66 ? 0.10</cell><cell>3.3</cell><cell>2.47 ? 0.04</cell><cell>3.4</cell><cell>2.61 ? 0.06</cell><cell>4.1</cell></row><row><cell>0.5</cell><cell>2.92 ? 0.19</cell><cell>3.5</cell><cell>2.84 ? 0.13</cell><cell>4.2</cell><cell>2.71 ? 0.12</cell><cell>3.8</cell><cell>2.83 ? 0.15</cell><cell>3.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparison with state-of-the-art image classifiers on CIFAR-10. Means and standard deviations of our Shapley-NAS are obtained by repeated experiments with 4 random seeds. Comparison with state-of-the-art image classifiers on Im-ageNet under the mobile setting [23]. ? indicates the results obtained by searching on ImageNet, otherwise on CIFAR-10.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.35</cell></row><row><cell>Architecture DenseNet-BC [16] NASNet-A [49]</cell><cell cols="2">Test Error (%) 3.46 2.65</cell><cell cols="2">Params (M) 25.6 3.3</cell><cell cols="2">Search Cost (GPU days) -2000</cell><cell>Architecture parameters</cell><cell>0.1 0.2 0.3 0.4</cell><cell cols="3">max_pool_3x3 avg_pool_3x3 skip_connect sep_conv_3x3 sep_conv_5x5 dil_conv_3x3 dil_conv_5x5</cell><cell>Architecture parameters</cell><cell>0.10 0.15 0.20 0.25 0.30</cell><cell>max_pool_3x3 avg_pool_3x3 skip_connect sep_conv_3x3 sep_conv_5x5 dil_conv_3x3 dil_conv_5x5</cell></row><row><cell>AmoebaNet-A [30] AmoebaNet-B [30] PNAS [22]</cell><cell cols="2">3.34 ? 0.06 2.55 ? 0.05 3.41 ? 0.09</cell><cell>3.2 2.8 3.2</cell><cell></cell><cell></cell><cell>3150 3150 225</cell><cell></cell><cell>15</cell><cell>20 (a) Normal cell 25 30 35 Epoch</cell><cell>40</cell><cell>45</cell><cell>50</cell><cell>15</cell><cell>20 (b) Reduction cell 25 30 35 40 Epoch</cell><cell>45</cell><cell>50</cell></row><row><cell>ENAS [29]</cell><cell>2.89</cell><cell></cell><cell>4.6</cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NAONet [26]</cell><cell>3.53</cell><cell></cell><cell>3.1</cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RandomNAS [21]</cell><cell cols="2">2.85 ? 0.08</cell><cell>4.3</cell><cell></cell><cell></cell><cell>2.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DARTS (1st order) [23]</cell><cell cols="2">3.00 ? 0.14</cell><cell>3.3</cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DARTS (2nd order) [23] 2.76 ? 0.09</cell><cell>3.3</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SNAS(moderate) [39]</cell><cell cols="2">2.85 ? 0.02</cell><cell>2.8</cell><cell></cell><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GDAS [10]</cell><cell>2.93</cell><cell></cell><cell>3.4</cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BayesNAS [46]</cell><cell cols="2">2.81 ? 0.04</cell><cell>3.4</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProxylessNAS [4]</cell><cell>2.08</cell><cell></cell><cell>5.7</cell><cell></cell><cell></cell><cell>4.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P-DARTS [8]</cell><cell>2.50</cell><cell></cell><cell>3.4</cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PC-DARTS [40]</cell><cell cols="2">2.57 ? 0.07</cell><cell>3.6</cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SGAS (Cri 1. avg) [20]</cell><cell cols="2">2.66 ? 0.24</cell><cell>3.7</cell><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SDARTS-RS [6]</cell><cell cols="2">2.61 ? 0.02</cell><cell>3.4</cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DrNAS [7]</cell><cell cols="2">2.54 ? 0.03</cell><cell>4.0</cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DARTS+PT [36]</cell><cell cols="2">2.61 ? 0.08</cell><cell>3.0</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shapley-NAS(avg.)</cell><cell cols="2">2.47 ? 0.04</cell><cell>3.4</cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shapley-NAS(best)</cell><cell>2.43</cell><cell></cell><cell>3.6</cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Architecture</cell><cell>Test Error (%)</cell><cell cols="2">Params (M)</cell><cell cols="2">?+ (M)</cell><cell>Search Cost (GPU days)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inception-v1 [34]</cell><cell>30.1</cell><cell></cell><cell>6.6</cell><cell cols="2">1448</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNet [15]</cell><cell>29.4</cell><cell></cell><cell>4.2</cell><cell cols="2">569</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ShuffleNet 2? (v1) [45]</cell><cell>26.4</cell><cell></cell><cell>? 5</cell><cell cols="2">524</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ShuffleNet 2? (v2) [27]</cell><cell>25.1</cell><cell></cell><cell>? 5</cell><cell cols="2">591</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NASNet-A [49]</cell><cell>26.0</cell><cell></cell><cell>5.3</cell><cell cols="2">564</cell><cell>2000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AmoebaNet-C [30]</cell><cell>24.3</cell><cell></cell><cell>6.4</cell><cell cols="2">570</cell><cell>3150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PNAS [22]</cell><cell>25.8</cell><cell></cell><cell>5.1</cell><cell cols="2">588</cell><cell>225</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MnasNet-92 [35]</cell><cell>25.2</cell><cell></cell><cell>4.4</cell><cell cols="2">388</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DARTS (2nd) [23]</cell><cell>26.7</cell><cell></cell><cell>4.7</cell><cell cols="2">574</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SNAS (mild) [39]</cell><cell>27.3</cell><cell></cell><cell>4.3</cell><cell cols="2">522</cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GDAS [10]</cell><cell>26.0</cell><cell></cell><cell>5.3</cell><cell cols="2">545</cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BayesNAS [46]</cell><cell>26.5</cell><cell></cell><cell>3.9</cell><cell>-</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProxylessNAS (GPU) [4]  ?</cell><cell>24.9</cell><cell></cell><cell>7.1</cell><cell cols="2">465</cell><cell>8.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P-DARTS [8]</cell><cell>24.4</cell><cell></cell><cell>4.9</cell><cell cols="2">557</cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PC-DARTS [40]</cell><cell>25.1</cell><cell></cell><cell>5.3</cell><cell cols="2">586</cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PC-DARTS [40]  ?</cell><cell>24.2</cell><cell></cell><cell>5.3</cell><cell cols="2">582</cell><cell>3.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SGAS (Cri 1. best) [20]</cell><cell>24.2</cell><cell></cell><cell>5.3</cell><cell cols="2">585</cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SDARTS-ADV [6]</cell><cell>25.6</cell><cell></cell><cell>6.1</cell><cell>-</cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DrNAS [7]  ?</cell><cell>24.2</cell><cell></cell><cell>5.2</cell><cell cols="2">644</cell><cell>3.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DARTS+PT [36]  ?</cell><cell>25.5</cell><cell></cell><cell>4.7</cell><cell cols="2">538</cell><cell>3.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shapley-NAS</cell><cell>24.3</cell><cell></cell><cell>5.1</cell><cell cols="2">566</cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shapley-NAS  ?</cell><cell>23.9</cell><cell></cell><cell>5.4</cell><cell cols="2">582</cell><cell>4.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">portant operations and derive the best architecture from the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">large search space. On the ImageNet-16-120 dataset, we</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison results with state-of-the-art NAS methods on NAS-Bench-201. ? denotes the results are obtained by searching on CIFAR-10, otherwise by directly searching on the evaluation dataset. Shapley-NAS 91.61 ? 0.00 94.37 ? 0.00 73.49 ? 0.00 73.51 ? 0.00 46.57 ? 0.08 46.85 ? 0.12</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell cols="3">CIFAR-10 validation</cell><cell>test</cell><cell>CIFAR-100 validation</cell><cell>test</cell><cell>ImageNet-16-120 validation test</cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet [14]</cell><cell cols="2">90.83</cell><cell></cell><cell>93.97</cell><cell>70.42</cell><cell>70.86</cell><cell>44.53</cell><cell>43.63</cell></row><row><cell></cell><cell></cell><cell cols="4">Random (baseline) 90.93 ? 0.36</cell><cell cols="2">93.70 ? 0.36</cell><cell>70.60 ? 1.37</cell><cell>70.65 ? 1.38</cell><cell>42.92 ? 2.00</cell><cell>42.96 ? 2.15</cell></row><row><cell></cell><cell></cell><cell cols="2">RSPS [21]</cell><cell cols="2">84.16 ? 1.69</cell><cell cols="2">87.66 ? 1.69</cell><cell>45.78 ? 6.33</cell><cell>46.60 ? 6.57</cell><cell>31.09 ? 5.65</cell><cell>30.78 ? 6.12</cell></row><row><cell></cell><cell></cell><cell cols="4">REINFORCE [49]  ? 91.09 ? 0.37</cell><cell cols="2">93.85 ? 0.37</cell><cell>71.61 ? 1.12</cell><cell>71.71 ? 1.09</cell><cell>45.05 ? 1.02</cell><cell>45.24 ? 1.18</cell></row><row><cell></cell><cell></cell><cell cols="2">ENAS [29]</cell><cell cols="2">39.77 ? 0.00</cell><cell cols="2">54.30 ? 0.00</cell><cell>10.23 ? 0.12</cell><cell>10.62 ? 0.27</cell><cell>16.43 ? 0.00</cell><cell>16.32 ? 0.00</cell></row><row><cell></cell><cell></cell><cell cols="2">DARTS [23]  ?</cell><cell cols="2">39.77 ? 0.00</cell><cell cols="2">54.30 ? 0.00</cell><cell>15.03 ? 0.00</cell><cell>15.61 ? 0.00</cell><cell>16.43 ? 0.00</cell><cell>16.32 ? 0.00</cell></row><row><cell></cell><cell></cell><cell cols="2">DARTS [23]</cell><cell cols="2">39.77 ? 0.00</cell><cell cols="2">54.30 ? 0.00</cell><cell>38.57 ? 0.00</cell><cell>38.97 ? 0.00</cell><cell>18.87 ? 0.00</cell><cell>18.41 ? 0.00</cell></row><row><cell></cell><cell></cell><cell cols="2">SNAS [39]</cell><cell cols="2">90.10 ? 1.04</cell><cell cols="2">92.77 ? 0.83</cell><cell>69.69 ? 2.39</cell><cell>69.34 ? 1.98</cell><cell>42.84 ? 1.79</cell><cell>43.16 ? 2.64</cell></row><row><cell></cell><cell></cell><cell cols="2">GDAS [10]</cell><cell cols="2">90.01 ? 0.46</cell><cell cols="2">93.23 ? 0.23</cell><cell>24.05 ? 8.12</cell><cell>24.20 ? 8.08</cell><cell>40.66 ? 0.00</cell><cell>41.02 ? 0.00</cell></row><row><cell></cell><cell></cell><cell cols="2">PC-DARTS [40]</cell><cell cols="2">89.96 ? 0.15</cell><cell cols="2">93.41 ? 0.30</cell><cell>67.12 ? 0.39</cell><cell>67.48 ? 0.89</cell><cell>40.83 ? 0.08</cell><cell>41.31 ? 0.22</cell></row><row><cell></cell><cell></cell><cell cols="2">iDARTS [44]  ?</cell><cell cols="2">89.96 ? 0.60</cell><cell cols="2">93.58 ? 0.32</cell><cell>70.57 ? 0.24</cell><cell>70.83 ? 0.48</cell><cell>40.38 ? 0.59</cell><cell>40.89 ? 0.68</cell></row><row><cell></cell><cell></cell><cell cols="2">DrNAS [7]</cell><cell cols="2">91.55 ? 0.00</cell><cell cols="2">94.36 ? 0.00</cell><cell>73.49 ? 0.00</cell><cell>73.51 ? 0.00</cell><cell>46.37 ? 0.00</cell><cell>46.34 ? 0.00</cell></row><row><cell></cell><cell></cell><cell cols="2">optimal</cell><cell cols="2">91.61</cell><cell></cell><cell>94.37</cell><cell>73.49</cell><cell>73.51</cell><cell>46.77</cell><cell>47.31</cell></row><row><cell></cell><cell>94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy (%)</cell><cell>86 88 90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>84</cell><cell></cell><cell cols="3">DARTS, = 0.128 Shapley-NAS, = 0.526</cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell>0.10</cell><cell>0.15 Average Operation Strength 0.20 0.25 0.30</cell><cell>0.35</cell><cell>0.40</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Explaining deep neural networks with a polynomial time algorithm for shapley value approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz?ztireli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01795</idno>
		<title level="m">Shapley value as principled metric for structured network pruning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Polynomial calculation of the shapley value based on sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Tejada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1726" to="1730" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stabilizing differentiable architecture search via perturbation-based regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10355</idno>
		<title level="m">Drnas: Dirichlet neural architecture search</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00326</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data shapley: Equitable valuation of data for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirata</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirata</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09815</idno>
		<title level="m">Neuron shapley: Discovering the responsible neurons</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ce Zhang, Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the shapley value</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<meeting><address><addrLine>Frances Ann Hubis, Nick Hynes; Bo Li</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1167" to="1176" />
		</imprint>
	</monogr>
	<note>Nezihe Merve G?rel</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From local explanations to global understanding with explainable ai for trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Erion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De-Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bala</forename><surname>Jordan M Prutkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronit</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisha</forename><surname>Himmelfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-In</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07233</idno>
		<title level="m">Neural architecture optimization</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Mase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Art</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00467</idno>
		<title level="m">Explaining black box decisions by shapley cohort refinement</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The Shapley value: essays in honor of Lloyd S. Shapley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><forename type="middle">E</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A value for n-person games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to the Theory of Games</title>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="page" from="307" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An efficient explanation of individual classifications using game theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Strumbelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04392</idno>
		<title level="m">Rethinking architecture selection in differentiable nas</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">I</forename><surname>Inouye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02297</idno>
		<title level="m">Shapley explanation networks</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Apq: Joint search for network architecture, pruning and quantization policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2078" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">Snas: stochastic neural architecture search</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05737</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Who&apos;s responsible? jointly quantifying the contribution of the learning algorithm and data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Yona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirata</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIES</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1034" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09656</idno>
		<title level="m">Yassine Marrakchi, Thomas Brox, and Frank Hutter. Understanding and robustifying differentiable architecture search</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10784</idno>
		<title level="m">idarts: Differentiable architecture search with stochastic implicit gradients</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bayesnas: A bayesian approach for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7603" to="7613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploiting operation importance for differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xukai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun-Yuan</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TNNLS, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
