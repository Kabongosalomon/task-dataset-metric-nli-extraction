<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TDAN: Temporally Deformable Alignment Network for Video Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TDAN: Temporally Deformable Alignment Network for Video Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video super-resolution (VSR) aims to restore a photorealistic high-resolution (HR) video frame from both its corresponding low-resolution (LR) frame (reference frame) and multiple neighboring frames (supporting frames). Due to varying motion of cameras or objects, the reference frame and each support frame are not aligned. Therefore, temporal alignment is a challenging yet important problem for VSR. Previous VSR methods usually utilize optical flow between the reference frame and each supporting frame to wrap the supporting frame for temporal alignment. Therefore, the performance of these image-level wrapping-based models will highly depend on the prediction accuracy of optical flow, and inaccurate optical flow will lead to artifacts in the wrapped supporting frames, which also will be propagated into the reconstructed HR video frame. To overcome the limitation, in this paper, we propose a temporal deformable alignment network (TDAN) to adaptively align the reference frame and each supporting frame at the feature level without computing optical flow. The TDAN uses features from both the reference frame and each supporting frame to dynamically predict offsets of sampling convolution kernels. By using the corresponding kernels, TDAN transforms supporting frames to align with the reference frame. To predict the HR video frame, a reconstruction network taking aligned frames and the reference frame is utilized. Experimental results demonstrate the effectiveness of the proposed TDAN-based VSR model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of video super-resolution (VSR) is to reconstruct a high-resolution (HR) video frame from its corresponding low-resolution (LR) video frame (reference frame) and multiple neighboring LR video frames (supporting frames). HR video frames contain more image details  <ref type="figure">Figure 1</ref>. VSR results for a frame in the walk sequence. We can find that our method can restore more accurate image structures and details than the recent DUF network.</p><p>and are more preferred to humans. Therefore, the VSR technique is desired in many real applications, such as video surveillance and high-definition television (HDTV).</p><p>To super-resolve the LR reference frame, VSR will exploit both the LR reference frame and multiple LR supporting frames. However, the LR reference frame and each supporting frame are likely not fully aligned due to the motion of camera or objects. Thus, a vital issue in VSR is how to align the supporting frames with the reference frame.</p><p>Previous methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b21">22]</ref> usually exploit optical flow to predict motion fields between the reference frame and supporting frames, then wrap the supporting frames using their corresponding motion fields. Therefore, the optical flow prediction is crucial for these approaches, and any errors in the flow computation or the image-level wrapping operation may introduce artifacts around image structures in the aligned supporting frames.</p><p>To alleviate the above issues, we propose a temporally deformable alignment network (TDAN) in this paper that performs one-stage temporal alignment without using optical flow. Unlike previous optical flow-based VSR methods, our approach can adaptively align the reference frame and supporting frames at a feature level without explicit motion estimation and image wrapping operations. Therefore, the aligned LR video frames will have less annoying image artifacts, and the image quality of the reconstructed HR video frame will be improved. In specific, inspired by the deformable convolution <ref type="bibr" target="#b3">[4]</ref>, the proposed TDAN uses features from both the reference frame and each supporting frame to dynamically predict offsets of sampling convolution kernels, and then apply these dynamic kernels on features from supporting frames to employ the temporal alignment. Here, given different reference and supporting frame pairs, the module will generate their corresponding sampling kernels, which makes the TDAN have strong capability and flexibility to handle various motion conditions in temporal scenes. With the aligned supporting frames and the reference frame, a reconstruction network is utilized to predict an HR video frame corresponding to the LR reference frame.</p><p>We conduct extensive experiments on a widely-used VSR benchmark and two real-world LR video sequences. The experimental results show that our framework achieves state-of-the-art performance. In <ref type="figure">Fig. 1</ref>, we show a visual comparison to the recent DUF network <ref type="bibr" target="#b8">[9]</ref>, and we observe that our method reconstructs more image details.</p><p>The contributions of this paper are three-fold: (1) we propose a novel temporally deformable alignment network (TDAN) for feature-level alignment, which avoids the twostage process adopted by previous optical flow-based methods; <ref type="bibr" target="#b1">(2)</ref> we propose an end-to-end trainable VSR framework based on the TDAN; and (3) our method achieves state-of-the-art VSR performance on Vid4 <ref type="bibr" target="#b15">[16]</ref> benchmark dataset. The source code and pre-trained models will be released. A video demo is available on https://www. youtube.com/watch?v=eZExENE50I0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we discuss three related works: single image super-resolution (SISR), video super-resolution (VSR), and deformable convolution <ref type="bibr" target="#b3">[4]</ref>.</p><p>Single Image Super-Resolution (SISR): Dong et al. <ref type="bibr" target="#b4">[5]</ref> firstly proposed an end-to-end image super-resolution convolutional neural network (SRCNN). Kim et al. <ref type="bibr" target="#b10">[11]</ref> introduced a 20-layer deep network: VDSR with residual learning. Shi et al. <ref type="bibr" target="#b22">[23]</ref> learned an efficient sub-pixel convolution layer to upscale the final LR feature maps into the HR output for accelerating SR networks. Deeper networks like LapSRN <ref type="bibr" target="#b12">[13]</ref>, DRRN <ref type="bibr" target="#b24">[25]</ref>, and MemNet <ref type="bibr" target="#b25">[26]</ref>, were explored to further improve SISR performance. However, training images used in the previous methods have limited resolution, which makes the training of even deeper and wider networks very difficult. Recently, Timofte et al. introduced a novel large dataset (DIV2K) consisting of 1000 DIVerse 2K resolution RGB images in NTIRE 2017 Chal-lenge <ref type="bibr" target="#b27">[28]</ref>. Current state-of-the-art SISR networks, like EDSR <ref type="bibr" target="#b14">[15]</ref>, RDN <ref type="bibr" target="#b34">[35]</ref>, DBPN <ref type="bibr" target="#b5">[6]</ref>, and RCAN <ref type="bibr" target="#b33">[34]</ref>, trained on the DIV2K outperformed previous networks by a substantial margin. A recent survey about deep learning-based SISR methods is in <ref type="bibr" target="#b32">[33]</ref>.</p><p>Video Super-Resolution (VSR): It has been observed that temporal alignment critically affects the performance of VSR systems. Previous methods usually adopted two-stage approaches based on optical flow. They will conduct motion estimation by computing optical flow in the first stage and utilize the estimated motion fields to perform image wrapping/motion compensation in the second stage. For example, Liao et al. <ref type="bibr" target="#b13">[14]</ref> used two classical optical flow methods: TV-L 1 and MDP flow <ref type="bibr" target="#b30">[31]</ref> with different parameters to generate HR SR-drafts, and then predicted the final HR frame by a deep draft-ensemble network. Kappeler et al. <ref type="bibr" target="#b9">[10]</ref> took interpolated flow-wrapped frames as inputs of a CNN to predict HR video frames. However, the pioneering methods both used classical optical flow algorithms, which are separated from the frame reconstruction CNN and much slower than the flow CNN during inference.</p><p>To address the issue, Caballero et al. <ref type="bibr" target="#b2">[3]</ref> introduced the first end-to-end VSR network: VESCPN, which jointly trains flow estimation and spatiotemporal networks. Liu et al. <ref type="bibr" target="#b16">[17]</ref> proposed a temporal adaptive neural network to adaptively select the optimal range of temporal dependency and a rectified optical flow alignment method for better motion estimation. Tao et al. <ref type="bibr" target="#b26">[27]</ref> computed LR motion field based on optical flow network and designed a new layer to utilize sub-pixel information from motion and simultaneously achieve sub-pixel motion compensation (SPMC) and resolution enhancement. Xue et al. <ref type="bibr" target="#b31">[32]</ref> exploited task-oriented motion cues via Task-Oriented Flow (TOFlow), which achieves better VSR results than fixed flow algorithms. Sajjadi et al. <ref type="bibr" target="#b21">[22]</ref> extended the conventional VSR models to a frame-recurrent VSR framework. However, sufficiently high-quality motion estimation is not easy to obtain even with state-of-the-art optical flow estimation networks. Even with accurate motion fields, the image-wrapping based motion compensation will produce artifacts around image structures, which may be propagated into final reconstructed HR frames. The proposed TDAN performs a feature-wise one-stage temporal alignment without relying on optical flow, which will alleviate the issues in these previous optical flow-based VSR networks.</p><p>Deformable Convolution: CNNs have the inherent limitation in modeling geometric transformations due to the fixed kernel configuration. For enhancing the transformation modeling capability of regular CNNs, Dai et al. <ref type="bibr" target="#b3">[4]</ref> proposed a deformable convolution operation. It has been applied to address several high-level vision tasks such as object detection <ref type="bibr" target="#b3">[4]</ref>, video object detection <ref type="bibr" target="#b1">[2]</ref>, semantic seg-  <ref type="figure">Figure 2</ref>. The proposed TDAN-based VSR framework. Here, we only show the framework with one supporting frame. In our implementation, 4 neighboring supporting frames are exploited for exploring more temporal information.</p><p>mentation <ref type="bibr" target="#b3">[4]</ref>, and human pose estimation <ref type="bibr" target="#b23">[24]</ref>. Although the deformable convolution has shown superiority on several high-level vision tasks, it was rarely explored for lowlevel vision problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>First, we define the problem and give an overview of our framework in Sec. 3.1. Upon this framework, we propose the temporally deformable alignment network in Sec. 3.2, the SR reconstruction network in Sec. 3.3, and the loss functions in Sec. 3.4. Finally, in Sec. 3.5, we discuss the merits of the proposed alignment network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Let I LR t ? R H?W ?C be the t-th LR video frame, and I HR t ? R sH?sW ?C be the corresponding HR video frame, where s is the upscaling factor. Our goal is to restore the HR video frame I HR t from the reference LR frame I LR t and 2N supporting LR frames {I LR t?N , ...I LR t?1 , I LR t+1 , ..., I LR t+N }. Therefore, our VSR framework takes the consecutive 2N + 1 frames {I i } t+N i=t?N as the input to predict the HR frame I HR t , which is illustrated in <ref type="figure">Fig. 2</ref>. It consists of two main sub-networks: a temporally deformable alignment network (TDAN) to align each supporting frame with the reference frame and a SR reconstruction network to predict the HR frame.</p><p>The TDAN takes an LR supporting frame I LR i and the LR reference frame I LR t as inputs to predict the corresponding aligned LR frame I LR i for the supporting frame:</p><formula xml:id="formula_0">I LR i = f T DAN (I LR t , I LR i ) .<label>(1)</label></formula><p>Feeding the 2N supporting frames into the TDAN separately, we can obtain 2N corresponding aligned LR frames</p><formula xml:id="formula_1">{I LR t?N , ...I LR t?1 , I LR t+1 , .</formula><p>.., I LR t+N }. The SR reconstruction network will utilize the 2N aligned frames along with the reference frame to restore the HR video frame I HR</p><formula xml:id="formula_2">t = f SR (I LR t?N , ..., I LR t?1 , I LR t , I LR t+1 , ..., I LR t+N ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporally Deformable Alignment Network</head><p>Given an LR supporting frame I LR i and the LR reference frame I LR t , the proposed TDAN will temporally align</p><formula xml:id="formula_3">I LR i with I LR t .</formula><p>It mainly consists of three modules: feature extraction, deformable alignment, and aligned frame reconstruction. respectively via a shared feature extraction network. The network consists of one convolutional layer and k 1 residual blocks <ref type="bibr" target="#b6">[7]</ref> with Re-LUs as the activation functions. In our implementation, we adopted a modified residual structure from <ref type="bibr" target="#b14">[15]</ref>. The extracted features will be utilized for feature-wise temporal alignment. Deformable Alignment: The deformable alignment module takes the F LR i and F LR t as inputs to predict sampling parameters ? for the feature F LR i :</p><formula xml:id="formula_4">? = f ? (F LR i , F LR t ) .<label>(2)</label></formula><p>Here, ? = { p n | n = 1, . . . , |R|} refers to the offsets of the convolution kernels, where R = {(?1, ?1), (?1, 0), ..., (0, 1), (1, 1)} donates a regular grid of a 3?3 kernel. With ? and F LR i , the aligned feature F LR i of the supporting frame can be computed by the deformable convolution:</p><formula xml:id="formula_5">F LR i = f dc (F LR i , ?) .<label>(3)</label></formula><p>More specifically, for each position p 0 on the aligned feature map F LR i , we have:</p><formula xml:id="formula_6">F LR i (p 0 ) = pn?R w(p n )F LR i (p 0 + p n + p n ) . (4)</formula><p>The convolution will be operated on the irregular positions p n + p n , where the p n may be fractional. To address the issue, the operation is implemented by using bilinear interpolation, which is the same as that proposed in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Here, the deformable alignment module consists of several regular and deformable convolutional layers. For the sampling parameter generation function f ? , it concatenates F LR i and F LR t , and uses a 3 ? 3 bottleneck layer to reduce the channel number of the concatenated feature map. Then, the sampling parameters are predicted by a convolutional layer with the kernel size |R| as the output channel number. Finally, the aligned feature F LR i is obtained from ? and F LR i based on deformable convolution operation. In practice, we use three additional deformable convolutional layers before and after the f dc for enhancing the transformation flexibility and capability of the module. Section 4.3 contains the ablation study on the performance of the module with different numbers of the additional deformable convolutional layers.</p><p>We note that the feature of the reference frame F LR t is only used for computing the offset, and its information will not be propagated into the aligned feature of the supporting frame F LR i . Besides, the adaptively learned offset will implicitly capture motion cues and also explore neighboring features within the same image structures for temporal alignment.</p><p>Aligned Frame Reconstruction: Although the deformable alignment has the potential to capture motion cues and align F LR i with F LR t , the implicit alignment is very difficult to learn without a supervision. To make the implicit alignment be possible, we restore an aligned LR frame I LR i for I LR i and utilize a alignment loss for enforcing the deformable alignment module to capture motions and align two frames at the feature level. More discussions can be found in Sec. 5. The aligned LR frame I LR i ? R H?W ?C can be reconstructed from the aligned feature map with a 3 ? 3 convolutional layer.</p><p>After feeding 2N reference and supporting frame pairs into the TDAN, we can obtain the corresponding 2N aligned LR frames, which will be used to predict the HR video frame I HR t in the SR reconstruction network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SR Reconstruction Network</head><p>We use a SR reconstruction network to restore the HR video frame I HR i from the aligned LR frames and the reference frame. The network contains three modules: temporal fusion, nonlinear mapping, and HR frame reconstruction, which will aggregate temporal information from different frames, predict high-level visual features, and restore the HR frame for the LR reference frame, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Fusion:</head><p>To fuse different frames cross the timespace, we directly concatenate the 2N + 1 frames and then feed them into a 3?3 convolutional layer to output the fused feature map.</p><p>Nonlinear Mapping: The nonlinear mapping module with k 2 stacked residual blocks <ref type="bibr" target="#b14">[15]</ref> will take the shadow fused features as input to predict deep features.</p><p>HR Frame Reconstruction: After extracting deep features in the LR space, inspired by the EDSR <ref type="bibr" target="#b14">[15]</ref>, we utilize an upscaling layer to increase the resolution of the feature map with a sub-pixel convolution as proposed by Shi et al. <ref type="bibr" target="#b22">[23]</ref>. In practice, for ?4 upscaling, two sub-pixel convolution modules will be used. The final HR video frame estimation I HR t will be obtained by a convolutional layer from the zoomed feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>Two loss functions L align and L sr are used to train the TDAN and SR reconstruction network, respectively. Note that we have no groundtruth of the aligned LR frames. To optimize the TDAN, we utilize the reference frame as the label and make the aligned LR frames close to the reference frame:</p><formula xml:id="formula_7">L align = 1 2N t+N i=t?N, =t I LR i ? I LR t 1 .<label>(5)</label></formula><p>The objective function of the SR reconstruction network is defined via L 1 reconstruction loss:</p><formula xml:id="formula_8">L sr = I HR t ? I HR t 1 .<label>(6)</label></formula><p>Combining the two loss terms, we have the overall loss function for training our VSR framework:</p><formula xml:id="formula_9">L = L align + L sr .<label>(7)</label></formula><p>The two loss terms are simultaneously optimized when training our VSR framework. Therefore, our TDAN-based VSR network is end-to-end trainable. In addition, the TDAN can be trained with a self-supervision without requiring any annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Analyses of the Proposed TDAN</head><p>Given a reference frame and a set of supporting frames, the proposed TDAN can employ temporal alignment to align the supporting frames with the reference frame. It has several merits: One-Stage Temporal Alignment: Most previous temporal alignment methods are based on optical flow, which will split the temporal alignment problem into two subproblems: flow/motion estimation and motion compensation. As discussed in the paper, the performance of these methods will highly rely on the accuracy of flow estimation, and the flow-based image wrapping will introduce annoying artifacts. Unlike these two-stage temporal alignments, our TDAN is a one-stage approach, which aligns the supporting frames at the feature level. It implicitly captures motion cues via adaptive sampling parameter generation without explicitly estimating the motion field, and reconstructs the aligned frames from the aligned features. Self-Supervised Training: The optical flow estimation is crucial for the two-stage methods. For ensuring the accuracy of flow estimation, some VSR networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b16">17]</ref> utilized additional flow estimation algorithms. Unlike these methods, there is no flow estimation inside the TDAN, and it can be trained in a self-supervised manner without relying on any extra supervisions. Exploration: For each location in a frame, its motion field computed by optical flow only refers to one potential position p. It means that each pixel in a wrapped frame will only copy one pixel at p or use an interpolated value for a fractional position. However, beyond utilizing information at p, our deformable alignment module can adaptively explore more features at sampled positions, which may share the same image structure as p, and it will help to aggregate more contexts for better-aligned frame reconstruction. Therefore, the proposed TDAN has stronger exploration capability than optical flow-based models. Generic: The proposed TDAN is a generic temporal alignment framework and can be easily utilized to replace flowbased motion compensation for several other tasks, such as video denoising <ref type="bibr" target="#b18">[19]</ref>, video deblocking <ref type="bibr" target="#b17">[18]</ref>, video deblurring <ref type="bibr" target="#b7">[8]</ref>, video frame interpolation <ref type="bibr" target="#b0">[1]</ref>, and even video prediction <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>First, we introduce the experimental settings: datasets, evaluation metrics, degradation methods, and training settings in Sec. 4.1. Then, we compare the proposed VSR framework with other state-of-the-art VSR and SISR approaches on different degradation for fair comparisons in Sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Datasets: A large-scale dataset is important for training high-performance networks. For SR tasks, besides the scale of the training data, resolution of the training images is another important factor. With the emergence of the DIV2K dataset <ref type="bibr" target="#b27">[28]</ref>, which contains 1000 2K-resolution HR images, training very deep network (e.g. beyond 400 convolutional layers in RCAN <ref type="bibr" target="#b33">[34]</ref>) or very wide network (e.g. EDSR <ref type="bibr" target="#b14">[15]</ref>) becomes possible. As a result, the SISR performance has been greatly improved recently. Similar to SISR, for training a high-capability VSR network, a large video dataset containing very HR frames is desired. Previous methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref> usually collect 720P, 1080P, or even 4K videos from the internet, and then crop training patches from the collected samples. Jo et al. <ref type="bibr" target="#b8">[9]</ref> created a large dataset with various contents including wildlife, activity, and landscape, and augmented the dataset by a temporal augmentation method. Sajjadi et al. introduced another high-quality VSR dataset in <ref type="bibr" target="#b21">[22]</ref>. However, these datasets are all not publicly available 1 . Thanks to Xue et al. <ref type="bibr" target="#b31">[32]</ref> for releasing a VSR dataset: Vimeo Super-Resolution dataset containing 64612 training samples. Each sample in the dataset contains seven consecutive frames with 448 ? 256 resolution. However, unlike the previous methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref>, the authors did not create HR training samples by cropping full-resolution frames (or slightly down-sampled frames). Instead, they resized the original frames to 448 ? 256 resolution, which will smooth out many image details.</p><p>In our experiments, we used Vimeo Super-Resolution dataset as our training dataset and 31 frames from the Temple sequence <ref type="bibr" target="#b13">[14]</ref> as the validation dataset. We evaluated our models on the Vid4 benchmark <ref type="bibr" target="#b15">[16]</ref>, which contains four video sequences: city, walk, calendar, and foliage, as other methods. Evaluation Metrics: PSNR and SSIM <ref type="bibr" target="#b28">[29]</ref> are used as evaluation metrics to compare different VSR networks quantitatively. Excepting comparison with the TOFlow, the first and last two frames are not used for evaluation, and four spatial boundary pixels are ignored. Degradation Methods: We compare our TDAN-based network with current state-of-the-art VSR and SISR networks: VSRnet <ref type="bibr" target="#b9">[10]</ref>, ESPCN <ref type="bibr" target="#b22">[23]</ref>, VESCPN <ref type="bibr" target="#b2">[3]</ref>, Liu et al. <ref type="bibr" target="#b16">[17]</ref>, TOFlow <ref type="bibr" target="#b31">[32]</ref>, DBPN <ref type="bibr" target="#b5">[6]</ref>, RND <ref type="bibr" target="#b34">[35]</ref>, RCAN <ref type="bibr" target="#b33">[34]</ref>, SPMC <ref type="bibr" target="#b26">[27]</ref>, FSRVSR <ref type="bibr" target="#b21">[22]</ref>, and DUF <ref type="bibr" target="#b8">[9]</ref>. The first seven networks adopted the Matlab function imresize with the option bicubic (BI) to generate LR video frames. SPMC, FS-RVSR, and DUF obtained LR frames by first blurring HR frames via a Gaussian kernel and then downsampling via selecting every s-th pixel (denote as BD for short). Note that we compare the FRVSR-3-64 and DUF-16L models, which have similar model sizes as our TDAN-based VSR network. We train two different TDAN models with the two different degradation methods for fair comparisons. Training Settings: We use a downsampling factor: s = 4 in all our experiments. Each training batch contains 64 ? 5 LR RGB patches with the size 48 ? 48, where 64 means the batch size and 5 refers to the number of consecutive input frames. We implement our network with Pytorch <ref type="bibr" target="#b20">[21]</ref> and adopt Adam <ref type="bibr" target="#b11">[12]</ref> as the optimizer. The learning rate is initialized to 10 ?4 for all layers and decreases half for every 100 epochs. For every 100 epochs, training our network takes roughly 1.7 days with an NVIDIA 1080TI GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Comparisons</head><p>Results with BI Degradation: <ref type="table" target="#tab_1">Tables 1 and 2</ref>  and other VSR methods did not release their training data or training source code. Therefore we compare with their methods based directly on the provided results. We can see that our TDAN achieves the best performance among all compared state-of-the-art flow-based VSR networks and SISR networks.</p><p>Visual results of City and Calendar for 4? VSR on the BI configuration are illustrated in <ref type="figure">Fig. 3</ref>. We can find that the SISR networks without using the supporting frames: DPBN, RDN, and RCAN, fail to restore missing image details such as the building structures in City and the window in Calendar. With motion compensation, the VESCPN [3], Liu et al. <ref type="bibr" target="#b16">[17]</ref>, and TOFlow <ref type="bibr" target="#b31">[32]</ref> can compensate missing details from the supporting frames. Our TDAN recovers more fine image details than others, which demonstrates the effectiveness of the proposed one-stage temporal alignment framework.</p><p>Results with BD Degradation: <ref type="table" target="#tab_4">Table 3</ref> shows quantitative comparisons on the BD configuration. Our method outperforms the flow-based networks: SPMC and recent FRVSR method but is worse than the dynamic filtering-based DUF on SSIM; the latter implicitly handles the motion by learning to output dynamic upsampling filters and utilizes a large HR training dataset with an additional temporal augmentation. The DUF does not well restore borders of frames; thus the PSNR value of DUF is lower than the TDAN. In addition, the DUF and the FRVSR take 7 and 10 frames as inputs respectively, and our TDAN with 5 frames as inputs exploits less sequence information.</p><p>Visual results of Walk and Foliage for 4? VSR on the BD configuration are illustrated in <ref type="figure">Fig. 3</ref>. In comparison with SPMC <ref type="bibr" target="#b26">[27]</ref> and DUF <ref type="bibr" target="#b8">[9]</ref>, benefiting from the strong temporal alignment network, the proposed TDAN is more effective in exploiting information in supporting frames, and therefore it is more capable of restoring image structures, for example, the baby face and the stripe on the cloth in Walk, and the white and black cars in Foliage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vid4</head><p>Bicubic SPMC <ref type="bibr" target="#b26">[27]</ref> FSRVSR <ref type="bibr" target="#b21">[22]</ref>    Model Sizes: <ref type="table" target="#tab_5">Table 4</ref> shows parameter numbers of several networks with the leading VSR performance. We can see that the state-of-the-art SISR networks: RDN, RCAN, and TOFlow, have larger model sizes than the TDAN, and the proposed TDAN has comparable parameter numbers with the FRVSR and DUF. Even with such a light-weight model, the proposed TDAN still achieves promising VSR performance, which further validates the effectiveness of the proposed one-stage temporal alignment framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To further investigate the proposed TDAN, we compare it with two baselines: SISR and MFSR models, that trained on the Vimeo Super-Resolution dataset. The SISR model only uses the reference frame as the input, and the MFSR directly concatenates the supporting and reference frames as inputs without temporal alignments. The SISR and MFSR networks have similar network structures with the proposed SR reconstruction network in Sec. 3.3. In addition, we compare our TDAN models with different numbers: 2, 3, 4, and 5 of deformable convolutional layers, and we denote these networks as D2, D3, D4, and D5, respectively. <ref type="figure">Figure 4</ref> illustrates convergence curves of the SISR, MFSR, D2, D3, D4, and D5 networks. We can see that the MFSR outperforms the SISR; D2, D3, D4, and D5 are better than the MFSR; more deformable layers in the proposed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Real-World Examples</head><p>To further validate the robustness of the proposed TDAN, we also conduct experiments on two real-world video sequences: bldg and CV Book from <ref type="bibr" target="#b13">[14]</ref>. For the two sequences, the HR video frames are not available, and the degradation methods are unknown. We compare the proposed TDAN with TOFlow on the BI configuration and DUF on the BD configuration. As illustrated in <ref type="figure">Fig. 5</ref>, we find that the proposed TDAN can produce sharper edges (see bldg results) and more image details (see CV Book results) than the compared state-of-the-art VSR networks.</p><p>The comparison results show that our TDAN can robustly handle even unknown degradation, which further demonstrates the superiority of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitation and Failure Exploration</head><p>As discussed in Sec. 4.2, the resolution of HR video frames in Vimeo Super-Resolution dataset is only 448 ? 256. It is hard to train a very deep network on the dataset for recovering even finer image structures and details. One failure case of the TDAN is shown in <ref type="figure">Fig. 6</ref>. We can see that the TDAN fails to recover the structures in the building, but the very deep SISR network: RCAN trained on the DIV2K dataset can accurately reconstruct them, which demonstrates that the LR reference frame can provide enough cues for restoring the structures without requiring additional information from the LR supporting frames. Therefore, it is worth to build a publicly available large VSR dataset with HR (e.g. 1080P, 2K, even 4K) video frames for training very deep VSR architectures.</p><p>In Sec. 3.3, we adopt a naive and simple method to perform temporal fusion, which directly concatenates a LR reference frame and the corresponding LR aligned frames, and then uses a convolutional layer to obtain temporally fused features. A more advanced fusion network may further improve the VSR performance of the TDAN.</p><p>In our initial design, we directly took the output feature map F LR i (see Eq. 3) of the deformable alignment module and the feature map F LR t of the LR reference frame as inputs to a SR reconstruction network without predicting the aligned frames and the loss term L align . With this design, it will become a unified VSR framework without explicitly intermediate temporal alignment. However, the performance of the framework is even worse than the baseline model: MFSR. Unlike high-level problems in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2]</ref>, the deformable convolution taking features from pre-trained strong backbones as inputs is much easier to learn to capture motions of objects. Therefore, we think that the loss L align is important for this end-to-end VSR network.</p><p>In the TDAN, we use the LR reference frame as the label to define the L align . However, the LR reference frame is not exactly same as real aligned LR frames, which will make the label be noisy. Robust algorithms like <ref type="bibr" target="#b19">[20]</ref> for learning under label noise can be considered to further improve the L align .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a one-stage temporal alignment network: TDAN for video super-resolution. Unlike previous optical flow-based methods, which splits the temporal alignment problem into two sub-problems: motion estimation and motion compensation, the TDAN implicitly captures motion cues via a deformable sampling module at the feature level and directly predicts aligned LR video frames from sampled features without image-wise wrapping operations. In addition, the TDAN is capable of exploring image contextual information. With the advanced one-stage temporal alignment design and the strong exploration capability, the proposed TDAN-based VSR framework outperforms the compared flow-based state-of-the-art VSR networks. In the future, we would like to adopt the proposed TDAN to solve other video restoration tasks, such as video denoising, video deblurring, and video frame interpolation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. 4.2. Furthermore, ablation studies on different settings of our TDAN are shown in Sec. 4.3. Finally, some results on real-world videos are illustrated in Sec. 4.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .Figure 6 .</head><label>456</label><figDesc>Convergence analysis of TDAN with different numbers of the deformable convolutional layers and two baselines: SISR and MFSR. The curves show PSNR values of different models on the validation set in 200 epochs. Visual results on real-world video sequences with scaling factor 4. The two rows show VSR results for videos bldg and CV Book from [14], respectively. A failure case of the TDAN. The very deep SISR network: RCAN trained on the DIV2K can accurately recover the structures of the shown image region in the city video frame, but TOFlow and TDAN failed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>PSNR (dB) and SSIM of the TOFlow<ref type="bibr" target="#b31">[32]</ref> and our method on Vid4 dataset with an upscale factor 4 on the BI configuration. Note that, the first and last three frames are not used for evaluation since the TOFlow does not predict them; we used the same training dataset as the TOFlow.</figDesc><table><row><cell>PSNR</cell><cell>City</cell><cell cols="4">Walk Calendar Foliage Avg.</cell></row><row><cell cols="3">TOFlow 26.77 29.05</cell><cell>22.54</cell><cell>25.32</cell><cell>25.92</cell></row><row><cell>TDAN</cell><cell cols="2">26.97 29.48</cell><cell>22.98</cell><cell>25.50</cell><cell>26.23</cell></row><row><cell>SSIM</cell><cell>City</cell><cell cols="4">Walk Calendar Foliage Avg.</cell></row><row><cell cols="3">TOFlow 0.740 0.880</cell><cell>0.731</cell><cell>0.710</cell><cell>0.765</cell></row><row><cell>TDAN</cell><cell cols="2">0.756 0.889</cell><cell>0.7556</cell><cell>0.717</cell><cell>0.779</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>show quantitative comparisons on the BI configuration. Note that our TDAN used the same training dataset as the TOFlow, PSNR Bicubic VSRnet [10] ESPCN [23] VESCPN [3] Liu et al. [17] DBPN [6] RDN [35] RCAN<ref type="bibr" target="#b33">[34]</ref> TDAN PSNR (dB) and SSIM of different networks on Vid4 dataset with upscale factor 4 on BI configuration. Note that, the first and last two frames are not used for evaluation as Liu et al.<ref type="bibr" target="#b16">[17]</ref>. The top-2 results are highlighted with red and blue colors. Compared with recent state-of-the-art SISR and VSR methods on BI configuration, our method can achieve the best performance.</figDesc><table><row><cell>City</cell><cell>25.13</cell><cell>25.62</cell><cell>25.82</cell><cell>26.17</cell><cell>26.50</cell><cell>25.80</cell><cell>26.04</cell><cell>26.06</cell><cell>26.99</cell></row><row><cell>Walk</cell><cell>26.06</cell><cell>27.54</cell><cell>28.05</cell><cell>28.31</cell><cell>28.35</cell><cell>28.64</cell><cell>28.61</cell><cell>28.64</cell><cell>29.50</cell></row><row><cell>Calendar</cell><cell>20.54</cell><cell>21.34</cell><cell>21.76</cell><cell>21.98</cell><cell>22.13</cell><cell>22.29</cell><cell>22.32</cell><cell>22.33</cell><cell>22.98</cell></row><row><cell>Foliage</cell><cell>23.50</cell><cell>24.41</cell><cell>24.58</cell><cell>24.91</cell><cell>25.09</cell><cell>24.73</cell><cell>24.76</cell><cell>24.77</cell><cell>25.51</cell></row><row><cell>Avg.</cell><cell>23.81</cell><cell>24.73</cell><cell>25.06</cell><cell>25.34</cell><cell>25.52</cell><cell>25.37</cell><cell>25.43</cell><cell>25.45</cell><cell>26.24</cell></row><row><cell>SSIM</cell><cell cols="9">Bicubic VSRnet [10] ESPCN [23] VESCPN [3] Liu et al. [17] DBPN [6] RDN [35] RCAN [34] TDAN</cell></row><row><cell>City</cell><cell>0.601</cell><cell>0.654</cell><cell>0.670</cell><cell>0.696</cell><cell>0.723</cell><cell>0.682</cell><cell>0.688</cell><cell>0.694</cell><cell>0.757</cell></row><row><cell>Walk</cell><cell>0.798</cell><cell>0.844</cell><cell>0.859</cell><cell>0.861</cell><cell>0.863</cell><cell>0.872</cell><cell>0.872</cell><cell>0.873</cell><cell>0.890</cell></row><row><cell>Calendar</cell><cell>0.571</cell><cell>0.644</cell><cell>0.679</cell><cell>0.691</cell><cell>0.707</cell><cell>0.715</cell><cell>0.721</cell><cell>0.723</cell><cell>0.756</cell></row><row><cell>Foliage</cell><cell>0.566</cell><cell>0.645</cell><cell>0.652</cell><cell>0.673</cell><cell>0.701</cell><cell>0.661</cell><cell>0.663</cell><cell>0.664</cell><cell>0.717</cell></row><row><cell>Avg.</cell><cell>0.634</cell><cell>0.697</cell><cell>0.715</cell><cell>0.730</cell><cell>0.748</cell><cell>0.732</cell><cell>0.736</cell><cell>0.738</cell><cell>0.780</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>PSNR (dB) and SSIM of different networks with upscale factor 4 on the BD configuration. Values of the FSRVSR are from the original paper.</figDesc><table><row><cell>Methods</cell><cell cols="5">RDN RCAN TOFlow FRVSR DUF TDAN</cell></row><row><cell cols="2">Param./M 22.30 15.50</cell><cell>6.20</cell><cell>2.00</cell><cell>1.90</cell><cell>1.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Parameter numbers (?10 6 ) of several networks with leading VSR performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Visual comparisons for 4? VSR on the BI and BD configurations. We observe that the proposed TDAN restores better image structures and details than other state-of-the-art VSR networks, which demonstrates the strong capability of the TDAN in temporal alignment leveraging informative pixels from LR supporting frames. TDAN the better VSR performance. These observations demonstrate that exploiting supporting frames even without temporal alignment can improve VSR performance; the proposed temporal alignment network is effective in utilizing information from supporting frames; more deformable layers can enhance the capability of TDAN. For setting TDAN with comparable model size as FRVSR and DUF, we used D4 in our experiments. From qualitative and quantitative comparisons in Sec. 4.2, we find that even D4 has achieved state-of-the-art VSR performance.</figDesc><table><row><cell></cell><cell>HR</cell><cell>Bicubic</cell><cell>VSRnet [10]</cell><cell>VESCPN [3]</cell><cell>Liu et al. [17]</cell></row><row><cell>City/BI</cell><cell>DBPN [6]</cell><cell>RDN [35]</cell><cell>RCAN [34]</cell><cell>TOFlow [32]</cell><cell>TDAN</cell></row><row><cell></cell><cell>HR</cell><cell>Bicubic</cell><cell>VSRnet [10]</cell><cell>VESCPN [3]</cell><cell>Liu et al. [17]</cell></row><row><cell>Calendar/BI</cell><cell>DBPN [6]</cell><cell>RDN [35]</cell><cell>RCAN [34]</cell><cell>TOFlow [32]</cell><cell>TDAN</cell></row><row><cell></cell><cell>HR</cell><cell>Bicubic</cell><cell>SPMC [27]</cell><cell>DUF [9]</cell><cell>TDAN</cell></row><row><cell>Walk/BD</cell><cell>HR</cell><cell>Bicubic</cell><cell>SPMC [27]</cell><cell>DUF [9]</cell><cell>TDAN</cell></row><row><cell></cell><cell>HR</cell><cell>Bicubic</cell><cell>SPMC [27]</cell><cell>DUF [9]</cell><cell>TDAN</cell></row><row><cell>Foliage/BD</cell><cell>HR</cell><cell>Bicubic</cell><cell>SPMC [27]</cell><cell>DUF [9]</cell><cell>TDAN</cell></row><row><cell>Figure 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although Sajjadi et al. provided the URLs of used videos, some videos are not accessible, and the raw data needs additional pre-processing and cleaning.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was supported in part by NSF IIS-1813709, IIS-1741472, and CHE-1764415. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving frame interpolation with spatial motion smoothing for pixel domain distributed video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ascenso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th EURASIP Conference on Speech and Image Processing</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalized video deblurring for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5426" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep video superresolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video superresolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition (CVPR) workshops</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video denoising, deblocking, and enhancement through separable 4-d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3952" to="3966" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fast image and video denoising via nonlocal means of similar neighborhoods. IEEE signal processing letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="839" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Framerecurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6626" to="6634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04768</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09078</idno>
		<title level="m">Video enhancement with task-oriented flow</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03344</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
