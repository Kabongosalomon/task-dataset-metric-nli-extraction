<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACNET: ATTENTION BASED NETWORK TO EXPLOIT COMPLEMENTARY FEATURES FOR RGBD SEMANTIC SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Optical Science and Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Optical Science and Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Optical Science and Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Optical Science and Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ACNET: ATTENTION BASED NETWORK TO EXPLOIT COMPLEMENTARY FEATURES FOR RGBD SEMANTIC SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Attention</term>
					<term>Complementary</term>
					<term>RGBD se- mantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Compared to RGB semantic segmentation, RGBD semantic segmentation can achieve better performance by taking depth information into consideration. However, it is still problematic for contemporary segmenters to effectively exploit RGBD information since the feature distributions of RGB and depth (D) images vary significantly in different scenes. In this paper, we propose an Attention Complementary Network (ACNet) that selectively gathers features from RGB and depth branches. The main contributions lie in the Attention Complementary Module (ACM) and the architecture with three parallel branches. More precisely, ACM is a channel attention-based module that extracts weighted features from RGB and depth branches. The architecture preserves the inference of the original RGB and depth branches, and enables the fusion branch at the same time. Based on the above structures, ACNet is capable of exploiting more high-quality features from different channels. We evaluate our model on SUN-RGBD and NYUDv2 datasets, and prove that our model outperforms state-of-the-art methods. In particular, a mIoU score of 48.3% on NYUDv2 test set is achieved with ResNet50. We will release our source code based on PyTorch and the trained segmentation model at https://github.com/anheidelonghu/ACNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Semantic segmentation is a basic task of computer vision, whose purpose is to partition an image into several coherent semantically-meaningful parts. Compared with traditional approaches that need to be deployed in complex separate ways, semantic segmentation can be utilized to unify diverse detection tasks desired by navigation systems, at least in standard outdoor conditions <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>.</p><p>In contrast, indoor semantic segmentation that has not been thoroughly investigated, remains challenging in several This work has been partially funded through the project Research on Vision Sensor Technology Fusing Multidimensional Parameters (111303-I21805) by Hangzhou SurImage Technology Co., Ltd and supported by Hangzhou KrVision Technology Co., Ltd (krvision.cn). The authors would like to acknowledge Juan Wang for the GPU support. aspects. For example, it suffers from uneven illumination and messy spatial overlapping. With the emergence and development of RGBD cameras (such as RealSense, Kinect, Xition etc.), indoor semantic segmentation can be benefited from RGBD observations that encode real-world geometric information, which theoretically leads to better segmentation performance compared to RGB semantic segmentation. Towards this end, there were a few attempts like <ref type="bibr" target="#b2">[3]</ref>[4] that treated depth image as an additional channel, and used the method similar to RGB semantic segmentation to implement RGBD semantic segmentation. In <ref type="bibr" target="#b4">[5]</ref>, two neural networks branches were designed for RGB input and depth input, which were merged before upsampling. In <ref type="bibr" target="#b2">[3]</ref>, depth images was decomposed into three channels, namely disparity, height and angle which were also treated as an RGB image. In <ref type="bibr" target="#b5">[6]</ref>, instead of simply using traditional convolution neural networks, novel graph-based networks were applied to excavate scene geometric information more sufficiently. In <ref type="bibr" target="#b6">[7]</ref>, traditional convolution was modified according to depth values. These networks designed for RGBD semantic segmentation have achieved break-through results. However, there are still some issues that need to be solved:</p><p>? Although the geometric information encoded in the depth image can clearly provide additional benefits for image segmentation, the information contained in RGB image and depth image are not equivalent for each scene (shown in <ref type="figure" target="#fig_0">Fig. 1</ref>). In other words, features extracted from RGB branch and depth branch by current networks may be not appropriate.</p><p>? Conventional RGBD segmentation network can be divided into two types of architectures. One of them, such as <ref type="bibr" target="#b7">[8]</ref>, employs two encoders to extract features from RGB and depth image respectively, and combines the features of both before or during upsampling. The other like <ref type="bibr" target="#b4">[5]</ref>[9] just fuses the RGBD features at the downsampling stage. The former can't sufficiently combine RGBD information, and the latter tends to lose original RGB and depth branches since the fusion branches take the place of them.</p><p>In this paper, we propose ACNet (shown in <ref type="figure" target="#fig_1">Fig. 2</ref>) to combine RGB and depth features by a proportion determined by the input. In ACNet, there are two independent branches based on ResNet <ref type="bibr" target="#b9">[10]</ref> to extract features for RGB and depth image separately. Several Attention Complementary Module (ACMs) are designed to obtain features from the aforementioned branches, which are determined by the amount of information they carry. There's another branch based on ResNet to process the merged features. The proposed architecture is able to keep original RGBD features flow as well as to utilize merged features in an integrated network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FRAMEWORK</head><p>Attention Complementary Module (ACM). As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the information contained in RGB image and depth image vary in different regions of indoor scenes. In order to gather features selectively from RGB branch and depth branch, we have designed a set of attention modules <ref type="bibr" target="#b10">[11]</ref> to make the network focus on more informative regions. More precisely, the proposed ACM is based on channel attention <ref type="bibr" target="#b11">[12]</ref> (shown in <ref type="figure" target="#fig_2">Fig. 3</ref>). Assuming the input feature maps A = [A 1 , ? ? ? , A C ] ? R C?H?W , we first apply global average pooling, to have the output Z ? R C?1?1 , where C denotes the number of channels, H, W denote the height and width of feature maps respectively. The k-th (k ? [1, C]) of Z can be expressed as:</p><formula xml:id="formula_0">Z k = 1 H ? W H i W j A k (i, j)<label>(1)</label></formula><p>Then Z is reorganized by a 1 ? 1 convolution layer with the same number of channels as Z. A 1 ? 1 convolution layer is able to excavate correlations between channels, thus eliciting an appropriate weight distribution for these channels. A sigmoid function is applied to activate the convolution result, constraining the value of weight vector V ? R C?1?1 between 0 and 1. Finally, we perform an outer product for A and V , and the result U ? R C?H?W can be expressed as:</p><formula xml:id="formula_1">U = A ? ?[?(Z)]<label>(2)</label></formula><p>where ? denotes outer product, ? denotes sigmoid function, and ? denotes 1 ? 1 convolution. In this way, feature maps U are converted into new feature maps U , which contain more valid information. Architecture for Feature Fusion. A majority of stateof-the-art RGBD semantic segmentation networks use an encoder that fuses RGBD features either too early or too late <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b7">[8]</ref>, which ruins the original RGB and depth information or results in a low efficiency of exploiting the carried information. In order to keep the original RGB and depth features flow during downsampling, we propose a specialized architecture for RGBD feature fusion. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, two complete ResNets are deployed to extract RGB and depth features separately. Note that here the ResNet can be replaced with other networks, e.g., ERF-PSPNet <ref type="bibr" target="#b1">[2]</ref> in efficiency-critical domains. Vitally, these two branches can preserve RGB and depth features before upsampling. After that, the fusion branch is leveraged to extract features from the merged feature maps.</p><p>Attention Complementary Network (ACNet). We design an integrated network called ACNet for RGBD semantic segmentation. The backbone of ACNet is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. RGB image and depth image are inputted, and are processed by ResNet branches separately. During inference, each aforementioned branch provides a group of feature maps at every module stage, such as Conv, Layer1, etc. Then the feature maps are reorganized by ACM. After passing through Conv, the feature maps are further element-wisely added as input of fusion branch, while others are added to the output of fusion branch. In this way, both low-level and high-level features can be extracted, reorganized and fused by our ACNet. As for upsampling, we apply the skip connection like <ref type="bibr" target="#b4">[5]</ref>, which appends the features in downsampling to upsampling with a quite low computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We evaluate our method on two public datasets:</p><p>NYUDv2 <ref type="bibr" target="#b12">[13]</ref>: The NYU-Depth V2 data set (NYUDv2) contains 1,449 RGBD images with dense pixel-wise annotation. We divide the dataset into 795 training images and 654 testing images according to the official setting. We use the version with annotations on 40 classes (common ones used in the literature).</p><p>SUN-RGBD <ref type="bibr" target="#b13">[14]</ref>: We use SUN-RGBD V1 which have 37 categories and contains 10,335 RGBD images with dense pixel-wise annotations, 5,285 images for training and 5,050 for testing.</p><p>As for metrics, we use the prevailing mean Intersectionover-Union over all classes (mIoU) to evaluate the performance of different semantic segmenters. Implementation Details. As for data augmentation, we apply random scaling, cropping and flipping to both RGB and depth images, and normalize them separately. For RGB images, we also randomly change their color in HSV space. For all the experiments, we use ResNet50 as the encoder, which is pre-trained on ImageNet <ref type="bibr" target="#b14">[15]</ref>. Since depth image contains one channel, we average the three channels in ResNet50's first layer to one channels for the depth branch. We use focal loss <ref type="bibr" target="#b15">[16]</ref> with the focusing parameter ? = 2 to supervise the training of our network. During training stage, we also calculate the average loss of outputs from up1 to up5 in <ref type="figure" target="#fig_1">Fig. 2</ref> to better optimize our network. During testing stage, we only evaluate the metrics of last output to ensure consistency with the state of the art. We use SGD optimizer with initial learning rate 0.002, momentum 0.9 and weight decay 0.004. Batch size is set to 4 when training on one NVIDIA TITAN Xp. The learning rate is multiplied by 0.8 for every 20 iterations on SUN-RGBD and 100 iterations for NYUDv2.</p><p>Analysis of the ACM. To understand ACM better, we visualize the feature maps from layer2 (shown in <ref type="figure" target="#fig_3">Fig. 4</ref>) since layer2's low-level features are more consistent with visual intuitions. Note that we only visualize the first 16 of 128 feature maps for better illustration. Regarding the weights as matrices starting from (0,0), they correspond to the features maps one by one, where we can find some insightful results. At <ref type="figure">Fig. 5</ref>. Quantitative analysis of all ACM in our model (0,0), feature map of RGB branch contains more valid information than the feature map from depth branch visually, so that ACM tends to give a higher weight to the RGB branch. In contrast, at (2,2), feature map of depth branch contains more information, therefore, depth branch gets higher weight. Finally, feature maps of the two branches are element-wisely added into feature maps of fusion branch to supplement the RGBD information.</p><p>We also evaluate weights generated by ACM at all levels (shown in <ref type="figure">Fig. 5</ref>). First, we focus on averages of weights, which indicate the amount of information contained in the feature maps before inputting to ACM. The averages of RGB branches' weights are higher than those of depth branches in Conv and Layer1. This reveals that RGB branches always contain more valid information at lower levels since RGB images often contain many redundant textures. In addition, weights of the two branches in Layer2, Layer3 and Layer4 are queie close, which means RGB and D branches contain nearly equivalent valid information at higher levels. In addition, the average weights of Layer4 are quite low, indicating that the fusion branch might gather enough features. The metrics: std (standard deviation), min and max can reflect the distribution of information across channels. From Conv to Layer3, the std becomes increasingly smaller, which indicates that ACNet can homogenize the distribution of information. However, Layer4 should decide to select useful features as well as eliminate redundant features as it is the last module in the encoder. Therefore, the std of Layer4 is quite high. This experiment demonstrates that our network flattens the distribution of information across channels where complementary features are effectively exploited, which is essential for RGBD semantic segmentation.</p><p>Ablation Study. To verify functionality of both ACM and the multi-branch architecture, we perform an ablation study by comparing the original model with two defective models: Model-1 and Model-2. In Model-1, we remove all ACMs and the RGB and D branches after Conv Layer. In Model-2, we remove all ACMs but retain the multi-branch architecture. Our ablation study on NYUDv2 turns out that, the mIoU of Model-1 and Model-2 are 44.3% and 46.8%, verifying the multi-branch architecture and ACM lead to significant accuracy boost of 2.5% and 1.5%, respectively.</p><p>Comparison with state-of-the-art networks. We com- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>NYUDv2 SUN-RGBD 3DGNN <ref type="bibr" target="#b5">[6]</ref> 39.9% 44.1% RefineNet (ResNet152) <ref type="bibr" target="#b16">[17]</ref> 46.5% 45.9% Depth-aware CNN <ref type="bibr" target="#b6">[7]</ref> 43.9% 42.0% LSD <ref type="bibr" target="#b7">[8]</ref> 45.9% -CFN (VGG-16) <ref type="bibr" target="#b17">[18]</ref> 41.7% 42.5% CFN (RefineNet-152) <ref type="bibr" target="#b17">[18]</ref> 47.7% 48.1% ACNet (ResNet-50)</p><p>48.3% 48.1%</p><p>pare our ACNet with state-of-the-art methods to prove its effectiveness. Note we adopt the most universally-used mIoU as the evaluation metric. <ref type="table" target="#tab_0">Table 1</ref> shows the result of our ACNet on NYUDv2 and SUN-RGBD test sets. The result shows that on NYUDv2, our ACNet outperforms other state-of-the-art models by 0.6%, yielding the new record of mIoU accuracy 48.3% on NYUDv2. On SUN-RGBD, our model (ResNet-50) is able to reach the same mIoU as CFN (RefineNet-152) <ref type="bibr" target="#b17">[18]</ref> by using a more lightweight backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we propose a novel multi-branch attention based network for RGBD semantic segmentation. The multi-branch architecture is able to gather features efficiently and doesn't destroy original RGB and depth branches' inference. The attention module can selectively gather features from RGB and depth branches according to the amount of information they contain, and complement the fusion branch by using these weighted features. Our model can resolve the problem that RGB images and depth images always contain unequal amount of information as well as different context distributions. We evaluate our model on NYUDv2 and SUN-RGBD datasets, and the experiments show that our model can outperform state-of-the-art methods.</p><p>In the future, we will explore ways to improve the realtime performance of pixel-wise image segmentation not only for RGBD semantic cognition but also for panoramic annular surrounding perception.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>RGB and depth images have different distributions of features that are appropriately exploited by our ACNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overview of our proposed ACNet. RGB image and depth image are processed by two ResNets separately. Red arrows represent the data flow of feature maps reorganized by ACM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Attention Complementary Module (ACM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>How ACM fuses complementary RGBD features into fusion branch. * denotes element-wise product and ? denotes element-wise add. The feature maps are visualized from layer2. (f) and (g) depict the weights calculated from the feature maps by ACM, which are multiplied to feature maps separately, and added into the merged features from the fusion branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with other state-of-the-art methods on NYUDv2 test set and SUN-RGBD test set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unifying terrain awareness through real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxue</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Can we pass beyond the field of view? panoramic annular semantic segmentation for realworld surrounding perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Std2p: Rgbd semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saarland Informatics</forename><surname>Campus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7158" to="7167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rednet: Residual encoder-decoder network for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lunan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01054</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Depthaware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06791</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Localitysensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Squeeze-andexcitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1320" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
