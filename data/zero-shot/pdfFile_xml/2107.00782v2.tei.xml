<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Polarized Self-Attention: Towards High-quality Pixel-wise Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqiang</forename><surname>Liu</surname></persName>
							<email>fuqiangl@andrew.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
							<email>donghuang@cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Polarized Self-Attention: Towards High-quality Pixel-wise Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pixel-wise regression is probably the most common problem in fine-grained computer vision tasks, such as estimating keypoint heatmaps and segmentation masks. These regression problems are very challenging particularly because they require, at low computation overheads, modeling long-range dependencies on high-resolution inputs/outputs to estimate the highly nonlinear pixel-wise semantics. While attention mechanisms in Deep Convolutional Neural Networks(DCNNs) has become popular for boosting long-range dependencies, element-specific attention, such as Nonlocal blocks, is highly complex and noise-sensitive to learn, and most of simplified attention hybrids try to reach the best compromise among multiple types of tasks. In this paper, we present the Polarized Self-Attention(PSA) block that incorporates two critical designs towards highquality pixel-wise regression: (1) Polarized filtering: keeping high internal resolution in both channel and spatial attention computation while completely collapsing input tensors along their counterpart dimensions. (2) Enhancement: composing non-linearity that directly fits the output distribution of typical fine-grained regression, such as the 2D Gaussian distribution (keypoint heatmaps), or the 2D Binormial distribution (binary segmentation masks). PSA appears to have exhausted the representation capacity within its channel-only and spatial-only branches, such that there is only marginal metric differences between its sequential and parallel layouts. Experimental results show that PSA boosts standard baselines by 2 ? 4 points, and boosts stateof-the-arts by 1 ? 2 points on 2D pose estimation and semantic segmentation benchmarks. Codes are released 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent trends from the coarse-grained (such as imagewise classification <ref type="bibr" target="#b37">[38]</ref> and bounding box detection <ref type="bibr" target="#b14">[15]</ref>) to the fine-grained computer vision tasks (such as keypoint estimation <ref type="bibr" target="#b30">[31]</ref> and segmentation segmentation <ref type="bibr" target="#b62">[62]</ref>) have <ref type="bibr">Figure 1</ref>. Polarized Self-Attention(PSA) block (ours), keeps high internal resolution along the channel (C/2) and spatial dimension([W, H]) while collapses the input tensor [C ? W ? H] along their counterparts dimensions, and fits output distributions of pixel-wise regression with a softmax-sigmoid composition. At minor computation-memory overheads upon the vanilla DC-NNs, PSA produces significantly higher-quality person keypoint heatmaps and semantic segmentation masks (also see <ref type="table" target="#tab_2">Table 2</ref>-3 for the boosts in metrics). received booming advances in both research and industrial communities. Comparing to the coarse-grained tasks, perception at the pixel-wise level is increasingly appealing in autonomous driving <ref type="bibr" target="#b42">[42]</ref>, augment reality <ref type="bibr" target="#b6">[7]</ref>, medical image processing <ref type="bibr" target="#b28">[29]</ref>, and public surveillance <ref type="bibr" target="#b46">[46]</ref>.</p><p>The goal of the pixel-wise regression problem is to map every image pixels of the same semantics to the same scores. For instance, mapping all the background pixels to 0 and all the foreground pixels to their class indices, respectively. Two typical tasks are keypoint heatmap regression arXiv:2107.00782v2 [cs.CV] 8 Jul 2021 and segmentation mask regression. Most DCNN models for regression problems take an encoder-decoder architecture. The encoder usually consists of a backbone network, such as ResNet <ref type="bibr" target="#b17">[18]</ref>, that sequentially reduces the spatial resolution and increases the channel resolution, while the decoder usually contains de-convolution/up-sampling operations that recover the spatial resolution and decrease the channel resolution. Typically the tensor connecting the encoder and decoder has an element number smaller than both the input image tensor and the output tensor. The reduction of elements is necessary for computation/memory efficiency and stochastic optimization reasons <ref type="bibr" target="#b15">[16]</ref>. However, the pixel appearances and patch shapes of the same semantics are highly nonlinear in nature and therefore difficult to be encoded with a reduced number of features. Moreover, high input-output resolutions are preferred for fine details of objects and object parts <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b44">44]</ref>. Comparing to the image classification task where an input image is collapsed to an output vector of class indices, the pixel-wise regression problem has a higher problem complexity by the order of output element numbers. From the model design perspective, the pixel-wise regression problem faces special challenges: (1) Keeping high internal resolution at a reasonable cost; <ref type="bibr" target="#b1">(2)</ref> Fitting output distribution such as that of the keypoint heatmaps or segmentation masks. Based on the tremendous success in new DCNNs architectures, we focus on a plug-and-play solution that could consistently improve an existing (vanilla) network, i.e., inserting attention blocks <ref type="bibr" target="#b43">[43]</ref>  <ref type="bibr" target="#b2">[3]</ref>. Most of above hybrids try to reach the best compromise among multiple types of tasks, for instance, image classification, object detection, as well as for instance segmentation. These generalized goals are partially the reason that channel-only attention (SE <ref type="bibr" target="#b19">[20]</ref>, GE <ref type="bibr" target="#b18">[19]</ref> and GCNet <ref type="bibr" target="#b2">[3]</ref>) are among the most popular blocks. Channel-only attention blocks put the same weights on different spatial locations, such that the classification task still benefits since its spatial information eventually collapses by pooling, and the anchor displacement regression in object detection benefits since the channel-only attention unanimously highlights all foreground pixels. Unfortunately, due to critical differences in attention designs, the channel-spatial compositional attention blocks, (e.g., DA <ref type="bibr" target="#b13">[14]</ref>, CBAM <ref type="bibr" target="#b48">[48]</ref>), did not show significant overall advantages from the latest channel-only attentions such as GCNet <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this paper, we present the Polarized Self-Attention (PSA) block (See <ref type="figure">Figure 1)</ref> for high-quality pixel-wise regression.</p><p>To preserve the potential loss of highresolution information in vanilla/baseline DCNNs by pooling/downsampling, PSA keeps the highest internal resolution in attention computation among existing attention blocks (see also <ref type="table" target="#tab_1">Table 1</ref>). To fitting the output distribution of typical fine-grained regression, PSA fuse softmax-sigmoid composition in both channel-only and spatial-only attention branches. Comparing to existing channel-spatial compositions <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b13">14]</ref> that favor particular layouts, there is only marginal metric differences between PSA layouts. This indicates PSA may have exhausted the representation capacity within its channel-only and spatial-only branches. We conducted extensive experiments to demonstrate the direct performance gain of PSA on standard baselines as well as state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Pixel-wise Regression Tasks: The advances of DCNNs for pixel-wise regression are basically pursuing higher resolution. For body keypoint estimation, Simple-Baseline <ref type="bibr" target="#b51">[51]</ref> consists of conventional components ResNet+deconvolution. HRnet <ref type="bibr" target="#b40">[40]</ref> address the resolution challenge of Simple-Baseline with 4 parallel high-to-low resolution branches and their pyramid fusion. Other most recent variants, DARK-Pose <ref type="bibr" target="#b56">[56]</ref> and UDP-Pose <ref type="bibr" target="#b20">[21]</ref>, both compensate for the loss of resolution due to the preprocessing, post-processing, and propose techniques to achieve a sub-pixel estimation of keypoints. Note that, besides the performance gain among network designs, the same models with and 388 ? 284 inputs are usually better than that with 256 ? 192 inputs. This constantly reminds researchers of the importance of keeping high-resolution information. For Semantic segmentation, <ref type="bibr" target="#b3">[4]</ref> introduces atrous convolution in the decoder head of Deeplab for wide receptive field on high-resolution inputs. To overcome the limitation of ResNet backbones in Deeplab, all the latest advances are based on HRnet <ref type="bibr" target="#b44">[44]</ref>, in particular, HRNet-OCR <ref type="bibr" target="#b41">[41]</ref> and its variants are the current state-of-the-art. There are many other multitask architecture <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b5">6]</ref> that include pixelwise regression as a component.</p><p>PSA further pursues the high-resolution goals of the above efforts from the attention perspective and further boosts the above DCNNs.</p><p>Self-attention and its Variants. Attention mechanisms have been introduced into many visual tasks to address the weakness of standard convolutions <ref type="bibr" target="#b34">[35]</ref>[2][1][37] <ref type="bibr" target="#b2">[3]</ref>. In the self-attention mechanism, each input tensor is used to compute an attention tensor and is then re-weighted by this attention tensor. Self-attention <ref type="bibr" target="#b43">[43]</ref>[35] <ref type="bibr" target="#b7">[8]</ref> emerged as a standard component to capture long-range interactions, after it success in sequence modeling and generative modeling tasks. Cordonnier et al. <ref type="bibr" target="#b7">[8]</ref> has proven that a multi-head self-attention layer with a sufficient number of heads is at least as expressive as any convolutional layer. In some vision tasks, such as object detection and image classification, self-attention augmented convolution models <ref type="bibr" target="#b1">[2]</ref> or standalone self-attention models <ref type="bibr" target="#b36">[37]</ref> have yielded remarkable gains. While most self-attention blocks were inserted after convolution blocks, attention-augmented convolution <ref type="bibr" target="#b1">[2]</ref> demonstrates that parallelizing the convolution layer and attention block is a more powerful structure to handle both short and long-range dependency.</p><p>PSA advances self-attention for pixel-wise regression and could also be used in other variants such as the convolution-augmented attentions.</p><p>Full-tensor and simplified attention blocks. The basic non-local block (NL) <ref type="bibr" target="#b47">[47]</ref> and its variants, such as a residual form <ref type="bibr" target="#b59">[59]</ref> second-order non local <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b50">[50]</ref>, and asymmetric non-local <ref type="bibr" target="#b64">[64]</ref>, produce full-tensor attentions and have successfully improved person re-identification, image super-resolution, and semantic segmentation tasks. To capture pair-wise similarities among all feature elements, the NL block computes an extremely large similarity matrix between the key feature maps and query feature maps, leading to huge memory and computational costs. EA <ref type="bibr" target="#b39">[39]</ref> produces a low-rank approximation of NL block for computation efficiency. BAM <ref type="bibr" target="#b32">[33]</ref>,DAN <ref type="bibr" target="#b13">[14]</ref> and CBAM <ref type="bibr" target="#b48">[48]</ref> produce different compositions of the channel-only and spatial-only attentions. Squeeze-and-Excitation (SENet) <ref type="bibr" target="#b19">[20]</ref>, Gather-Excite <ref type="bibr" target="#b18">[19]</ref> and GCNet <ref type="bibr" target="#b2">[3]</ref> only re-weight feature channels using signals aggregated from global context modeling. Most of above attention blocks were designed as a compromise among multiple types of tasks, and do not address the specific challenges in fine-grained regression.</p><p>PSA address the specific challenges in fine-grained regression by keeping the highest attention resolution among existing attention blocks, and directly fitting the typical output distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>Notations: 2 Denote X ? Cin?H?W ? as a feature tensor of one sample (e.g., one image), where C in , H, W are the number of elements along the height, width, and channel dimension of X, respectively. X = {x i } HW i=1 where x i ? Cin is a feature vector along the channel dimension. A self-attention block A(?) takes X as input, and produces a tensor Z as output, where Z ? Cout?H?W . A DCNN block is formulated as a nonlinear mapping ? : X ? Z. The possible operators of the network block include: the convolution layer W(?), the batch norm layer BN (?), the ReLU activation layer RU (?), softmax SM (?). Without losing generality, all the convolution layers in attention blocks are the (1 ? 1) convolution, denoted by W. For simplicity, we only consider the case where the input tensor X and output tensor Z of a DCNN block have the same dimension C ? H ? W (i.e., C in = C out ). <ref type="bibr" target="#b1">2</ref> All non-bold letters represent scalars. Bold capital letter X denotes a matrix; Bold lower-case letters x is a column vector. x i represents the i th column vector of the matrix X. x j denotes the j th element of x.</p><p>x, y? = x T y denotes the inner-product between two vectors or metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Attention for Pixel-wise Regression</head><p>A DCNN for pixel-wise regression learns a weighted combination of features along two dimensions: (1) channelspecific weighting to estimate the class-specific output scores; (2) spatial-specific weighting to detect pixels of the same semantics. The self-attention mechanism applied to the DCNN is expected to further highlight features for both above goals.</p><p>Ideally, with a full-tensor self-attention Z = A(X) X (A(X) ? C?H?W ), the highlighting could potentially be achieved at the element-wise granularity (C ? H ? W elements). However, the attention tensor A is very complex and noise-prone to learn directly. In the Non-Local selfattention block <ref type="bibr" target="#b47">[47]</ref>, A is calculated as,</p><formula xml:id="formula_0">A = W z (F sm (X T W T k W q X)W v X).<label>(1)</label></formula><p>There are four (1 ? 1) convolution kernels, i.e., W z ,W k , W q , and W v , that learns the linear combination of spatial features among different channels. Within the same channels, the HW ? HW outer-product between W k X and W q X activates any features at different spatial locations that have a similar intensity. The joint activation mechanism of spatial features is very likely to highlight the spatial noise. The only actual weights, Ws, are channel-specific instead of spatial-specific, making the Non-Local attention exceptionally redundant at the huge memory-consumption of the HW ? HW matrix. For efficient computation, reduction of NL leads to many possibilities: Low rank approximation of A (EA), Channel-only self-attention A ch ? C?1?1 that highlight the same global context for all pixels(GC <ref type="bibr" target="#b2">[3]</ref> and SE <ref type="bibr" target="#b18">[19]</ref> ), Spatial-only self-attention A sp ? 1?W ?H not powerful enough to be recognized as a standalone model, Channel-spatial composition A sp , where the parallel composition: Z = A ch ch X + A sp sp X and the sequential composition: Z = A ch ch (A sp sp X) introduce different order of non-linearity. Different conclusions were empirically drawn, such as CBAM <ref type="bibr" target="#b48">[48]</ref> (sequential&gt;parallel) and DA <ref type="bibr" target="#b13">[14]</ref> (parallel&gt;sequential), which partially indicates that the intended non-linearity of the tasks are not fully modeled within the attention blocks.</p><p>These issues are typical examples of general attention design that does not target the pixel-wise regression problem. With the help of <ref type="table" target="#tab_1">Table 1</ref>, we re-visit critical design aspects of existing attention blocks and raise challenges on how to achieve both channel-specific and spatialspecific weighting for pixel-wise regression. (All the attention blocks are compared with their top-performance configurations.)</p><p>Internal Attention Resolution. Recall that most pixelwise regression DCNNs use the same backbone networks, e.g., ResNet, as the classification (i.e., image recognition) and coordinate regression(i.e. bbox detection, instance segmentation) tasks. For robustness and computational effi-   <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b40">40]</ref>. Using these backbones in pixel-wise regression, selfattention blocks are expected to preserve high-resolution semantics in attention computation. However, in <ref type="table" target="#tab_1">Table 1</ref>, all the reductions of NL reach their top performance at a lower internal resolution. Since their performance metrics are far from perfect, the natural question to ask is: are there better non-linearity that could leverages higher resolution information in attention computation?</p><formula xml:id="formula_1">NL[47] C [W, H] SM C 2 W H + CW 2 H 2 GC [3] C/4 - SM+ReLU CW H SE [19] C/4 - ReLU+SD CW H CBAM [48] C/16 [W, H] SD CW H DA [14] C/8 [W, H] SM C 2 W H + CW 2 H 2 EA [39] d k ( C) dv ( min(W, H)) SM CW H PSA(ours) C/2 [W, H] SM+SD CW H</formula><p>Output Distribution/Non-linearity. In DCNNs for pixel-wise regression, outputs are usually encoded as 3D tensors. For instance, the 2D keypoint coordinates are encoded as a stack of 2D Gaussian maps [#keypoint type ? W ? H]. The pixel-wise class indices are encoded as a stack of binary maps [#semantic classes?W ?H] which follows the Binormial distribution. Non-linearity that directly fits the distribution upon linear transformations (such as convolution) could potentially alleviate the learning burden of DCNNs. The natural nonlinear functions to fit the above distributions are SoftMax for 2D Gaussian maps, and Sigmoid for 2D Binormial Distribution. However, none of the existing attention blocks in <ref type="table" target="#tab_1">Table 1</ref> contains such a combination of nonlinear functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Polarized Self-Attention (PSA) Block</head><p>Our solution to the above challenges is to conduct "polarized filtering" in attention computation. A self-attention block operates on an input tensor X to highlight or suppress features, which is very much like optical lenses filtering the light. In photography, there are always random lights in transverse directions that produce glares/reflections. Polarized filtering, by only allowing the light pass orthogonal to the transverse direction, can potentially improve the contrast of the photo. Due to the loss of total intensity, the light after filtering usually has a small dynamic range, therefore needs a additional boost, e.g. by High Dynamic Range (HDR), to recover the details of the original scene.</p><p>We borrow the key factors of photography, and propose the Polarized Self-Attention (PSA) mechanism: (1) Filtering: completely collapse features in one direction while preserving high-resolution in its orthogonal direction; (2) HDR: increase the dynamic range of attention by Softmax normalization at the bottleneck tensor (smallest feature tensor in attention block), followed by tone-mapping with the Sigmoid function. Formally, we instantiate the PSA mechanism as a PSA block below (also see diagram in <ref type="figure" target="#fig_1">Figure 2</ref>):</p><p>Channel-only branch A ch (X) ? C?1?1 :</p><formula xml:id="formula_2">A ch (X) = F SG W z|?1 (? 1 (W v (X))?F SM (? 2 (W q (X))) ,<label>(2)</label></formula><p>where W q , W v and W z are 1 ? 1 convolution layers respectively, ? 1 and ? 2 are two tensor reshape operators, and F SM (?) is a SoftMax operator and "?" is the matrix dotproduct operation F SM (X) = Np j=1 e x j Np m=1 e xm x j . The internal number of channels, between W v |W q and W z , is C/2. The output of channel-only branch is Z ch = A ch (X) ch X ? C?H?W , where ch is a channel-wise multiplication operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial-only branch A sp (X) ? 1?H?W :</head><p>A sp (X) = F SG ? 3 F SM (? 1 (F GP (W q (X))))?? 2 (W v (X)) ,</p><p>(3) where W q and W v are standard 1 ? 1 convolution layers respectively, ? 2 is an intermediate parameter for these channel convolutions, and ? 1 , ? 2 and ? 3 are three tensor reshape operators, and F SM (?) is the SoftMax operator. F GP (?) is a global pooling operator F GP (X) = 1 H?W H i=1 W j=1 X(:, i, j), and ? is the matrix dotproduct operation. The output of spatial-only branch is Z sp = A sp (X) sp X ? C?H?W , where sp is a spatialwise multiplication operator.</p><p>Composition: The outputs of above two branches are composed either under the parallel layout</p><formula xml:id="formula_3">P SA p (X) = Z ch + Z sp (4) = A ch (X) ch X + A sp (X) sp X,</formula><p>or under the sequential layout</p><formula xml:id="formula_4">P SA s (X) = Z sp (Z ch ) (5) = A sp (A ch (X) ch X) sp A ch (X) ch X.</formula><p>where "+" is the element-wise addition operator.</p><p>Relation of PSA to other Self-Attentions: We add PSA to <ref type="table" target="#tab_1">Table 1</ref> and make the following observations:</p><p>? Internal Resolution vs Complexity: Comparing to existing attention blocks under their top configuration, PSA preserves the highest attention resolution for both the channel (C/2) <ref type="bibr" target="#b2">3</ref> and spatial ([W, H]) dimension.</p><p>Moreover, in our channel-only attention, the Softmax re-weighting is fused with squeeze-excitation leveraging Softmax as the nonlinear activation at the bottleneck tensor of size C/2 ? W ? H. The channel numbers C-C/2-C follow a squeeze-excitation pattern that benefited both GC and SE blocks. Our design conducts higher-resolution squeeze-and-excitation while at comparable computation complexity of the GC block.</p><p>Our spatial-only attention not only keeps the full [W, H] spatial resolution, but also internally keeps 2?C?C/2 learnable parameters in W q and W v for the nonlinear Softmax re-weighting, which is more powerful structure than existing blocks. For instance, the spatial-only attention in CBAM is parameterized by a 7?7?2 convolution (a linear operator), and EA learns C ? d k + C ? d v parameters for linear re-weighting (d k , d v C ).</p><p>? Output Distribution/Non-linearity. Both the PSA channel-only and spatial-only branches use a Softmax-Sigmoid composition. Considering the Softmax-Sigmoid composition as a probability distribution function, both the multi-mode Gaussian maps (keypoint heatmaps) and the piece-wise Binomial maps (segmentation masks) can be approximated upon linear transformations, i.e. 1 ? 1 convolutions in PSA. We therefore expect the non-linearity could fully leverage the high resolution information preserved within in PSA attention branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Implementation details. For any baseline networks with the bottleneck or basic residual blocks, such as ResNet and HRnet, we add PSAs after the first 3 ? 3 convolution in every residual blocks, respectively. For 2D pose estimation, we kept the same training strategy and hyperparameters as the baseline networks. For semantic segmentation, we added a warming-up training phase of 5000 iterations, stretched the total training iteration by 30%, and kept all the rest training strategy and hyper-parameters of the baseline networks. Empirically, these changes allow PSA to train smoothly on semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">PSA vs. Baselines</head><p>We first add PSA blocks to standard baseline networks of the following tasks.</p><p>Top-Down 2D Human Pose Estimation: Among the DCNN approaches for 2D human pose estimation, the topdown approaches generally dominate the top metrics. This top-down pipeline consists of a person bounding box detector and a keypoint heatmap regressor. Specifically, we use the pipelines in <ref type="bibr" target="#b51">[51]</ref> and <ref type="bibr" target="#b40">[40]</ref> as our baselines. An input image is first processed by a human detector <ref type="bibr" target="#b51">[51]</ref> of 56.4AP (Average Precision) on MS-COCO val2017 dataset <ref type="bibr" target="#b27">[28]</ref>. Then all the detected human image patches are cropped from the input image and resized to 384 ? 288. Finally, the 384 ? 288 image patches are used for keypoint heatmap regression by a single person pose estimator. The output heatmap size is 96 ? 72.</p><p>We add PSA on Simple-Baseline <ref type="bibr" target="#b51">[51]</ref> with the Resnet50/152 backbones and HRnet <ref type="bibr" target="#b40">[40]</ref> with the HRnet-w32/w48 backbones. The results on MS-COCO val2017 are shown in <ref type="table" target="#tab_2">Table 2</ref>. PSA boosts all the baseline networks by 2.6 to 4.3 AP with minor overheads of computation (Flops) and the number of parameters(mPara). Even without ImageNet pre-training, PSA with "Res50" backbone gets 76.5 AP, which is not only 4.3 better than Simple-Baseline with Resnet50 backbone, but also better than Simple-Baseline even with Resnet152 backbone. A similar benefit is also observed on PSA with HRNet-W32 backbone outperforms the baseline with "HR-w48" back-  bone. This giant performance gains of PAS and the small overheads make PSA+HRNet-W32 the most cost-effective model among all models in <ref type="table" target="#tab_2">Table 2</ref>. Semantic Segmentation. This task maps an input image to a stack of segmentation masks, one output mask for one semantic class. In <ref type="table" target="#tab_3">Table 3</ref>, we compare PSA with the DeepLabV3Plus <ref type="bibr" target="#b3">[4]</ref> baseline on the Pascal VOC2012 Aug <ref type="bibr" target="#b11">[12]</ref> (21 classes, input image size 513 ? 513, output mask size 513 ? 513). PSA boosts all the baseline networks by 1.8 to 2.6mIoU(mean Intersection over Union) with minor overheads of computation (Flops) and the number of parameters (mPara). PSA with "Res50" backbone got 79.0 mIoU, which is not only 1.8 better than the DeepLabV3Plus with the Resnet50 backbone, but also better than DeepLabV3Plus even with Resnet101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparing with State-of-the Arts</head><p>We then apply PSA to the current state-of-the-arts of above tasks. Top-down 2D Human Pose Estimation. To our knowledge, the current state-of-the-art results by single models were achieved by UDP-HRnet with 65.1mAP bbox detector on the MS-COCO keypoint testdev set. In <ref type="table" target="#tab_4">Table 4</ref>, we add PSA to the UDP-Pose with HRnet-W48 backbone and achieve a new state-of-the-art AP of 79.5. PSA boosts UDP-Pose (baseline) by 1.7 points (see <ref type="figure" target="#fig_2">Figure 3</ref> (a) for their qualitative comparison).</p><p>Note that there is only a subtle metric difference between the parallel (p) and sequential(s) layout of PSA. We believe this partially validate that our design of the channel-only and spatial-only attention blocks has exhausted the representation power along the channel and spatial dimension.</p><p>Semantic Segmentation. To our knowledge, the current state-of-the-art results by single models were produced by HRNet-OCR(MA) <ref type="bibr" target="#b41">[41]</ref> on the Cityscapes validation set <ref type="bibr" target="#b8">[9]</ref>(19 classes, input image size 1024 ? 2048, output mask size 1024 ? 2048). In <ref type="table" target="#tab_5">Table 5</ref>, we add PSA to the basic configuration of HRNet-OCR and achieve the new state-ofthe-arts mIoU of 86.95. PSA boosts HRNet-OCR (strong baseline) by 2 points(see <ref type="figure" target="#fig_2">Figure 3</ref> (b) for their qualitative comparison). Again that there is only a subtle metric difference between the PSA results under the parallel(p) layout and the sequential(s) layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In <ref type="table">Table 6</ref>, we conduct an ablation study of PSA configurations on Simple-Baseline(Resnet50) <ref type="bibr" target="#b51">[51]</ref> and compare PSAs with other related self-attention methods. All the overheads, such as Flops, mPara, inference GPU memory("Mem."), and inference time ("Time")) are inference costs of one sample. To reduce the randomness in CUDA and Pytorch scheduling, we ran inference on MS-COCO val2017 using 4 TITAN RTX GPUs, batchsize 128 (batchsize 32/GPU), and averaged over the number of samples.</p><p>From the results of "PSA ablation" in <ref type="table">Table 6</ref>, we observe that (1) the channel-only block (A ch ) outperform spacial-only attention (A sp ), but can be further boosted by their parallel ([A ch |A sp ]) or sequential (A sp (A ch )) compositions; (2) The parallel ([A ch |A sp ]) or sequential (A sp (A ch )) compositions has similar AP, Flops, mPara, inference memory(Mem.), and inference (Time.).</p><p>From the results of "related self-attention methods", we observe that (1) the NL block costs the most memory while produces the least boost (2.3AP) over the baseline, indicating that NL is highly redundant. (2) The channel-only attention GC is better than SE since it includes SE. GC is even better than channel+spatial attention CBAM because the inner-product-based attention mechanism in GC is more powerful than the convolution/MLP-based CBAM. (3) PSA A ch is the best channel-only attention block over GC and SE. We believe PSA benefits from its highest channel resolution (C/2) and its output design. (4) The channel+spatial attention CBAM with a relatively early design is still better than the channel-only attention SE. (5) Under the same sequential layout of spatial and channel attention, PSA is significantly better than CBAM. Finally, (6) At similar overheads, both the parallel and sequential PSAs are better than the compared blocks.   <ref type="table">Table 6</ref>. Ablation study of PSA and comparison with related attention blocks(human pose estimation on the MS-COCO val2017 dataset with human detector <ref type="bibr" target="#b51">[51]</ref> of 56.4AP, input size 384 ? 288.) A ch denotes channel-only self-attention. A ch denotes spatial-only selfattention. [A ch |A sp ] denotes the parallel layout of the channel-only and spatial-only self-attention. A ch (A sp ) denotes the sequentially layout. "Mem" and "Time" are inference costs of one sample, which are averaged over the val2017 set.  <ref type="table" target="#tab_4">Table 4</ref>) and (b) Semantic segmentation(HRNetV2-OCR, <ref type="table" target="#tab_5">Table 5</ref> ). The white eclipses highlight the fine-grained details that PSAs outperform the strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We presented the Polarized Self-Attention(PSA) block towards high-quality pixel-wise regression. PSA significantly boosts all compared DCNNs for two critical designs (1) keeping high internal resolution in both polarized channel-only and spatial-only attention branches, and (2) incorporating a nonlinear composition that fully leverages the high-resolution information preserved in the PSA branches. PSA can potentially benefit any computer vision tasks with pixel-wise regression.</p><p>It is still not clear how PSA would best benefit pixel-wise regression embedded with the classification and displace-ment regression in complex DCNN heads, such as those in the instance segmentation, anchor-free object detection and panoptic segmentation tasks. To our knowledge, most existing work with self-attention blocks only inserted blocks in the backbone networks. Our future work is to explore the use of PSAs in DCNN heads.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Method ch. resolution sp. resolution non-linearity complexity O(?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The Polarized Self-Attention (PSA) block under (upper) the parallel layout, and (lower) the sequential layout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative comparison of PSA(ours) and Strong Baselines: (a) Human Pose Estimation(UDP-Pose,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Re-visit critical design aspects in existing attention blocks. All the attention blocks are compared in their topperformance configurations. SM: SoftMax, SD: Sigmoid. Complexity is estimated assuming C &lt; W H.ciency, these backbones produce low-resolution features, for instance 1?1?512 for the classification and [W/r, H/r] for bbox detection, where r is the longest side pixels of the smallest object bounding box. Pixel-wise regression cannot afford such loss of resolution, especially because the highly</figDesc><table /><note>non-linearity in object edges and body parts are very diffi- cult to encode in low-resolution features</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>PSA vs. Baselines for top-down human pose estimation on the MS-COCO val2017 dataset. All results were computed with an human detector [51] of 56.4 AP on COCO val2017 dataset. All detected human image patches were resized to 384 ? 288.</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone</cell><cell cols="3">ImageNet Pretrain</cell><cell>AP ?</cell><cell cols="4">AP50? AP75? APM ? APL? AR ? Flops mPara</cell></row><row><cell cols="2">Simple-Baseline [51]</cell><cell>Res50</cell><cell></cell><cell>Y</cell><cell></cell><cell>72.2</cell><cell>89.3</cell><cell>78.9</cell><cell>68.1</cell><cell>79.7</cell><cell>77.6</cell><cell>20.0G 34.0M</cell></row><row><cell>+PSA</cell><cell></cell><cell>Res50</cell><cell></cell><cell>N</cell><cell></cell><cell>76.5(+4.3)</cell><cell>93.6</cell><cell>83.6</cell><cell>73.2</cell><cell>81.0</cell><cell>79.0</cell><cell>20.9G 36.1M</cell></row><row><cell cols="2">Simple-Baseline [51]</cell><cell>Res152</cell><cell></cell><cell>Y</cell><cell></cell><cell>74.3</cell><cell>89.6</cell><cell>81.1</cell><cell>70.5</cell><cell>81.6</cell><cell>79.7</cell><cell>35.3G 68.6M</cell></row><row><cell>+PSA</cell><cell></cell><cell>Res152</cell><cell></cell><cell>N</cell><cell></cell><cell>78.0(+3.7)</cell><cell>93.6</cell><cell>84.8</cell><cell>75.2</cell><cell>82.3</cell><cell>80.5</cell><cell>37.5G 75.2M</cell></row><row><cell>HRNet [40]</cell><cell cols="2">HRNet-W32</cell><cell></cell><cell>Y</cell><cell></cell><cell>75.8</cell><cell>90.6</cell><cell>82.5</cell><cell>72.0</cell><cell>82.7</cell><cell>80.9</cell><cell>16.0G 28.5M</cell></row><row><cell>+PSA</cell><cell cols="2">HRNet-W32</cell><cell></cell><cell>Y</cell><cell></cell><cell>78.7(+2.9)</cell><cell>93.6</cell><cell>85.9</cell><cell>75.6</cell><cell>83.5</cell><cell>81.1</cell><cell>17.1G 31.4M</cell></row><row><cell>HRNet [40]</cell><cell cols="2">HRNet-W48</cell><cell></cell><cell>Y</cell><cell></cell><cell>76.3</cell><cell>90.8</cell><cell>82.9</cell><cell>72.3</cell><cell>83.4</cell><cell>81.2</cell><cell>32.9G 63.6M</cell></row><row><cell>+PSA</cell><cell cols="2">HRNet-W48</cell><cell></cell><cell>Y</cell><cell></cell><cell>78.9(+2.6)</cell><cell>93.6</cell><cell>85.7</cell><cell>75.8</cell><cell>83.8</cell><cell>81.4</cell><cell>35.2G 70.0M</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>mIoU ?</cell><cell></cell><cell>Flops</cell><cell>mPara</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepLabV3Plus [4]</cell><cell>MobileNet</cell><cell>71.1</cell><cell></cell><cell>16.9G</cell><cell>5.22M</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+PSA</cell><cell>MobileNet</cell><cell cols="2">73.7(+2.6)</cell><cell>17.1G</cell><cell>5.22M</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepLabV3Plus [4]</cell><cell>Res50</cell><cell>77.2</cell><cell></cell><cell>62.5G</cell><cell>39.8M</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+PSA</cell><cell>Res50</cell><cell cols="2">79.0(+1.8)</cell><cell>65.2G</cell><cell>42.3M</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepLabV3Plus [4]</cell><cell>Res101</cell><cell>78.3</cell><cell></cell><cell>83.2G</cell><cell>58.8M</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+PSA</cell><cell>Res101</cell><cell cols="2">80.3(+2.0)</cell><cell>87.7G</cell><cell>63.5M</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>PSA vs. Baselines for semantic segmentation on the Pascal VOC2012 Aug database.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison with State-of-the-Art top-down 2D pose estimation approaches on the MS-COCO keypoint testdev set. Note that only<ref type="bibr" target="#b20">[21]</ref>Strong Baseline used extra training data.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Input Size</cell><cell>AP</cell><cell cols="2">AP50</cell><cell cols="2">AP75</cell><cell>APM</cell><cell>APL</cell><cell>AR</cell><cell>Flops</cell><cell>mPara</cell></row><row><cell>8-stage Hourglass [32]</cell><cell>8-stage Hourglass</cell><cell cols="2">256 ? 192</cell><cell>66.9</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>14.3G</cell><cell>25.1M</cell></row><row><cell>CPN [5]</cell><cell>ResNet50</cell><cell cols="2">256 ? 192</cell><cell>68.6</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.2G</cell><cell>27.0M</cell></row><row><cell>CPN + OHKM [5]</cell><cell>ResNet50</cell><cell cols="2">256 ? 192</cell><cell>69.4</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.2G</cell><cell>27.0M</cell></row><row><cell>SimpleBaseline [51]</cell><cell>ResNet50</cell><cell cols="2">256 ? 192</cell><cell>70.4</cell><cell cols="2">88.6</cell><cell>78.3</cell><cell></cell><cell>67.1</cell><cell>77.2</cell><cell>76.3</cell><cell>8.90G</cell><cell>34.0M</cell></row><row><cell>SimpleBaseline [51]</cell><cell>ResNet101</cell><cell cols="2">256 ? 192</cell><cell>71.4</cell><cell cols="2">89.3</cell><cell>79.3</cell><cell></cell><cell>68.1</cell><cell>78.1</cell><cell>77.1</cell><cell>12.4G</cell><cell>53.0M</cell></row><row><cell>SimpleBaseline [51]</cell><cell>ResNet152</cell><cell cols="2">256 ? 192</cell><cell>72.0</cell><cell cols="2">89.3</cell><cell>79.8</cell><cell></cell><cell>68.7</cell><cell>78.9</cell><cell>77.8</cell><cell>15.7G</cell><cell>72.0M</cell></row><row><cell>HRNet-W32 [40]</cell><cell>HRNet</cell><cell cols="2">256 ? 192</cell><cell>74.4</cell><cell cols="2">90.5</cell><cell>81.9</cell><cell></cell><cell>70.8</cell><cell>81.0</cell><cell>78.9</cell><cell>7.10G</cell><cell>28.9M</cell></row><row><cell>HRNet-W48 [40]</cell><cell>HRNet</cell><cell cols="2">256 ? 192</cell><cell>75.1</cell><cell cols="2">90.6</cell><cell>82.2</cell><cell></cell><cell>71.5</cell><cell>81.8</cell><cell>80.4</cell><cell>14.6G</cell><cell>63.6M</cell></row><row><cell>Dark-Pose [56]</cell><cell>HRNet-W32</cell><cell cols="2">256 ? 192</cell><cell>75.6</cell><cell cols="2">90.5</cell><cell>82.1</cell><cell></cell><cell>71.8</cell><cell>82.8</cell><cell>80.8</cell><cell>7.1G</cell><cell>28.5M</cell></row><row><cell>UDP-Pose [21]</cell><cell>HRNet-W48</cell><cell cols="2">256 ? 192</cell><cell>77.2</cell><cell cols="2">91.8</cell><cell>83.7</cell><cell></cell><cell>73.8</cell><cell>83.7</cell><cell>82.0</cell><cell>14.7G</cell><cell>63.8M</cell></row><row><cell>SimpleBaseline [51]</cell><cell>ResNet152</cell><cell cols="2">384 ? 288</cell><cell>74.3</cell><cell cols="2">89.6</cell><cell>81.1</cell><cell></cell><cell>70.5</cell><cell>79.7</cell><cell>79.7</cell><cell>35.6G</cell><cell>68.6M</cell></row><row><cell>HRNet-W32 [40]</cell><cell>HRNet</cell><cell cols="2">384 ? 288</cell><cell>75.8</cell><cell cols="2">90.6</cell><cell>82.7</cell><cell></cell><cell>71.9</cell><cell>82.8</cell><cell>81.0</cell><cell>16.0G</cell><cell>28.5M</cell></row><row><cell>HRNet-W48 [40]</cell><cell>HRNet</cell><cell cols="2">384 ? 288</cell><cell>76.3</cell><cell cols="2">90.8</cell><cell>82.9</cell><cell></cell><cell>72.3</cell><cell>83.4</cell><cell>81.2</cell><cell>32.9G</cell><cell>63.6M</cell></row><row><cell>Dark-Pose [56]</cell><cell>HRNet-W48</cell><cell cols="2">384 ? 288</cell><cell>76.8</cell><cell cols="2">90.6</cell><cell>83.2</cell><cell></cell><cell>72.8</cell><cell>84.0</cell><cell>81.7</cell><cell>32.9G</cell><cell>63.6M</cell></row><row><cell>UDP-Pose [21]</cell><cell>HRNet-W48</cell><cell cols="2">384 ? 288</cell><cell>76.2</cell><cell cols="2">92.5</cell><cell>83.6</cell><cell></cell><cell>72.5</cell><cell>82.4</cell><cell>81.1</cell><cell>33.0G</cell><cell>63.8M</cell></row><row><cell>UDP-Pose [21] (Strong Baseline)</cell><cell>HRNet-W48</cell><cell cols="2">384 ? 288</cell><cell>77.8</cell><cell cols="2">92.0</cell><cell>84.3</cell><cell></cell><cell>74.2</cell><cell>84.5</cell><cell>82.5</cell><cell>33.0G</cell><cell>63.8M</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UDP-Pose-PSA(p)</cell><cell>HRNet-W48</cell><cell cols="2">256 ? 192</cell><cell>78.9</cell><cell cols="2">93.6</cell><cell>85.8</cell><cell></cell><cell>76.1</cell><cell>83.6</cell><cell>81.4</cell><cell>15.7G</cell><cell>70.1M</cell></row><row><cell>UDP-Pose-PSA(p)</cell><cell>HRNet-W48</cell><cell cols="2">384 ? 288</cell><cell>79.5</cell><cell cols="2">93.6</cell><cell>85.9</cell><cell></cell><cell>76.3</cell><cell>84.3</cell><cell>81.9</cell><cell>35.4G</cell><cell>70.1M</cell></row><row><cell>UDP-Pose-PSA(s)</cell><cell>HRNet-W48</cell><cell cols="2">384 ? 288</cell><cell>79.4</cell><cell cols="2">93.6</cell><cell>85.8</cell><cell></cell><cell>76.1</cell><cell>84.1</cell><cell>81.7</cell><cell>35.4G</cell><cell>69.1M</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>Backbone</cell><cell></cell><cell cols="6">mIoU iIoU cla. IoU cat. iIoU cat.</cell></row><row><cell>GridNet [13]</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>69.5</cell><cell></cell><cell>44.1</cell><cell cols="2">87.9</cell><cell>71.1</cell></row><row><cell>LRR-4x</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>69.7</cell><cell></cell><cell>48.0</cell><cell cols="2">88.2</cell><cell>74.7</cell></row><row><cell>DeepLab [4]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>70.4</cell><cell></cell><cell>42.6</cell><cell cols="2">86.4</cell><cell>67.7</cell></row><row><cell>LC</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>71.1</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Piecewise [27]</cell><cell></cell><cell></cell><cell></cell><cell>VGG-16</cell><cell></cell><cell>71.6</cell><cell></cell><cell>51.7</cell><cell cols="2">87.3</cell><cell>74.1</cell></row><row><cell>FRRN [36]</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>71.8</cell><cell></cell><cell>45.5</cell><cell cols="2">88.9</cell><cell>75.1</cell></row><row><cell>RefineNet [26]</cell><cell></cell><cell></cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell>73.6</cell><cell></cell><cell>47.2</cell><cell cols="2">87.9</cell><cell>70.6</cell></row><row><cell>PEARL [23]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>75.4</cell><cell></cell><cell>51.6</cell><cell cols="2">89.2</cell><cell>75.1</cell></row><row><cell>DSSPN [25]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>76.6</cell><cell></cell><cell>56.2</cell><cell cols="2">89.6</cell><cell>77.8</cell></row><row><cell>LKM [34]</cell><cell></cell><cell></cell><cell></cell><cell>ResNet-152</cell><cell></cell><cell>76.9</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>DUC-HDC [45]</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>77.6</cell><cell></cell><cell>53.6</cell><cell cols="2">90.1</cell><cell>75.2</cell></row><row><cell>SAC [58]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>78.1</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>DepthSeg [24]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>78.2</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>ResNet38 [49]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">WResNet-38</cell><cell>78.4</cell><cell></cell><cell>59.1</cell><cell cols="2">90.9</cell><cell>78.1</cell></row><row><cell>BiSeNet [53]</cell><cell></cell><cell></cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell>78.9</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>DFN [54]</cell><cell></cell><cell></cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell>79.3</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>PSANet [61]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>80.1</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>PADNet [52]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>80.3</cell><cell></cell><cell>58.8</cell><cell cols="2">90.8</cell><cell>78.5</cell></row><row><cell>CFNet [57]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>79.6</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="2">Auto-DeepLab [30]</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>80.4</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>DenseASPP [60]</cell><cell></cell><cell></cell><cell cols="3">WDenseNet-161</cell><cell>80.6</cell><cell></cell><cell>59.1</cell><cell cols="2">90.9</cell><cell>78.1</cell></row><row><cell>SVCNet [11]</cell><cell></cell><cell></cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell>81.0</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>ANN [65]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>81.3</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>CCNet [22]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>81.4</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>DANet [14]</cell><cell></cell><cell></cell><cell cols="3">D-ResNet-101</cell><cell>81.5</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>HRNetV2 [44]</cell><cell></cell><cell></cell><cell cols="3">HRNetV2-W48</cell><cell>81.6</cell><cell></cell><cell>61.8</cell><cell cols="2">92.1</cell><cell>82.2</cell></row><row><cell cols="2">HRNetV2+OCR [55]</cell><cell></cell><cell cols="3">HRNetV2-W48</cell><cell>84.9</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="3">HRNetV2+OCR(MA) [41] (Strong Baseline)</cell><cell cols="3">HRNetV2-W48</cell><cell>85.4</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">HRNetV2-OCR+PSA(p)</cell><cell></cell><cell cols="3">HRNetV2-W48</cell><cell cols="2">86.95</cell><cell>71.6</cell><cell cols="2">92.8</cell><cell>85.0</cell></row><row><cell cols="2">HRNetV2-OCR+PSA(s)</cell><cell></cell><cell cols="3">HRNetV2-W48</cell><cell cols="2">86.72</cell><cell>71.3</cell><cell cols="2">92.3</cell><cell>82.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison with State-of-the-Art semantic segmentation approaches on the Cityscapes validation set. Method AP ? AP50? AP75? APM ? APL? AR ? Flops ? mPara ? Mem.(MiB) ? Time(ms)?</figDesc><table><row><cell>Simple-Baseline(ResNet50) [51]</cell><cell>72.2</cell><cell>89.3</cell><cell>78.9</cell><cell>68.1</cell><cell>79.7 77.6 20.0G 34.0M</cell><cell>1.43</cell><cell>2.56</cell></row><row><cell>PSA ablation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+A ch</cell><cell cols="2">76.3(+4.1) 92.6</cell><cell>83.6</cell><cell>73.0</cell><cell>80.8 78.9 20.4G 35.3M</cell><cell>1.49</cell><cell>2.58</cell></row><row><cell>+A sp</cell><cell cols="2">75.0(+2.8) 92.6</cell><cell>81.6</cell><cell>71.5</cell><cell>80.2 77.7 20.7G 35.3M</cell><cell>1.45</cell><cell>2.63</cell></row><row><cell>+[A ch |A sp ] (PSA(p))</cell><cell cols="2">76.5(+4.3) 93.6</cell><cell>83.6</cell><cell>73.2</cell><cell>81.0 79.0 20.9G 36.5M</cell><cell>1.54</cell><cell>2.70</cell></row><row><cell>+A sp (A ch ) (PSA(s))</cell><cell cols="2">76.6(+4.4) 93.6</cell><cell>83.6</cell><cell>73.2</cell><cell>81.2 79.1 20.9G 36.5M</cell><cell>1.52</cell><cell>2.71</cell></row><row><cell>Related self-attention methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+A (NL [47])</cell><cell cols="2">74.5(+2.3) 92.6</cell><cell>81.5</cell><cell>70.9</cell><cell>79.9 77.3 21.1G 36.5M</cell><cell>10.97</cell><cell>2.76</cell></row><row><cell>+A ch (GC [3])</cell><cell cols="2">76.1(+3.9) 92.6</cell><cell>82.7</cell><cell>72.9</cell><cell>80.9 78.7 20.2G 34.3M</cell><cell>1.47</cell><cell>2.69</cell></row><row><cell>+A ch (SE [20])</cell><cell cols="2">75.7(+3.5) 93.6</cell><cell>82.6</cell><cell>72.4</cell><cell>80.8 78.3 20.2G 34.2M</cell><cell>1.29</cell><cell>2.94</cell></row><row><cell>+A sp (A ch ) (CBAM [48])</cell><cell cols="2">75.9(+3.7) 92.6</cell><cell>82.7</cell><cell>72.9</cell><cell>80.7 78.7 20.2G 34.3M</cell><cell>1.49</cell><cell>2.96</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">C/2 is the smallest channel number when PSA produces the best metrics, and is used throughout our experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolution, attention and structure embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Andreoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Augmented reality driving using semantic geo-registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Pang</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Villamil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Drew</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supun</forename><surname>Samarasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic correlation promoted shape-variant context for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Residual convdeconv grid network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Fourure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Tremeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video scene parsing with predictive feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamicstructured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Dan Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bejnordi Arnaud, Arindra Adiyoso, and Setio Francesco. A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thijs</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Ehteshami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the heatmap regression for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bam: bottleneck attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Joon-Young Lee, and In SoKweon</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large kernel matters -improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Selfattention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<imprint>
			<publisher>Andrej Karpathy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Efficient attention: Attention with linear complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Andreas Mayr, Martin Heusel, Markus Hofmarcher, Michael Widrich, Bernhard Nessler, and Sepp Hochreiter. Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schuberth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit Abd Llion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="5686" to="5696" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Pixel-wise crowd understanding via synthetic data. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="225" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Second-order non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Bryan ; Ning) Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cooccurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Residual non-local attention networks for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Psanet: Pointwise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Squeeze-and-attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Zhong Qiu Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Bidart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><forename type="middle">Ben</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Daya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11168</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
