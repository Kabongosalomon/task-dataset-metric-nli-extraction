<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging Category-level and Instance-level Semantic Image Segmentation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<postCode>5005</postCode>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<postCode>5005</postCode>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<postCode>5005</postCode>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging Category-level and Instance-level Semantic Image Segmentation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an approach to instance-level image segmentation that is built on top of category-level segmentation. Specifically, for each pixel in a semantic category mask, its corresponding instance bounding box is predicted using a deep fully convolutional regression network. Thus it follows a different pipeline to the popular detect-then-segment approaches that first predict instances' bounding boxes, which are the current state-of-the-art in instance segmentation. We show that, by leveraging the strength of our state-of-the-art semantic segmentation models, the proposed method can achieve comparable or even better results to detect-thensegment approaches. We make the following contributions. (i) First, we propose a simple yet effective approach to semantic instance segmentation. (ii) Second, we propose an online bootstrapping method during training, which is critically important for achieving good performance for both semantic category segmentation and instance-level segmentation. (iii) As the performance of semantic category segmentation has a significant impact on the instance-level segmentation, which is the second step of our approach, we train fully convolutional residual networks to achieve the best semantic category segmentation accuracy. On the PASCAL VOC 2012 dataset, we obtain the currently best mean intersection-over-union score of 79.1%. (iv) We also achieve state-of-the-art results for instance-level segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic category-level image segmentation amounts to predicting the category of individual pixels in an image, which has been one of the most active topics in the field of image understanding and computer vision for a long time. Most of the recently proposed approaches to this task are based on deep convolutional networks. Particularly, the fully convolutional network (FCN) <ref type="bibr" target="#b0">[1]</ref> is efficient and at the same time has achieved the state-of-the-art performance. By reusing the computed feature maps for an image, FCN avoids redundant re-computation for classifying individual pixels in the image. FCN has become the de facto approach to dense prediction, and many methods have been proposed to further improve this framework, e.g., the DeepLab <ref type="bibr" target="#b1">[2]</ref> and the Adelaide-Context model <ref type="bibr" target="#b2">[3]</ref>. One key reason for the success of these methods is that they are based on rich features learned from the very large ImageNet <ref type="bibr" target="#b3">[4]</ref> dataset, often in the form of a 16-layer VGGNet <ref type="bibr" target="#b4">[5]</ref>. However, currently, there exist much improved models for image classification, e.g., the ResNet <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Semantic instance-level segmentation aims to identify the individual instances of different semantic categories, i.e., simultaneous object detection and segmentation. Instance segmentation is supposed to be only one more step beyond semantic segmentation. However, most recently proposed approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> to instance segmentation are based on bounding-box detection methods. Generally speaking, these methods follow the pipeline of a typical object detection method, e.g., Fast R-CNN <ref type="bibr" target="#b10">[11]</ref>, to locate instances of semantic objects with bounding-boxes as the first step, and then predict binary object masks within these boxes via background-foreground segmentation. In spite of the recent fast development of powerful semantic-category segmentation methods, there is little work in the literature towards developing an instance segmentation method on top of, and fully exploiting the power of such methods. One notable work is the proposal-free network (PFN) <ref type="bibr" target="#b11">[12]</ref>, which for all pixels predicts semantic scores and instance bounding-boxes, as well as category-wise numbers of instances per image, based on which a clustering algorithm is applied for post-processing to locate instances. As pointed out by Uhrig et al. <ref type="bibr" target="#b12">[13]</ref>, PFN has a complex architecture with many interleaved building blocks, which makes training very complex. More importantly, its performance seemingly depends critically on the correct prediction of instance numbers, which is sometimes infeasible. Especially, in a complex scene usually there are many small instances, and the number of training samples per number of instances can be very small, leading to mistakes in their estimation. Consequently, the available cues for clustering may significantly deviate from the estimated number of instances. This might be one of the reasons why their performance reported in <ref type="bibr" target="#b11">[12]</ref> is considerably poorer than most recent results. On the other hand, the concurrent work to ours by Uhrig et al. <ref type="bibr" target="#b12">[13]</ref> relies on elaborately designed template matching to detect instances, which is very case-specific. Furthermore, good performance in <ref type="bibr" target="#b12">[13]</ref> heavily relies on accurate depth estimation, which only works well for indoor/outdoor scene images. Based on the above considerations, here we attempt to develop a concise and generic yet accurate method of this kind, and show the great potential of the less explored route.</p><p>In summary, we highlight the main contributions of this work as follows:</p><p>? We propose a new approach to instance-level segmentation, in which semantic score maps are transformed into Hough-like maps. We can easily detect the instances of different semantic objects from these transformed maps. ? We propose an online bootstrapping method for training, and show that it is critically important in achieving the best performance both for semantic and instance segmentation. ? We extensively evaluate different variations of a fully convolutional residual network (FCRN) so as to find the best configuration, including the number of layers, the resolution of feature maps, and the size of field-of-view. ? We introduce dropout regularization to some of the residual blocks in an FCRN, replace the top-most linear classifier with a multi-layer non-linear one, and adopt the multi-view testing technique, by which we further improve the performance. Our method achieves the currently best results on the PASCAL VOC 2012 dataset in terms of semantic segmentation. Our mean intersection-over-union score reaches 79.1% even though we use only the augmented PASCAL VOC training data, which is a new record 2 . ? We achieve on par or even better results in terms of instance segmentation on the PASCAL VOC 2012 dataset, compared with the previous best performers. In particular, we significantly improve the mean region average precision at an overlap of 0.7 by 5.1%, from 41.5% to 46.6%. We empirically show that, as the accuracy of semantic segmentation improves, our method has the potential to improve by a remarkable margin.</p><p>Related work Next we briefly review the most recent developments within four topics, which are closely related to this paper.</p><p>Very deep convolutional networks. The recent boom in deep convolution networks originated when Krizhevsky et al. <ref type="bibr" target="#b13">[14]</ref> won the first place in the ILSVRC 2012 competition <ref type="bibr" target="#b3">[4]</ref> with the 8-layer AlexNet. In the next year, 'Clarifai' <ref type="bibr" target="#b3">[4]</ref> still had the same number of layers. However, in 2014, the VGGNets <ref type="bibr" target="#b4">[5]</ref> were composed of up to nineteen layers, while the even deeper 22-layer GoogLeNet <ref type="bibr" target="#b14">[15]</ref> won the competition <ref type="bibr" target="#b3">[4]</ref>. In 2015, the much deeper ResNets <ref type="bibr" target="#b5">[6]</ref> achieved the best performance <ref type="bibr" target="#b3">[4]</ref>, showing deeper networks indeed learn better features.</p><p>The most impressive part was that, by replacing the VGGNets in Fast RCNN <ref type="bibr" target="#b10">[11]</ref> with their ResNets, He et al. <ref type="bibr" target="#b5">[6]</ref> won in the object detection task with a remarkable margin. This result showed the importance of features in image understanding tasks. The main contribution that enabled them to train such deep networks was that they connected some of the layers with shortcuts. These shortcuts directly passed through the signals, and thus avoid the gradient vanishing effect, which may be a problem for very deep plain networks. In a more recent work, they redesigned their residual blocks to avoid over-fitting, which enabled them to train an even deeper 200-layer residual network. Deep ResNets can be seen as a simplified version of the highway network <ref type="bibr" target="#b15">[16]</ref>.</p><p>Fully convolutional networks for semantic segmentation. Long et al. <ref type="bibr" target="#b0">[1]</ref> first proposed the framework of fully convolutional networks for semantic segmentation, which was both effective and efficient. They also enhanced the final feature maps with those from intermediate layers, which enabled their model to make finer predictions. Chen et al. <ref type="bibr" target="#b1">[2]</ref> increased the resolution of feature maps by spontaneously removing some of the down-sampling operations and accordingly introducing kernel dilation into their networks. They also found that a classifier composed of small kernels with a large dilation performed as well as a classifier with large kernels, and that reducing the size of field-of-view had an adverse impact on performance. As post-processing, they applied dense CRFs <ref type="bibr" target="#b16">[17]</ref> to the network-predicted category-wise score maps for further improvement. Zheng et al. <ref type="bibr" target="#b17">[18]</ref> simulated the dense CRFs with an recurrent neural network (RNN), which can be trained end-to-end together with the down-lying convolution layers. Lin et al. <ref type="bibr" target="#b2">[3]</ref> jointly trained CRFs with down-lying convolution layers. Thus they were able to capture both 'patch-patch' and 'patch-background' context with CRFs, rather than just pursuing local smoothness as most of the previous methods did.</p><p>Bounding-box detection based approaches to instance segmentation. Most of the best performers for instance-level segmentation in the literature can be attributed as bounding-box detection based approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Usually, they were composed of two steps, i.e., first locating objects with bounding-boxes, and second generating masks from these boxes via foreground segmentation. However, our method in this paper is built upon semantic image segmentation. An object is detected as a local maxima on a transformed semantic score map, which has some connections to the generalized Hough transform based approaches to object detection and segmentation <ref type="bibr" target="#b18">[19]</ref>.</p><p>Online bootstrapping for training deep convolutional networks. There are some recent works in the literature exploring sampling methods during training, which are concurrent with ours. Loshchilov and Hutter <ref type="bibr" target="#b19">[20]</ref> studied mini-batch selection in terms of image classification. They picked hard training images from the whole training set according to their current losses, which were lazily updated once an image had been forwarded through the network being trained. Shrivastava et al. <ref type="bibr" target="#b20">[21]</ref> in a concurrent work to ours selected hard regions-of-interest (RoIs) for object detection. They only computed the feature maps of an image once, and forwarded all RoIs of the image on top of these feature maps. Thus they were able to find the hard RoIs with a small additional computational cost.</p><p>The method of <ref type="bibr" target="#b19">[20]</ref> is similar to ours in the sense that they both select hard training samples based on the current losses of individual data-points. However, we only search for hard pixels within the current mini-batch, rather than the whole training set. In this sense, the method of <ref type="bibr" target="#b20">[21]</ref> is more similar to ours. Nevertheless, to our knowledge, the method here is the first to propose online bootstrapping of hard pixel samples for the problems of instance and semantic image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed method</head><p>We first introduce the proposed pipeline for instance segmentation, then demonstrate our online bootstrapping approach, which is one of the key components in our high-performance fully convolutional residual networks (FCRNs) for both semantic and instance segmentation, and finally explain how to build up our FCRNs from residual networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proposed pipeline for instance segmentation</head><p>The pipeline of our approach is illustrated in <ref type="figure">Fig. 1</ref>. During testing, we obtain the instance segmentation result of an image following the steps shown below.</p><p>1) Calculate the category-wise score maps via semantic category segmentation. For clarity, we only depict the score maps corresponding to the 'sheep' category and discard the remaining nineteen object categories in the figure.</p><p>2) Calculate the category-wise transform maps via bounding-box regression. Again, only the transform map for the 'sheep' category is depicted.</p><p>3) Apply the obtained transform maps to their corresponding scores maps. 4) Search for local maxima on the transformed maps by non-maximal suppression (NMS), and keep the obtained maxima as detected instance hypotheses.  <ref type="figure">Figure 1</ref>: The pipeline of our proposed approach to instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>Trace back to all of the suppressed pixels and recover a mask for each instance hypothesis. 6) Generate the final instance segmentation result by region based NMS <ref type="bibr" target="#b7">[8]</ref>.</p><p>We train the two networks separately. The semantic segmentation net is trained with the classic Logistic regression loss, and the localization network is trained with the smoothed 1 loss <ref type="bibr" target="#b10">[11]</ref>. Our regression targets include the vertical and horizontal offsets from the current pixel to the bounding-box center of the instance that it belongs to, and also the height and width of that instance. To balance the contributions of instances in different sizes, we re-weight their pixels in the training loss according to their heights and widths. However, we let pixels in the same instance have the same loss weight, since the central and peripheral pixels are of the same importance in order to reconstruct the whole instance.</p><p>To combine the outputs of both networks during testing, we start by applying the transform maps to the semantic score maps. Note that we ignore the background and only transform the foreground pixels, e.g., only those labeled as 'sheep' in <ref type="figure">Fig. 1</ref>. To improve the recall rate of instances, we use top-n 'sheep' masks. In other words, we generate n masks per category. The top-n masks are obtained by keeping the pixels whose scores for the 'sheep' category are among their top-n highest scores.</p><p>The next step is to find the instances. We need to detect the modes of transformed pixels in the 2D spatial image space. To this end, here we resort to a simple off-the-shelf approach, although more sophisticated clustering methods may lead to improved results. Namely, we apply non-maximum suppression (NMS) to the bounding-boxes previously predicted by our localization net. The pixels with locally high semantic scores will be kept as instance hypotheses. For each of them, we view that pixel and its suppressed pixels as a cluster, and average their semantic scores to compute the confidence of their corresponding instance hypothesis. Note that the above process has some connections with the generalized Hough transform based approaches to object detection and segmentation <ref type="bibr" target="#b18">[19]</ref>, and that predicting bounding-boxes is not an indispensable component but a trivial implementation of maxima searching in our proposed method. However, in most of the recent methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, bounding-boxes are one of the required inputs for their segmentation components to generate binary masks for individual instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Online bootstrapping of hard training pixels</head><p>When we train an FCRN, depending on the size of image crops, there may be tens of thousands of labeled pixels to predict per crop. However, sometimes many of them can easily be discriminated from others, especially those lying at the center of a large semantic region. Continuing to learn from these pixels cannot improve our objective. For this reason, we propose an online bootstrapping method, which forces networks to focus on hard (and so more valuable) pixels during training.</p><p>We first describe the proposed online bootstrapping in the context of semantic category-level segmentation. Let there be K different categories c j in a label space. For simplicity, suppose that there is only one image crop per mini-batch, and let there be N pixels a i to predict in this crop. Let y i denote the ground-truth label of pixel a i , and p ij denote the predicted probability of pixel a i belonging to category c j . Then, the loss function can be defined as,</p><formula xml:id="formula_0">= ? 1 N i K j 1{y i = j and p ij &lt; t} ( N i K j 1{y i = j and p ij &lt; t} log p ij )<label>(1)</label></formula><p>where t ? (0, 1] is a threshold. Here 1{?} equals one when the condition inside the brackets holds, and otherwise equals zero. In other words, we drop pixels when they are too easy for the current model, i.e., their losses are below t. However, in practice, we hope that there should be at least a reasonable number of pixels kept per mini-batch. Otherwise, the computed gradient can become very noisy. Hence, we increase the threshold t accordingly if our current model performs fairly well on a specific mini-batch; and decrease if the model does not work well.</p><p>The idea to introduce online bootstrapping into our localization network is similar, i.e., to drop those too easy pixels. However, the definition of 'easy' is slightly different. Here, we threshold the intersection-over-union (IoU) scores between ground-truth and predicted bounding-boxes, rather than directly thresholding the regression loss. Our intuition is straightforward. We do not care how accurate the heights, widths or offsets are regressed, but we do care if we can get the correct instances after the following NMS on transformed score maps. Since NMS is based on IoU scores between bounding-boxes, our choice is more natural than thresholding regression losses of the four targets respectively.</p><p>Note that this online bootstrapping approach is also capable of automatically balancing biased training data, which is one of the significant and common problems for a pixel labeling task. Usually, there are remarkably more background pixels than object pixels. Sometimes, one category has many more pixels than the others. The mechanism works as follows. As the training process goes on, our model may become overly-learned for those majority categories. Therefore, pixels belonging to these categories will become easy to discriminate, which means that the training losses for these pixels will go down. At the same time, the losses for pixels belonging to minority categories keep unchanged or even go up. At some point, our bootstrapping approach will find them and keep the model learning from them until their losses go down to the same level as the majority category pixels. Similar argument was also presented in a concurrent work to ours by Shrivastava et al. <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fully convolutional residual network</head><p>We initialize an FCRN from the version of ResNet in <ref type="bibr" target="#b5">[6]</ref>. We replace the linear classification layer with a convolution layer so as to make one prediction per spatial location. Besides, we also remove the 7?7 pooling layer. This layer can enlarge the field-of-view (FoV) <ref type="bibr" target="#b1">[2]</ref> of features, which is sometimes useful considering the fact that we human usually tell the category of a pixel by referring to its surrounding context region. However, this pooling layer at the same time smoothes the features.</p><p>In pixel labeling tasks, features of adjacent pixels should be distinct from each other when they respectively belong to different categories, which may conflict with the pooling layer. Therefore we remove this layer and let the linear convolution layer be on top to deal with the FoV.</p><p>Up to now, the feature maps below the added linear convolution layer only has a resolution of 1/32, which is too low to precisely discriminate individual pixels. Long et al. <ref type="bibr" target="#b0">[1]</ref> learned extra up-sampling layers for this issue. However, Chen et al. <ref type="bibr" target="#b1">[2]</ref> reported that the hole algorithm (or the ?trous algorithm by Mallat <ref type="bibr" target="#b21">[22]</ref>) can be more efficient. Intuitively, the hole algorithm can be seen as dilating the convolution kernels before applying them to their input feature maps. With this technique, we can build up a new network generating feature maps of any higher resolution, without changing the weights. When there is a layer with down-sampling, we skip the down-sampling part and increase the dilations of subsequent convolution kernels accordingly. See DeepLab <ref type="bibr" target="#b1">[2]</ref> for an explanation.</p><p>A sufficiently large FoV was reported to be important by Chen et al. <ref type="bibr" target="#b1">[2]</ref>. Intuitively, we need to present context information of a pixel to the top-most classification layer. However, the features at different locations should be discriminative at the same time so that the classifier can tell the differences between adjacent pixels which belong to different categories. Therefore, a natural way is to let the classifier to handle the FoV, which can be achieved by enlarging its kernel size. Unfortunately, the required size can be so large that it can blow up the number of parameters in the classifier. Nevertheless, we can resort to the hole algorithm again. In other words, we use small kernels with large dilations, in order to realize a large FoV.</p><p>Following the above three steps, we design our baseline FCRN for semantic category-level segmentation. Although the ResNet has shown its advantages in terms of many tasks due to much richer learned features, we observe that this baseline FCRN is not powerful enough to beat the best algorithm for semantic segmentation <ref type="bibr" target="#b2">[3]</ref>, which is based on the VGGNet <ref type="bibr" target="#b4">[5]</ref>. However, we empirically show that FCRN can achieve the best performance together with our proposed online bootstrapping and a few modifications. We design our localization networks similarly. We do not regress bounding-boxes for background pixels, thus there are only eighty channels in the top-most layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we show that our method achieves the state-of-the-art performance both in terms of semantic category-level and instance-level segmentation. We implement our algorithm with the Caffe <ref type="bibr" target="#b22">[23]</ref> toolkit throughout all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic category-level segmentation results</head><p>We first evaluate the semantic category segmentation component in our method, which is derived based on the FCN <ref type="bibr" target="#b0">[1]</ref> and the ResNet <ref type="bibr" target="#b5">[6]</ref>. We name it as the fully convolutional residual network (FCRN), whose hyper-parameters are carefully evaluated, including the network depth, the resolution of feature maps, the kernel size and dilation of the top-most classifier in the network. For evaluation, we use three popular and challenging datasets, i.e., the PASCAL VOC 2012 <ref type="bibr" target="#b23">[24]</ref>, the Cityscapes <ref type="bibr" target="#b24">[25]</ref>, and the PASCAL-Context <ref type="bibr" target="#b25">[26]</ref> dataset. We report, 1) the pixel accuracy, which is the percentage of correctly labeled pixels on a whole test set, 2) the mean pixel accuracy, which is the mean of class-wise pixel accuracies, and 3) the mean class-wise intersection-over-union (IoU) scores. Also note that we only show these three scores when it is possible for the individual datasets. For example, we will not show the first two kinds of scores for the test set of PASCAL VOC 2012, since only the mean IoU is available.</p><p>PASCAL VOC 2012. This dataset consists of photos taken in human daily life. Besides the background category, there are twenty semantic categories, including bus, car, cat, sofa, monitor, etc. There are 1,464 fully labeled images in the train set and another 1,449 in the val set. Ground-truth labels of the 1,456 images in the test set are not public, but there is an online evaluation server.</p><p>Following the conventional settings in the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, we augment the train set with extra labeled PASCAL VOC images from the semantic boundaries dataset <ref type="bibr" target="#b26">[27]</ref>. So, in total there are 10,582 images in the augmented train set.</p><p>According to results on the val set in <ref type="table" target="#tab_1">Table 1</ref>, we notice the below three points. 1) Increasing the depth from 50 to 101 brings a significant improvement. However, we observe no further improvement when increasing the depth to 152, probably, due to over-fitting. 2) Increasing the resolution of feature maps is beneficial. 3) Increasing the size of field-of-view (FoV) to more than 224 is also helpful. Note that 224 is the setting used in FCNs <ref type="bibr" target="#b0">[1]</ref>.</p><p>The above results suggest using deeper networks, generating high resolution feature maps, and enlarging the FoV of classifiers. This makes sense because we can obtain richer and finer features, and classifiers learn from larger context regions. However, it also makes computational cost heavier and may be limited by the amount of GPU memories. Besides, as for the size of FoV, there is another important factor to consider. Note that all of the images in PASCAL VOC are no larger than 500?500, and we feed a network with original images (without resizing) during testing. Thus, we have to limit the size of FoV below 500 pixels on this dataset. Otherwise, the dilated kernels of a classifier will be larger than the size of feature maps. As a result, the outer part of the kernels must be applied to padded zeros, which may cause adversarial impact. Similarly, if the size of FoV is larger than the size of image crops during training, the outer part of the kernels cannot be properly learned. In <ref type="table" target="#tab_1">Table 1</ref>, our largest evaluated size of FoV is 392. No matter what is the depth, networks with this setting always achieve the best performance. To realize such a large FoV, we can either enlarge the kernel size of the classifier or increase the dilation of these kernels. However, this dilation should not be too large, since the feature vector per location can only cover a limited size of area. For example, models with a dilation of eighteen show no advantages over those with a dilation of twelve.</p><p>We compare our method with the previous best performers on the test set in the bottom part of <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Being trained with only the augmented PASCAL VOC data, our model outperforms the previous best performer by 3.8% and wins the first place for 18 out of the 20 object categories. Generally speaking, our method usually loses for those very hard categories, e.g., 'bicycle' and 'chair', for which most of the methods can only achieve scores below 60.0%. Instances of these categories are usually of large diversity and in occluded situations. More importantly, some of them are given with quite elaborated annotations, e.g., a bicycle with carefully labeled skeletons. The above facts suggest that more training data are required for these categories. But unfortunately, this is not the case for this dataset. Some works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3]</ref> pre-trained their models with the Microsoft COCO <ref type="bibr" target="#b28">[29]</ref> data, which is composed of about 120k labeled images. In this case, the best previous result is 77.8% <ref type="bibr" target="#b2">[3]</ref>.</p><p>With this setting, we can only observe a rather limited improvement. It seems like that more efforts  should be put in domain adaption from COCO to PASCAL VOC. Here, we leave this problem as one of our future works, and focus on the augmented PASCAL VOC dataset. Nevertheless, even with less training data, our method still beats the previous best performer by 1.3%.</p><p>Cityscapes. This dataset consists of street scene images taken using car-carried cameras. There are nineteen semantic categories, including road, car, pedestrian, bicycle, etc. There are 2975 fully labeled images in the train set and another 500 in the val set. Ground-truth labels of images in the test set are not public, but there is an online evaluation server also. All of the images in this dataset are in the same size. They are 1024 pixels high and 2048 pixels wide.</p><p>We show results on the val set of Cityscapes in <ref type="table" target="#tab_3">Table 3</ref>. Most of the observations on this dataset are consistent with those on PASCAL VOC 2012, as demonstrated previously. Two notable exceptions are as follows. First, the problem of over-fitting seems less severe. One possible reason is that the resolution of images in this dataset are higher than those in PASCAL VOC 2012, so the total number of pixels are actually larger. On the other hand, the diversity of images in this dataset is smaller than those in PASCAL VOC 2012. In this sense, even less training data can cover a larger proportion of possible situations, which can reduce over-fitting. Second, 392 seems still smaller than the optimal size of FoV. Since the original images are in a size of 1024?2048, we can feed a 50-layer network with larger image crops during both training and testing. In this case, a network may prefer even larger FoV. Therefore, to some extent, the ideal size of FoV depends on the size of image crops during training and testing.</p><p>Our best model achieves an IoU score of 74.6% on the val set, as shown in <ref type="table" target="#tab_7">Table 7</ref>, which is compared with the previously reported best result 68.6% <ref type="bibr" target="#b2">[3]</ref>.</p><p>PASCAL-Context. This dataset is composed of PASCAL VOC 2010 images with extra object and stuff labels, e.g., bag, food, sign, ceiling, ground and snow. Including the background category, in total there are sixty semantic categories. At present, labels on the test set are not released yet. So, we train networks using the 4,998 images in the train set, and evaluate them using the 5,105 images in  the val set. For this dataset, we just evaluate the same model as the one used for PASCAL VOC 2012. Our method outperforms the previous best performers with a clear margin in terms of all the three kinds of scores, as shown in <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance-level segmentation results</head><p>We now evaluate the whole framework in terms of instance segmentation on the PASCAL VOC 2012 dataset, following the common protocols used in several recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. We use the annotations in SBD <ref type="bibr" target="#b26">[27]</ref>. According to the PASCAL VOC 2012 splits, there are 5,623 images in the train set and 5,732 in the val set. Since there is no annotation for the test set, we train models with the train set and evaluate them with the val set. We report two mean region average precisions mAP r 0.5 and mAP r 0.7 , and also the mean volume region average precisions mAP r vol , which were proposed by Hariharan et al. <ref type="bibr" target="#b7">[8]</ref>.</p><p>We show results on the val set of PASCAL VOC 2012 in <ref type="table" target="#tab_6">Table 6</ref>. Our method can finally achieve on par or even better performance compared with the previous best performers. Especially, in terms of the mean AP at an overlap of 0.7, our method outperforms the previous best one by 3.1%, which is a significant improvement. By pre-training the semantic segmentation network using the COCO dataset <ref type="bibr" target="#b28">[29]</ref>, we can further improve the performance by 2.0%. In a pilot experiment, we generate semantic score maps with ground-truth masks, while still compute transform maps with our best localization network. The performance goes up to 73.0% in mAP r 0.5 and 60.6% in mAP r 0.7 , which shows the great potential of our method to benefit from the improvement of semantic category-level segmentation approaches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Importance of online bootstrapping of hard training pixels</head><p>We show the results of two best models for semantic segmentation in <ref type="table" target="#tab_7">Table 7</ref>. In both cases, the best setting is to keep the 512 most hardest pixels. Particularly, the proposed bootstrapping improves the mean IoU by 3.1% on Cityscapes, which is a significant margin. The problem of biased data is more severe on this dataset, since there are clearly much more 'sky' and 'road' pixels than 'traffic sign' in a street scene. We also show the category-wise results in <ref type="table" target="#tab_2">Tables 2 and 4</ref>. Generally, the proposed bootstrapping can clearly improve the performance for those categories which are less frequent in training data, e.g., 'cow' and 'horse' on PASCAL VOC 2012, 'traffic light' and 'train' on Cityscapes. Similarly, the proposed online bootstrapping also contributes clearly to our localization networks, as shown in <ref type="table" target="#tab_6">Table 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Qualitative results</head><p>We show qualitative results for semantic segmentation in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we have built a state-of-the-art fully convolutional residual network for semantic category-level segmentation. On top of this model, we have proposed a new pipeline for instancelevel segmentation, which is intrinsically different from the currently commonly-used bounding-box detection based methods. We have also proposed an online bootstrapping approach, which contributes greatly in both of the above two tasks. Finally, on the PASCAL VOC 2012 dataset, we have achieved the currently best mean IoU score for semantic segmentation, and the state-of-the-art performance for instance-level segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Truth Prediction Image Truth Prediction Image Truth Prediction  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Truth Prediction Image Truth Prediction Image Truth Prediction</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figs. 2 ,</head><label>2</label><figDesc>3 and 4, as well as for instance segmentation in Figs. 5. Note that the white borders and regions in the ground-truth in Figs. 2 and 5 are excluded during evaluation, as well as those black regions in the ground-truth in Fig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The qualitative results of our method on PASCAL VOC 2012 for semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>The qualitative results of our method on PASCAL-Context for semantic segmentation. The qualitative results of our method on Cityscapes for semantic segmentation. The qualitative results of our method on PASCAL VOC 2012 for instance segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of our vanilla FCRNs on the val set of PASCAL VOC 2012.</figDesc><table><row><cell cols="8">Depth Resolution Kernel Dilation FoV Pixel acc. % Mean acc. % Mean IoU %</cell></row><row><cell>50</cell><cell>1/16</cell><cell>3</cell><cell>6</cell><cell>208</cell><cell>92.74</cell><cell>78.68</cell><cell>69.09</cell></row><row><cell>50</cell><cell>1/8</cell><cell>3</cell><cell>6</cell><cell>104</cell><cell>92.50</cell><cell>77.60</cell><cell>67.61</cell></row><row><cell>50</cell><cell>1/8</cell><cell>3</cell><cell>12</cell><cell>200</cell><cell>93.03</cell><cell>79.51</cell><cell>69.94</cell></row><row><cell>50</cell><cell>1/8</cell><cell>3</cell><cell>18</cell><cell>296</cell><cell>93.02</cell><cell>79.28</cell><cell>70.01</cell></row><row><cell>50</cell><cell>1/8</cell><cell>5</cell><cell>6</cell><cell>200</cell><cell>92.98</cell><cell>79.34</cell><cell>69.81</cell></row><row><cell>50</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>392</cell><cell>93.25</cell><cell>79.84</cell><cell>71.10</cell></row><row><cell>50</cell><cell>1/8</cell><cell>7</cell><cell>6</cell><cell>296</cell><cell>93.14</cell><cell>79.54</cell><cell>70.67</cell></row><row><cell>101</cell><cell>1/16</cell><cell>3</cell><cell>6</cell><cell>208</cell><cell>93.22</cell><cell>80.16</cell><cell>70.93</cell></row><row><cell>101</cell><cell>1/8</cell><cell>3</cell><cell>6</cell><cell>104</cell><cell>93.20</cell><cell>79.87</cell><cell>70.20</cell></row><row><cell>101</cell><cell>1/8</cell><cell>3</cell><cell>12</cell><cell>200</cell><cell>93.68</cell><cell>81.29</cell><cell>72.34</cell></row><row><cell>101</cell><cell>1/8</cell><cell>3</cell><cell>18</cell><cell>296</cell><cell>93.67</cell><cell>81.15</cell><cell>72.37</cell></row><row><cell>101</cell><cell>1/8</cell><cell>5</cell><cell>6</cell><cell>200</cell><cell>93.52</cell><cell>81.00</cell><cell>71.97</cell></row><row><cell>101</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>392</cell><cell>93.87</cell><cell>81.87</cell><cell>73.41</cell></row><row><cell>101</cell><cell>1/8</cell><cell>7</cell><cell>6</cell><cell>296</cell><cell>93.61</cell><cell>81.34</cell><cell>72.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of category-wise and mean IoU scores on the test set of PASCAL VOC 2012. Bs. 88.3 40.4 86.5 66.6 80.1 91.6 84.3 90.1 36.6 83.7 53.6 84.5 85.1 79.9 83.9 59.0 83.3 44.6 81.1 74.5 74.8 Results on the test set FCN-8s [1] 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2 DeepLab [2] 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2 80.8 59.7 82.2 50.4 73.1 63.7 71.6 CRFasRNN [18] 87.5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1 80.6 59.5 82.8 47.8 78.3 67.1 72.0 DeconvNet [30] 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6 80.2 58.8 83.4 54.3 80.7 65.0 72.5 DPN [28] 87.7 59.4 78.4 64.9 70.3 89.3 83.5 86.1 31.7 79.9 62.6 81.9 80.0 83.5 82.3 60.5 83.2 53.4 77.9 65.0 74.1 UoAContext [3] 90.6 37.6 80.0 67.8 74.4 92.0 85.2 86.2 39.1 81.2 58.9 83.8 83.9 84.3 84.8 62.1 83.2 58.2 80.8 72.3 75.3 ours 91.9 48.1 93.4 69.3 75.5 94.2 87.5 92.8 36.7 86.9 65.2 89.1 90.2 86.5 87.2 64.6 90.1 59.7 85.5 72.7 79.1</figDesc><table><row><cell>Method</cell><cell>aeroplane</cell><cell>bicycle</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>diningtable</cell><cell>dog</cell><cell>horse</cell><cell>motorbike</cell><cell>person</cell><cell>pottedplant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tvmonitor</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Results on the val set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="22">FCRN 86.7 39.5 85.5 66.9 79.3 90.7 84.7 90.6 34.0 79.1 51.6 83.9 80.6 80.0 83.0 55.7 80.6 40.3 82.7 72.9 73.4</cell></row><row><cell>FCRN +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of our vanilla FCRNs on the val set of Cityscapes.</figDesc><table><row><cell cols="8">Depth Resolution Kernel Dilation FoV Pixel acc. % Mean acc. % Mean IoU %</cell></row><row><cell>50</cell><cell>1/16</cell><cell>3</cell><cell>6</cell><cell>208</cell><cell>93.83</cell><cell>74.67</cell><cell>66.41</cell></row><row><cell>50</cell><cell>1/8</cell><cell>3</cell><cell>6</cell><cell>104</cell><cell>94.38</cell><cell>74.89</cell><cell>66.58</cell></row><row><cell>50</cell><cell>1/8</cell><cell>3</cell><cell>12</cell><cell>200</cell><cell>94.47</cell><cell>75.91</cell><cell>67.68</cell></row><row><cell>50</cell><cell>1/8</cell><cell>3</cell><cell>18</cell><cell>296</cell><cell>94.53</cell><cell>76.52</cell><cell>68.38</cell></row><row><cell>50</cell><cell>1/8</cell><cell>5</cell><cell>6</cell><cell>200</cell><cell>94.48</cell><cell>76.17</cell><cell>68.04</cell></row><row><cell>50</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>392</cell><cell>94.61</cell><cell>76.68</cell><cell>68.71</cell></row><row><cell>50</cell><cell>1/8</cell><cell>5</cell><cell>18</cell><cell>584</cell><cell>94.64</cell><cell>76.34</cell><cell>68.53</cell></row><row><cell>50</cell><cell>1/8</cell><cell>7</cell><cell>6</cell><cell>296</cell><cell>94.58</cell><cell>76.88</cell><cell>68.79</cell></row><row><cell>50</cell><cell>1/8</cell><cell>7</cell><cell>12</cell><cell>584</cell><cell>94.64</cell><cell>76.57</cell><cell>68.79</cell></row><row><cell>101</cell><cell>1/16</cell><cell>3</cell><cell>6</cell><cell>208</cell><cell>94.11</cell><cell>76.26</cell><cell>67.62</cell></row><row><cell>101</cell><cell>1/8</cell><cell>3</cell><cell>6</cell><cell>104</cell><cell>94.68</cell><cell>77.15</cell><cell>68.58</cell></row><row><cell>101</cell><cell>1/8</cell><cell>3</cell><cell>12</cell><cell>200</cell><cell>94.78</cell><cell>78.30</cell><cell>69.99</cell></row><row><cell>101</cell><cell>1/8</cell><cell>3</cell><cell>18</cell><cell>296</cell><cell>94.82</cell><cell>78.21</cell><cell>70.00</cell></row><row><cell>101</cell><cell>1/8</cell><cell>5</cell><cell>6</cell><cell>200</cell><cell>94.75</cell><cell>78.11</cell><cell>69.89</cell></row><row><cell>101</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>392</cell><cell>94.87</cell><cell>79.17</cell><cell>71.16</cell></row><row><cell>101</cell><cell>1/8</cell><cell>7</cell><cell>6</cell><cell>296</cell><cell>94.75</cell><cell>78.43</cell><cell>70.40</cell></row><row><cell>152</cell><cell>1/16</cell><cell>3</cell><cell>6</cell><cell>208</cell><cell>94.26</cell><cell>76.89</cell><cell>68.30</cell></row><row><cell>152</cell><cell>1/8</cell><cell>3</cell><cell>6</cell><cell>104</cell><cell>94.82</cell><cell>78.30</cell><cell>69.69</cell></row><row><cell>152</cell><cell>1/8</cell><cell>3</cell><cell>12</cell><cell>200</cell><cell>94.94</cell><cell>78.79</cell><cell>70.66</cell></row><row><cell>152</cell><cell>1/8</cell><cell>3</cell><cell>18</cell><cell>296</cell><cell>94.93</cell><cell>79.19</cell><cell>70.92</cell></row><row><cell>152</cell><cell>1/8</cell><cell>5</cell><cell>6</cell><cell>200</cell><cell>94.88</cell><cell>78.77</cell><cell>70.61</cell></row><row><cell>152</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>392</cell><cell>95.00</cell><cell>79.38</cell><cell>71.51</cell></row><row><cell>152</cell><cell>1/8</cell><cell>7</cell><cell>6</cell><cell>296</cell><cell>94.91</cell><cell>79.08</cell><cell>70.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of category-wise and mean IoU scores on the val set of Cityscapes. 80.3 90.8 47.6 53.8 53.1 58.1 70.2 91.2 59.6 93.2 77.1 54.4 93.0 67.1 79.4 62.2 57.3 72.7 71.5 FCRN+Bs. 97.6 82.0 91.7 52.3 56.2 57.0 65.7 74.4 91.7 62.5 93.8 79.8 59.6 94.0 66.2 83.7 70.3 64.2 75.5 74.6</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Results on val set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FCRN 97.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison on the val set of PASCAL-Context.</figDesc><table><row><cell>Method</cell><cell cols="3">Pixel acc. % Mean acc. % Mean IoU %</cell></row><row><cell>FCN-8s [1]</cell><cell>65.9</cell><cell>46.5</cell><cell>35.1</cell></row><row><cell>BoxSup [31]</cell><cell>-</cell><cell>-</cell><cell>40.5</cell></row><row><cell>UoA-Context [3]</cell><cell>71.5</cell><cell>53.9</cell><cell>43.3</cell></row><row><cell>ours</cell><cell>72.9</cell><cell>54.8</cell><cell>44.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison on the val set of PASCAL VOC 2012. Bs. means 'bootstrapping'.</figDesc><table><row><cell>Method</cell><cell cols="3">mAP r 0.5 % mAP r 0.7 % mAP r vol %</cell></row><row><cell>SDS [8]</cell><cell>49.7</cell><cell>25.3</cell><cell>41.4</cell></row><row><cell>Hypercolumn [9]</cell><cell>60.0</cell><cell>40.4</cell><cell>-</cell></row><row><cell>MNC [10]</cell><cell>63.5</cell><cell>41.5</cell><cell>-</cell></row><row><cell>50-layer</cell><cell>57.2</cell><cell>40.5</cell><cell>52.5</cell></row><row><cell>50-layer, Bs.</cell><cell>58.7</cell><cell>42.0</cell><cell>53.8</cell></row><row><cell>50-layer, Bs., weighted loss</cell><cell>59.8</cell><cell>43.5</cell><cell>54.7</cell></row><row><cell>101-layer, Bs., weighted loss</cell><cell>60.9</cell><cell>44.6</cell><cell>55.5</cell></row><row><cell>101-layer, Bs., weighted loss, COCO</cell><cell>61.5</cell><cell>46.6</cell><cell>56.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results showing the impact of online bootstrapping.</figDesc><table><row><cell cols="4">Depth Resolution Kernel Dilation</cell><cell>Bs.</cell><cell cols="3">Pixel acc. % Mean acc. % Mean IoU %</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">PASCAL VOC 2012</cell><cell></cell><cell></cell></row><row><cell>101</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>F</cell><cell>93.87</cell><cell>81.87</cell><cell>73.41</cell></row><row><cell>101</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>256</cell><cell>94.11</cell><cell>81.44</cell><cell>74.41</cell></row><row><cell>101</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>512</cell><cell>94.23</cell><cell>82.09</cell><cell>74.80</cell></row><row><cell>101</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>1024</cell><cell>94.08</cell><cell>81.84</cell><cell>74.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cityscapes</cell><cell></cell><cell></cell></row><row><cell>152</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>F</cell><cell>95.00</cell><cell>79.38</cell><cell>71.51</cell></row><row><cell>152</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>256</cell><cell>95.41</cell><cell>81.37</cell><cell>73.97</cell></row><row><cell>152</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>512</cell><cell>95.46</cell><cell>82.04</cell><cell>74.64</cell></row><row><cell>152</cell><cell>1/8</cell><cell>5</cell><cell>12</cell><cell>1024</cell><cell>95.38</cell><cell>81.00</cell><cell>73.45</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://host.robots.ox.ac.uk:8080/anonymous/MZVIPW.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Exploring context with deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03183</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ImageNet Large Scale Visual Rcognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simultaneously detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.05096</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online batch selection for faster training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A wavelet tour of signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-12" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
	<note>3rd ed</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
