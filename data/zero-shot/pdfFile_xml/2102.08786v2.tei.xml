<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Learning with 1D Convolutions on Random Walks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Toenshoff</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Hinrikus Wolf RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Learning with 1D Convolutions on Random Walks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose CRAWL (CNNs for Random Walks), a novel neural network architecture for graph learning. It is based on processing sequences of small subgraphs induced by random walks with standard 1D CNNs. Thus, CRAWL is fundamentally different from typical message passing graph neural network architectures. It is inspired by techniques counting small subgraphs, such as the graphlet kernel and motif counting, and combines them with random walk based techniques in a highly efficient and scalable neural architecture. We demonstrate empirically that CRAWL matches or outperforms state-of-the-art GNN architectures across a multitude of benchmark datasets for classification and regression on graphs.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph data is ubiquitous across multiple domains, reaching from cheminformatics and social network analysis to knowledge graphs. Being able to effectively learn on such graph data is thus extremely important. We propose a novel neural network architecture called CRAWL (CNNs for Random Walks) that is based on random walks and standard 1D CNNs. Essentially, CRAWL samples a set of random walks and extracts features that fully describe the subgraphs visible within a sliding window over these walks. The walks with the subgraph features are then processed with standard 1D convolutions. We experimentally verify that this approach consistently achieves state-of-the-art performance. For example, CRAWL outperforms all other approaches on the standard graph learning benchmarks MOLPCBA (graph classification; <ref type="bibr" target="#b19">Hu et al., 2020)</ref> and ZINC (graph regression; <ref type="bibr" target="#b11">Dwivedi et al., 2020)</ref>.</p><p>The CRAWL architecture was originally motivated from the empirical observation that in many application scenarios random walk based methods perform surprisingly well in comparison with graph neural networks (GNNs). A notable example is node2vec <ref type="bibr" target="#b17">(Grover &amp; Leskovec, 2016)</ref> in combination with various classifiers. A second observation is that standard GNNs are not very good at detecting small subgraphs, for example, cycles of length 6 <ref type="bibr" target="#b33">(Morris et al., 2019;</ref><ref type="bibr" target="#b49">Xu et al., 2019)</ref>.The distribution of such subgraphs in a graph carries relevant information about the structure of a graph, as witnessed by the extensive research on motif detection and counting (e.g. <ref type="bibr" target="#b2">Alon, 2007)</ref>.</p><p>We believe that the key to the strength of CRAWL is a favorable combination of engineering and expressiveness aspects. Even large numbers of random walks can be sampled very efficiently. Once the random walks are available, we can rely on existing highly optimized code for 1D CNNs, which allows us to fully exploit the strengths of modern hardware. Sampling small subgraphs in a sliding window on random walks has the advantage that even in sparse graphs it usually yields meaningful subgraph patterns. In terms of expressiveness, CRAWL detects both the global connectivity structure in a graph by sampling longer random walks as well as the full local structure within its window size. The gain in expressiveness compared to GNNs is mainly due to the detailed view on the local structure in the sliding window, which standard message passing GNNs (e.g. <ref type="bibr" target="#b15">Gilmer et al., 2017)</ref> do not have. We show that the expressiveness of CRAWL is incomparable to that of GNNs (Theorem 1). In particular, CRAWL even detects features that are not even accessible by higher-order GNNs.</p><p>CRAWL empirically outperforms advanced message passing GNN architectures on major benchmark datasets. On the molecular regression dataset ZINC <ref type="bibr" target="#b11">(Dwivedi et al., 2020)</ref>, CRAWL improves the best results currently listed on the leaderboard by roughly 40% (and 20% compared to the best published approach). CRAWL also places first on the leaderboard for MOLPCBA, a large molecular property prediction dataset from the OGB Project <ref type="bibr" target="#b19">(Hu et al., 2020)</ref>.</p><p>A basic requirement for graph learning methods is their isomorphism invariance, which guarantees that the result of a computation only depends on the structure and not on the specific representation of the input graph. A CRAWL model represents a random variable defined on graphs. This random variable is invariant (in the sense of <ref type="bibr" target="#b32">Maron et al., 2019b)</ref>, which means that it does not depend on a particular node numbering, but only on the isomorphism type of the input graph. Note that it does not contradict the invariance that on every single random walk that we sample we see the vertices in a specific order and can process the vertices in this order by 1D CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Over the last few years, message passing GNNs (MPGNNs) have been the dominant type of architecture in all kinds of graph related learning tasks . Thus, MPGNNs constitute the main baselines in our experiments. Many variants of this architecture exist, such as GCN <ref type="bibr" target="#b22">(Kipf &amp; Welling, 2017)</ref>, GIN , GAT <ref type="bibr" target="#b46">(Veli?kovi? et al., 2018)</ref>, GraphSage <ref type="bibr" target="#b18">(Hamilton et al., 2017)</ref>, and GatedGCN <ref type="bibr" target="#b5">(Bresson &amp; Laurent, 2017)</ref>. A novel variant of MPGNNs is PNA  which combines multiple types of local aggregation to improve performance. Another recent advance is DeeperGCN (DGCN) by <ref type="bibr" target="#b29">Li et al. (2020)</ref> which is designed for significantly deeper GNNs. PHC-GNNs <ref type="bibr" target="#b27">(Le et al., 2021)</ref> are GNNs with complex and hypercomplex feature vectors with learned multiplication strategies.</p><p>Multiple extensions to the standard message passing framework have been proposed that strengthen the theoretical expressiveness which otherwise is bounded by the 1-dimensional Weisfeiler-Leman algorithm. With 3WLGNN, <ref type="bibr" target="#b31">Maron et al. (2019a)</ref> suggested a higher-order GNN, which is equivalent to the 3-dimensional Weisfeiler-Leman kernel and thus more expressive than standard MPGNNs. In HIMP , the backbone of a molecule graph is extracted and then two GNNs are run in parallel on the backbone and the full molecule graph. This allows HIMP to detect structural features that are otherwise neglected. Explicit counts of fixed substructures such as cycles or small cliques have been added to the node and edge features by <ref type="bibr" target="#b4">Bouritsas et al. (2020)</ref>  <ref type="bibr">(GSN)</ref>. Similarly, <ref type="bibr" target="#b41">Sankar et al. (2017)</ref>, <ref type="bibr" target="#b28">Lee et al. (2019), and</ref><ref type="bibr" target="#b39">Peng et al. (2020)</ref> added the frequencies of motifs, i.e., common connected induced subgraphs, to improve the predictions of GNNs. <ref type="bibr" target="#b42">Sankar et al. (2020)</ref> introduce motif-based regularization, a framework that improves multiple MPGNNs. A novel approach with strong empirical performance is GINE+ <ref type="bibr" target="#b6">(Brossard et al., 2020)</ref>. It is based on GIN and aggregates information from higher-order neighborhoods, allowing it to detect small substructures such as cycles. Combining GINE+ with APPNP <ref type="bibr" target="#b23">(Klicpera et al., 2019)</ref>, a propagation scheme based on the personalized pagerank, improves the performance even further <ref type="bibr" target="#b40">(Rozemberczki, 2019)</ref>. <ref type="bibr" target="#b3">Beaini et al. (2020)</ref> proposed DGN, which incorporates directional awareness into message passing.</p><p>A different way to learn on graph data is to use similarity measures on graphs with graph kernels <ref type="bibr" target="#b26">(Kriege et al., 2020)</ref>. Graph kernels often count induced subgraphs such as graphlets, label sequences, or subtrees, which relates them conceptually to our approach. The graphlet kernel <ref type="bibr" target="#b44">(Shervashidze et al., 2009)</ref> counts the occurrences of all 5-node (or more general k-node) subgraphs. The Weisfeiler-Leman kernel <ref type="bibr" target="#b45">Shervashidze et al. (2011)</ref> is based on iterated degree sequences and effectively counts occurrences of local subtrees. The Weisfeiler-Leman algorithm is the traditional yardstick for the expressiveness of GNN architectures.</p><p>A few previous approaches utilize either random walks or conventional CNNs in the context of end-to-end graph learning, two concepts our method is also based on. <ref type="bibr" target="#b37">Nikolentzos &amp; Vazirgiannis (2020)</ref> propose a differentiable version of the random walk kernel and integrate it into a GNN architecture. In <ref type="bibr" target="#b14">Geerts (2020)</ref>, the -walk MPGNN adds random walks directly as features to the f (v 1 ) 0000000 0000 000</p><formula xml:id="formula_0">f (v 2 ) g(v 1 , v 2 ) 0000 000 f (v 3 ) g(v 2 , v 3 ) 0000 100 f (v 4 ) g(v 3 , v 4 ) 0000 010 f (v 5 ) g(v 4 , v 5 ) 0000 001 f (v 6 ) g(v 5 , v 6 ) 0000 110 f (v 4 ) g(v 6 , v 4 ) 0010 101 . . . . . . . . . . . . ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>n o d e l a b e l e d g e l a b e l i d e n t i t y c o n n e c t i v i t y <ref type="figure">Figure 1</ref>: Example of the information flow in a CRAWL layer for a graph with 8 nodes. We sample a walk W and compute the feature matrix X based on node embeddings f, edge embeddings g, and a window size of s = 4. To this matrix we apply a 1D CNN with receptive field r = 5 and pool the output into the nodes to update their embeddings.</p><formula xml:id="formula_1">1 2 3 4 5 6 c 1 c 2 c 3 c 4 c 5 . . . ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? X(W, f, g, 4) random walk W = {v 1 , v 2 , v 3 , v 4 , v 5 , v 6 , v 4 , . . . } 1D CNN Pool Update</formula><p>nodes and connects the architecture theoretically to the 2-dimensional Weisfeiler-Leman algorithm. Patchy-SAN <ref type="bibr" target="#b36">(Niepert et al., 2016)</ref> normalizes graphs in such a way that they can be interpreted by CNN layers. <ref type="bibr" target="#b52">Zhang et al. (2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>CRAWL processes random walks with convolutional neural networks. We initially sample a large enough set of (relatively long) random walks. Each CRAWL layer uses these walks to update a latent node embedding as follows. For each of the walks, the layer constructs features that contain the sequences of node and edge labels that occurred. Additionally, for every position in each walk, the features encode to which of its s predecessors the current node is identical or adjacent. The window size s is a hyperparameter of the model. These walk features are then processed by a 1D CNN. The output of the CNN is an embedding for every position in each random walk. These embeddings are pooled into the nodes, that is, for each node in the graph, we average over the embeddings of all the positions at which it occurs in the random walks. Finally, the CRAWL layer uses a simple MLP to produce the new node embedding from this information. In the full network, each layer uses the embedding produced by the preceding layer as node labels. After the last layer, the node embeddings can be pooled to perform graph level tasks.</p><p>Effectively, through the CNN, we extract structural information from many small subgraphs of the size of the CNN's receptive field. Those subgraphs are always connected since they are induced from the nodes of a random walk.</p><p>The process of sampling random walks in a graph is not deterministic and therefore the final output of CRAWL is a random variable. However, the output of a trained CRAWL model has low variance such that the inherent randomization does not limit our method's usefulness in real world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Random Walks</head><formula xml:id="formula_2">A walk of length ? N in a graph G = (V, E) is a sequence of nodes (v 0 , . . . , v ) ? V +1 with v i?1 v i ? E for all i ? [ ].</formula><p>A random walk in a graph is obtained by starting at some initial node v 0 ? V and then iteratively sampling the next node v i+1 randomly from the neighbors N G (v i ) of the current node v i . We consider two different random walk strategies: uniform and non-backtracking.</p><p>The uniform walks are obtained by sampling the next node uniformly from all neighbors:</p><formula xml:id="formula_3">v i+1 ? U N G (v i ) .</formula><p>On sparse graphs with nodes of small degree (such as molecules) this walk strategy has a tendency to backtrack often. This slows the traversal of the graph and interferes with the discovery of long-range patterns. The non-backtracking walk strategy addresses this issue by excluding the previous node from the sampling (unless the degree is one):</p><formula xml:id="formula_4">v i+1 ? D NB (v i ) with D NB (v i ) = U N G (v i ) , if i = 0 ? deg(v i ) = 1 U N G (v i )\{v i?1 } , else.</formula><p>The choice of the walk strategy is a hyperparameter of CRAWL. In our experiments the nonbacktracking strategy usually performs better as shown in Section 3.5.</p><p>CRAWL initially samples m random walks ? = {W 1 , . . . , W m } of length from the input graph G. The values for m and are not fixed hyperparameters of the model but instead can be chosen at runtime. By default, we start one walk at every node, i.e., m = |V |. We noted that reducing the number of walks during training can help against overfitting and of course is a way to reduce the memory footprint which is important for large graphs. If we choose to use fewer random walks, we sample m = p * ? |V | starting nodes uniformly at random from the nodes of the graph with chosen probability p * . We typically choose ? 50, practically ensuring that each node appears multiple times in the walks. For inference, we choose a larger of up to 150 which improves the predictions.</p><p>While, in theory, every layer of CRAWL may use different random walks, we sample the random walks once in the beginning of a run and then make use of the same walks in every layer. This allows us to increase the number of random walks that each layer may work with as the total number of walks is bounded by the GPU memory. This empirically improves stability in training and also the overall performance.</p><p>We call contiguous segments W [i : j] := (w i , . . . , w j ) of a walk W = (w 0 , . . . , w ) walklets. The center of a walklet (w i , . . . , w j ) of even length j ? i is the node w (i+j)/2 . For each walklet w = (w i , . . . , w j ), by G[w] we denote the subgraph induced by G on the set {w i , . . . , w j }. Note that G[w] is connected as it contains all edges w k w k+1 for i ? k &lt; j and may contain additional edges. Also note that the w k are not necessarily distinct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Walk Features</head><p>Based on the walks and a local window size s, we define feature vectors which can then be processed by 1D CNNs. Those feature vectors consist of four parts: one for node features, one for edge features along the walk, and the last two for local structural information. <ref type="figure">Figure 1</ref> depicts an example of a walk feature matrix and its use in a CRAWL layer.</p><p>Given a walk W ? V of length ? 1 in a graph G = (V, E), a d-dimensional node embedding f : V ? R d , a d -dimensional edge embedding g : E ? R d , and a local window size s &gt; 0 we define the walk feature matrix X(W, f, g, s) ? R ?d X with feature dimension d X = d+d +s+(s?1) as X(W, f, g, s) = (f W g W I s W A s W ). For ease of notation, the first dimensions of the matrices f W , g W , I s W , A s W are indexed from 0 to ? 1. Here, the node feature sequence f W ? R ?d and the edge feature sequence g W ? R ?d are defined as the concatenation of node and edge features, respectively. Formally,</p><formula xml:id="formula_5">(f W ) i,_ = f (v i ) and (g W ) i,_ = 0, if i = 0 g(v i?1 v i ), else.</formula><p>We define the local identity relation I s W ? {0, 1} ?s and the local adjacency relation</p><formula xml:id="formula_6">A s W ? {0, 1} ?(s?1) as (I s W ) i,j = 1, if i?j ? 0 ? v i = v i?j 0, else and (A s W ) i,j = 1, if i?j ? 1 ? v i v i?j?1 ? E 0, else.</formula><p>Intuitively, I s W and A s W are binary matrices that contain one row for every node v i in the walk W. The bitstring for v i in I s W encodes which of the s predecessors of v i in W are identical to v i , that is, where the random walk looped or backtracked. Similarly, A s W stores to which of its predecessors v i has an edge in G. The direct predecessor v i?1 must share an edge with v i and is thus omitted in A s W . Note that we do not leverage edge labels of edges not on the walk, only the existence of such edges within the local window is encoded in A s W . For any walklet w = W [i : i + s], the restriction of the walk feature matrix to rows i, . . . , i + s contains a full description about the induced subgraphs G[w]. Hence, when we apply a CNN with receptive field of size at most s + 1 to the walk feature matrix, the CNN filter has full access to the subgraph induced by the walklet within its scope.</p><formula xml:id="formula_7">CRaWl Layer X(?, h t?1 , F E , s) h t?1 ? R |V |?d ? ? V m? 1D CNN C t node pool MLP U t h t ? R |V |?d</formula><p>Let ? = {W 1 , . . . , W m } be the sampled set of walks. By stacking the individual feature matrices for each walk, we get the walk feature tensor X(?, f, g, s) ? R m? ?d X defined as X(?, f, g, s) i = X(W i , f, g, s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CRAWL Layer</head><p>A CRAWL network iteratively updates latent embeddings for each node. Let G = (V, E) be a graph with initial node and edge feature maps</p><formula xml:id="formula_8">F V : V ? R d V and F E : E ? R d E .</formula><p>The function h t : V ? R dt stores the output of the t-th layer of CRAWL and the initial node features are stored in h 0 = F V . In principle, the size of the output node embedding d t is an independent hyperparameter for each layer. In practice, we use the same size d for the output node embeddings of all layers for simplicity.</p><p>The t-th layer of a CRAWL network constructs the walk feature tensor X t = X(?, h t?1 , F E , s) using h t?1 as its input node embedding and the graph's edge features F E . This walk feature tensor is then processed by a convolutional network CNN t based on 1D CNNs. The first dimension of X t of size m is viewed as the batch dimension. The convolutional filters move along the second dimension (and therefore along each walk) while the third dimension contains the feature channels. Each CNN t consists of 3 convolutional layers combined with ReLU activations and batch normalization. A detailed description is provided in Appendix B. The stack of operations has a receptive field of s+1 and effectively applies an MLP to each subsection of this length in the walk feature matrices. In each CNN t , we use Depthwise Separable Convolutions <ref type="bibr" target="#b8">(Chollet, 2017)</ref> for efficiency. Each such CNN network uses O(d 2 + sd) trainable parameters. Both the time and the memory complexity of applying CNN t to X t are therefore in O m ? ? (d 2 + sd) .</p><p>The output of CNN t is a tensor C t ? R m?( ?s)?d . Note that the second dimension is ?s instead of as no padding is used in the convolutions. Through its receptive field, the CNN operates on walklets of size s+1 and produces embeddings for those. We pool those embeddings into the nodes of the graph by collecting for each node v ? V all embeddings of walklets centered at v. Let ? = {W 1 , . . . , W m } be a set of walks with</p><formula xml:id="formula_9">W i = {v i,1 , . . . , v i, }. Then C t i,j?s/2 ? R d is the embedding computed by CNN t for the walklet w = W i [j ? s 2 : j + s 2 ] centered at v i,j .</formula><p>Then, the pooling operation is given as</p><formula xml:id="formula_10">p t (v) = mean (i,j)?center(?,s,v) C t i,j?s/2 with center(?, s, v) = (i, j) v i,j = v, i ? [m], s 2 &lt; j &lt; ? s 2 .</formula><p>where center(?, s, v) encodes the walklets of length s+1 in which v occurs as center. An illustration of how the output of the CNN is pooled into the nodes of the graph can be found in <ref type="figure">Figure 1</ref>. The output of the pooling step is a vector p t (v) ? R d for each v. This vector is then processed by a trainable MLP U t with a single hidden layer of dimension 2d to compute the next intermediate node embedding h t (v). Formally, the update procedure of a CRAWL layer is defined by</p><formula xml:id="formula_11">h t (v) = U t p t (v) .</formula><p>The upper part of <ref type="figure" target="#fig_1">Figure 2</ref> gives an overview over the elements of a CRAWL layer. The runtime of each CRAWL layer is linear in the number of walk steps m ? for the CNN and nodes |V | for the final MLP. The initial generation of the random walks is in O m ? . Note that m and are not fixed hyperparameters. They can be chosen freely at runtime and have no effect on the number of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Architecture</head><p>The architecture we use in the experiments, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (bottom), works as follows. The first step for running CRAWL is to compute a set of random walks ? as described in Section 2.1. We then apply multiple CRAWL layers with residual connections. In each CRAWL layer, we typically choose s = 8. After the final CRAWL layer, we apply batch normalization and a ReLU activation to the latent node embeddings before we perform a global pooling step. As pooling we use either sum-pooling or mean-pooling. Finally, a simple feedforward neural network is used to produce a graph-level output which can then be used in classification and regression tasks. In our experiments, we use either an MLP with one hidden layer of dimension d or a single linear layer.</p><p>Since CRAWL layers are based on iteratively updating latent node embeddings, they are fully compatible with conventional message passing layers and related techniques such as virtual nodes <ref type="bibr" target="#b15">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b30">Li et al., 2017;</ref><ref type="bibr" target="#b20">Ishiguro et al., 2019)</ref>. In our experiments, we use virtual nodes whenever this increases validation performance. A detailed explanation of our virtual node layer is provided in the Appendix. Combining CRAWL with message passing layers is left as future work.</p><p>We implemented CRAWL in PyTorch <ref type="bibr" target="#b38">(Paszke et al., 2019;</ref>, a public repository is available at GitHub 1 . Crucially, the random walks and the feature matrices are computed entirely on the GPU, increasing speed and reducing the data exchange between CPU and GPU. As a downside of this approach, the current implementation struggles to stay within the available RAM of most GPUs for large graphs such as those occurring in many node classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Recently, two initiatives were launched by Dwivedi et al. (2020) (Benchmarking GNNs) and Hu et al.</p><p>(2020) (Open Graph Benchmark, OGB) to improve the experimental standards used in graph learning research. Both projects aim to solve common problems of previous experimental settings. Those problems included varying training and evaluation protocols as well as the use of small datasets without standardized splits into training, validation, and test sets. This made the results hard to compare. Both projects introduced novel benchmark datasets with fixed splits and specified training and evaluation procedures. Here, we will use datasets from both projects to evaluate the empirical capabilities of CRAWL. In Appendix A we provide additional results for some of the formerly more common datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>From the OGB project, we use the molecular property prediction dataset MOLPCBA with more than 400k molecules. Each of its 128 binary targets states whether or not a molecule is active towards a </p><formula xml:id="formula_12">- - 0.2842 ? 0.0043 PHC-GNN 0.164 ? 0.003 - - 0.2947 ? 0.0026 GINE+ - - - 0.2917 ? 0.0015 GINE+&amp;APPNP - - - 0.2979 ? 0.0030 OTHER HIMP * 0.151 ? 0.006 - - - DGN ? 0.168 ? 0.003 - 72.840 ? 0.420 - GSN * 0.108 ? 0.018 - - - OUR CRAWL 0.085 ? 0.004 97.944 ? 0.050 69.013 ? 0.259 0.2986 ? 0.0025</formula><p>particular bioassay (a method that quantifies the effect of a substance on a particular kind of living cells or tissues). The dataset is adapted from the MoleculeNet <ref type="bibr" target="#b47">(Wu et al., 2018)</ref> and represents molecules as graphs of atoms. It contains multidimensional node and edge features which encode information such as atomic number and chirality. Additionally, it provides a train/val/test split that separates structurally different types of molecules for a more realistic experimental setting. On MOLPCBA, the performance is measured in terms of the average precision (AP).</p><p>From the other initiative, started by <ref type="bibr" target="#b11">Dwivedi et al. (2020)</ref>, we use 4 datasets. The first dataset ZINC is a molecular regression dataset. It is a subset of 12K molecules from the larger ZINC database. The aim is to predict the constrained solubility, an important chemical property of molecules. The node label is the atomic number and the edge labels specify the bond type. The datasets CIFAR10 and MNIST are graph datasets derived from the corresponding image classification tasks and contain 60K and 70K graphs, respectively. The original images are modeled as networks of super-pixels. Both datasets are 10-class classification problems. The last dataset CSL is a synthetic dataset containing 150 Cyclic Skip Link graphs <ref type="bibr" target="#b35">(Murphy et al., 2019)</ref>. Those are 4-regular graphs obtained by adding cords of a fixed length to a cycle. The formal definition and an example are provided in the appendix. The aim is to classify the graphs by their isomorphism class. Since all graphs are 4-regular and no node or edge features are provided, this task is unsolvable for most message passing architectures such as standard GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setting</head><p>We adopt the training procedure specified by <ref type="bibr" target="#b11">Dwivedi et al. (2020)</ref>. In particular, the learning rate is initialized as 10 ?3 and decays with a factor of 0.5 if the performance on the validation set stagnates for 10 epochs. The training stops once the learning rate falls below 10 ?6 . <ref type="bibr" target="#b11">Dwivedi et al. (2020)</ref> also specify that networks need to stay within parameter budgets of either 100K or 500K parameters. This ensures a fairer comparison between different methods. For ZINC, we use the larger budget of 500K parameters. For MNIST, CIFAR10 and CSL we build CRAWL models with the smaller budget of 100K since more baseline results are available in the literature. The OGB Project does not specify a standardized training procedure or parameter budgets. For MOLPCBA, we train for 60 epochs and decay the learning rate once with a factor of 0.1 after epoch 50.</p><p>During training, we always set the walk length to = 50. For evaluation, we use walks of length = 150, except for MOLPCBA where we use = 100 for efficiency. All hyperparameters and the exact number of trainable parameters are listed in the appendix. There, we also specify the sets of hyperparameters we searched for each dataset. For each dataset, we train 5 models with different random seeds, except for MOLPCBA where we trained 10 models to meet the submission requirements of the OGB project. We report the mean performance and standard deviation across those models. During inference, the output of each model depends on the sampled random walks. Thus, we evaluate each model on 10 different seeds used for the generation of random walks and take the average of those runs as the model's performance.</p><p>In the appendix we provide extended results that additionally specify the internal model deviation, that is, the impact of the random walks on the performance. Since this internal model deviation is substantially lower than the differences between the models, they are comparatively insignificant when comparing CRAWL to other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compare the results obtained with CRAWL to a wide range of graph learning methods. We report values that are currently listed on the leaderboard for the benchmark datasets as well as additional results from the literature that are not officially listed yet. Our main baselines are numerous message passing GNN architectures that have been proposed in recent years (see Section 1.1). Additional methods not yet mentioned in Section 1.1 are MPNN by <ref type="bibr" target="#b9">Corso et al. (2020)</ref> as well as FLAG <ref type="bibr" target="#b25">(Kong et al., 2020)</ref> which was proposed to improve the training of GNNs with adversarial data augmentation. <ref type="table" target="#tab_1">Table 1</ref> provides our results on ZINC, MNIST, CIFAR10, and MOLPCBA. On the ZINC dataset, CRAWL achieves an MAE of 0.085. This is approximately a 40% improvement over the current first place (PNA) of the official leaderboard. CRAWL's performance on MNIST dataset is on par with PNA (within standard deviation), which is also the state of the art on this dataset. On CIFAR10, CRAWL achieves the fourth highest accuracy among the eleven compared approaches. For MOLPCBA, we report the baseline results from the leaderboard of the OGB project. On MOLPCBA, CRAWL yields state-of-the-art results and beats all other architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>The results on CSL are reported in <ref type="table" target="#tab_2">Table 2</ref>. We consider two variants of CSL, the pure task and an easier variant in which node features based on Laplacian eigenvectors are added as suggested by <ref type="bibr" target="#b11">Dwivedi et al. (2020)</ref>. Without additional node features, CRAWL achieves an accuracy of 100% which indicates that the task is comparatively easy for it. None of the 5 trained CRAWL models misclassified a single graph in the test folds. 3WLGNN is theoretically capable of solving the task without additional node features but unlike CRAWL does not achieve 100% accuracy. Without the Laplacian features that essentially encode the solution, MPGNNs cannot distinguish the 4-regular CSL graphs and achieve at most 10% accuracy. With Laplacian features, all but 3WLGNN achieve very good performance.</p><p>Overall, CRAWL performs very well on a variety of datasets across several domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>In an ablation study on the ZINC, MOLPCBA, and CSL datasets, we evaluated the importance of the identity and adjacency encodings in the walk features and the effects of uniform walks and nonbacktracking walks. The structural encodings improve CRAWL's performance on all three datasets.</p><p>On ZINC and CSL, the structural encodings give a significant benefit on the performance, while on MOLPCBA the improvement is only marginal. Furthermore, non-backtracking walks consistently outperform uniform walks on all three datasets. The detailed results are provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Expressiveness</head><p>In this section, we report on theoretical results comparing the expressiveness of CRAWL with that of other methods. The additional strength of CRAWL is mainly derived from the fact that it detects small subgraphs (of size determined by the window size hyperparameter s) and can sample such subgraphs from a non-uniform, but well-defined, distribution determined by the random walks. In this sense, it is similar to network analysis techniques based on motif detection <ref type="bibr" target="#b2">(Alon, 2007)</ref> and graph kernels based on counting subgraphs, such as the graphlet kernel <ref type="bibr" target="#b44">(Shervashidze et al., 2009</ref>).</p><p>The following results are concerned with the basic expressiveness question which graphs can be distinguished by the various methods, assuming that optimal parameters for the models are available. They do not discuss how such parameters can be learned. This limits the scope of these results, but they still give useful intuition about the different approaches.</p><p>It is known that the expressiveness of GNNs corresponds exactly to that of the 1-dimensional Weisfeiler-Leman algorithm (1-WL) <ref type="bibr" target="#b33">(Morris et al., 2019;</ref><ref type="bibr" target="#b49">Xu et al., 2019)</ref>, in the sense that two graphs are distinguished by 1-WL if and only if they can be distinguished by a GNN. It is also known that higher-dimensional versions of WL characterize the expressiveness of higher-order GNNs <ref type="bibr" target="#b33">(Morris et al., 2019)</ref>. Theorem 1.</p><p>(1) For every k ? 1 there are graphs that are distinguishable by CRAWL, but not by k-WL (and hence not by k-dimensional GNNs). <ref type="formula">(2)</ref> There are graphs that are distinguishable by 1-WL (and hence by GNNs), but not by CRAWL.</p><p>We state a precise quantitative version of the theorem and give a proof in Appendix D. Let us just note that for assertion <ref type="formula" target="#formula_21">(1)</ref> we need a window size s and walk length quadratic in k. However, the execution cost of CRAWL remains linear in the graph size n, compared to the ?(n k )-execution cost for even a single layer of k-dimensional GNN. For assertion <ref type="formula">(2)</ref> of the theorem, we can allow CRAWL to use a window size and path length linear in the size of the graphs. It can also be shown that CRAWL with a window size polynomial in the size of the graphlets is strictly more expressive than graphlet kernels. We omit the precise result, which can be proved similarly to Theorem 1 (1), due to space limitations.</p><p>Let us finally remark that the expressiveness of GNNs can be considerably strengthened by adding a random node initialization <ref type="bibr" target="#b0">(Abboud et al., 2020;</ref><ref type="bibr" target="#b43">Sato et al., 2020)</ref>. The same can be done for CRAWL, but so far the need for such a strengthening (at the cost of a higher variance) did not arise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced a novel neural network architecture CRAWL for graph learning that is based on random walks and 1D CNNs. Thus, CRAWL is fundamentally different from standard graph neural networks. We demonstrated that this approach works very well across a variety of graph level tasks and is able to outperform state-of-the-art GNNs. By construction, CRAWL can detect arbitrary substructures up to the size of its local window. In particular, on the regular graphs of CSL where pure MPGNNs fail because of the lack of expressiveness, CRAWL is able to extract useful features and solve this task. Future work includes extending the experimental framework to node-level tasks and to motif counting. In both cases, one needs to scale CRAWL to work on individual large graphs instead of many medium sized ones.</p><p>CRAWL can be viewed as an attempt to process random walks and the structures they induce with end-to-end neural networks. The strong empirical performance demonstrates the potential of this general approach. However, many variations remain to be explored, including different walk strategies, variations in the walk features, and alternative pooling functions for pooling walklet embeddings into nodes or edges. In view of the incomparability of the expressiveness of GNNs and CRAWL, hybrid approaches that interleave CRAWL layers and GNN layers seem attractive as well.</p><p>Beyond plain 1D-CNNs, other deep learning architectures for sequential data, such as transformer networks, could be used to process random walks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Extended Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Additional Experiments</head><p>In this section we evaluate CRAWL on commonly used benchmark datasets from the domain of social networks. We use a subset from the TUDataset <ref type="bibr" target="#b26">(Morris et al., 2020)</ref>, a list of typically small graph datasets from different domains e.g. chemistry, bioinformatics, and social networks. We focus on three datasets originally proposed by <ref type="bibr" target="#b50">Yanardag &amp; Vishwanathan (2015)</ref>: COLLAB, a scientific collaboration dataset, IMDB-MULTI, a multiclass dataset of movie collaboration of actors/actresses, and REDDIT-BIN, a balanced binary classification dataset of Reddit users which discussed together in a thread. These datasets do not have any node or edge features and the tasks have to be solved purely with the structure of the graphs.</p><p>We stick to the experimental protocol suggested by <ref type="bibr" target="#b49">Xu et al. (2019)</ref>. Specifically, we perform a 10-fold cross validation. Each dataset is split into 10 stratified folds. We perform 10 training runs where each split is used as test data once, while the remaining 9 are used for training. We then select the epoch with the highest mean test accuracy across all 10 runs. We report this mean test accuracy as the final result. This is not the most realistic setup for simulating real world tasks, since there is no clean split between validation and test data. But in fact, it is the most commonly used experimental setup for these datasets and is mainly justified by the comparatively small number of graphs. Therefore, we adopt the same procedure for the sake of comparability to the previous literature. For COLLAB and IMDB-MULTI we use the same 10-fold split used by <ref type="bibr" target="#b52">Zhang et al. (2018)</ref>. For REDDIT-BIN we computed our own stratified splits. We also computed separate stratified 10-fold splits for hyperparameter tuning.</p><p>We adapt the training procedure of CRAWL towards this setup. Here, the learning rate decays with a factor of 0.5 in fixed intervals. These intervals are chosen to be 20 epochs on COLLAB and REDDIT-BINARY and as 50 epochs on IMDB-MULTI. We train for 200 epochs on COLLAB and REDDIT-BINARY and for 500 epochs on IMDB-MULTI. This ensures a consistent learning rate profile across all 10 runs for each dataset. <ref type="table" target="#tab_3">Table 3</ref> reports the achieved accuracy of CRAWL and several key baselines on those datasets. For the baselines, we provide the results as reported in the literature. For comparability, we only report values for baselines with the same experimental protocol. On IMDB-MULTI, the smallest of the three datasets, CRAWL yields a slightly lower accuracy than most baselines. On COLLAB, our method performs similarly to standard MPGNN architectures such as GIN. CRAWL outperforms all baselines that report values for REDDIT-BIN. Note that GSN, the method with the best results on COLLAB and IMDB-MULTI, does not scale as well as CRAWL and is infeasible for REDDIT-BIN which contains graphs with several thousand nodes. <ref type="table" target="#tab_4">Table 4</ref> provides the full results from our experimental evaluation. It reports the performance on the train, validation, and test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Detailed Results for all Experiments</head><p>Recall that the output of CRAWL is a random variable. The predictions for a given input graph may vary when different random walks are sampled. To quantify this additional source of randomness, . When evaluating (both on test and validation data), we evaluate each model r ? N times, with different random walks in each evaluation run. Let p i,j ? R measure the performance achieved by the model m i in its j-th evaluation run. Note that the unit of p i,j varies between experiments (Accuracy, MAE, . . . ). We formally define the internal model deviation as</p><formula xml:id="formula_13">IMD = 1 q ? 1?i?q STD ({p i,j | 1 ? j ? r}) ,</formula><p>where STD(?) is the standard deviation of a given distribution. Intuitively, the IMD measures how much the performance of a trained model varies when applying it multiple times to the same input.</p><p>It quantifies how the model performance depends on the random walks that are sampled during evaluation.</p><p>We formally define the cross model deviation as</p><formula xml:id="formula_14">CMD = STD ? ? ? ? ? 1 r ? 1?j?r p i,j | 1 ? i ? q ? ? ? ? ? .</formula><p>The CMD measures the deviation of the average model performance between different training runs. It therefore quantifies how the model performance depends on the random initialization of the network parameters before training.</p><p>In the main section, we only reported the CMD for simplicity. Note that the CMD is significantly larger then the IMD across all experiments. Therefore, trained CRAWL models can reliably produce high quality predictions, despite their dependence on randomly sampled walks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model and Setup Details B.1 Convolution Module</head><p>Here, we describe the architecture used for the 1D CNN network CNN t in each layer t. <ref type="figure">Let  Conv1D(d, d , k)</ref> be a standard 1D convolution with input feature dimension d, output feature dimension d , kernel size k and no bias. This module has d ? d ? k trainable parameters. The term scales poorly for larger hidden dimensions d, since the square of this dimension is scaled with an additional factor of k, which we typically set to 9 or more.</p><p>To address this issue we leverage Depthwise Separable Convolutions, as suggested by <ref type="bibr" target="#b8">Chollet (2017)</ref>. This method is most commonly applied to 2D data in Computer Vision, but it can also be utilized for 1D convolutions. It decomposes one convolution with kernel size k into two convolutions: The first convolution is a standard 1D convolution with kernel size 1. The second convolution is a depthwise convolution with kernel size k, which convolves each channel individually and therefore only requires k ? d parameters. The second convolution is succeeded by a Batch Norm layer and a ReLU activation function. Note that there is no non-linearity between the two convolutions. These After the ReLU activation, we apply an additional (standard) convolution with kernel size 1, followed by another ReLU non-linearity. This final convolution increases the expressiveness of our convolution module which could otherwise only learn linearly separable functions. This would limit its ability to distinguish the binary patterns that encode identity and adjacency.</p><p>The full stack of operations effectively applies a 2-layer MLP to each sliding window position of the walk feature tensor. Overall, CNN t is composed of the following operations:</p><formula xml:id="formula_15">Conv1D(d, d , 1) ? Conv1D dw (d , d , k) ? BatchNorm ? ReLU ? Conv1D(d , d , 1) ? ReLU</formula><p>Here, Conv1D is a standard 1D convolution and Conv1D dw is a depthwise convolution. The total number of parameters of one such module (without the affine transformation of the Batch Norm) is equal to dd + kd + d 2 . <ref type="table" target="#tab_5">Table 5</ref> provides the hyperparameters used in each experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Hyperparameters</head><p>Hyperparameters that define/modify the network architecture:</p><p>? The number of layers L. We tried out L ? {2, 3, 4} on all datasets, except for MOLPCBA, where we searched through L ? {3, 5, 7}.</p><p>? The latent state size d. On ZINC, CIFAR10, MNIST and CSL we chose sizes that would roughly use the chosen parameter budgets. On COLLAB, IMDB-MULTI and REDDIT-BIN we set d = 100 and for MOLPCBA we chose the largest feasible size for our hardware (d = 400).</p><p>? The local window size s.</p><p>? The global pooling function (either mean or sum)</p><p>? The architecture of the final output network (either mlp or linear)</p><p>? The number of random walk steps during training ( train ) and evaluation ( eval ).</p><p>? The dropout rate. We searched through {0.0, 0.25, 0.5}. One dropout layer is placed behind the global pooling step.</p><p>? Whether or not a virtual node (VN) is used as an intermediate update layer. Hyperparameters for the walks and the training procedure:</p><p>? The probability of starting a walk from each node during training p * . We choose p * = 1 by default. On MOLPCBA we set p * = 0.2 to reduce overfitting. ? The walk strategy (either uniform (un) or non-backtracking (nb)) ? The number of evaluation runs with different random walks for validation (r val ) and testing (r test ). ? The initial learning rate lr was chosen as 0.001 in all experiments. ? The patience for learning rate decay (with a factor of 0.5) is 10 by default. ? The batch size <ref type="table" target="#tab_6">Table 6</ref> provides the number of trainable parameters in each model. Additionally, we report the runtime observed during training. All experiments were run on a machine with 64GB RAM, an Intel Xeon 8160 CPU and an Nvidia Tesla V100 GPU with 16GB GPU memory. The resources were provided by our internal compute cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Model Size and Runtime</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Virtual Node</head><p>Gilmer et al. <ref type="formula" target="#formula_21">(2017)</ref>, <ref type="bibr" target="#b30">Li et al. (2017)</ref>, and <ref type="bibr" target="#b20">Ishiguro et al. (2019)</ref> suggested the use of a virtual node to enhance GNNs for chemical datasets. Intuitively, a special node is inserted into the graph that is connected to all other nodes. This node aggregates the states of all other nodes and uses this information to update its own state. The virtual node has its own distinct update function which is not shared by other nodes. The updated state is then sent back to all nodes in the graph. Effectively, a virtual node allows global information flow after each layer.</p><p>Formally, a virtual node updates a latent state h t vn ? R d , where h t vn is computed after the t-th layer and h 0 vn is initialized as a zero vector. The update procedure is defined by:</p><formula xml:id="formula_16">h t vn = U t vn h t?1 vn + v?V h t (v) h t (v) = h t (v) + h t vn .</formula><p>Here, U t vn is a trainable MLP and h t is the latent node embedding computed by the t-th CRAWL layer.h t is an updated node embedding that is used as the input for the next CRAWL layer instead of h t . In our experiments, we choose U t vn to contain a single hidden layer of dimension d. When using a virtual node, we perform this update step after every CRAWL layer, except for the last one.</p><p>Note that we view the virtual node as an intermediate update step that is placed between our CRAWL layers to allow for global communication between nodes. No additional node is actually added to   <ref type="bibr" target="#b35">Murphy et al., 2019)</ref> with 11 nodes and a skip distance of 2 and 3 respectively. the graph and, most importantly, the "virtual node" does not occur in the random walks sampled by CRAWL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Cross Validation on CSL</head><p>Let us briefly discuss the experimental protocol used for the CSL dataset. Unlike the other benchmark datasets provided by <ref type="bibr" target="#b11">Dwivedi et al. (2020)</ref>, CSL is evaluated with 5-fold cross-validation. We use the 5-fold split <ref type="bibr" target="#b11">Dwivedi et al. (2020)</ref> provide in their repository. In each training run, three folds are used for training and one is used for validation and model selection. After training, the remaining fold is used for testing. Finally, <ref type="figure" target="#fig_3">Figure 3</ref> provides an example of two skip-link graphs. The task of CSL is to classify such graphs by their isomorphism class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablation Study</head><p>We perform an ablation study to understand how the key aspects of CRAWL influence the empirical performance. We aim to answer two main questions:</p><p>? How useful are the identity and adjacency features we construct for the walks? ? How do different strategies for sampling random walks impact the performance?</p><p>Here, we use the ZINC, MOLPCBA, and CSL datasets to answer these questions empirically. We trained multiple versions of CRAWL with varying amounts of structural features used in the walk feature matrices. The simplest version only uses the sequences of node and edge features without any structural information. For ZINC and CSL, we also train intermediate versions using either the identity or the adjacency encoding, but not both. We omit these for MOLPCBA to save computational resources. Finally, we measure the performance of the standard CRAWL architecture, where both encodings are incorporated into the walk feature matrices. For each version, we compute the performance with both walk strategies.</p><p>On each dataset, the experimental setup and hyperparameters are identical to those used in the previous experiments on both datasets. In particular, we train five models with different seeds and provide the average performance as well as the standard deviation across models. Note that we repeat the experiment independently for each walk strategy. Switching walk strategies between training and evaluation does not yield good results. <ref type="table" target="#tab_7">Table 7</ref> reports the performance of each studied version of CRAWL. On ZINC, the networks without any structural encoding yield the worst predictions. Adding either the adjacency or the identity encoding improves the results substantially. The best results are obtained when both encodings are utilized and non-backtracking walks are used. On MOLPCBA, the best performance is also obtained with full structural encodings. However, the improvement over the version without the encodings is only marginal. Again, non-backtracking walks perform significantly better than uniform walks. On CSL, the only version to achieve a perfect accuracy of 100% is the one with all structural encodings and non-backtracking walks. Note that the version without any encodings can only guess on CSL since this dataset has no node features (we are not using the Laplacian features here).</p><p>Overall, the structural encodings of the walk feature matrices yield a measurable performance increase on all three datasets. However, the margin of the improvement varies significantly and depends on the specific dataset. For some tasks, such as MOLPCBA, CRAWL yields highly competitive results even when only the sequences of node and edge features are considered in the walk feature matrices.</p><p>Finally, the non-backtracking walks consistently outperform the uniform walks. This could be attributed to their ability to traverse sparse substructures quickly. On sparse graphs with limited degree, such as molecules, uniform walks will backtrack often. This slows down the traversal of the graph. Each substructure will be traversed less frequently and the average size of the subgraphs induced by the walklets decreases. On the three datasets used here these effects seem to cause a significant loss in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Theory</head><p>In this appendix, we prove Theorem 1 and discuss its context. Let us first recall the setting and introduce some additional notation. Throughout the paper, graphs are undirected and simple (that is, without self-loops and parallel edges). <ref type="bibr" target="#b54">2</ref> In this appendix, all graphs will be unlabeled. All result can easily be extended to (vertex and edge) labelled graphs. In fact, the (harder) inexpressivity results only become stronger by restricting them to the subclass of unlabelled graphs. We further assume that graphs have no isolated nodes, which enables us to start a random walk from every node. This makes the setup cleaner and avoids tedious case distinctions, but again is no serious restriction.</p><p>We denote the edge set of a graph G by V (G) and the node set by E(G). The order |G| of G is the number of nodes, that is, |G| := |V (G)|. For a set X ? V (G), the induced subgraph G[X] is the graph with node set X and edge set</p><formula xml:id="formula_17">{vw ? E(G) | v, w ? X}. A walk of length in G is a sequence W = (w 0 , . . . , w ) ? V (G) +1 such that w i?1 w i ? E(G) for 1 ? i ? . The walk is non-backtracking if for 1 &lt; i &lt; we have w i+1 = w i?1 unless the degree of vertex w i is 1.</formula><p>Before we prove the theorem, let us precisely specify what it means that CRAWL distinguishes two graphs. Recall that CRAWL has three (walk related) hyperparameters:</p><p>? the window size s;</p><p>? the walk length ;</p><p>? the samples size m.</p><p>Recall furthermore that with every walk W = (w 0 , . . . , w ) we associate a walk feature matrix X ? R ( +1)?(d+d +s+(s?1)) . For 0 ? i ? , the first d entries of the i-th row of X describe the current embedding of the node w i , the next d entries the embedding of the edge w i?1 w i (0 for i = 0), the following s entries are indicators for the equalities between w i and the nodes w i?j for j = 1, . . . , s (1 if w i = w i?j , 0 if i ? j &lt; 0 or w i = w i?j ), and the remaining s ? 1 entries are indicators for the adjacencies between w i and the nodes w i?j for j = 2, . . . , s (1 if w i , w i?j are adjacent in G, 0 if i ? j &lt; 0 or w i , w i?j are non-adjacent; note that w i , w i?1 are always be adjacent because W is a walk in G). Note that in the unlabelled graphs we consider here the initial node and edge embeddings are the same for all nodes and for all edges, and therefore they do not contribute to the expressivity. As our expressiveness results are based on the initial feature matrices-they carry all information that CRAWL extracts from the graph-we can safely ignore these embeddings and focus on the subgraph features encoded in the last 2s ? 1 columns. For simplicity, we regard X as an ( + 1) ? (2s ? 1) matrix with only these features in the following. We denote the entries of the matrix X by X i,j and the rows by X i,? . So X i,? = (X i,1 , . . . , X i,d+d +2s?1 ) ? {0, 1} 2s?1 . We denote the walk feature matrix of a walk W by X(W ). It is immediate from the definitions that for walks W = (w 1 , . . . , w ), W = (w 1 , . . . , w ) in graphs G, G with feature matrices X := X(W ), X := X(W ), we have:</p><p>1. if X i?j,? = X i?j,? for j = 0, . . . , s ? 1 then the mapping w i?j ? w i?j for j = 0, . . . , s is an isomorphism from the induced subgraph G[{w i?j | j = 0, . . . , s}] to the induced subgraph G [{w i?j | j = 0, . . . , s}];</p><p>2. if the mapping w i?j ? w i?j for j = 0, . . . , 2s ? 1 is an isomorphism from the induced subgraph G[{w i?j | j = 0, . . . , 2s ? 1}] to the induced subgraph G [{w i?j | j = 0, . . . , 2s ? 1}], then X i?j,? = X i?j,? for j = 0, . . . , s ? 1.</p><p>The reason that we need to include the vertices w i?2s+1 , . . . , w i?s and w i?2s+1 , . . . , w i?s into the subgraphs in <ref type="formula">(2)</ref> is that row X i?s+1,? of the feature matrix records edges and equalities between w i?s+1 and w i?2s+1 , . . . , w i?s .</p><p>For every graph G we denote the distribution of random walks on G starting from a node chosen uniformly at random by W(G) and W nb (G) for the non-backtracking walks. We let X (G) and X nb (G)) be the push-forward distributions on {0, 1} ( +1)?(2s?1) , that is, for every X ? {0, 1} ( +1)?(2s?1) we let Pr</p><formula xml:id="formula_18">X (G) (X) = Pr W(G) {W | X(W ) = X}</formula><p>A CRAWL run on G takes m samples from X (G). So to distinguish two graphs G, G , CRAWL must detect that the distributions X (G), X (G ) are distinct using m samples.</p><p>As a warm-up, let us prove the following simple result.</p><p>Theorem 2. Let G be a cycle of length n and G the disjoint union of two cycles of length n/2. Then G and G cannot be distinguished by CRAWL with window size s &lt; n/2 (for any choice of parameters and m).</p><p>Proof. With a window size smaller than the length of the shortest cycle, the graph CRAWL sees in its window is always a path. Thus for every walk W in either G or G the feature matrix X(W ) only depends on the backtracking pattern of W . This means that X (G) = X (G ).</p><p>It is worth noting that the graphs G, G of Theorem 2 can be distinguished by 2-WL (the 2-dimensional Weisfeiler-Leman algorithm), but not by 1-WL.</p><p>Proving that two graphs G, G have identical feature-matrix distributions X (G) = X (G ) is the ultimate way of proving that they are not distinguishable by CRAWL. Yet for more interesting graphs, we rarely have identical feature-matrix distributions. However, if the distributions are sufficiently close we will still not be able to distinguish them. To quantify closeness, we use the total variation distance of the distributions. Recall that the total variation distance between two probability distributions D, D on the same finite sample space ? is</p><formula xml:id="formula_19">dist T V (D, D ) := max S?? | Pr D (S) ? Pr D (S)|.</formula><p>It is known that the total variation distance is half the 1 -distance between the distributions, that is,</p><formula xml:id="formula_20">dist T V (D, D ) = 1 2 D ? D 1 = 1 2 ??? | Pr D ({?}) ? Pr D ({?})|.</formula><p>Let ? &gt; 0. We say that two graphs G, G are ?-indistinguishable by CRAWL with window size s, walk length , and sample size m if</p><formula xml:id="formula_21">dist T V (X (G), X (G )) &lt; ? m .<label>(1)</label></formula><p>The rationale behind this definition is that if dist T V (X (G), X (G )) &lt; ? m then for every property of feature matrices that CRAWL may want to use to distinguish the graphs, the expected numbers of samples with this property that CRAWL sees in both graphs are close together (assuming ? is small).</p><p>Often, we want to make asymptotic statements, where we have two families of graphs (G n ) n?1 and (G n ) n?1 , typically of order |G n | = |G n | = ?(n), and classes S, L, M of functions, such as the class O(log n) of logarithmic or the class n O(1) of polynomial functions. We say that (G n ) n?1 and (G n ) n?1 are indistinguishable by CRAWL with window size S, walk length L, and sample size M if for all ? &gt; 0 and all s ? S, ? L, m ? M there is an n such that G n , G n are ?-indistinguishable by CRAWL with window size s(n), walk length (n), and sample size m(n).</p><p>We could make similar definitions for distinguishability, but we omit them here and deal with distinguishability in an ad-hoc fashion (in the following subsection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Proof of Theorem 1(1)</head><p>Here is a precise quantitative version of the first part of Theorem 1. Theorem 3. For all k ? 1 there are families of graphs (G n ) n?1 , (G n ) n?1 of order |G n | = |G n | = n + O(k) that are distinguishable by CRAWL with window size s = O(k 2 ), walk length s = O(k 2 ), and samples size m = O(n), but not by k-WL (and hence not by k-dimensional GNNs).</p><p>Fortunately, to prove this theorem we do not need to know any details about the Weisfeiler-Leman algorithm (the interested reader is deferred to <ref type="bibr">(Grohe, 2021;</ref><ref type="bibr" target="#b21">Kiefer, 2020)</ref>). We can use the following well-known inexpressibility result as a black box.</p><p>Theorem 4 <ref type="bibr" target="#b7">(Cai et al. (1992)</ref>). For all k ? 1 there are graphs H k , H k such that |H k | = |H k | = O(k), the graphs H k and H k are 3-regular, and k-WL cannot distinguish H k and H k .</p><p>It is a well-known fact that the cover time of a connected graph of order n with m edges, that is, the expected time it takes a random walk starting from a random node to visit all nodes of the graph, is bounded from above by 4nm <ref type="bibr" target="#b1">(Aleliunas et al., 1979)</ref>. By Markov's inequality, a path of length 8nm visits all nodes with probability at least 1/2. Sampling several such paths, we can bring the success probability arbitrarily close to 1.</p><p>Proof of Theorem 3. Let k ? 1 and let H k , H k be the graphs obtained from the Theorem 4. Let n k := |H k |. For every n ? 1, we let G n be the disjoint union of H k with a path of length n, and let G n be defined in the same way from H k . Then |G n | = |G n | = n k + n + 1.</p><p>Let m k = 3 2 n k be the number of edges of the 3-regular graphs H k and H k , and let s := 8n k m k = O(k 2 ). This will be our window size, and in fact also our walk length: := s. Let ? &gt; 0. We choose a sufficiently large m = m(n) ? O(n) to make sure that a sample of m nodes from V (G n ) or V (G n ) contains sufficiently many nodes in the subset V (H k ) ? V (G k ) resp. V (H k ) ? V (G k ). Then, if we sample m paths of length from W(G n ), with probability at least 1 ? ?, one of these paths covers V (H k ). This means that m random walks of length will detect the subgraphs H k , and as the window size s is equal to , these subgraphs will appear in the feature matrix. Since the subgraph H k does not appear as a subgraph of G n , this means that with probability at least 1 ? ?, CRAWL can distinguish the two graphs. To prove the second part of the theorem, it will be necessary to briefly review the 1-dimensional Weisfeiler-Leman algorithm , which is also known as color refinement and as naive node classification. The algorithm iteratively computes a partition of the nodes of its input graph. It is convenient to think of the classes of the partition as colors of the nodes. Initially, all nodes have the same color. Then in each iteration step, for all colors c in the current coloring and all nodes v, w of color c, the nodes v and w get different colors in the new coloring if there is some color d such that v and w have different numbers of neighbors of color d. This refinement process is repeated until the coloring is stable, that is, any two nodes v, w of the same color c have the same number of neighbors of any color d. We say that 1-WL distinguishes two graphs G, G if, after running the algorithm on the disjoint union G G of the two graphs, in the stable coloring of G G there is a color c such that G and G have a different number of nodes of color c.</p><p>For the results so far, it has not mattered if we allowed backtracking or not. Here, it makes a big difference. For the non-backtracking version, we obtain a stronger result with an easier proof. The following theorem is a precise quantitative statement of Theorem 1(2). Theorem 5. There are families (G n ) n?1 , (G n ) n?1 of graphs of order |G n | = |G n | = 3n ? 1 with the following properties.</p><p>So, let us turn to the proof that with walks of linear length this is not possible, that is, assertion (c). The reason for this is simple: random walks of length O(n) are very unlikely to traverse a path of length at least n ? 1 from x to y. It is well known that the expected traversal time is ?(n 2 ) (this follows from the analysis of the gambler's ruin problem). However, this does not suffice for us. We need to bound the probability that a path of length O(n) is a traversal. Using a standard, Chernoff type tail bound, it is straightforward to prove that for every constant c ? 0 there is a constant d ? 1 such that the probability that a random walk of length cn in either G n or G n visits both x and y is at most exp(?n/d). As only walks visiting both x and y can differentiate between the two graphs, this gives us an upper bound of exp(?n/d) for the total variation distance between X (G n ) and X (G n ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Top: Update procedure of latent node embeddings h t in a CRAWL layer. ? is a set of random walks. Bottom: Architecture of a 3-layer CRAWL network as used in the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Two cyclic skip-link graphs (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>G 3 G 3 Figure 4 :</head><label>334</label><figDesc>The graphs G 3 and G 3 in the proof of Theorem 5 with their stable coloring computed by 1-WL D.2 Proof of Theorem 1<ref type="bibr" target="#b54">(2)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance achieved on ZINC, MNIST, CIFAR10, and MOLPCBA. A result marked with " ?" indicates that the parameter budget was smaller than for CRAWL and " * " marks results where no parameter budget was reported.</figDesc><table><row><cell></cell><cell>METHOD</cell><cell>ZINC</cell><cell>MNIST</cell><cell>CIFAR10</cell><cell>MOLPCBA</cell></row><row><cell></cell><cell></cell><cell>TEST MAE</cell><cell>TEST ACC (%)</cell><cell>TEST ACC (%)</cell><cell>TEST AP</cell></row><row><cell></cell><cell>GIN</cell><cell>0.526 ? 0.051</cell><cell>96.485 ? 0.252</cell><cell>55.255 ? 1.527</cell><cell>0.2703 ? 0.0023</cell></row><row><cell></cell><cell>GRAPHSAGE</cell><cell>0.398 ? 0.002</cell><cell>97.312 ? 0.097</cell><cell>65.767 ? 0.308</cell><cell>-</cell></row><row><cell>LEADERBOARD</cell><cell>GAT GCN 3WLGNN GATEDGCN MPNN PNA DGCN&amp;FLAG</cell><cell>0.384 ? 0.007 0.367 ? 0.011 0.303 ? 0.068 0.214 ? 0.006 0.145 ? 0.007 0.142 ? 0.010 -</cell><cell>95.535 ? 0.205 90.705 ? 0.218 95.075 ? 0.961 97.340 ? 0.143 97.690 ? 0.220 97.940 ? 0.120</cell><cell>64.223 ? 0.455 55.710 ? 0.381 59.175 ? 1.593 67.312 ? 0.311 70.860 ? 0.270 70.350 ? 0.630</cell><cell>-0.2483 ? 0.0037 ---0.2838 ? 0.0035</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy achieved on CSL with and without Laplacian eigenvectors<ref type="bibr" target="#b11">(Dwivedi et al., 2020)</ref>. As these node features already encode the solution, unsurprisingly most models perform well.</figDesc><table><row><cell>METHOD</cell><cell>CSL</cell><cell>CSL+LAP</cell></row><row><cell></cell><cell>TEST ACC (%)</cell><cell>TEST ACC (%)</cell></row><row><cell>GIN</cell><cell>? 10.0</cell><cell>99.333 ? 1.333</cell></row><row><cell>GRAPHSAGE</cell><cell>? 10.0</cell><cell>99.933 ? 0.467</cell></row><row><cell>GAT</cell><cell>? 10.0</cell><cell>99.933 ? 0.467</cell></row><row><cell>GCN</cell><cell>? 10.0</cell><cell>100.000 ? 0.000</cell></row><row><cell>3WLGNN</cell><cell>95.700 ?14.850</cell><cell>30.533 ? 9.863</cell></row><row><cell>GATEDGCN</cell><cell>? 10.0</cell><cell>99.600 ? 1.083</cell></row><row><cell>CRAWL</cell><cell cols="2">100.000 ? 0.000 100.000 ? 0.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on Social Datasets.</figDesc><table><row><cell>Method</cell><cell></cell><cell>COLLAB</cell><cell>IMDB-MULTI</cell><cell>REDDIT-BIN</cell></row><row><cell></cell><cell></cell><cell>Test Acc</cell><cell>Test Acc</cell><cell>Test Acc</cell></row><row><cell cols="2">WL-Kernel (Shervashidze et al., 2011)</cell><cell>78.9 ? 1.9</cell><cell>50.9 ? 3.8</cell><cell>81.0 ? 3.1</cell></row><row><cell>WEGL</cell><cell>(Kolouri et al., 2020)</cell><cell>79.8 ? 1.5</cell><cell>52.0 ? 4.1</cell><cell>92.0 ? 0.8</cell></row><row><cell>GNTK</cell><cell>(Du et al., 2019)</cell><cell>83.6 ? 1.0</cell><cell>52.8 ? 4.6</cell><cell>-</cell></row><row><cell>DGCNN</cell><cell>(Zhang et al., 2018)</cell><cell>73.8 ? 0.5</cell><cell>47.8 ? 0.9</cell><cell>-</cell></row><row><cell>3WLGNN</cell><cell>(Maron et al., 2019a)</cell><cell>80.7 ? 1.7</cell><cell>50.5 ? 3.6</cell><cell>-</cell></row><row><cell>GIN</cell><cell>(Xu et al., 2019)</cell><cell>80.2 ? 1.9</cell><cell>52.3 ? 2.8</cell><cell>92.4 ? 2.5</cell></row><row><cell>GSN</cell><cell>(Bouritsas et al., 2020)</cell><cell>85.5 ? 1.2</cell><cell>54.3 ? 3.3</cell><cell>-</cell></row><row><cell>CRAWL</cell><cell></cell><cell cols="3">80.40% ? 1.50 47.77% ? 3.87 92.75% ? 2.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Extended results for CRAWL on all datasets. Note that different metrics are used to measure the performance on the datasets. For each experiment we provide the cross model deviation (CMD) and the internal model deviation (IMD). IMD). For clarity, let us define these terms formally. For each experiment, we perform q ? N training runs with different random seeds. Let m i be the model obtained in the</figDesc><table><row><cell>DATASET / MODEL</cell><cell>METRIC</cell><cell></cell><cell>TEST</cell><cell></cell><cell></cell><cell>VALIDATION</cell><cell></cell><cell></cell><cell>TRAIN</cell></row><row><cell></cell><cell></cell><cell>SCORE</cell><cell>CMD</cell><cell>IMD</cell><cell>SCORE</cell><cell>CMD</cell><cell>IMD</cell><cell>SCORE</cell><cell>CMD</cell></row><row><cell>ZINC</cell><cell>MAE</cell><cell>0.08456</cell><cell>? 0.00352</cell><cell>? 0.00116</cell><cell>0.11398</cell><cell>? 0.00447</cell><cell>? 0.00121</cell><cell>0.04913</cell><cell>? 0.00887</cell></row><row><cell>CIFAR10</cell><cell>ACC.</cell><cell>0.69013</cell><cell>? 0.00259</cell><cell>? 0.00158</cell><cell>0.70052</cell><cell>? 0.00307</cell><cell>? 0.00060</cell><cell>0.79180</cell><cell>? 0.01956</cell></row><row><cell>MNIST</cell><cell>ACC.</cell><cell>0.97944</cell><cell>? 0.00050</cell><cell>? 0.00055</cell><cell>0.98106</cell><cell>? 0.00110</cell><cell>? 0.00030</cell><cell>0.99044</cell><cell>? 0.00090</cell></row><row><cell>CSL</cell><cell>ACC.</cell><cell>1.00000</cell><cell>? 0.00000</cell><cell>? 0.00000</cell><cell>1.00000</cell><cell>? 0.00000</cell><cell>? 0.00000</cell><cell>1.00000</cell><cell>? 0.00000</cell></row><row><cell>MOLPCBA</cell><cell>AP</cell><cell>0.29863</cell><cell>? 0.00249</cell><cell>? 0.00055</cell><cell>0.30746</cell><cell>? 0.00195</cell><cell>? 0.00027</cell><cell>0.54889</cell><cell>? 0.01021</cell></row><row><cell>COLLAB</cell><cell>ACC.</cell><cell>0.80400</cell><cell>? 0.01497</cell><cell>? 0.00540</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.85827</cell><cell>? 0.00518</cell></row><row><cell>IMDB-MULTI</cell><cell>ACC.</cell><cell>0.47767</cell><cell>? 0.03870</cell><cell>? 0.00900</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.44133</cell><cell>? 0.01017</cell></row><row><cell>REDDIT-BIN</cell><cell>ACC.</cell><cell>0.92750</cell><cell>? 0.02162</cell><cell>? 0.00450</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.94417</cell><cell>? 0.00702</cell></row><row><cell cols="10">we measure two deviations for each experiment: The cross model deviation (CMD) and the internal</cell></row><row><cell cols="2">model deviation (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>i-th training run with i ? [q]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters used in each experiment.</figDesc><table><row><cell></cell><cell cols="3">ZINC CIFAR10 MNIST</cell><cell>CSL</cell><cell cols="4">MOLPCBA COLLAB IMDB-M REDDIT-B</cell></row><row><cell>L</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>5</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>d</cell><cell>147</cell><cell>75</cell><cell>75</cell><cell>90</cell><cell>400</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>s</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell></row><row><cell>pool</cell><cell>sum</cell><cell>mean</cell><cell>mean</cell><cell>mean</cell><cell>mean</cell><cell>sum</cell><cell>sum</cell><cell>sum</cell></row><row><cell>out</cell><cell>mlp</cell><cell>mlp</cell><cell>mlp</cell><cell>mlp</cell><cell>linear</cell><cell>mlp</cell><cell>mlp</cell><cell>mlp</cell></row><row><cell>train</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell></row><row><cell>eval</cell><cell>150</cell><cell>150</cell><cell>150</cell><cell>150</cell><cell>100</cell><cell>50</cell><cell>50</cell><cell>50</cell></row><row><cell>p *</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.2</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>walk strat.</cell><cell>nb</cell><cell>nb</cell><cell>nb</cell><cell>nb</cell><cell>nb</cell><cell>nb</cell><cell>nb</cell><cell>nb</cell></row><row><cell>rval</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>5</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>rtest</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>dropout</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.25</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>lr</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>patience</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>batch size</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>100</cell><cell>50</cell><cell>10</cell><cell>10</cell></row><row><cell>VN</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell cols="9">operations effectively simulate a standard convolution with kernel size k but require substantially less</cell></row><row><cell cols="2">memory and runtime.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Number of parameters and runtime for each model in our experiments. The reported times are averaged over all training runs and include the time used to perform a validation run after each training epoch.</figDesc><table><row><cell>MODEL</cell><cell>#PARAM.</cell><cell>TRAIN TIME (H:MM)</cell></row><row><cell>ZINC</cell><cell>497 743</cell><cell>0:53</cell></row><row><cell>CIFAR10</cell><cell>109 660</cell><cell>6:00</cell></row><row><cell>MNIST</cell><cell>109 360</cell><cell>6:00</cell></row><row><cell>CSL</cell><cell>104 140</cell><cell>0:01</cell></row><row><cell>MOLPCBA</cell><cell>6 115 728</cell><cell>6:22</cell></row><row><cell>COLLAB</cell><cell>190 103</cell><cell>1:05</cell></row><row><cell>IMDB-MULTI</cell><cell>190 103</cell><cell>1:34</cell></row><row><cell>REDDIT-BIN</cell><cell>189 901</cell><cell>2:07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results of our ablation study. Node features F V , edge features F E , adjacency encoding A, and identity encoding I. Walk strategies no-backtrack (NB) and uniform (UN).</figDesc><table><row><cell>FEATURES</cell><cell>WALKS</cell><cell>ZINC (MAE)</cell><cell>MOLPCBA (AP)</cell><cell>CSL (ACC)</cell></row><row><cell>F V +F E</cell><cell>UN</cell><cell cols="3">0.19768 ? 0.01159 0.28364 ? 0.00201 0.06000 ? 0.04422</cell></row><row><cell>F V +F E</cell><cell>NB</cell><cell cols="3">0.15475 ? 0.00350 0.29613 ? 0.00209 0.06000 ? 0.04422</cell></row><row><cell>F V +F E +A</cell><cell>UN</cell><cell>0.10039 ? 0.00514</cell><cell cols="2">-0.97467 ? 0.02587</cell></row><row><cell>F V +F E +A</cell><cell>NB</cell><cell>0.08656 ? 0.00310</cell><cell cols="2">-0.99933 ? 0.00133</cell></row><row><cell>F V +F E +I</cell><cell>UN</cell><cell>0.10940 ? 0.00698</cell><cell cols="2">-0.70733 ? 0.07658</cell></row><row><cell>F V +F E +I</cell><cell>NB</cell><cell>0.09345 ? 0.00219</cell><cell cols="2">-0.97133 ? 0.00859</cell></row><row><cell>F V +F E +I +A</cell><cell>UN</cell><cell cols="3">0.09368 ? 0.00232 0.28522 ? 0.00317 0.96267 ? 0.02037</cell></row><row><cell>F V +F E +I +A</cell><cell>NB</cell><cell cols="3">0.08456 ? 0.00352 0.29863 ? 0.00249 1.00000 ? 0.00000</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/toenshoff/CRaWl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">It is possible to simulate directed edges and parallel edges through edge labels and loops through node labels, but so far, we have only worked with undirected simple, though possibly labeled graphs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the German Research Foundation (DFG) under grants GR 1492/16-1 and GRK 2236 UnRAVeL.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01179</idno>
		<title level="m">The surprising power of graph neural networks with random node initialization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random walks, universal traversal sequences, and the complexity of maze problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aleliunas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lov?sz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rackoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS79</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="page" from="218" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network motifs: theory and experimental approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrg2102</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="450" to="461" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li?</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02863</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">P. Directional graph networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15069</idno>
		<title level="m">Graph convolutions that can finally model local structure</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>F?rer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05718</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/663fd3c5144fd10bd5ca6611a9a5b92d-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5723" to="5733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Walk message passing neural networks and second-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09499</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth International Conference on Machine Learning (ICML)</title>
		<meeting>the Thirty-Fourth International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The logic of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.14624" />
		<imprint/>
	</monogr>
	<note>ArXiv, 2104.14624 [cs.LG</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rastogi</forename></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939754</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>R.</editor>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graph warp module: an auxiliary module for boosting the power of graph neural networks in molecular graph analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01020</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Weisfeiler-Leman algorithm: An exploration of its power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGLOG News</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5" to="27" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wasserstein embedding for graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naderializadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Flag: Adversarial data augmentation for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morris</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16584</idno>
		<title level="m">Parameterized hypercomplex graph neural networks for graph classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with motif-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepergcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">All you need to train deeper GCNs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning graph-level representation for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03741</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TUDataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="URLwww.graphlearning.io" />
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>GRL+ 2020</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Random walk graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">; H</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Lin, H.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>Larochelle,</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Motif-matching based subgraph-level attentional convolutional network for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5387" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<ptr target="https://github.com/benedekrozemberczki/APPNP" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05697</idno>
		<title level="m">Motif-based convolutional neural network on graphs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Beyond localized graph neural networks: An attributed motif regularization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sundaram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05197</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<idno>abs/2002.03155</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cucurull</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MoleculeNet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Learning Representations (ICLR)</title>
		<meeting>the Seventh International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Node2seq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01849</idno>
		<title level="m">Towards trainable convolutions in graph neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">For all n ? 1, 1-WL distinguishes G n and G n</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">G n ) n?1 , (G n ) n?1 are indistinguishable by the non-backtracking version of CRAWL with window size s(n) = o(n) (regardless of the walk length and sample size)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">G n ) n?1 (G n ) n?1 are indistinguishable by CRAWL with walk length (n) = O(n), and samples size m(n) = n O(1) (regardless of the window size)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The graphs G n and G n both consist of three internally disjoint paths with the same endnodes x and y. In G n the lengths of all three paths is n</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Proof</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In G n , the length of the paths is n ? 1, n, n + 1</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Then for all i and j with i ? s ? j ? i we have w i = w j , and unless j = i ? 1, there is no edge between w i and w j . Thus X(W ) = X(W ) for all walks W of the same length , and since it does not matter which of the two graphs G n , G n the walks are from</title>
	</analytic>
	<monogr>
		<title level="m">To prove assertion (b), let s := 2n ? 3</title>
		<imprint/>
	</monogr>
	<note>Then the length of the shortest cycle in G n , G n is s + 2. Now consider a non-backtracking walk W = (w 1 , . . . , w ) in either G n or G n (of arbitrary length ). It follows that X nb (G n ) = X nb (G n</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The reason is that by going back and forth between a node and all its neighbors within its window, CRAWL can distinguish the two degree-3 nodes x, y from the remaining degree-2 nodes. Thus, the feature matrix reflects traversal times between degree-3 nodes, and the distribution of traversal times is different in G n and G n</title>
		<imprint/>
	</monogr>
	<note>Before we prove (c), we remark that the backtracking version of CRAWL can distinguish (G n ) n?1 and (G n ) n?1 with a constant window size 6, walk length n O(1) , and samples size n O(1). With sufficiently many samples, CRAWL can detect this</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
