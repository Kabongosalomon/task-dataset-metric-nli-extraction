<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Da</forename><surname>Xu</surname></persName>
							<email>xuhengda@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongli</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Cloud Xiaowei</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
							<email>qingyuzhou@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Cloud Xiaowei</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhen</forename><surname>Wang</surname></persName>
							<email>zizhenwang@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Cloud Xiaowei</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
							<email>yunbocao@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Cloud Xiaowei</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tencent Cloud Xiaowei</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chinese Spell Checking (CSC) aims to detect and correct erroneous characters for usergenerated text in Chinese language. Most of the Chinese spelling errors are misused semantically, phonetically or graphically similar characters. Previous attempts notice this phenomenon and try to utilize the similarity relationship for this task. However, these methods use either heuristics or handcrafted confusion sets to predict the correct character. In this paper, we propose a Chinese spell checker called REALISE, by directly leveraging the multimodal information of the Chinese characters. The REALISE model tackles the CSC task by (1) capturing the semantic, phonetic and graphic information of the input characters, and (2) selectively mixing the information in these modalities to predict the correct output. Experiments 1 on the SIGHAN benchmarks show that the proposed model outperforms strong baselines by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Chinese Spell Checking (CSC) task aims to identify erroneous characters and generate candidates for correction. It has attracted much research attention, due to its fundamental and wide applications such as search query correction <ref type="bibr" target="#b19">(Martins and Silva, 2004;</ref><ref type="bibr">Gao et al., 2010)</ref>, optical character recognition (OCR) <ref type="bibr" target="#b0">(Afli et al., 2016)</ref>, automatic essay scoring <ref type="bibr" target="#b11">(Dong and Zhang, 2016)</ref>. Recently, rapid progress <ref type="bibr" target="#b40">(Zhang et al., 2020;</ref><ref type="bibr" target="#b6">Cheng et al., 2020)</ref> has been made on this task, because of the success of large pretrained language models <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref>.</p><p>In alphabetic languages such as English, spelling errors often occur owing to one or more wrong <ref type="table">Table 1</ref>: Two examples of Chinese spelling errors and their candidate corrections. "Sent./Cand./Trans." are short for sentence/candidates/translation respectively. The wrong/candidate/correct characters with their pronunciation and translation are in red/orange/blue color. characters, resulting in the written word not in the dictionary problem <ref type="bibr" target="#b24">(Tachibana and Komachi, 2016)</ref>. However, Chinese characters are valid if they can be typed in computer systems, which causes that the spelling errors are de facto misused characters in the context of computer-based language processing. Considering the formation of Chinese characters, a few of them were originally pictograms or phono-semantic compound characters <ref type="bibr">(Jerry, 1988)</ref>. Thus, in Chinese, the spelling errors are not only the misused characters with confusing semantic meaning, but also the characters which are phonetically or graphically similar <ref type="bibr" target="#b16">(Liu et al., 2010</ref><ref type="bibr" target="#b15">(Liu et al., , 2011</ref>. <ref type="table">Table 1</ref> shows two examples of Chinese spelling error. In the first example, phonetic information of "?" (flat) is needed to get the correct character "?" (bottle) since they share the same pronunciation "p?ng". The second example needs not only phonetic, but also graphic information of the erroneous character "?" (light). The correct one, "?" (go), has the same right radical as "?" and similar pronunciation <ref type="bibr">("q?ng" and "j?ng")</ref>. Therefore, considering the intrinsic nature of Chinese, it is essential to leverage the phonetic and graphic knowledge of the Chinese characters along with the textual semantics for the CSC task.</p><p>In this paper, we propose REALISE (Read, Listen, and See), a Chinese spell checker which leverages the semantic, phonetic and graphic information to correct the spelling errors. The REALISE model employs three encoders to learn informative representations from textual, acoustic and visual modalities. First, BERT <ref type="bibr" target="#b10">(Devlin et al., 2019</ref>) is adopted as the backbone of the semantic encoder to capture the textual information. For the acoustic modality, Hanyu Pinyin (pinyin), the romanization spelling system for the sounds of Chinese characters, is used as the phonetic features. We design a hierarchical encoder to process the pinyin letters at the character-level and the sentence-level. Meanwhile, for the visual modality, we build character images with multiple channels as the graphic features, where each channel corresponds to a specific Chinese font. Then, we use ResNet <ref type="bibr">(He et al., 2016)</ref> blocks to encode the images to get the graphic representation of characters.</p><p>With the representation of three different modalities, one challenge is how to fuse them into one compact multimodal representation. To this end, a selective modality fusion mechanism is designed to control how much information of each modality can flow to the mixed representation. Furthermore, as the pretrain-finetune procedure has been proven to be effective on various NLP tasks <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b39">Dong et al., 2019;</ref><ref type="bibr" target="#b23">Sun et al., 2020)</ref>, we propose to pretrain the phonetic and the graphic encoders by predicting the correct character given input in the corresponding modality.</p><p>We conduct experiments on the SIGHAN benchmarks <ref type="bibr" target="#b33">(Wu et al., 2013;</ref><ref type="bibr" target="#b26">Tseng et al., 2015)</ref>. By leveraging multimodal information, REALISE outperforms all previous state-ofthe-art models by a large margin. Compared to previous methods using confusion set <ref type="bibr" target="#b10">(Lee et al., 2019)</ref> to capture the character similarity relationships, such as the SOTA SpellGCN <ref type="bibr" target="#b6">(Cheng et al., 2020)</ref>, REALISE achieves an averaging 2.4% and 2.6% F1 improvements at detection-level and correctionlevel. Further analysis shows that our model performs better on the errors which are not defined in the handcrafted confusion sets. This indicates that leveraging the phonetic and graphic information of Chinese characters can better capture the easily-misused characters.</p><p>To summarize, the contributions of this paper include: (i) we propose to leverage phonetic and graphic information of Chinese characters besides textual semantics for the CSC task; (ii) we introduce the selective fusion mechanism to integrate multimodal information; (iii) we propose acoustic and visual pretraining tasks to further boost the model performance; (iv) to the best of our knowledge, the proposed REALISE model achieves the best results on the SIGHAN CSC benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Chinese Spell Checking</head><p>The CSC task is to detect and correct spelling errors in Chinese sentences. Early works design various rules to deal with different errors <ref type="bibr" target="#b8">Chu and Lin, 2015)</ref>. Next, traditional machine learning algorithms are brought to this field, such as Conditional Random Field and Hidden Markov Model <ref type="bibr" target="#b31">(Wang and Liao, 2015;</ref>. Then, neural-based methods have made great progress in CSC. <ref type="bibr" target="#b29">Wang et al. (2018)</ref> treat the CSC task as a sequence labeling problem, and use a bidirectional LSTM to predict the correct characters. With the great success of large pretrained language models (e.g., BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>), <ref type="bibr">Hong et al. (2019)</ref> propose the FASpell model, which use a BERT-based denoising autoencoder to generate candidate characters and uses some empirical measures to select the most likely ones. Besides, the Soft-Masked BERT model <ref type="bibr" target="#b40">(Zhang et al., 2020)</ref> leverages a cascading architecture where GRU is used to detect the erroneous positions and BERT is used to predict correct characters.</p><p>Previous works <ref type="bibr" target="#b36">(Yu and Li, 2014;</ref><ref type="bibr" target="#b6">Cheng et al., 2020)</ref> using handcrafted Chinese character confusion set <ref type="bibr" target="#b10">(Lee et al., 2019)</ref> aim to correct the errors by discovering the similarity of the easily-misused characters.  leverage the pointer network <ref type="bibr" target="#b28">(Vinyals et al., 2015)</ref> by picking the correct character from the confusion set. <ref type="bibr" target="#b6">Cheng et al. (2020)</ref> propose a Spell-GCN model which models the character similarity through Graph Convolution Network (GCNs) (Kipf and Welling, 2016) on the confusion set. However, the character confusion set is predefined and fixed, which cannot cover all the similarity relations, nor can it distinguish the divergence in the similarity ! ! " q i n g   Transformer Block L ? <ref type="figure">Figure 1</ref>: Architecture overview of the REALISE model. The semantic, phonetic and graphic encoders, are used to capture the information in textual, acoustic and visual modalities. The fusion module selectively fuses the information from three encoders. In the example input, to correct the erroneous character, "?" (q?ng, light), we need not only the contextual text information, but also the phonetic and graphic information of the character itself.</p><p>of Chinese characters. In this work, we discard the predefined confusion set and directly use the multimodal information to discover the subtle similarity relationship between all Chinese characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Learning</head><p>There has been much research to integrate information from different modalities to achieve better performance. Tasks such as Multimodal Sentiment Analysis <ref type="bibr" target="#b38">(Zadeh et al., 2016;</ref><ref type="bibr" target="#b39">Zhang et al., 2019)</ref>, Visual Question Answering <ref type="bibr" target="#b1">(Antol et al., 2015;</ref><ref type="bibr" target="#b5">Chao et al., 2018)</ref> and Multimodal Machine Translation <ref type="bibr">(Hitschler et al., 2016;</ref><ref type="bibr" target="#b3">Barrault et al., 2018)</ref> have made much progress. Recently, multimodal pretraining models have been proposed, such as VL-BERT <ref type="bibr" target="#b22">(Su et al., 2020)</ref>, Unicoder-VL , and LXMERT <ref type="bibr" target="#b25">(Tan and Bansal, 2019)</ref>. In order to incorporate the visual information of Chinese characters into language models, Meng et al.</p><p>(2019) design a Tianzige-CNN to facilitate some NLP tasks, such as named entity recognition and sentence classification. To the best of our knowledge, this paper is the first work to leverage multimodal information to tackle the CSC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The REALISE Model</head><p>In this section, we introduce the REALISE model, which utilizes the semantic, phonetic, and graphic information to distinguish the similarities of Chinese characters and correct the spelling errors. As shown in <ref type="figure">Figure 1</ref>, multiple encoders are firstly employed to capture valuable information from textual, acoustic and visual modalities. Then, we develop a selective modality fusion module to obtain the context-aware multimodal representations. Finally, the output layer predicts the probabilities of error corrections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Semantic Encoder</head><p>We adopt BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> as the backbone of the semantic encoder. BERT provides rich contextual word representation with the unsupervised pretraining on large corpora. The input tokens X = (x 1 , . . . , x N ) are first projected into H t 0 through the input embedding. Then the computation of Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> encoder layers can be formulated as:</p><formula xml:id="formula_0">H t l = Transformer l (H t l?1 ), l ? [1, L] (1)</formula><p>where L is the number of Transformer layers. Each layer consists of a multi-head attention module and a feed-forward network with the residual connection <ref type="bibr">(He et al., 2016)</ref> and layer normalization <ref type="bibr" target="#b2">(Ba et al., 2016)</ref>. The output of the last layer</p><formula xml:id="formula_1">H t = H t L = (h t 1 , . . . , h t N )</formula><p>is used as the contextualized semantic representation of the input tokens in textual modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Phonetic Encoder</head><p>Hanyu Pinyin (pinyin) is the romanization for Chinese to "spell out" the sounds of characters. We use it to calculate the phonetic representation in this paper. The pinyin of a Chinese character consists of three parts: initial, final, and tone. The initial (21 in total) and final (39 in total) are written with letters in the English alphabet. The 5 kinds of tones (take the final "a" as an example, {?,?,?,?, a }) can be mapped into numbers {1, 2, 3, 4, 0}. Though the vocabulary size of pinyin for all Chinese characters is a fixed number, we use a sequence of letters in REALISE to capture the subtle phonetic difference between Chinese characters. For example, the pinyin of "?" (middle) and "?" (brown) are "zh?ng" and "z?ng" respectively. The two characters have very similar sounds but quite different meanings. We thus represent pinyin as a symbol sequence, e.g., {z, h, o, n, g, 1} for "?". We denote the pinyin of the i-th character in the input sentence as p i = (p i,1 , . . . , p i,|p i | ), where |p i | is the length of pinyin p i .</p><p>In REALISE, we design a hierarchical phonetic encoder, which consists of a character-level encoder and a sentence-level encoder.</p><p>The Character-level Encoder is to model the basic pronunciation and capture the subtle sound difference between characters. It is a single-layer uni-directional GRU <ref type="bibr" target="#b7">(Cho et al., 2014)</ref>, which encodes the pinyin of the i-th character x i as:</p><formula xml:id="formula_2">h a i,j = GRU(h a i,j?1 , E(p i,j ))<label>(2)</label></formula><p>where E(p i,j ) is the embedding of the pinyin symbol p i,j , andh a i,j is the j-th hidden states of the GRU. The last hidden state is used as the characterlevel phonetic representation of x i .</p><p>The Sentence-level Encoder is a 4-layer Transformer with the same hidden size as the semantic encoder. It is designed to obtain the contextualized phonetic representation for each Chinese character. As the independent phonetic vectors are not distinguished in order, we add the positional embedding to each vector in advance. Then, we pack these phonetic vectors together, and apply the Transformer layers to calculate the contextualized representation in acoustic modality, denoted as H a = (h a 1 , h a 2 , ..., h a N ). Note that owing to the Transformer architecture, this representation is also normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Graphic Encoder</head><p>We apply the ResNet (He et al., 2016) as the graphic encoder. The graphic encoder has 5 layers of ResNet blocks (denoted as ResNet5) followed by a layer normalization <ref type="bibr" target="#b2">(Ba et al., 2016)</ref> operation. We formulate this procedure as follows:</p><formula xml:id="formula_3">h v i = ResNet5(I i ) h v i = LayerNorm(h v i )<label>(3)</label></formula><p>where I i is the image of the i-th character x i in the input sentence, and LayerNorm means layer normalization.</p><p>In order to extract graphic information effectively, each block in ResNet5 halves the width and height of the image, and increases the number of channels. Thus, the final output is a vector with the length equal to the number of output channels, i.e., both height and width become 1. Furthermore, we set the number of output channels to the hidden size in the semantic encoder for the follow-up modality fusion. We denote the representation in visual modality of the input sentence as</p><formula xml:id="formula_4">H v = (h v 1 , h v 2 , . . . , h v N )</formula><p>. The character image of x i is read from preset font files. Since the scripts of Chinese characters have evolved for thousands of years, to capture the graphic relationship between character as much as possible, we select three fonts, namely Gothic typefaces (??, h?it?) in both Simplified and Traditional Chinese, and Small Seal Script (? ?, xi?ozhu?n). The three fonts correspond to the three channels of the character images, whose size is set to 32 ? 32 pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Selective Modality Fusion Module</head><p>After applying the previously mentioned semantic, phonetic and graphic encoders, we get the representation vectors H t , H a and H v in textual, acoustic and visual modalities. To predict the final correct Chinese characters, we develop a selective modality fusion module to integrate these vectors in different modalities. This module fuses information in two levels, i.e., character-level and sentence-level.</p><p>First, for each modality, a selective gate unit is employed to control how much information can flow to the mixed multimodal representation. For example, if a character is misspelled due to its similar pronunciation to the correct one, then more information in the acoustic modality should flow into the mixed representation. The gate values are computed by a fully-connected layer followed by a sigmoid function. The inputs include the character representation of three modalities and the mean of the semantic encoder output H t to capture the overall semantics of the input sentence. Formally, we denote the gate values for the textual, acoustic and visual modalities as g t , g a and g v . The mixed multimodal representationh i of the i-th character is computed as follows:</p><formula xml:id="formula_5">h t = 1 N N i=1 h t i g t i = ?(W t ? [h t i , h a i , h v i ,h t ] + b t ) g a i = ?(W a ? [h t i , h a i , h v i ,h t ] + b a ) g v i = ?(W v ? [h t i , h a i , h v i ,h t ] + b v ) h i = g t i ? h t i + g a i ? h a i + g v i ? h v i (4) where W t , W a , W v , b t , b a , b v are learnable pa- rameters,</formula><p>? is the sigmoid function, and [?] means the concatenation of vectors. Then, we apply the Transformer to fully learn the semantic, phonetic and visual information at the sentence-level. The mixed representations of all characters are packed together into H 0 = [h 1 ,h 2 , ...,h N ], and the probability distribution? i of what the i-th character should be is derived as:</p><formula xml:id="formula_6">H l = Transformer l (H l?1 ), l ? [1, L ] y i = softmax(W o h i + b o ), h i ? H L<label>(5)</label></formula><p>where L is the number of Transformer layers, W o and b o are learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Acoustic and Visual Pretraining</head><p>While acoustic and visual information is essential to the CSC task, equally important is how to associate them with the correct character. In order to learn the acoustic-textual and visual-textual relationships, we propose to pretrain the phonetic and the graphic encoders. For the phonetic encoder, we design an Input Method pretraining objective, that the encoder should recover the Chinese character sequence given the input pinyin sequence. This is what the  Chinese input methods do. We add a linear layer on the top of the encoder to transform the hidden states to the probability distributions over the Chinese character vocabulary. We pretrain the phonetic encoder with the pinyin of the sentences with spelling errors in the training data, and make it recover the character sequences without spelling errors. For the graphic encoder, we design an Optical Character Recognition (OCR) pretraining objective. Given the Chinese character images, the graphic encoder learns the visual information to predict the corresponding character over the Chinese character vocabulary. This is like what the OCR task does, but our recognition is only conducted on the character level and typed scripts. During the pretraining, we also add a linear layer on the top to perform the classification.</p><p>Finally, we load the pretrained weights of the semantic encoder, phonetic encoder, and graphic encoder, and conduct the final training process with the CSC training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we introduce experimental details and results on the SIGHAN benchmarks <ref type="bibr" target="#b33">(Wu et al., 2013;</ref><ref type="bibr" target="#b26">Tseng et al., 2015)</ref>. We then verify the effectiveness of our model by conducting ablation studies and analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Metrics</head><p>Following previous works <ref type="bibr" target="#b6">Cheng et al., 2020)</ref>, we use the SIGHAN training data and the generated pseudo data <ref type="bibr">(Wang et al., 2018, denoted</ref>   in 2013, 2014 and 2015 (denoted as SIGHAN13, SIGHAN14 and SIGHAN15). <ref type="table" target="#tab_3">Table 2</ref> shows the data statistics. Originally, the SIGHAN datasets are in the Traditional Chinese. Following previous works <ref type="bibr" target="#b6">Cheng et al., 2020;</ref><ref type="bibr" target="#b40">Zhang et al., 2020)</ref>, we convert them to the Simplified Chinese using the OpenCC tool 2 .</p><p>Results are reported at the detection level and the correction level. At the detection level, a sentence is considered to be correct if and only if all the spelling errors in the sentence are detected successfully. At the correction level, the model must not only detect but also correct all the erroneous characters to the right ones. We report the accuracy, precision, recall and F1 scores on both levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The REALISE model is implemented using Py-Torch framework <ref type="bibr" target="#b21">(Paszke et al., 2019)</ref> with the Transformer library <ref type="bibr" target="#b32">(Wolf et al., 2020)</ref>. The architecture of the semantic encoder is same as the BERT BASE <ref type="bibr" target="#b10">(Devlin et al., 2019</ref>) model (i.e. 12 transformer layers with 12 attention heads, hidden 2 https://github.com/BYVoid/OpenCC size of 768). We initialize the semantic encoder with the weights of BERT-wwm model <ref type="bibr" target="#b9">(Cui et al., 2019)</ref>. For the phonetic sentence-level encoder, we set the number of layers to 4, and initialize its position embedding with BERT's position embedding. The selective modality fusion module has 3 transformer layers, i.e., L = 3, and the prediction matrix W o is tied with the word embedding matrix of the semantic encoder. All the embeddings and hidden states have the dimension of 768. We use the Pillow library to extract the Chinese character images. When processing the special tokens (e.g.</p><p>[CLS] and [SEP] of BERT), we use the tensor with all zero values as their image inputs. We train our REALISE model with the AdamW <ref type="bibr" target="#b18">(Loshchilov and Hutter, 2017)</ref> optimizer for 10 epochs. The learning rate is set to 5e-5, the batch size is set to 32, and the model is trained with learning rate warming up and linear decay.</p><p>In the SIGHAN13 test set, the annotation quality is relatively poor, that quite a lot of the mixed usage of auxiliary "?", "?", and "?" are not annotated <ref type="bibr" target="#b6">(Cheng et al., 2020)</ref>. Therefore, a wellperformed model may obtain bad scores on it. To alleviate the problem, <ref type="bibr" target="#b6">Cheng et al. (2020)</ref> proposes to continue finetuning the model on the SIGHAN13 training set before testing. We argue that it's not a good practice because it reduces the model performance. Instead, we use a simple and effective post-processing method. We simply remove all the detected and corrected "?", "?", and "?" characters from the model output and then evaluate with the ground truth of SIGHAN13 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare REALISE with the following baselines: KUAS , NTOU <ref type="bibr" target="#b8">(Chu and Lin, 2015)</ref>, NCTU-NTUT <ref type="bibr" target="#b31">(Wang and Liao, 2015)</ref>, HanSpeller++ , LMC <ref type="bibr" target="#b34">(Xie et al., 2015)</ref> mainly utilize heuristics or traditional machine learning algorithms, such as n-gram language model, Conditional Random Field and Hidden Markov Model. Sequence Labeling <ref type="bibr" target="#b29">(Wang et al., 2018)</ref> treats CSC as a sequence labeling problem and applies a BiLSTM model. FASpell (Hong et al., 2019) utilizes a denoising autoencoder (DAE) to generate candidate characters. Soft-Masked BERT <ref type="bibr" target="#b40">(Zhang et al., 2020</ref>) utilizes the detection model to help the correction model learn the right context. SpellGCN <ref type="bibr" target="#b6">(Cheng et al., 2020)</ref> incorporates the predefined character confusion sets to the BERT-based correction model through Graph Convolutional Networks (GCNs). BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> is to directly fine-tune the BERT BASE model with the CSC training data. <ref type="table" target="#tab_5">Table 3</ref> shows the evaluation scores at detection and correction levels on the SIGHAN 13/14/15 test sets. The REALISE model performs significantly better than all the previous state-of-the-art models on all test sets. It can be seen that, by capturing valuable information from acoustic and visual modalities, REALISE yields consistent gain with a large margin against BERT. Specifically, at the correctionlevel, REALISE exceeds BERT by 5.2% F1 on SIGHAN13, 3.8% F1 on SIGHAN14, and 4.4% F1 on SIGHAN15. The results on SIGHAN13 are improved significantly with simple post-processing described in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>There are several successful applications of BERT on the CSC task, such as FASpell and Spell-GCN, which also consider the Chinese character similarity. They attempt to calculate the similarity as the confidence of filtering candidates, or construct similarity graphs from predefined confusion sets. Instead, in our method, multiple encoders are  <ref type="table">Table 4</ref>: Ablation results of the REALISE model averaged on SIGHAN test sets. We apply the following changes to REALISE: removing the phonetic encoder (-Phonetic), removing the graphic encoder (-Graphic), using only one font to build the graphic inputs (-Multi-Fonts), removing acoustic and visual pretraining (-Pretraining), replacing the selective modality fusion mechanism with simple summation (-Selective-Fusion).</p><p>directly applied to derive more informative representation from the acoustic and visual modalities. Compared with SpellGCN <ref type="bibr" target="#b6">(Cheng et al., 2020)</ref>, the SOTA CSC model, our REALISE model achieves an averaging 2.4% F1 improvements at detectionlevel and an averaging 2.6% F1 improvements at correction-level. This indicates that, compared with other extensions of BERT, the explicit utilization of multimodal information of Chinese characters is more beneficial to the CSC task.</p><p>With the simple post-processing as described in Section 4.2, results of each model on the SIGHAN13 test set are improved significantly. Compared with BERT and SpellGCN, we can see that, after the post-processing, the REALISE model is ahead of all the baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We explore the contribution of each component in REALISE by conducting ablation studies with the following settings: 1) removing the phonetic encoder, 2) removing the graphic encoder, 3) using only one font (Gothic typefaces in Simplified Chinese) for the graphic encoder, 4) removing the acoustic and visual pretraining objectives, 5) replacing the selective modality fusion mechanism with simple summation. 0.99 1.00 0.97 1.00 1.00 1.00 0.98 1.00 0.97 1.00 1.00 1.00 0.15 0.29 0.20 0.18 0.24 0.12 0.18 0.19 0.17 0.19 0.25 0.15 0.04 0.06 0.13 0.08 0.03 0.03 0.06 0.12 0.19 0.08 0.59 0.07 1.00 1.00 1.00 1.00 1.00 0.97 1.00 0.99 1.00 1.00 1.00 1.  <ref type="figure">Figure 2</ref>: Selective modality fusion visualization. "I" is the input sentence. "O" is the output of REALISE (also the ground truth), and "T" is the translation. g t , g a , g v are the gate values for the textual, acoustic, and visual modality respectively. We highlight the wrong/correct characters in red/blue color. <ref type="table">Table 4</ref> shows the averaged scores 3 on three SIGHAN test sets. The main motivation of this paper is to discover the character similarity relationships by incorporating the acoustic and visual information. If removing the phonetic or graphic encoder, we can see that the model performance drops at two levels but still outperforms BERT significantly. This suggests that the checking model can benefit from the multimodal information. No matter which component we remove, the performance of REALISE drops, which fully demonstrates the effectiveness of each part in our model. <ref type="figure">Figure 2</ref> gives two examples to analyze the selective modality fusion module. In the first example, the acoustic and visual selective gate values of "?", i.e. g a and g v , are much larger than most other characters since "?(p?)" and "?(p?)" have the same pronunciation and right radical "?". This shows that the selective fusion module can judge whether to introduce phonetic or graphic information into the mixed representation. The second example shows a similar trend for the pronunciation of "?(d?i)" and "?(d?i)". More selective fusion visualization can be found in the Appendix A.2. Besides, we calculate the averaged gate values of erroneous characters for each modality on SIGHAN15. The largest one is the textual modality that the value is almost equal to 1.0. The second one is the acoustic modality that the averaged <ref type="bibr" target="#b43">3</ref> Full ablation results can be found in the Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis of the Selective Modality Fusion Module</head><p>In: ??????????????????? I am going to popular to France, would you like to go with me?</p><p>Out: ??????????????????? I am going to travel to France, would you like to go with me?</p><p>In: ???????????? After returning home, I will go to your house with.</p><p>Out: ???????????? After returning home, I will go to your house soon. value is 0.334, and the smallest one is the visual modality that the value is 0.229. It means that the information from the semantic encoder is the most important for correcting the spelling errors. The acoustic modality is more important than the visual modality, which is consistent with the fact that the spelling errors caused by similar pronunciations are more frequent than errors caused by similar character shapes <ref type="bibr" target="#b16">(Liu et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case Study</head><p>In the first example in <ref type="table" target="#tab_7">Table 5</ref>, "?" is the erroneous character. If ignoring the Chinese character similarities, we can find that there are multiple candidate corrections to replace the "?" character. For instance, we can replace it with "?" and the English translation is "I am going to parade in France". However, the REALISE's output is the best correction, because "?(li?)" and "?(l??)" have a similar pronunciation. In the second example, not only the phonetic information, but also the visual information is important for correcting "?(g?n)" to "?(h?n)". In detail, the two characters share the same final "en" in pronunciation, and have the same right radical "?". The errors in the above examples are not corrected by SpellGCN, since they are not defined as confusing character pairs in the handcrafted confusion sets <ref type="bibr" target="#b10">(Lee et al., 2019)</ref>. Specifically, in the SIGHAN15 test set, there are 16% erroneouscorrected character pairs not in the predefined confusion sets. SpellGCN corrects 64.6% of them but REALISE performs better with 73.5% correction. Besides, for the easily-confused pairs in the predefined sets, SpellGCN corrects 82.5% of them and REALISE corrects 85.8%. This indicates that leveraging multimodal information of Chinese charac-ters helps the model generalize better in capturing the character similarity relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a model called REALISE for Chinese spell checking. Since the spelling errors in Chinese are often semantically, phonetically or graphically similar to the correct characters, RE-ALISE leverages information in textual, acoustic and visual modalities to detect and correct the errors. The REALISE model captures information in these modalities using tailored semantic, phonetic and graphic encoders. Besides, a selective modality fusion mechanism is proposed to control the information flow of these modalities. Experiments on the SIGHAN benchmarks show that the proposed REALISE outperforms the baseline models using only textual information by a large margin, which verifies that leveraging acoustic and visual information helps the Chinese spell checking task. The affair also happened from this point 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.32 0.20 0.09 0.13 0.22 0.22 0.21 0.17 0.22 0.32 0.28 0.32 0.24 0.25 0.07 0.09 0.12 0.07 0.10 0.14 0.10 0.08 0.07 0.11 0.53 0.11 0.12 0.09 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1. <ref type="formula">00</ref>   <ref type="figure">Figure 3</ref>: Selective modality fusion visualization. "I" is the input sentence. "O" is the output of REALISE (also the ground truth), and "T" is the translation. g t , g a , g v are the gate values for the textual, acoustic, and visual modality respectively. We highlight the wrong/correct characters in red/blue color.</p><p>5. -Selective-Fusion: replacing the selective modality fusion mechanism with simple summation.</p><p>We can see that, when we remove anything from our model, the REALISE performance drops consistently, and it drops most apparently in the SIGHAN14 test set. These results suggest that each part of our model is an effective means for boosting the checking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Selective Modality Fusion Visualization</head><p>We show more examples in <ref type="figure">Figure 3</ref>. We can see that, if the misused characters are phonetically similar to the correct ones, the acoustic gate values tend to be larger, and if they are graphically similar, the visual gate values are larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Detection  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>00 1.00 0.20 0.20 0.14 0.34 0.18 0.13 0.37 0.25 0.19 0.31 0.25 0.23 0.17 0.09 0.04 0.13 0.12 0.08 0.06 0.52 0.09 0.04 0.04 0.06 0.08 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the used datasets. All the training data are merged to train the REALISE model. The test sets are used separately to evaluate the model performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>as Wang271K) as the training set. We evaluate our model on the SIGHAN test sets Hong et al., 2019) 63.1 76.2 63.2 69.1 60.5 73.1 60.5 66.2 BERT (Cheng et al., 2020) -79.0 72.8 75.8 -77.7 71.6 74.6 SpellGCN (Cheng et al., 2020) -80.1 74.4 77.2 -78.3 72.7 75.4 SpellGCN ? (Our reimplementation) 78.8 85.7 78.8 82.1 77.8 84.6 77.8 81.0</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Acc</cell><cell>Detection Level Pre Rec</cell><cell cols="2">F1 Acc</cell><cell cols="2">Correction Level Pre Rec</cell><cell>F1</cell></row><row><cell></cell><cell>Sequence Labeling (Wang et al., 2018)</cell><cell cols="3">-54.0 69.3 60.7</cell><cell>-</cell><cell>-</cell><cell>-52.1</cell></row><row><cell>SIGHAN13</cell><cell>FASpell (BERT  ?</cell><cell cols="6">77.0 85.0 77.0 80.8 75.2 83.0 75.2 78.9</cell></row><row><cell></cell><cell>REALISE  ?</cell><cell cols="6">82.7 88.6 82.5 85.4 81.4 87.2 81.2 84.1</cell></row><row><cell></cell><cell>Sequence Labeling (Wang et al., 2018)</cell><cell cols="3">-51.9 66.2 58.2</cell><cell>-</cell><cell>-</cell><cell>-56.1</cell></row><row><cell></cell><cell>FASpell (Hong et al., 2019)</cell><cell cols="6">70.0 61.0 53.5 57.0 69.3 59.4 52.0 55.4</cell></row><row><cell>SIGHAN14</cell><cell>SpellGCN (Cheng et al., 2020)</cell><cell cols="3">-65.1 69.5 67.2</cell><cell cols="3">-63.1 67.2 65.3</cell></row><row><cell></cell><cell>BERT</cell><cell cols="6">75.7 64.5 68.6 66.5 74.6 62.4 66.3 64.3</cell></row><row><cell></cell><cell>REALISE</cell><cell cols="6">78.4 67.8 71.5 69.6 77.7 66.3 70.0 68.1</cell></row><row><cell></cell><cell>KUAS (Chang et al., 2015)</cell><cell cols="6">53.2 57.5 24.6 34.4 51.5 53.7 21.1 30.3</cell></row><row><cell></cell><cell>NTOU (Chu and Lin, 2015)</cell><cell cols="6">42.2 42.2 41.8 42.0 39.0 38.1 35.2 36.6</cell></row><row><cell></cell><cell>NCTU-NTUT (Wang and Liao, 2015)</cell><cell cols="6">60.1 71.7 33.6 45.7 56.4 66.3 26.1 37.5</cell></row><row><cell></cell><cell>HanSpeller++ (Zhang et al., 2015)</cell><cell cols="6">70.1 80.3 53.3 64.0 69.2 79.7 51.5 62.5</cell></row><row><cell></cell><cell>LMC (Xie et al., 2015)</cell><cell cols="6">54.6 63.8 21.5 32.1 52.3 57.9 16.7 26.0</cell></row><row><cell>SIGHAN15</cell><cell>Sequence Labeling (Wang et al., 2018)</cell><cell cols="3">-56.6 69.4 62.3</cell><cell>-</cell><cell>-</cell><cell>-57.1</cell></row><row><cell></cell><cell>FASpell (Hong et al., 2019)</cell><cell cols="6">74.2 67.6 60.0 63.5 73.7 66.6 59.1 62.6</cell></row><row><cell></cell><cell cols="7">Soft-Masked BERT (Zhang et al., 2020) 80.9 73.7 73.2 73.5 77.4 66.7 66.2 66.4</cell></row><row><cell></cell><cell>SpellGCN (Cheng et al., 2020)</cell><cell cols="3">-74.8 80.7 77.7</cell><cell cols="3">-72.1 77.7 75.9</cell></row><row><cell></cell><cell>BERT</cell><cell cols="6">82.4 74.2 78.0 76.1 81.0 71.6 75.3 73.4</cell></row><row><cell></cell><cell>REALISE</cell><cell cols="6">84.7 77.3 81.3 79.3 84.0 75.9 79.9 77.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>The performance of our model and all baseline models on SIGHAN test sets. The " ?" symbol means we apply post-processing (Section 4.2) to the model outputs on SIGHAN13. Results of REALISE on all SIGHAN test sets outperforms all the corresponding baselines with a significance level p &lt; 0.05.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Examples of the input and output of our RE-</cell></row><row><cell>ALISE model. We highlight the wrong/correct charac-</cell></row><row><cell>ters in red/blue color.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>tem for search query spelling correction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358-366, Beijing, China. Coling 2010 Organizing Committee. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog-</figDesc><table><row><cell>T</cell><cell>I plan to watch a movie with my girlfriend</cell></row><row><cell>O I</cell><cell>nition. In 2016 IEEE Conference on Computer Vi-</cell></row><row><cell>! !</cell><cell>sion and Pattern Recognition, CVPR 2016, Las Ve-1.00 1.00 1.00 1.00 1.00 0.97 1.00 0.99 1.00 1.00 1.00 1.00 1.00</cell></row><row><cell>! "</cell><cell>gas, NV, USA, June 27-30, 2016, pages 770-778. 0.20 0.20 0.14 0.34 0.18 0.13 0.37 0.25 0.19 0.31 0.25 0.23 0.17</cell></row><row><cell>! #</cell><cell>IEEE Computer Society. 0.09 0.04 0.13 0.12 0.08 0.06 0.52 0.09 0.04 0.04 0.06 0.08 0.05</cell></row><row><cell cols="2">Julian Hitschler, Shigehiko Schamoni, and Stefan Rie-</cell></row><row><cell></cell><cell>zler. 2016. Multimodal pivots for image caption</cell></row><row><cell></cell><cell>translation. In Proceedings of the 54th Annual Meet-</cell></row><row><cell></cell><cell>ing of the Association for Computational Linguistics,</cell></row><row><cell></cell><cell>ACL 2016, August 7-12, 2016, Berlin, Germany, Vol-</cell></row><row><cell></cell><cell>ume 1: Long Papers. The Association for Computer</cell></row><row><cell></cell><cell>Linguistics.</cell></row><row><cell cols="2">0.99 1.00 1.00 1.00 0.99 1.00 0.99 1.00 0.99 1.00 0.99 Yuzhong Hong, Xianguo Yu, Neng He, Nan Liu, 0.26 0.23 0.24 0.17 0.37 0.34 0.21 0.21 0.21 0.30 0.21 and Junhui Liu. 2019. Faspell: A fast, adapt-able, simple, powerful chinese spell checker based 0.22 0.62 0.24 0.07 0.18 0.23 0.09 0.06 0.04 0.05 0.05</cell></row><row><cell></cell><cell>on dae-decoder paradigm. In Proceedings of the</cell></row><row><cell></cell><cell>5th Workshop on Noisy User-generated Text, W-</cell></row><row><cell></cell><cell>NUT@EMNLP 2019, Hong Kong, China, November</cell></row><row><cell></cell><cell>4, 2019, pages 160-169. Association for Computa-</cell></row><row><cell></cell><cell>tional Linguistics.</cell></row><row><cell cols="2">Norman Jerry. 1988. Chinese (cambridge language sur-</cell></row><row><cell></cell><cell>veys).</cell></row><row><cell cols="2">Thomas N Kipf and Max Welling. 2016. Semi-</cell></row><row><cell></cell><cell>supervised classification with graph convolutional</cell></row><row><cell></cell><cell>networks. arXiv preprint arXiv:1609.02907.</cell></row><row><cell cols="2">Lung Hao Lee, Wun Syuan Wu, Jian Hong Li, Yu Chi</cell></row><row><cell></cell><cell>Lin, and Yuen Hsien Tseng. 2019. Building a con-</cell></row><row><cell></cell><cell>fused character set for chinese spell checking. In</cell></row><row><cell></cell><cell>27th International Conference on Computers in Edu-</cell></row><row><cell></cell><cell>cation, ICCE 2019, pages 703-705. Asia-Pacific So-</cell></row><row><cell></cell><cell>ciety for Computers in Education.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Fusion 84.4 76.8 81.2 78.9 83.6 75.4 79.7 77.5</figDesc><table><row><cell></cell><cell></cell><cell>Level</cell><cell></cell><cell>Correction Level</cell></row><row><cell></cell><cell></cell><cell>Acc Pre Rec</cell><cell>F1</cell><cell>Acc Pre Rec</cell><cell>F1</cell></row><row><cell></cell><cell>BERT</cell><cell cols="3">77.0 85.0 77.0 80.8 75.2 83.0 75.2 78.9</cell></row><row><cell></cell><cell>REALISE</cell><cell cols="3">82.7 88.6 82.5 85.4 81.4 87.2 81.2 84.1</cell></row><row><cell></cell><cell>-Phonetic</cell><cell cols="3">82.4 87.4 82.3 84.8 81.2 86.1 81.1 83.5</cell></row><row><cell>SIGHAN13</cell><cell>-Graphic</cell><cell cols="3">82.1 88.1 82.1 85.0 80.9 86.7 80.8 83.7</cell></row><row><cell></cell><cell>-Multi-Fonts</cell><cell cols="3">82.2 87.5 82.2 84.8 81.2 86.4 81.2 83.7</cell></row><row><cell></cell><cell>-Pretraining</cell><cell cols="3">82.8 88.2 82.7 85.4 81.4 86.7 81.3 83.9</cell></row><row><cell></cell><cell cols="4">-Selective-Fusion 82.0 87.3 82.0 84.6 81.0 86.2 81.0 83.5</cell></row><row><cell></cell><cell>BERT</cell><cell cols="3">75.7 64.5 68.6 66.5 74.6 62.4 66.3 64.3</cell></row><row><cell></cell><cell>REALISE</cell><cell cols="3">78.4 67.8 71.5 69.6 77.7 66.3 70.0 68.1</cell></row><row><cell></cell><cell>-Phonetic</cell><cell cols="3">77.1 65.5 69.2 67.3 76.3 63.8 67.5 65.6</cell></row><row><cell>SIGHAN14</cell><cell>-Graphic</cell><cell cols="3">78.0 67.3 69.6 68.4 77.1 65.6 67.9 66.7</cell></row><row><cell></cell><cell>-Multi-Fonts</cell><cell cols="3">76.9 65.0 69.6 67.2 76.2 63.6 68.1 65.7</cell></row><row><cell></cell><cell>-Pretraining</cell><cell cols="3">77.5 65.6 70.4 67.9 76.7 64.0 68.7 66.2</cell></row><row><cell></cell><cell cols="4">-Selective-Fusion 77.6 66.5 69.0 67.7 76.9 64.8 67.3 66.0</cell></row><row><cell></cell><cell>BERT</cell><cell cols="3">82.4 74.2 78.0 76.1 81.0 71.6 75.3 73.4</cell></row><row><cell></cell><cell>REALISE</cell><cell cols="3">84.7 77.3 81.3 79.3 84.0 75.9 79.9 77.8</cell></row><row><cell></cell><cell>-Phonetic</cell><cell cols="3">84.2 76.2 81.7 78.9 83.3 74.5 79.9 77.1</cell></row><row><cell>SIGHAN15</cell><cell>-Graphic</cell><cell cols="3">84.3 76.6 79.9 78.2 83.5 75.0 78.2 76.6</cell></row><row><cell></cell><cell>-Multi-Fonts</cell><cell cols="3">84.5 76.5 81.9 79.1 83.5 74.6 79.9 77.1</cell></row><row><cell></cell><cell>-Pretraining</cell><cell cols="3">84.2 75.7 81.3 78.4 83.7 74.9 80.4 77.5</cell></row><row><cell></cell><cell>-Selective-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Ablation results of the REALISE model on each SIGHAN dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Ablation</head><p>We conduct an ablation study to verify the effectiveness of the proposed method. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using smt for ocr error correction of historical texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haithem</forename><surname>Afli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?raic</forename><surname>Sheridan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="962" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.279</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Layer normalization. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the third shared task on multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiraag</forename><surname>Lala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-6402</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="304" to="323" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introduction to a proofreading tool for chinese spelling check task of SIGHAN-8</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao-Hsing</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Chih</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Han</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-3109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Eighth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07-30" />
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Being negative but constructively: Lessons learnt from creating better visual question answering datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1040</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="431" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spellgcn: Incorporating phonological and visual similarities into language models for chinese spelling check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="871" to="881" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Aglar G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/d14-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">NTOU chinese spelling check system in sighan-8 bake-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Jie</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-3121</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Eighth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07-30" />
			<biblScope unit="page" from="137" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pre-training with whole word masking for chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno>abs/1906.08101</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA; Long and Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic features for essay scoring-an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1072" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Micol</surname></persName>
		</author>
		<title level="m">Chris Quirk, and Xu Sun. 2010. A large scale ranker-based sys</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visually and phonologically similar characters in incorrect chinese words: Analyses, identification, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/1967293.1967297</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visually and phonologically similar characters in incorrect simplified Chinese words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hua</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Ying</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Posters</title>
		<meeting><address><addrLine>Beijing, China. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="739" to="747" />
		</imprint>
	</monogr>
	<note>Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spelling correction for search engine queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?rio</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Natural Language Processing</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="372" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glyce: Glyph-vectors for chinese character representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="2742" to="2753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">VL-BERT: pretraining of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ERNIE 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analysis of english spelling errors in a word-typing game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="385" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LXMERT: learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="5099" to="5110" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to SIGHAN 2015 bake-off for chinese spelling check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuen-Hsien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lung-Hao</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Ping</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-3106</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Eighth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07-30" />
			<biblScope unit="page" from="32" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pointer networks. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A hybrid approach to automatic corpus generation for chinese spelling check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1273</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="2517" to="2527" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Confusionset-guided pointer networks for chinese spelling check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1578</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5780" to="5785" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word vector/conditional random field-based chinese spelling error detection for SIGHAN-2015 evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih-Ru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fu</forename><surname>Liao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-3108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Eighth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07-30" />
			<biblScope unit="page" from="46" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Chinese spelling check evaluation at SIGHAN bake-off 2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Hung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lung-Hao</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, SIGHAN@IJCNLP 2013</title>
		<meeting>the Seventh SIGHAN Workshop on Chinese Language Processing, SIGHAN@IJCNLP 2013<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10-14" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Chinese spelling check system based on n-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiduo</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-3120</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Eighth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-30" />
			<biblScope unit="page" from="128" to="136" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Chinese spelling error detection and correction based on language model, pronunciation, and shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third CIPS-SIGHAN Joint Conference on Chinese Language Processing</title>
		<meeting>The Third CIPS-SIGHAN Joint Conference on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="220" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overview of SIGHAN 2014 bake-off for chinese spelling check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chih</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lung-Hao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuen-Hsien</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-6820</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third CIPS-SIGHAN Joint Conference on Chinese Language Processing</title>
		<meeting>The Third CIPS-SIGHAN Joint Conference on Chinese Language Processing<address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10-20" />
			<biblScope unit="page" from="126" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.1109/MIS.2016.94</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Effective sentiment-relevant word selection for multi-modal sentiment analysis in spoken language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spelling error correction with soft-masked BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jicong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="882" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hanspeller++: A unified framework for chinese spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-3107</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Eighth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07-30" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Phonetic: removing the phonetic encoder. 2. -Graphic: removing the graphic encoder</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Multi-Fonts: using only one font (Gothic typefaces in Simplified Chinese) for the graphic encoder</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pretraining: removing the acoustic and visual pretraining objectives</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
