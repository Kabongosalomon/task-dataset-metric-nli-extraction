<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Song</surname></persName>
							<email>qingyusong@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<email>yingtai@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Yang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Localizing individuals in crowds is more in accordance with the practical demands of subsequent high-level crowd analysis tasks than simply counting. However, existing localization based methods relying on intermediate representations (i.e., density maps or pseudo boxes) serving as learning targets are counter-intuitive and error-prone. In this paper, we propose a purely point-based framework for joint crowd counting and individual localization. For this framework, instead of merely reporting the absolute counting error at image level, we propose a new metric, called density Normalized Average Precision (nAP), to provide more comprehensive and more precise performance evaluation. Moreover, we design an intuitive solution under this framework, which is called Point to Point Network (P2PNet). P2PNet discards superfluous steps and directly predicts a set of point proposals to represent heads in an image, being consistent with the human annotation results. By thorough analysis, we reveal the key step towards implementing such a novel idea is to assign optimal learning targets for these proposals. Therefore, we propose to conduct this crucial association in an one-to-one matching manner using the Hungarian algorithm. The P2PNet not only significantly surpasses state-of-the-art methods on popular counting benchmarks, but also achieves promising localization accuracy. The codes will be available at: TencentYoutuResearch/CrowdCounting-P2PNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Among all the related concrete tasks of crowd analysis, crowd counting is a fundamental pillar, aiming to estimate the number of individuals in a crowd. However, simply giving a single number is obviously far from being able to support the practical demands of subsequent higher-level crowd * Equal contribution. ? Corresponding author.  <ref type="figure">Figure 1</ref>. Illustrations for the comparison of our pipeline with existing methods, in which the predictions are marked in Red while the ground truths are marked as Green. Top flow: The dominated density map learning based methods fail to provide the exact locations of individuals. Middle flow: The estimated inaccurate ground truth bounding boxes make the detection based methods error-prone, such as the missing detections as indicated, especially for the NMS-like process. Bottom flow: Our pipeline directly predicts a set of points to represent the locations of individuals, which is simple, intuitive and competitive as demonstrated, bypassing those error-prone steps. Best viewed in color.</p><p>analysis tasks, such as crowd tracking, activity recognition, abnormality detection, flow/behavior prediction, etc.</p><p>In fact, there is an obvious tendency in this field for more challenging fine-grained estimation (i.e., the locations of individuals) beyond simply counting. Specifically, some approaches cast crowd counting as a head detection problem, but leaving more efforts on labor-intensive annotation for tiny-scale heads. Other approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35]</ref> attempted to generate the pseudo bounding boxes of heads with only point annotations provided, which however appears to be tricky or inaccurate at least. Also trying to directly locate individuals, several methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref> got stuck in suppressing or splitting over-close instance candidates, making themselves error-prone due to the extreme head scale variation, especially for highly-congested regions. To eschew the above problems, we propose a purely point-based framework for jointly counting and localizing individuals in crowds. This framework directly uses point annotations as learning targets and simultaneously outputs points to locate individuals, benefiting from the high-precision localization property of point representation and its relatively cheaper annotation cost. The pipeline is illustrated in <ref type="figure">Figure 1</ref>.</p><p>Additionally, in terms of the evaluation metrics, some farsighted works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37]</ref> encourage to adopt patch-level metrics for fine-grained evaluation, but they only provide a rough measure for localization. Other existing localization aware metrics either ignore the significant density variation across crowds <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35]</ref> or lack the punishment for duplicate predictions <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>. Instead, we propose a new metric called density Normalized Average Precision (nAP) to provide a comprehensive evaluation metric for both localization and counting errors. The nAP metric supports both box and point representation as inputs (i.e., predictions or annotations), without the defects mentioned above.</p><p>Finally, as an intuitive solution under this new framework, we develop a novel method to directly predict a set of point proposals with the coordinates of heads in an image and their confidences. Specifically, we propose a Point-to-Point Network (P2PNet) to directly receive a set of annotated head points for training and predict points too during inference. Then to make such an idea work correctly, we delve into the ground truth target assignation process to reveal the crucial of such association. The conclusion is that either the case when multiple proposals are matched to a single ground truth, or the opposite case, can make the model confused during training, leading to over-estimated or under-estimated counts. So we propose to perform an one-to-one matching by Hungarian algorithm to associate the point proposals with their ground truth targets, and the unmatched proposals should be classified as negatives. We empirically show that such a matching is beneficial to improving the nAP metric, serving as a key component for our solution under the new framework. This simple, intuitive and efficient design yields state-of-the-art counting performance and promising localization accuracy.</p><p>The major contributions of this work are three-fold: 1. We propose a purely point-based framework for joint counting and individual localization in crowds. This framework encourages fine-grained predictions, benefiting the practical demands of downstream tasks in crowd analysis.</p><p>2. We propose a new metric termed density Normalized Average Precision to account for the evaluation of both lo-calization and counting, as a comprehensive evaluation metric under the new framework.</p><p>3. We propose P2PNet as an intuitive solution following this conceptually simple framework. The method achieves state-of-the-art counting accuracy and promising localization performance, and might also be inspiring for other tasks relying on point predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, we review two kinds of crowd counting methods in recent literature. They are grouped according to whether locations of individuals could be provided. Since we focus on the estimation of locations, existing metrics accounting for localization errors are also discussed.</p><p>Density Map based Methods. The adoption of density map is a common choice of most state-of-the-art crowd counting methods, since it was firstly introduced in <ref type="bibr" target="#b17">[18]</ref>. And the estimated count is obtained by summing over the predicted density maps. Recently, many efforts have been devoted to pushing forward the counting performance frontier of such methods. They either conduct a pixel-wise density map regression <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11]</ref>, or resort to classify the count value of local patch into several bins <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Although many compelling models have been proposed, these density map learning based models still fail to provide the exact locations of individuals in crowds, not to mention their inherent flaws as pointed out in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b24">25]</ref>. Whereas the proposed method goes beyond counting and focuses on the direct prediction for locations of individuals, eschewing the defects of density maps and also benefiting the downstream practical applications.</p><p>Localization based Methods. These methods typically achieve counting by firstly predicting the locations of individuals. Motivating by cutting-edge object detectors, some counting methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> try to predict the bounding boxes for heads of individuals. However, with only the point annotations available, these methods rely on heuristic estimation for ground truth bounding boxes, which is error-prone or even infeasible. These inaccurate bounding boxes not only confuse the model training process, but also make the post-process, i.e., NMS, fail to suppress false detections. Without those inaccurate targets introduced, other methods locate individuals by points <ref type="bibr" target="#b23">[24]</ref> or blobs <ref type="bibr" target="#b16">[17]</ref>, but leaving more efforts to remove duplicates or split over-close detected individuals in congest regions. Instead, bypassing these tricky post-processing with an one-to-one matching, we propose to streamline the framework to directly estimate the point locations of individuals.</p><p>Localization Aware Metrics. Traditional universally agreed evaluation metrics only measure the counting errors, entirely ignoring the significant spatial variation of estimation errors in single image. To provide a more accurate eval-uation, some works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref> advocate to adopt patch-level or pixel-level absolute counting error as criteria, in lieu of the commonly used image-level metric. Other research <ref type="bibr" target="#b34">[35]</ref> proposes Mean Localization Error to compute the average pixel distance between the predictions and ground truths, merely evaluating the localization errors. Inspired by evaluation metric used in object detection, <ref type="bibr" target="#b12">[13]</ref> proposes to use the area under the Precision-Recall curve after a greedy association, which however ignores the punishment for duplicate predictions. Hence, <ref type="bibr" target="#b23">[24]</ref> proposes to adopt a sequential matching and then use the standard Average Precision (AP) for evaluation. In this paper, we propose a new metric, termed density Normalized Average Precision (nAP), as a comprehensive evaluation metric for both localization errors and false detections. In particular, the nAP metric introduces a density normalization to account for the large density variation problem in crowds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Work</head><p>We firstly introduce the proposed framework in detail (Sec. 3.1), and the new evaluation metric nAP is also presented (Sec. 3.2). Then we conduct a thorough analysis to reveal the key issue in improving the nAP metric under the new framework (Sec. 3.3). Inspired by the insightful analysis, we introduce the proposed P2PNet (Sec. 3.4), which directly predicts a set of point proposals to represent heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Purely Point-based Framework</head><p>The proposed framework directly receives point annotations as its learning targets and then provides the exact locations for individuals in a crowd, rather than simply counting the number of individuals within it. And the locations of individuals are typically indicated by the center points of heads, possibly with optional confidence scores.</p><p>Formally, given an image with N individuals, we use p i = (x i , y i ), i ? {1, .., N }, to represent the head's center point of the i-th individual, which is located in (x i , y i ). Then the collection of the center points for all individuals could be further denoted as P = {p i |i ? {1, .., N }}. Assuming a well-designed model M is trained to instantiate this new framework. And the model M predicts another two collectionsP = {p j |j ? {1, .., M }} and? = {? j |j ? {1, .., M }}, in which M is the number of predicted individuals, and? j is the confidence score of the predicted pointp j . Without loss of generality, we may assume thatp j is exactly the prediction for the ground truth point p i . Then our goal is to ensure that the distance betweenp j and p i is as close as possible with a sufficiently high score? j . As a byproduct, the number of predicted individuals M should also be close enough to the ground truth crowd number N . In a nutshell, the new framework could simultaneously achieve crowd counting and individual localization.</p><p>Compared with traditional counting methods, the individual locations provided by this framework are helpful to those motion based crowd analysis tasks, such as crowd tracking <ref type="bibr" target="#b48">[49]</ref>, activity recognition <ref type="bibr" target="#b7">[8]</ref>, abnormality detection <ref type="bibr" target="#b3">[4]</ref>, etc. Besides, without relying on laborintensive annotations, inaccurate pseudo boxes or tricky post-processing, this framework benefits from the highprecision localization property of original point representation, especially for highly-congested regions in crowds.</p><p>Therefore, this new framework is worth more attentions due to its advantages and practical values over traditional crowd counting. However, since the existence of severe occlusions, density variations, and annotation errors, it is quite challenging to tackle with such a task <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>, which even is considered as ideal but infeasible in <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Density Normalized Average Precision</head><p>It is natural to ask that how to evaluate the performance of model M under the above new framework. In fact, a well-performed model following this framework should not only produce as few as false positives or false negatives, but also achieve competitive localization accuracy. Therefore, motivated by the mean Average Precision (mAP) <ref type="bibr" target="#b22">[23]</ref> metric widely used in Object Detection, we propose a density Normalized Average Precision (nAP) to evaluate both the localization errors and counting performance.</p><p>The nAP is calculated based on the Average Precision, which is the area under the Precision-Recall (PR) curve. And the PR curve could be easily obtained by accumulating a binary list following the common practice in <ref type="bibr" target="#b22">[23]</ref>. In the binary list, a True Positive (TP) prediction is indicated by 1, and a False Positive (FP) prediction is indicated by 0. Specifically, given all predicted head pointsP, we firstly sort the point list with their confidence scores from high to low. Then we sequentially determine that the point under investigation is either TP or FP, according to a pre-defined density aware criterion. Different from the greedy association used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref>, we apply a sequential association in which those higher scored predictions are associated firstly. In this way, these TP predictions could be easily obtained by a simple threshold filtering during inference.</p><p>We introduce our density aware criterion as follows. A predicted pointp j is classified as TP only if it could be matched to certain ground truth p i , in which p i must not be matched before by any higher-ranked point. The matching process is guided by a pixel-level Euclidean distance based criterion 1(p j , p i ). However, directly using the pixel distance to measure the affinity ignores the side effects from the large density variation across crowds. Thus, we introduce a density normalization for this matching criterion to mitigate the density variation problem. The density around a certain ground truth point is estimated following <ref type="bibr" target="#b47">[48]</ref>.  Formally, the final criterion used in nAP is defined as:</p><formula xml:id="formula_0">1(p j , p i ) = 1, if d(p j , p i )/d kNN (p i ) &lt; ?, 0, otherwise,<label>(1)</label></formula><p>where d(p j , p i ) = ||p j ? p i || 2 denotes to the Euclidean distance, and d kNN (p i ) denotes the average distance to the k nearest neighbors of p i . We use a threshold ? to control the desired localization accuracy, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Our Approach</head><p>Our approach is an intuitive solution following the proposed framework, which directly predict a set of point proposals to represent the center points for heads of individuals. In fact, the idea of point prediction is not new to the vision community, although it is quite different here. To name a few, in the field of pose estimation, some methods adopt heatmap regression <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43]</ref> or direct point regression <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44]</ref> to predict the locations of pre-defined keypoints. Since the number of the keypoints to be predicted is fixed, the learning targets for these point proposals could be determined entirely before the training. Differently, the proposed framework aims to predict a point set of unknown size and is an open-set problem by nature <ref type="bibr" target="#b44">[45]</ref>. Thus, one crucial problem of such a methodology is to determine which ground truth point should the current prediction be responsible for.</p><p>We propose to solve this key problem with a mutually optimal one-to-one association strategy during the training stage. Let us conduct a thorough analysis to show the defects of the other two strategies for the ground truth targets assignment. Firstly, for each ground truth point, the proposal with the nearest distance should produce the best prediction. However, if we select the nearest proposal for every ground truth point, it is likely that one proposal might be matched to multiple ground truth points, as shown in <ref type="figure" target="#fig_4">Figure 3 (a)</ref>. In such a case, only one ground truth could be correctly predicted, leading to under-estimated counts, especially for the congested regions. Secondly, for each point proposal, we may assign the nearest ground truth point as  (c) Our One-to-One match is without the above two defects, thus is suitable for direct point prediction.</p><p>its target. Intuitively, this strategy might be helpful to alleviate the overall overhead of the optimization, since the nearest ground truth point is relatively easier to predict. However, in such an assignment, there may exist multiple proposals which simultaneously predict the same ground truth, as shown in <ref type="figure" target="#fig_4">Figure 3 (b)</ref>. Because there are no scale annotations available, it is tricky to suppress these duplicate predictions, which might lead to over-estimated. Consequently, the association process should take both sides into consideration and produces the mutually optimal one-toone matching results, as shown in <ref type="figure" target="#fig_4">Figure 3</ref> (c). Additionally, both the other two strategies have to determine a negative threshold, and the proposals whose distance with their matched targets are above this threshold will be considered as negatives. While using the one-to-one matching, those unmatched proposals are automatically remained as negatives, without any hyperparameter introduced. In a nutshell, the key to solve the open-set direct point prediction problem is to ensure a mutually optimal one-to-one matching between predicted and ground truth points.</p><p>After the ground truth targets are obtained, these point proposals could be trained through an end-to-end optimization. Finally, the positive proposals should be pushed toward their targets, while those negative proposals would be simply classified as backgrounds. Since the point proposals are dynamically updated along with the training process, those proposals which have the potential to perform better could be gradually selected by the one-to-one matching to serve as the final predictions.</p><p>Actually, the distance used in above matching could be any other cost measure beyond pixel distance, such as a combination of confidence score and pixel distance. We empirically show that taking confidence scores of proposals into consideration during the one-to-one matching is helpful to improve the proposed nAP metric. Let us consider two predicted proposals around the same ground truth point p i . If they have the same confidence score, the one closer to p i should be matched as positive and encouraged to achieve higher localization accuracy. While the other one proposal should be matched as negative and supervised to lower its confidence, thus might not be matched again during next training iteration. On the contrary, if the two proposals share the same distance from p i , the one with higher confidence should be trained to be closer to p i with a much higher confidence. Both the above two cases would encourage the positive proposals to have more accurate locations as well as relatively higher confidences, which is beneficial to the improvement of nAP under the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The P2PNet Model</head><p>In this part, we present the detailed pipeline of the proposed Point to Point Network (P2PNet). Begining with the generation of point proposals, we introduce our one-to-one association strategy in detail. Then we present the loss function and the network architecture for the P2PNet.</p><p>Point Proposal Prediction. Let us denote the deep feature map outputted from the backbone network by F s , in which s is the downsampling stride and F s is with a size of H ? W . Then based on F s , we adopt two parallel branches for point coordinate regression and proposal classification. For the classification branch, it outputs the confidence scores with a Softmax normalization. For the regression branch, it resorts to predict the offsets of the point coordinates due to the intrinsic translation invariant property of convolution layers. Specifically, each pixel on F s should correspond to a patch of size s ? s in the input image. In that patch, we firstly introduces a set of fixed reference points R = {R k |k ? {1, ..., K}} with pre-defined locations R k = (x k , y k ). These reference points could be either densely arranged on the patch or just set to the center of that patch, as shown in <ref type="figure" target="#fig_5">Figure 4</ref>. Since there are K reference points for each location on F s , the regression branch should produce totally H ? W ? K point proposals. Assuming the reference point R k predicts offsets (? k jx , ? k jy ) for its point proposalp j = (x j ,? j ), then the coordinate of p j is calculated as follows:</p><formula xml:id="formula_1">x j = x k + ?? k jx , y j = y k + ?? k jy ,<label>(2)</label></formula><p>where ? is a normalization term, which scales the offsets to rectify the relatively small predictions.</p><p>Proposal Matching. Following the symbols defined in Sec. 3.1, we assign the ground truth target fromP for every point proposal in P using an one-to-one matching strategy ?(P,P, D). The D is a pair-wise matching cost matrix with the shape N ? M , which measures the distance between two points in a pair. Instead of simply using the pixel distance, we also consider the confidence score of that proposal, since we encourage the positive proposals to have higher confidences. Formally, the cost matrix D is defined as follows:</p><formula xml:id="formula_2">D(P,P) = ? ||p i ?p j || 2 ?? j i?N,j?M ,<label>(3)</label></formula><p>where ||?|| 2 denotes to the l 2 distance, and? j is the confidence score of the proposalp j . ? is a weight term to balance the effect from the pixel distance. Based on the pair-wise cost matrix D, we conduct the association using the Hungarian algorithm <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref> as the matching strategy ?. Note that in our implementation, we ensure M &gt; N to produce many enough predictions, since those redundant proposals would be classified as negatives. From the perspective of the ground truth points, let us use a permutation ? of {1, ..., M } to represent the optimal matching result, i.e., ? = ?(P,P, D). That is to say, the ground truth point p i is matched to the proposalp ?(i) . Furthermore, those matched proposals (positives) could be represented as a setP pos = {p ?(i) |i ? {1, ..., N }}, and those unmatched proposals in the setP neg = p ?(i) |i ? {N + 1, ..., M } are labeled as negatives.</p><p>Loss Design. After the ground truth targets have been obtained, we calculate the Euclidean loss L loc to supervise the point regression, and use Cross Entropy loss L cls to train the proposal classification. The final loss function L is the summation of the above two losses, which is defined as:</p><formula xml:id="formula_3">L cls = ? 1 M N i=1 log? ?(i) + ? 1 M i=N +1 log 1 ?? ?(i) ,<label>(4)</label></formula><formula xml:id="formula_4">L loc = 1 N N i=1 p i ?p ?(i) 2 2 ,<label>(5)</label></formula><formula xml:id="formula_5">L = L cls + ? 2 L loc ,<label>(6)</label></formula><p>where ||?|| l2 denotes to the Euclidean distance, ? 1 is a reweight factor for negative proposals, and ? 2 is a weight term to balance the effect of the regression loss.</p><p>Network Design. As illustrated in <ref type="figure">Figure 5</ref>, we use the first 13 convolutional layers in VGG-16 bn <ref type="bibr" target="#b35">[36]</ref> to extract deep features. With the outputted feature map, we upsample its spatial resolution by a factor of 2 using nearest neighbor interpolation. Then the upsampled map is merged with , ? ? , ? , Classification Head <ref type="figure">Figure 5</ref>. The overall architecture of the proposed P2PNet. Built upon the VGG16, it firstly introduce an upsampling path to obtain finegrained deep feature map. Then it exploits two branches to simultaneously predict a set of point proposals and their confidence scores.</p><p>The key step in our pipeline is to ensure an one-to-one matching between point proposals and ground truth points, which determines the learning targets of those proposals.</p><p>the feature map from a lateral connection by element-wise addition. This lateral connection is used to reduce channel dimensions of the feature map after the fourth convolutional blcok. Finally, the merged feature map undergoes a 3 ? 3 convolutional layer to get F s , and the convolution in which is used to reduce the aliasing effect due to the upsampling. The prediction head in our P2PNet is consisted of two branches, which are both fed with F s and produce point locations and confidence scores respectively. For simplicity, the architecture of the two branches are kept same, which is consisted of three stacked convolutions interleaved with ReLU activations. We have empirically found this simple structure yield competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Dataset. We exploit existing publicly available datasets in crowd counting to demonstrate the superiority of our method. Specifically, extensive experiments are conducted on four challenging datasets, including ShanghaiTech PartA and PartB <ref type="bibr" target="#b47">[48]</ref>, UCF CC 50 <ref type="bibr" target="#b11">[12]</ref>, UCF-QNRF <ref type="bibr" target="#b12">[13]</ref> and NWPU-Crowd <ref type="bibr" target="#b39">[40]</ref>. For experiments on UCF CC 50, we conduct a five-fold cross validation following <ref type="bibr" target="#b11">[12]</ref>.</p><p>Data Augmentations. We firstly adopt random scaling with its scaling factor selected from [0.7, 1.3], keeping the shorter side not less than 128. Then we randomly crop an image patches with a fixed-size of 128 ? 128 from the resized image. Finally, random flipping with a probability of 0.5 is also adopted. For the datasets containing extremely large resolution, i.e., QNRF and NWPU-Crowd, we keep the max size of image no longer than 1408 and 1920, respectively, and keep the original aspect ratio.</p><p>Hyperparameters. We use the feature map of stride s = 8 for the prediction. The number K of the reference points is set to 4 (8 for QNRF dataset). And K is set according to the dataset statistics to ensure M &gt; N . For the point regression, we set the ? to 100. The weight term ? during the matching is set as 5e-2. In the loss function, the ? 1 is set to 0.5, and ? 2 is set to 2e-4. Adam algorithm <ref type="bibr" target="#b14">[15]</ref> with a fixed learning rate 1e-4 is used to optimize the model parameters. Since the weights in the backbone network have been pre-trained on the ImageNet, thus, we use a smaller learning rate 1e-5. The training batch size is set to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Evaluation</head><p>As a comprehensive criteria, the proposed nAP metric is firstly reported to evaluate the performance of our P2PNet model. As shown in <ref type="table">Table 1</ref>, the nAP is reported using three different thresholds of ?, which corresponds to the average precision under different localization accuracies of the predicted individual points. Typically, nAP 0.5 could satisfy the requirements of most practical applications, which means that the ground truth point is exactly the nearest neighbor for most points within this region. Besides, nAP 0.1 and nAP 0.25 are reported to account for some requirements of high localization accuracy. Following recent detection methods which report the average of AP under several thresholds to provide a single number for the overall performance, we adopt a similar metric. Specifically, we calculate multiple nAP ? with the ? starting from 0.05 to 0.50, with steps of 0.05. Then an average is done to get the overall average precision nAP {0.05:0.05:0.50} . From the <ref type="table">Table 1</ref>, we observe that our P2PNet achieves a promising average precision under different levels of localization accuracy. Specifically, its overall metric nAP {0.05:0.05:0.50} is around 60% on all datassets, which should already meet the requirements of many practical applications. In terms of the primary indicator nAP 0.5 , the P2PNet generally achieves a promising precision of more than 80%. For most datasets, the P2PNet could achieve a nAP 0.5 of nearly 90%, which demonstrates the effectiveness of our approach on individual localization. Even for the stricter metric nAP 0.25 , the precision is still higher than 55%. These results are encouraging, since we did not use any techniques like coordinate refinement in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b46">47]</ref> or exploiting multiple feature levels <ref type="bibr" target="#b21">[22]</ref>, which are both orthogonal to our contributions and should bring more improvements. Besides, the P2PNet achieves a relatively lower precision on the nAP 0.05 , which is reasonable since the effects of the labeling deviations might gradually become apparent under such high localization accuracy.</p><p>Besides, we also notice that the NWPU-Crowd dataset <ref type="bibr" target="#b39">[40]</ref> provides scarce yet valuable box annotations, so we report our localization performance using their metrics to compare with other competitors. And our P2PNet achieves an F1-measure/Precision/Recall of 71.2%/72.9%/69.5%, which is the best among published methods with similar backbones. For other localization based methods with official codes available, we also report their results in nAP metric (much lower than ours) in Supplementary.</p><p>Furthermore, we also evaluate the counting accuracy of our model. The estimated crowd number of our P2PNet is obtained by counting the predicted points with confidence scores higher than 0.5. We compare the P2PNet with stateof-the-art methods on several challenging datasets with various densities. Similar to <ref type="bibr" target="#b47">[48]</ref>, we also adopt Mean Absolute Error (MAE) and Mean Squared Error (MSE) as the evaluation metrics. The results are illustrated in <ref type="table">Table 2</ref> and <ref type="table" target="#tab_3">Table 3</ref>. The top performance is indicated by bold numbers and the second best is indicated by underlined numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NWPU-Crowd MAE[O] MSE[O] MAE[L] MAE[S]</head><p>CSRNet <ref type="bibr" target="#b19">[20]</ref> 121  the much wider range of counts. As shown in <ref type="table">Table 2</ref>, our P2PNet achieves an MAE of 85.32, which is much better than the Neural Architecture Search based method AM-SNet. Compared with the previous best method ADSCNet, although the accuracy of our method is not so competitive, it is still much higher than ADSCNet on all other datasets. Besides, among all the methods in <ref type="table">Table 2</ref>, only ours could provide exact individual locations. NWPU-Crowd. The NWPU-Crowd dataset is a large-scale congested dataset recently introduced in <ref type="bibr" target="#b39">[40]</ref>. As shown in  Layout of reference points. We firstly evaluate the effect from the layouts of the reference points. As shown in <ref type="table" target="#tab_4">Table 4</ref>, we compare two layouts in the <ref type="figure" target="#fig_5">Figure 4</ref>. Generally speaking, both the two layouts achieve state-of-the-art performance with minor difference, proving that the target association matters more than the layout of reference points. The Grid layout performs slightly better due to its dense arrangement of reference points, which is beneficial for the congested regions. Effect of feature levels. We exhibit the effect of different feature levels used for prediction. For fair comparison, we keep the total reference points the same when using feature levels with different strides. As shown in <ref type="table" target="#tab_5">Table 5</ref> P2PNet consistently achieves competitive results using different feature levels, which demonstrates the effectiveness of our point based solution. In particular, the feature level with a stride of 8 provides a trade-off for the various densities, thus yields better performance. In terms of the localization accuracy, we observe an obvious trend of improvement on nAP when we increase the feature map resolution, as shown in <ref type="table" target="#tab_5">Table 5</ref>. It implies that the finest feature map is beneficial for localization, which is also in accord with the consensus on other tasks. Besides, based on our baseline method, it would be interesting to introduce existing multi-scale feature fusion techniques such as <ref type="bibr" target="#b21">[22]</ref>, which are discarded in our P2PNet for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we go beyond crowd counting and propose a purely point-based framework to directly predict locations for crowd individuals. This new framework could better satisfy the practical demands of downstream tasks in crowd analysis. In conjunction with it, we advocate to use a new metric nAP for a more comprehensive accuracy evaluation on both localization and counting. Moreover, as an intuitive solution following this framework, we propose a novel network P2PNet, which is capable of directly taking point annotations as supervision whilst predicting the point locations during inference. P2PNet's key component is the oneto-one matching during the ground truth targets association, which is beneficial to the improvement of the nAP metric. This conceptually simple framework yields state-of-the-art counting performance and promising localization accuracy.</p><p>Similar to previous works in crowding counting, we adopt Mean Absolute Error (MAE) and Mean squared error (MSE) as our evaluation metrics which are defined as:</p><formula xml:id="formula_6">MAE = 1 N N i |? i ? z i | ,<label>(7)</label></formula><formula xml:id="formula_7">MSE = 1 N N i (? i ? z i ) 2 ,<label>(8)</label></formula><p>where? i and z i represent estimated crowd number and ground-truth crowd number of the i-th image, respectively. N denotes the total number of test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Discussion on Spatial Scale Problem</head><p>Despite its superior performance, the proposed P2PNet did not explicitly deal with the scale variation problem. Actually, different from bounding boxes, the head points themselves are scale ignorant in nature. In other words, the one-one matching ensures that no matter which scale the head is, only one optimal predicted proposal will be chosen as its prediction. Thus, some implicit scale cues might be learned automatically during the training process. Besides, the proposed framework is orthogonal to some previous works dealing with scale variations, such as FPN <ref type="bibr" target="#b21">[22]</ref>, PGCNet <ref type="bibr" target="#b45">[46]</ref>, CSRNet <ref type="bibr" target="#b19">[20]</ref>, MCNN <ref type="bibr" target="#b47">[48]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hyperparameters Analysis</head><p>We set the number of reference points (K) based on the nearest neighbour distance distribution of ground truth points. Specifically, based on the observation that nearly 95% (SHTech PartA) of the head points are within the nearest neighbour distance of 4 pixels, we set the number of the reference points K as 4 on the feature map with stride 8. We experimentally analyze the accuracy sensitivity of this parameter in <ref type="table" target="#tab_6">Table 6</ref>. As shown from the results, the model with K=1 still achieves state-of-the-art accuracy, although it's reference points are too few to cover all the heads in congested areas. Setting K to a value greater than 4 leads to inferior accuracy, which might be caused by the increase of negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Localization Performance</head><p>Thanks to the scarce yet valuable box annotations provided by the NWPU-Crowd dataset <ref type="bibr" target="#b39">[40]</ref>, we could compare the localization performance of our P2PNet with other competitors using their metrics. As shown in <ref type="table">Table 7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1-Measure Precision</head><p>Recall FasterRCNN <ref type="bibr" target="#b32">[33]</ref> 0.068 0.958 0.035 TinyFaces <ref type="bibr" target="#b9">[10]</ref> 0.567 0.529 0.611 RAZ <ref type="bibr" target="#b23">[24]</ref> 0.599 0.666 0.543 Crowd-SDNet <ref type="bibr" target="#b40">[41]</ref> 0.637 0.651 0.624 PDRNet <ref type="bibr" target="#b18">[19]</ref> 0.653 0.675 0.633 TopoCount <ref type="bibr" target="#b0">[1]</ref> 0.692 0.683 0.701 D2CNet <ref type="bibr" target="#b5">[6]</ref> 0.700 0.741 0.662 Ours 0.712 0.729 0.695 <ref type="table">Table 7</ref>. Comparison for the localization performance on NWPU.</p><p>P2PNet achieves the best F1 score among the published methods with similar computation complexity. Among a few existing localization-based methods, almost none of them have official codes or third-party reimplementations except for <ref type="bibr" target="#b34">[35]</ref>. So for a fair comparison, we evaluate the nAP 0:05:0:05:0:50 of <ref type="bibr" target="#b34">[35]</ref> on SHTech PartA, SHTech PartB and QNRF, which are 33.2%, 45.8% and 8.9% respectively. As shown from the results, our P2PNet achieves significantly higher localization performance in terms of nAP, especially on the challenging QNRF dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Visual Results for Qualitative Evaluation</head><p>In <ref type="figure" target="#fig_7">Figure 7</ref>-19, we exhibit the results of several example images with different densities from sparse, medium to dense. As seen from these results, our P2PNet achieves impressive localization and counting accuracy under various crowd density conditions. Additionally, from these qualitative results, we also find that P2PNet may fail on some extreme large heads and gray images (old photos). But similar failure cases could also be found in other top methods, such as ASNet (CVPR'20) <ref type="bibr" target="#b13">[14]</ref>, AMSNet (ECCV'20) <ref type="bibr" target="#b10">[11]</ref>, SDANet (AAAI'20) <ref type="bibr" target="#b31">[32]</ref>, etc. Fortunately, these might be alleviated to some extent by adding more relevant training data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Illustration for different levels of localization accuracy in nAP (k=3). The yellow circle indicates the region within d kNN (pi) pixels from the center GT point pi. A typical value for ? is 0.5, as indicated by the blue circle, which means that the nearest GT point of most pixels within this region should be pi. The red circle represents a threshold (?=0.25) for stricter localization accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>(a) Multiple ground truth points might be matched to the same proposal when selecting the nearest proposal for each of them, which leads to under-estimated counts. (b) Multiple proposals might be matched to the same ground truth point when selecting the nearest ground truth for each of them, which leads to over-estimated counts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Two types of layout for reference points (s = 2, K = 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Some qualitative results for the predicted individuals of our P2PNet. The white numbers denote to the ground truth or prediction counts. The visualizations demonstrate the superiority of our method under various densities in terms of both localization and counting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Visual results of sparse scenes<ref type="bibr" target="#b0">(1)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>SHTech PartA SHTech PartB UCF CC 50 UCF-QNRF NWPU-Crowd</figDesc><table><row><cell>? = 0.05</cell><cell></cell><cell>10.9%</cell><cell></cell><cell>23.8%</cell><cell></cell><cell>5.0%</cell><cell></cell><cell>5.9%</cell><cell cols="2">12.9%</cell></row><row><cell>? = 0.25</cell><cell></cell><cell>70.3%</cell><cell></cell><cell>84.2%</cell><cell></cell><cell>54.5%</cell><cell></cell><cell>55.4%</cell><cell cols="2">71.3%</cell></row><row><cell>? = 0.50</cell><cell></cell><cell>90.1%</cell><cell></cell><cell>94.1%</cell><cell></cell><cell>88.1%</cell><cell></cell><cell>83.2%</cell><cell cols="2">89.1%</cell></row><row><cell cols="2">? = {0.05 : 0.05 : 0.50}</cell><cell>64.4%</cell><cell></cell><cell>76.3%</cell><cell></cell><cell>54.3%</cell><cell></cell><cell>53.1%</cell><cell cols="2">65.0%</cell></row><row><cell></cell><cell></cell><cell cols="6">Table 1. The overall performance of our P2PNet.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Venue</cell><cell cols="4">SHTech PartA SHTech PartB MAE MSE MAE MSE</cell><cell cols="2">UCF CC 50 MAE MSE</cell><cell cols="2">UCF-QNRF MAE MSE</cell></row><row><cell>CAN [28]</cell><cell cols="2">CVPR'19</cell><cell>62.3</cell><cell>100.0</cell><cell>7.8</cell><cell>12.2</cell><cell>212.2</cell><cell>243.7</cell><cell>107.0</cell><cell>183.0</cell></row><row><cell>Bayesian+ [31]</cell><cell cols="2">ICCV'19</cell><cell>62.8</cell><cell>101.8</cell><cell>7.7</cell><cell>12.7</cell><cell>229.3</cell><cell>308.2</cell><cell>88.7</cell><cell>154.8</cell></row><row><cell>S-DCNet [45]</cell><cell cols="2">ICCV'19</cell><cell>58.3</cell><cell>95.0</cell><cell>6.7</cell><cell>10.7</cell><cell>204.2</cell><cell>301.3</cell><cell>104.4</cell><cell>176.1</cell></row><row><cell>SANet + SPANet [7]</cell><cell cols="2">ICCV'19</cell><cell>59.4</cell><cell>92.5</cell><cell>6.5</cell><cell>9.9</cell><cell>232.6</cell><cell>311.7</cell><cell>-</cell><cell>-</cell></row><row><cell>SDANet [32]</cell><cell cols="2">AAAI'20</cell><cell>63.6</cell><cell>101.8</cell><cell>7.8</cell><cell>10.2</cell><cell>227.6</cell><cell>316.4</cell><cell>-</cell><cell>-</cell></row><row><cell>ADSCNet [2]</cell><cell cols="2">CVPR'20</cell><cell>55.4</cell><cell>97.7</cell><cell>6.4</cell><cell>11.3</cell><cell>198.4</cell><cell>267.3</cell><cell>71.3</cell><cell>132.5</cell></row><row><cell>ASNet [14]</cell><cell cols="2">CVPR'20</cell><cell cols="2">57.78 90.13</cell><cell>-</cell><cell>-</cell><cell cols="4">174.84 251.63 91.59 159.71</cell></row><row><cell>AMRNet [29]</cell><cell cols="2">ECCV'20</cell><cell cols="2">61.59 98.36</cell><cell>7.02</cell><cell>11.00</cell><cell>184.0</cell><cell>265.8</cell><cell>86.6</cell><cell>152.2</cell></row><row><cell>AMSNet [11]</cell><cell cols="2">ECCV'20</cell><cell>56.7</cell><cell>93.4</cell><cell>6.7</cell><cell>10.2</cell><cell>208.4</cell><cell>297.3</cell><cell>101.8</cell><cell>163.2</cell></row><row><cell>DM-Count [39]</cell><cell cols="2">NeurIPS'20</cell><cell>59.7</cell><cell>95.7</cell><cell>7.4</cell><cell>11.8</cell><cell>211.0</cell><cell>291.5</cell><cell>85.6</cell><cell>148.3</cell></row><row><cell>Ours</cell><cell>-</cell><cell></cell><cell cols="2">52.74 85.06</cell><cell>6.25</cell><cell>9.9</cell><cell cols="3">172.72 256.18 85.32</cell><cell>154.5</cell></row></table><note>nAP?Table 2. Comparison of the counting accuracy with state-of-the-art methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">, our P2PNet achieves the best overall MAE, with a</cell></row><row><cell cols="2">reduction of 12.4% compared with the second best method</cell></row><row><cell cols="2">DM-Count. Since our predictions are only based on a single</cell></row><row><cell cols="2">scale feature map for simplicity, the result is slightly lower</cell></row><row><cell cols="2">than those best performance on MAE[S]. MAE[S] is the</cell></row><row><cell cols="2">average MAE of different scale levels, please refer to [40].</cell></row><row><cell cols="2">4.3. Ablation Studies</cell></row><row><cell cols="2">Layout MAE MSE nAP ?</cell></row><row><cell>Center</cell><cell>53.7 89.61 61.7</cell></row><row><cell>Grid</cell><cell>52.74 85.06 64.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The effect of the layout for reference points. For an overall comparison, we use ? = {0.05 : 0.05 : 0.50}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The ablation study on SHTech PartA. For an overall comparison, we use ? = {0.05 : 0.05 : 0.50}.</figDesc><table><row><cell>, the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>The performance change w.r.t. the number K for reference points. For an overall comparison, we use ? = {0.05 : 0.05 : 0.50}.</figDesc><table><row><cell>, our</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary 1. Counting Evaluation Metrics</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Localization in the crowd with topological constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahira</forename><surname>Abousamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive dilated network with self-correction supervision for counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting abnormal crowd behaviors based on the div-curl characteristics of flow fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Han</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decoupled two-stage crowd counting and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning spatial awareness to improve crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Qi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crowd-11: A dataset for fine grained crowd behaviour analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Luvison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extremely overlapping vehicle counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Guerrero-G?mez-Olmedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Torre-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>L?pez-Sastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saturnino</forename><surname>Maldonado-Basc?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Onoro-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nas-count: Counting-by-density with neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhmmad</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishan</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somaya</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention scaling for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Where are the blobs: Counting by localization with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pdr-net: Perception-inspired single image dehazing network with refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Density map regression guided detection network for rgb-d crowd counting and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent attentive zooming for joint crowd counting and precise localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Counting objects by blockwise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weighing counts: Sequential crowd counting by reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Geometric and physical constraints for drone-based head plane crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Contextaware crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive mixture regression network with local counting map for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenrui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Point in, box out: Beyond counting persons in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian loss for crowd count estimation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shallow feature based dense attention network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriluka</forename><surname>Mykhaylo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Amogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skand</forename><surname>Vishwanath Peri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukuntha</forename><surname>Narayanan Sundararaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Padnet: Pan-density crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Distribution matching for crowd counting. Advances in Neural Information Processing Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huidong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Hoai</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nwpucrowd: A large-scale benchmark for crowd counting and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A self-training approach for point-supervised object detection and counting in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lap-Pui</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence-to-segments networks for detecting segments in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Boyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoai</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jianming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Xiaohui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mech</forename><surname>Radomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaras</forename><surname>Dimitris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A2j: Anchor-tojoint regression network for 3d articulated pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boshen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taidong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">From open set to closed set: Counting objects by spatial divide-and-conquer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Perspective-guided convolution networks for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuo</forename><surname>Wangmeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yezhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Shilei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Errui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Crowd tracking by group structure evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
