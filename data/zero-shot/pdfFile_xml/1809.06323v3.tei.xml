<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic segmentation</term>
					<term>real-time</term>
					<term>fast network design</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-time semantic segmentation plays an important role in practical applications such as self-driving and robots. Most semantic segmentation research focuses on improving estimation accuracy with little consideration on efficiency. Several previous studies that emphasize high-speed inference often fail to produce high-accuracy segmentation results. In this paper, we propose a novel convolutional network named Efficient Dense modules with Asymmetric convolution (EDANet), which employs an asymmetric convolution structure and incorporates dilated convolution and dense connectivity to achieve high efficiency at low computational cost and model size. EDANet is 2.7 times faster than the existing fast segmentation network, ICNet, while it achieves a similar mIoU score without any additional context module, post-processing scheme, and pretrained model. We evaluate EDANet on Cityscapes and CamVid datasets, and compare it with the other state-of-art systems. Our network can run with the high-resolution inputs at the speed of 108 FPS on one GTX 1080Ti.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semantic segmentation is an essential area in computer vision. It performs pixel-level label prediction for images. In recent years, the development of deep convolutional neural networks (CNNs) has made notable progress in providing accurate segmentation results <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">34]</ref>. The achievements of these networks mainly rely on their complicated model designs, which consist of considerable depth and width, and need a huge number of parameters and long inference time. However, recent interests in many real-world applications, such as autonomous driving, augmented reality, robotic interaction, and intelligent surveillance, have generated a high demand for the scene understanding systems that are able to operate in real-time. Thus, it is paramount to develop effective convolutional networks for real-time semantic segmentation.</p><p>The challenge of designing neural networks by taking both efficiency and reliability into consideration can be seen in <ref type="figure">Figure 1</ref>. For example, most of the top performing methods, such as PSPNet <ref type="bibr" target="#b33">[33]</ref> and SegModel <ref type="bibr" target="#b24">[25]</ref>, focus on improving accuracy at the expense of large increases in computational cost. Therefore, in <ref type="figure">Figure 1</ref>, these methods are located near the area of high accuracy Project page: https://github.com/shaoyuanlo/EDANet (mean of intersection-over-union, mIoU) and low inference speed (frames per second, FPS). On the other hand, some approaches, such as ENet <ref type="bibr" target="#b20">[21]</ref> and ESPNet <ref type="bibr" target="#b18">[19]</ref>, emphasize on speed, but their accuracy drops notably. They are located at the bottom right.</p><p>In this paper, we propose a new network architecture, Efficient Dense modules with Asymmetric convolution (EDANet), which simultaneously accomplishes high efficiency and accuracy. Our method is not only among the few systems who exceeds 30 FPS (real-time), but also located at the upper right of <ref type="figure">Figure 1</ref>.</p><p>One important feature of EDANet is asymmetric convolution. It decomposes a standard 2D convolution into two 1D convolutions. That is, an original n? n convolution kernel is factorized into two convolution kernels, n? 1 and 1? n, respectively. This technique dramatically reduces the number of parameters with little performance degradation. We take the essence of the densely connected structure <ref type="bibr" target="#b12">[13]</ref>, and modify it for real-time semantic segmentation. Although DenseNet was initially created for image classification challenges, our experiments show that its capability of gathering the features extracted from different layers and aggregating multi-scale information is innately beneficial to the segmentation task. This structure can also reduce the number of parameters. Dilated convolution is employed by EDANet. The idea is enlarging the receptive fields of networks to retain feature map resolution and avoid losing spatial information. For a good balance in efficiency and reliability, we do not add any extra decoder structure, context module, and post-processing scheme into our system. We further build several EDANet variants to evaluate the performance of different network design choices. In short, EDANet is able to achieve remarkable inference speed and retain high accuracy at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Originally, CNNs were created for the image classification task <ref type="bibr" target="#b16">[17]</ref>. FCN <ref type="bibr" target="#b17">[18]</ref> is a pioneering CNN in semantic segmentation. It adapts VGG16 <ref type="bibr" target="#b27">[27]</ref> by replacing fully-connected layers with convolution layers to do pixel-level label prediction. Thereafter, semantic segmentation entered the era of CNN-based methods.</p><p>High accuracy networks. U-Net <ref type="bibr" target="#b23">[24]</ref> develops an encoderdecoder architecture to collect spatial information from the shallower layers to enhance the features in the deeper layers. DeconvNet <ref type="bibr" target="#b19">[20]</ref> proposes a decoder that is symmetric to its encoder to upsample the outputs of the encoder. These networks have a huge computational cost owing to their heavy decoders. Dilation10 <ref type="bibr" target="#b32">[32]</ref> creates a context module by stacking dilated convolution layers with increasing dilation rates for aggregating multi-scale contextual information. DeepLab <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> introduces an atrous spatial pyramid pooling (ASPP) module, which employs multiple parallel filters with different dilation rates to exploit multi-scale representations. Both modules require enormous computation and inference time. As a result, although the aforementioned networks are accurate, they are infeasible for practical applications.</p><p>High inference speed networks. ENet <ref type="bibr" target="#b20">[21]</ref> is one of the first networks aiming at semantic segmentation in real-time. It adapts the ResNet structure <ref type="bibr" target="#b10">[11]</ref> but trims the number of convolution filters to reduce computation. ESPNet <ref type="bibr" target="#b18">[19]</ref> designs an efficient spatial pyramid (ESP) module, which uses point-wise convolution in front of the spatial pyramids to reduce computational cost. These two networks improve efficiency greatly but significantly sacrifice accuracy. Recent studies, such as ICNet <ref type="bibr" target="#b34">[34]</ref> and BiSeNet <ref type="bibr" target="#b31">[31]</ref>, make a better balance between speed and performance, but there is still room for further improvement.</p><p>Densely connected networks. DenseNet <ref type="bibr" target="#b12">[13]</ref> achieves excellent performance on image classification. Some studies have extended DenseNet to semantic segmentation networks. FC-DenseNet <ref type="bibr" target="#b14">[15]</ref> uses DenseNet as an encoder and adds a decoder structure based on the conventional skip connections <ref type="bibr" target="#b23">[24]</ref> to build fully convolutional DenseNet. SDN <ref type="bibr" target="#b9">[10]</ref> takes DenseNet as their backbone model and combines it with the stacked deconvolutional architecture. These methods simply adopt DenseNet without extensive optimization. Their added complexity further increases the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>The architecture of the proposed EDANet is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. It consists of three downsampling blocks, two EDA blocks, and a projection layer. The first and the second EDA block are composed of 5 and 8 densely connected EDA modules respectively. EDANet does not include any additional decoder, context module and postprocessing scheme.</p><p>In this section, we first describe the core EDA module, then elaborate on the other important network design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EDA Module</head><p>The EDA module is the core of the entire EDANet. Its structure is based on the dense module of asymmetric convolution, as shown in <ref type="figure" target="#fig_1">Figure 3a</ref>. It consists of a point-wise convolution layer and two pairs of asymmetric convolution layers. The output of each EDA module is the concatenation of its input and the newly produced features. Below we discuss each component in the EDA module.</p><p>Point-wise convolution layer. The point-wise convolution layer is a 1?1 convolution at the beginning of each EDA module, which is used to reduce the number of input channels <ref type="bibr" target="#b10">[11]</ref>. This design can dramatically decrease the number of parameters and computations.</p><p>Asymmetric convolution. Asymmetric convolution is to factorize a standard two-dimensional convolution kernel into two onedimension convolution kernels. In other words, an n? 1 convolution followed by a 1? n convolution can substitute for an n? n convolution <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">29]</ref>. This mechanism can be expressed as:</p><formula xml:id="formula_0">? ? ( , ) ( ? , ? ) =? =? = ? ( ) [ ? ( ) ( ? , ? ) =? ] =?<label>(1)</label></formula><p>where I is a 2D image, W is a 2D kernel, Wx is a 1D kernel along xdimension, and Wy is a 1D kernel along y-dimension. When the kernel size is 3, the number of parameters and computational cost are saved significantly by 33%, and the performance degradation is often very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dilated convolution.</head><p>Dilated convolution is a particular type of convolution, which inserts zeros between two consecutive kernel values along each dimension <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">33]</ref>. This type of convolution can enlarge the effective receptive field of kernels without increasing the number of parameters. For instance, the effective size of an n? n convolution kernel with dilation rate r is [r(n-1)+1]? [r(n-1)+1].</p><p>In order to aggregate more contextual information for accuracy improvement, we employ dilated convolution at the second asymmetric convolution pair in an EDA module to form the dilated EDA modules, and thus is called "dilated asymmetric convolution". The last two EDA modules in EDA block 1 and all the eight EDA modules in EDA block 2 are the dilated EDA modules. The dilation rates in the system are 2, 2, 2, 2, 4, 4, 8, 8, 16, and 16, respectively. We choose this sequential placement for enlarging the receptive field in a gradual manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense connectivity. Dense connectivity was proposed by</head><p>DenseNet <ref type="bibr" target="#b12">[13]</ref>. We adopt this strategy in EDANet but modify it from layer-level to module-level connections. That is, each EDA module concatenates its input and the new learned features together to form the final output.</p><p>This densely connected structure can substantially increase processing efficiency because each module is only responsible for acquiring a few new features. Furthermore, the deeper layers have larger receptive fields <ref type="bibr" target="#b27">[27]</ref>. For example, a stack of two 3?3 convolution layers has the same effective receptive field as a single 5?5 convolution layer, and three such layers have an effective receptive field of 7?7. Thus, the dense connectivity, which concatenate the features learned from each module that has a different receptive field individually, allows our network to naturally gather multi-scale information together. This enables our system to achieve good performance at low computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Design Choices</head><p>In this subsection, we discuss the other crucial design choices on the downsampling, decoder, and composite function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downsampling.</head><p>We adopt the ENet <ref type="bibr" target="#b20">[21]</ref> initial block as our downsampling block. The structure is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. ENet uses the initial block to do the first downsampling, but we apply it to all the downsampling layers, and further extend it to two modes. When the number of output channels Wout of a block is less than the number of input channels Win, this block is simply a single 3?3 convolution layer with stride 2, where Wconv = Wout. Our third downsampling block, which divides the network into two EDA blocks, adopts this mode (see <ref type="figure" target="#fig_0">Figure 2</ref>). If Wout &gt; Win, a 2?2 maxpooling layer with stride 2 is included, then the concatenation of the features from the convolution and the max-pooling branches forms the final output. In this mode, Wconv = Wout -Win. The first two downsampling blocks adopt this mode (see <ref type="figure" target="#fig_0">Figure 2</ref>). This twobranch design saves the computation of convolution layers.</p><p>The downsampled feature maps enable networks to have larger receptive fields to collect more contextual information. However, reducing feature map resolution would lose spatial details, which is especially harmful to the pixel-wise segmentation. To address this problem, we find a balanced structure, which contains only three downsampling operations in our network. The ratio of the feature size at the end of EDANet to the full-resolution input images is 1/8. Compared to other networks like SegNet <ref type="bibr" target="#b0">[1]</ref>, whose ratio of feature map size to inputs is 1/32, EDANet remain more spatial details. We use dilated convolution to compensate for the receptive field.</p><p>Decoding. Many systems use a decoder to upsample feature maps at the expense of huge computation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>. Even choosing a relatively small decoder still increases the computational cost <ref type="bibr" target="#b22">[23]</ref>. Since EDANet aims at fast semantic segmentation, we discard the decoder structure. After EDA block 2, we add a 1?1 convolution layer as a projection layer to output C (the number of classes) feature maps, then use bilinear interpolation to upsample feature maps by a factor of 8 to the size of input images (see <ref type="figure" target="#fig_0">Figure 2</ref>). This strategy reduces accuracy slightly but saves many computations.</p><p>Composite function. In order to accelerate the actual inference speed, we choose the traditional post-activation composite function   instead of pre-activation <ref type="bibr" target="#b11">[12]</ref>. Specifically, the sequence of three operations is a convolution, followed by batch normalization <ref type="bibr" target="#b13">[14]</ref> and ReLU. The advantage is that each batch normalization layer can be merged with its preceding convolution layer during inference, which decreases the inference time. In the training phase, we place a dropout layer <ref type="bibr" target="#b28">[28]</ref> with dropout rate 0.02 in each module as a regularization measure (see <ref type="figure" target="#fig_1">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate our method on two challenging datasets, Cityscapes <ref type="bibr" target="#b6">[7]</ref> and CamVid <ref type="bibr" target="#b1">[2]</ref>. In this section, we first describe these datasets and our training setup. Then, we conduct a series of experiments to examine the proposed network. Finally, we report the comparisons with the other state-of-art systems.</p><p>Datasets. The Cityscapes dataset is an urban street scene dataset that contains 19 object classes. It consists 5000 fine-annotated images at the high-resolution of 1024?2048, which are split into three sets: 2975 images for training, 500 images for validation, and 1525 images for testing. There is another set of 19,998 images with coarse annotation, but we only use the fine annotation set for all experiments. Our network is trained and tested on the downsampled 512?1024 inputs. For evaluation, the output features are upsampled by bilinear interpolation to the original dataset resolution.</p><p>The CamVid dataset is another dataset for vehicle applications, which consists of 367 training and 233 testing images. It includes 11 classes and has resolution of 360?480.</p><p>Training. We train our networks by using the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with weight decay 1e -4 and batch size 10. We employ the poly learning rate policy, where the learning rate is multiplied by ( ? / _ ) with power 0.9 and initial learning rate 5e -4 . Inspired by ENet <ref type="bibr" target="#b20">[21]</ref>, we use the class weighting scheme defined by = ( + ) ?</p><p>, where we set k to 1.12. We include data augmentation in training by using random horizontal flip and the translation of 0~2 pixels on both axes. All the reported accuracy results are measured in the mIoU metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study</head><p>In this subsection, we perform a series of experiments to validate the potential of our network. All the following experiments are evaluated on the Cityscapes dataset.</p><p>Core module. The asymmetric convolution structure and the dense connection concept <ref type="bibr" target="#b12">[13]</ref> are two key elements in the proposed EDA module. In order to further investigate potential improvements, we design two variants of our module for comparisons.</p><p>The first one is a "non-asymmetric" variant, which replaces the two pairs of asymmetric convolution by two standard 3?3 convolution layers (see <ref type="figure" target="#fig_1">Figure 3b</ref>). The other one is a "non-dense" variant, which employs the conventional residual connection <ref type="bibr" target="#b10">[11]</ref> instead of the dense connection, and it removes the point-wise convolution layer (see <ref type="figure" target="#fig_1">Figure 3c</ref>). This variant is the same as the ERF module <ref type="bibr" target="#b22">[23]</ref>. In order to make comparison at the same computational cost, we set its width W (the number of feature maps) to 40 in block 1 and 80 in block 2 (64 and 128 in ERFNet respectively). We use the same layer placement as EDANet to build two networks composed of the two module variants respectively, and they are called EDA-non-asym and EDA-non-dense. As shown in <ref type="table" target="#tab_2">Table 1a</ref>, EDA-non-asym obtains almost the same accuracy as EDANet, but has 27% more computational cost. This indicates the advantage of our asymmetric convolution design. On the other hand, EDA-non-dense performs 1.18% lower accuracy than EDANet. Apparently, the densely connectivity is effective.</p><p>Extra context module. Dense connectivity allows EDANet to concatenate multi-scale features and go deeper simultaneously. We compare the ability of our EDA block and the atrous spatial pyramid pooling (ASPP) context module proposed in DeepLab <ref type="bibr" target="#b4">[5]</ref> to extract multi-scale representations. We construct EDA-shallow, which contains only four EDA modules in its EDA block 2 as a baseline. Then, we replace the last four EDA modules in EDANet with the ASPP as EDA-ASPP (see <ref type="figure" target="#fig_3">Figure 5</ref>). <ref type="table" target="#tab_2">Table 1b</ref> shows the results. EDANet attains 7.01% higher accuracy than EDA-shallow, while EDA-ASPP only improves 2.55%. Moreover, EDA-ASPP has 5 times more parameters and 4.6    times more computational cost than EDANet owing to its 5-branch structure. Therefore, we observe that a block of only four connected EDA modules is able to outperform a heavy ASPP context module because of its deeper structure and the excellent capability of aggregating multi-scale information.</p><p>Decoder. After going through the trade-off analysis between efficiency and accuracy, we do not include the decoder structure in our network design. In our investigation, we build a network called EDA-ERFdec, which adds an ERFNet decoder <ref type="bibr" target="#b22">[23]</ref>, for comparison. This decoder consists of two blocks of a deconvolution layer with stride 2 followed by two ERF modules (see <ref type="figure" target="#fig_1">Figure 3c</ref>), plus the last deconvolution layer with stride 2 for final output.</p><p>As <ref type="table" target="#tab_2">Table 1c</ref> shows, EDA-ERFdec obtains 0.46% better accuracy at the expense of 44% more computational cost. Obviously, when we focus on efficiency, adding the decoder does not seem benefit. Downsampling block. We choose the initial block of ENet <ref type="bibr" target="#b20">[21]</ref> as the foundation of our downsampling block. Then, we extend it to the two-mode configuration as described earlier. On the other hand, DenseNet <ref type="bibr" target="#b12">[13]</ref> uses a 7?7 convolution layer with stride 2 followed by a 3?3 max-pooling with stride 2 for early downsampling, and creates the transition layers that consist of a 1?1 convolution layer followed by a 2?2 average-pooling with stride 2 for the other downsampling operations. For comparing the downsampling approach of ours and the one proposed by DenseNet, we construct EDA-DenseDown by replacing our first two downsampling blocks and the third downsampling block with the early downsampling layers and the transition layer in DenseNet, respectively.</p><p>As shown in <ref type="table" target="#tab_2">Table 1d</ref>, EDANet attains significantly 3.47% higher accuracy than EDA-DenseDown with only a little more computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Results</head><p>We finally train our EDANet in two stages. In the first stage, we train it by the annotations downsampled to 1/8 to the input image <ref type="table">Table 2</ref>: Evaluation results on the Cityscapes test set. The methods whose speed is faster than 15 FPS are included. " ?": GTX 1080Ti, with 3,584 CUDA cores and 11,340 GFLOPS. " ? ?": Titan XP, with 3,854 CUDA cores and 12,150 GFLOPS. " ? ? ?": Titan X Pascal, with 3,584 CUDA cores and 10,974 GFLOPS. size. In the second stage, we train it again by the annotations of the same size as the inputs. In the evaluations, we do not adopt any testing tricks such as multi-crop and multi-scale testing. <ref type="table">Table 2</ref> reports our results and the comparisons with the other state-of-art networks in terms of mIoU and inference efficiency on the Cityscapes test set. EDANet achieves 67.3% mIoU, which is better than most of the existing methods that can run at 30 FPS or higher, such as ENet <ref type="bibr" target="#b20">[21]</ref> and ESPNet <ref type="bibr" target="#b18">[19]</ref>, and even outperforms many approaches with lower speed such as Dilation10 <ref type="bibr" target="#b32">[32]</ref> and FCN <ref type="bibr" target="#b17">[18]</ref>. EDANet attains 108.7 FPS and 81.3 FPS on a single GTX 1080Ti and Titan X, respectively. It is one of the fastest networks now.</p><p>We also evaluate our network on the CamVid dataset <ref type="bibr" target="#b1">[2]</ref>. As reported in <ref type="table" target="#tab_4">Table 3</ref>, EDANet achieves outstanding performance again in efficiency and accuracy. It is able to process a 360?480 CamVid image at the speed of 163 FPS on one GTX 1080Ti card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have proposed a real-time semantic segmentation network, EDANet, based on the efficient dense modules with asymmetric convolution. The experimental results demonstrate its capability of producing pretty accurate segmentation results with a rather small computational cost comparing to the other state-of-art systems. Going through an extensive investigation, we finally design a well-balanced network architecture for semantic segmentation, which leads to a good trade-off between reliability and efficiency for scene understanding applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. NETWORK DETAILS</head><p>In this appendix, we provide detailed descriptions for the network architectures of the proposed EDANet and all of the variants mentioned in the ablation study section. <ref type="table" target="#tab_5">Tables 4, 5</ref> <ref type="bibr">, 6, 7, 8, 9,</ref> and 10 correspond to EDANet, EDA-non-asym, EDA-non-dense, EDA-shallow, EDA-ASPP, EDA-ERFdec, and EDA-DenseDown, respectively. In all the following tables, the input sizes are 512?1024. The structures of EDA module, EDA-non-asymmetric module, EDA-non-dense module, downsampling block, and ASPP are shown in <ref type="figure" target="#fig_1">Figures 3a, 3b, 3c, 4</ref>, and 5, respectively.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RESULTS ON THE CITYSCAPES AND THE CAMVID DATASETS</head><p>In this section, we provide additional segmentation results of the proposed EDANet on Cityscapes <ref type="bibr" target="#b6">[7]</ref> and CamVid dataset <ref type="bibr" target="#b1">[2]</ref>. First, <ref type="table" target="#tab_2">Tables 11 and 12</ref> list the IoU scores for each class in the two datasets respectively. Then, more visual results are shown in <ref type="figure" target="#fig_5">Figures 6 and 7</ref>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The proposed EDANet architecture. The numbers of input and output channels of each block are marked in parentheses. The numbers in brackets are output feature size ratios to the full-resolution input images. "C": the number of object classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) The proposed EDA module structure. (b) "non-asymmetric" module variant. (c) "non-dense" module variant. The numbers of input and output channels of each layer are marked in parentheses. "(D)": possible dilated convolution. "BN": batch normalization. "k": growth rate, we set it to 40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Downsampling block structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Part of EDANet-ASPP structure. Image pooling is a global average pooling followed by a 1?1 convolution and bilinear interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Method</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Sample visual results of EDANet on Cityscapes validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Sample visual results of EDANet on CamVid test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Figure 1: Inference speed and mIoU accuracy on Cityscapes test set [7]. The speeds are measured on a Titan X. Networks [1,3,4,18,19,21,22,23,25,26,30,32,33,34] are included. Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic Segmentation</head><label></label><figDesc></figDesc><table /><note>Shao-Yuan Lo 1 Hsueh-Ming Hang 1 Sheng-Wei Chan 2 Jing-Jhih Lin 21 National Chiao Tung University 2 Industrial Technology Research Institute sylo95.eecs02@g2.nctu.edu.tw, hmhang@nctu.edu.tw, {ShengWeiChan, jeromelin}@itri.org.tw</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 : Ablation study results.</head><label>1</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 : Evaluation results on the CamVid test set.</head><label>3</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 : Layer disposal of the proposed EDANet.</head><label>4</label><figDesc></figDesc><table><row><cell>Name</cell><cell>Mode</cell><cell>Growth rate</cell><cell># Output channels</cell><cell>Output size</cell></row><row><cell>Downsampleing block 1</cell><cell>Win &lt; Wout</cell><cell></cell><cell>15</cell><cell>256?512</cell></row><row><cell>Downsampleing block 2</cell><cell>Win &lt; Wout</cell><cell></cell><cell>60</cell><cell>128?256</cell></row><row><cell>EDA module 1-1</cell><cell></cell><cell>40</cell><cell>100</cell><cell>128?256</cell></row><row><cell>EDA module 1-2</cell><cell></cell><cell>40</cell><cell>140</cell><cell>128?256</cell></row><row><cell>EDA module 1-3</cell><cell></cell><cell>40</cell><cell>180</cell><cell>128?256</cell></row><row><cell>EDA module 1-4</cell><cell>dilation 2</cell><cell>40</cell><cell>220</cell><cell>128?256</cell></row><row><cell>EDA module 1-5</cell><cell>dilation 2</cell><cell>40</cell><cell>260</cell><cell>128?256</cell></row><row><cell>Downsampleing block 3</cell><cell>Win &gt; Wout</cell><cell></cell><cell>130</cell><cell>64?128</cell></row><row><cell>EDA module 2-1</cell><cell>dilation 2</cell><cell>40</cell><cell>170</cell><cell>64?128</cell></row><row><cell>EDA module 2-2</cell><cell>dilation 2</cell><cell>40</cell><cell>210</cell><cell>64?128</cell></row><row><cell>EDA module 2-3</cell><cell>dilation 4</cell><cell>40</cell><cell>250</cell><cell>64?128</cell></row><row><cell>EDA module 2-4</cell><cell>dilation 4</cell><cell>40</cell><cell>290</cell><cell>64?128</cell></row><row><cell>EDA module 2-5</cell><cell>dilation 8</cell><cell>40</cell><cell>330</cell><cell>64?128</cell></row><row><cell>EDA module 2-6</cell><cell>dilation 8</cell><cell>40</cell><cell>370</cell><cell>64?128</cell></row><row><cell>EDA module 2-7</cell><cell>dilation 16</cell><cell>40</cell><cell>410</cell><cell>64?128</cell></row><row><cell>EDA module 2-8</cell><cell>dilation 16</cell><cell>40</cell><cell>450</cell><cell>64?128</cell></row><row><cell>Projection layer</cell><cell>1?1 conv.</cell><cell></cell><cell># Classes</cell><cell>64?128</cell></row><row><cell>Bilinear interpolation</cell><cell>?8</cell><cell></cell><cell># Classes</cell><cell>512?1024</cell></row><row><cell>Bilinear interpolation (inference only)</cell><cell>?2</cell><cell></cell><cell># Classes</cell><cell>1024?2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 : Layer disposal of EDA-non-asym.</head><label>5</label><figDesc></figDesc><table><row><cell>Name</cell><cell>Mode</cell><cell>Growth rate</cell><cell># Output channels</cell><cell>Output size</cell></row><row><cell>Downsampleing block 1</cell><cell>Win &lt; Wout</cell><cell></cell><cell>15</cell><cell>256?512</cell></row><row><cell>Downsampleing block 2</cell><cell>Win &lt; Wout</cell><cell></cell><cell>60</cell><cell>128?256</cell></row><row><cell>EDA-non-asymmetric module 1-1</cell><cell></cell><cell>40</cell><cell>100</cell><cell>128?256</cell></row><row><cell>EDA-non-asymmetric module 1-2</cell><cell></cell><cell>40</cell><cell>140</cell><cell>128?256</cell></row><row><cell>EDA-non-asymmetric module 1-3</cell><cell></cell><cell>40</cell><cell>180</cell><cell>128?256</cell></row><row><cell>EDA-non-asymmetric module 1-4</cell><cell>dilation 2</cell><cell>40</cell><cell>220</cell><cell>128?256</cell></row><row><cell>EDA-non-asymmetric module 1-5</cell><cell>dilation 2</cell><cell>40</cell><cell>260</cell><cell>128?256</cell></row><row><cell>Downsampleing block 3</cell><cell>Win &gt; Wout</cell><cell></cell><cell>130</cell><cell>64?128</cell></row><row><cell>EDA-non-asymmetric module 2-1</cell><cell>dilation 2</cell><cell>40</cell><cell>170</cell><cell>64?128</cell></row><row><cell>EDA-non-asymmetric module 2-2</cell><cell>dilation 2</cell><cell>40</cell><cell>210</cell><cell>64?128</cell></row><row><cell>EDA-non-asymmetric module 2-3</cell><cell>dilation 4</cell><cell>40</cell><cell>250</cell><cell>64?128</cell></row><row><cell>EDA-non-asymmetric module 2-4</cell><cell>dilation 4</cell><cell>40</cell><cell>290</cell><cell>64?128</cell></row><row><cell>EDA-non-asymmetric module 2-5</cell><cell>dilation 8</cell><cell>40</cell><cell>330</cell><cell>64?128</cell></row><row><cell>EDA-non-asymmetric module 2-6</cell><cell>dilation 8</cell><cell>40</cell><cell>370</cell><cell>64?128</cell></row><row><cell>EDA-non-asymmetric module 2-7</cell><cell>dilation 16</cell><cell>40</cell><cell>410</cell><cell>64?128</cell></row><row><cell>EDA-non-asymmetric module 2-8</cell><cell>dilation 16</cell><cell>40</cell><cell>450</cell><cell>64?128</cell></row><row><cell>Projection layer</cell><cell>1?1 conv.</cell><cell></cell><cell># Classes</cell><cell>64?128</cell></row><row><cell>Bilinear interpolation</cell><cell>?8</cell><cell></cell><cell># Classes</cell><cell>512?1024</cell></row><row><cell>Bilinear interpolation (inference only)</cell><cell>?2</cell><cell></cell><cell># Classes</cell><cell>1024?2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Name</cell><cell>Mode</cell><cell>Growth rate</cell><cell># Output channels</cell><cell>Output size</cell></row><row><cell>Downsampleing block 1</cell><cell>Win &lt; Wout</cell><cell></cell><cell>15</cell><cell>256?512</cell></row><row><cell>Downsampleing block 2</cell><cell>Win &lt; Wout</cell><cell></cell><cell>40</cell><cell>128?256</cell></row><row><cell>EDA-non-dense module 1-1</cell><cell></cell><cell></cell><cell>40</cell><cell>128?256</cell></row><row><cell>EDA-non-dense module 1-2</cell><cell></cell><cell></cell><cell>40</cell><cell>128?256</cell></row><row><cell>EDA-non-dense module 1-3</cell><cell></cell><cell></cell><cell>40</cell><cell>128?256</cell></row><row><cell>EDA-non-dense module 1-4</cell><cell></cell><cell></cell><cell>40</cell><cell>128?256</cell></row><row><cell>EDA-non-dense module 1-5</cell><cell></cell><cell></cell><cell>40</cell><cell>128?256</cell></row><row><cell>Downsampleing block 3</cell><cell>Win &lt; Wout</cell><cell></cell><cell>80</cell><cell>64?128</cell></row><row><cell>EDA-non-dense module 2-1</cell><cell>dilation 2</cell><cell></cell><cell>80</cell><cell>64?128</cell></row><row><cell>EDA-non-dense module 2-2</cell><cell>dilation 4</cell><cell></cell><cell>80</cell><cell>64?128</cell></row><row><cell>EDA-non-dense module 2-3</cell><cell>dilation 8</cell><cell></cell><cell>80</cell><cell>64?128</cell></row><row><cell>EDA-non-dense module 2-4</cell><cell>dilation 16</cell><cell></cell><cell>80</cell><cell>64?128</cell></row><row><cell>EDA-non-dense module 2-5</cell><cell>dilation 2</cell><cell></cell><cell>80</cell><cell>64?128</cell></row><row><cell>EDA-non-dense module 2-6</cell><cell>dilation 4</cell><cell></cell><cell>80</cell><cell>64?128</cell></row><row><cell>EDA-non-dense module 2-7</cell><cell>dilation 8</cell><cell></cell><cell>80</cell><cell>64?128</cell></row><row><cell>EDA-non-dense module 2-8</cell><cell>dilation 16</cell><cell></cell><cell>80</cell><cell>64?128</cell></row><row><cell>Projection layer</cell><cell>1?1 conv.</cell><cell></cell><cell># Classes</cell><cell>64?128</cell></row><row><cell>Bilinear interpolation</cell><cell>?8</cell><cell></cell><cell># Classes</cell><cell>512?1024</cell></row><row><cell>Bilinear interpolation (inference only)</cell><cell>?2</cell><cell></cell><cell># Classes</cell><cell>1024?2048</cell></row></table><note>Layer disposal of EDA-non-dense. This dilation rate placement is consistent with ERFNet [23].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 : Layer disposal of EDA-shallow.</head><label>7</label><figDesc></figDesc><table><row><cell>Name</cell><cell>Mode</cell><cell>Growth rate</cell><cell># Output channels</cell><cell>Output size</cell></row><row><cell>Downsampleing block 1</cell><cell>Win &lt; Wout</cell><cell></cell><cell>15</cell><cell>256?512</cell></row><row><cell>Downsampleing block 2</cell><cell>Win &lt; Wout</cell><cell></cell><cell>60</cell><cell>128?256</cell></row><row><cell>EDA module 1-1</cell><cell></cell><cell>40</cell><cell>100</cell><cell>128?256</cell></row><row><cell>EDA module 1-2</cell><cell></cell><cell>40</cell><cell>140</cell><cell>128?256</cell></row><row><cell>EDA module 1-3</cell><cell></cell><cell>40</cell><cell>180</cell><cell>128?256</cell></row><row><cell>EDA module 1-4</cell><cell>dilation 2</cell><cell>40</cell><cell>220</cell><cell>128?256</cell></row><row><cell>EDA module 1-5</cell><cell>dilation 2</cell><cell>40</cell><cell>260</cell><cell>128?256</cell></row><row><cell>Downsampleing block 3</cell><cell>Win &gt; Wout</cell><cell></cell><cell>130</cell><cell>64?128</cell></row><row><cell>EDA module 2-1</cell><cell>dilation 2</cell><cell>40</cell><cell>170</cell><cell>64?128</cell></row><row><cell>EDA module 2-2</cell><cell>dilation 2</cell><cell>40</cell><cell>210</cell><cell>64?128</cell></row><row><cell>EDA module 2-3</cell><cell>dilation 4</cell><cell>40</cell><cell>250</cell><cell>64?128</cell></row><row><cell>EDA module 2-4</cell><cell>dilation 4</cell><cell>40</cell><cell>290</cell><cell>64?128</cell></row><row><cell>Projection layer</cell><cell>1?1 conv.</cell><cell></cell><cell># Classes</cell><cell>64?128</cell></row><row><cell>Bilinear interpolation</cell><cell>?8</cell><cell></cell><cell># Classes</cell><cell>512?1024</cell></row><row><cell>Bilinear interpolation (inference only)</cell><cell>?2</cell><cell></cell><cell># Classes</cell><cell>1024?2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 : Layer disposal of EDA-ASPP. The ASPP structure is consistent with DeepLabv3 [5].</head><label>8</label><figDesc></figDesc><table><row><cell>Name</cell><cell></cell><cell>Mode</cell><cell cols="2">Growth rate</cell><cell cols="2"># Output channels</cell><cell>Output size</cell></row><row><cell>Downsampleing block 1</cell><cell></cell><cell>Win &lt; Wout</cell><cell></cell><cell></cell><cell></cell><cell>15</cell><cell>256?512</cell></row><row><cell>Downsampleing block 2</cell><cell></cell><cell>Win &lt; Wout</cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell>128?256</cell></row><row><cell>EDA module 1-1</cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell>100</cell><cell>128?256</cell></row><row><cell>EDA module 1-2</cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell>140</cell><cell>128?256</cell></row><row><cell>EDA module 1-3</cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell>180</cell><cell>128?256</cell></row><row><cell>EDA module 1-4</cell><cell></cell><cell>dilation 2</cell><cell>40</cell><cell></cell><cell></cell><cell>220</cell><cell>128?256</cell></row><row><cell>EDA module 1-5</cell><cell></cell><cell>dilation 2</cell><cell>40</cell><cell></cell><cell></cell><cell>260</cell><cell>128?256</cell></row><row><cell>Downsampleing block 3</cell><cell></cell><cell>Win &gt; Wout</cell><cell></cell><cell></cell><cell></cell><cell>130</cell><cell>64?128</cell></row><row><cell>EDA module 2-1</cell><cell></cell><cell>dilation 2</cell><cell>40</cell><cell></cell><cell></cell><cell>170</cell><cell>64?128</cell></row><row><cell>EDA module 2-2</cell><cell></cell><cell>dilation 2</cell><cell>40</cell><cell></cell><cell></cell><cell>210</cell><cell>64?128</cell></row><row><cell>EDA module 2-3</cell><cell></cell><cell>dilation 4</cell><cell>40</cell><cell></cell><cell></cell><cell>250</cell><cell>64?128</cell></row><row><cell>EDA module 2-4</cell><cell></cell><cell>dilation 4</cell><cell>40</cell><cell></cell><cell></cell><cell>290</cell><cell>64?128</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ASPP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Branch</cell><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell><cell>(4)</cell><cell></cell><cell>Branch</cell><cell>(5)</cell></row><row><cell>Convolution</cell><cell>1?1</cell><cell>3?3</cell><cell>3?3</cell><cell>3?3</cell><cell></cell><cell>Average-pooling</cell><cell>64?128</cell></row><row><cell>Dilation rate</cell><cell>-</cell><cell>6</cell><cell>12</cell><cell>18</cell><cell></cell><cell># Output channels</cell><cell>290</cell></row><row><cell># Output channels</cell><cell>290</cell><cell>290</cell><cell>290</cell><cell>290</cell><cell></cell><cell>Output size</cell><cell>1?1</cell></row><row><cell>Output size</cell><cell>64?128</cell><cell>64?128</cell><cell>64?128</cell><cell cols="2">64?128</cell><cell>Convolution</cell><cell>1?1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell># Output channels</cell><cell>290</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output size</cell><cell>1?1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Interpolation</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell># Output channels</cell><cell>290</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output size</cell><cell>64?128</cell></row><row><cell>Concatenation</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell></row><row><cell># Output channels</cell><cell></cell><cell></cell><cell></cell><cell>1450</cell><cell></cell><cell></cell></row><row><cell>Output size</cell><cell></cell><cell></cell><cell></cell><cell>64?128</cell><cell></cell><cell></cell></row><row><cell>Convolution</cell><cell></cell><cell></cell><cell></cell><cell>1?1</cell><cell></cell><cell></cell></row><row><cell># Output channels</cell><cell></cell><cell></cell><cell></cell><cell>290</cell><cell></cell><cell></cell></row><row><cell>Output size</cell><cell></cell><cell></cell><cell></cell><cell>64?128</cell><cell></cell><cell></cell></row><row><cell>Projection layer</cell><cell></cell><cell>1?1 conv.</cell><cell></cell><cell></cell><cell></cell><cell># Classes</cell><cell>64?128</cell></row><row><cell>Bilinear interpolation</cell><cell></cell><cell>?8</cell><cell></cell><cell></cell><cell></cell><cell># Classes</cell><cell>512?1024</cell></row><row><cell cols="2">Bilinear interpolation (inference only)</cell><cell>?2</cell><cell></cell><cell></cell><cell></cell><cell># Classes</cell><cell>1024?2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 : Layer disposal of EDA-ERFdec. The decoder structure is consistent with ERFNet.</head><label>9</label><figDesc></figDesc><table><row><cell>Name</cell><cell>Mode</cell><cell>Growth rate</cell><cell># Output channels</cell><cell>Output size</cell></row><row><cell>Downsampleing block 1</cell><cell>Win &lt; Wout</cell><cell></cell><cell>15</cell><cell>256?512</cell></row><row><cell>Downsampleing block 2</cell><cell>Win &lt; Wout</cell><cell></cell><cell>60</cell><cell>128?256</cell></row><row><cell>EDA module 1-1</cell><cell></cell><cell>40</cell><cell>100</cell><cell>128?256</cell></row><row><cell>EDA module 1-2</cell><cell></cell><cell>40</cell><cell>140</cell><cell>128?256</cell></row><row><cell>EDA module 1-3</cell><cell></cell><cell>40</cell><cell>180</cell><cell>128?256</cell></row><row><cell>EDA module 1-4</cell><cell>dilation 2</cell><cell>40</cell><cell>220</cell><cell>128?256</cell></row><row><cell>EDA module 1-5</cell><cell>dilation 2</cell><cell>40</cell><cell>260</cell><cell>128?256</cell></row><row><cell>Downsampleing block 3</cell><cell>Win &gt; Wout</cell><cell></cell><cell>130</cell><cell>64?128</cell></row><row><cell>EDA module 2-1</cell><cell>dilation 2</cell><cell>40</cell><cell>170</cell><cell>64?128</cell></row><row><cell>EDA module 2-2</cell><cell>dilation 2</cell><cell>40</cell><cell>210</cell><cell>64?128</cell></row><row><cell>EDA module 2-3</cell><cell>dilation 4</cell><cell>40</cell><cell>250</cell><cell>64?128</cell></row><row><cell>EDA module 2-4</cell><cell>dilation 4</cell><cell>40</cell><cell>290</cell><cell>64?128</cell></row><row><cell>EDA module 2-5</cell><cell>dilation 8</cell><cell>40</cell><cell>330</cell><cell>64?128</cell></row><row><cell>EDA module 2-6</cell><cell>dilation 8</cell><cell>40</cell><cell>370</cell><cell>64?128</cell></row><row><cell>EDA module 2-7</cell><cell>dilation 16</cell><cell>40</cell><cell>410</cell><cell>64?128</cell></row><row><cell>EDA module 2-8</cell><cell>dilation 16</cell><cell>40</cell><cell>450</cell><cell>64?128</cell></row><row><cell>Deconvolution 1</cell><cell>2?2, stride 2</cell><cell></cell><cell>64</cell><cell>128?256</cell></row><row><cell>EDA-non-asymmetric module d1-1</cell><cell></cell><cell></cell><cell>64</cell><cell>128?256</cell></row><row><cell>EDA-non-asymmetric module d1-2</cell><cell></cell><cell></cell><cell>64</cell><cell>128?256</cell></row><row><cell>Deconvolution 2</cell><cell>2?2, stride 2</cell><cell></cell><cell>16</cell><cell>256?512</cell></row><row><cell>EDA-non-asymmetric module d2-1</cell><cell></cell><cell></cell><cell>16</cell><cell>256?512</cell></row><row><cell>EDA-non-asymmetric module d2-2</cell><cell></cell><cell></cell><cell>16</cell><cell>256?512</cell></row><row><cell>Deconvolution 2</cell><cell>2?2, stride 2</cell><cell></cell><cell># Classes</cell><cell>512?1024</cell></row><row><cell>Bilinear interpolation (inference only)</cell><cell>?2</cell><cell></cell><cell># Classes</cell><cell>1024?2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 : Layer disposal of EDA-DenseDown. The dowsampling layers are consistent with DenseNet [13].</head><label>10</label><figDesc></figDesc><table><row><cell>Name</cell><cell>Mode</cell><cell>Growth rate</cell><cell># Output channels</cell><cell>Output size</cell></row><row><cell>Convolution</cell><cell>7?7, stride 2</cell><cell></cell><cell>60</cell><cell>256?512</cell></row><row><cell>Max-pooling</cell><cell>3?3, stride 2</cell><cell></cell><cell>60</cell><cell>128?256</cell></row><row><cell>EDA module 1-1</cell><cell></cell><cell>40</cell><cell>100</cell><cell>128?256</cell></row><row><cell>EDA module 1-2</cell><cell></cell><cell>40</cell><cell>140</cell><cell>128?256</cell></row><row><cell>EDA module 1-3</cell><cell></cell><cell>40</cell><cell>180</cell><cell>128?256</cell></row><row><cell>EDA module 1-4</cell><cell>dilation 2</cell><cell>40</cell><cell>220</cell><cell>128?256</cell></row><row><cell>EDA module 1-5</cell><cell>dilation 2</cell><cell>40</cell><cell>260</cell><cell>128?256</cell></row><row><cell>Convolution</cell><cell>1?1</cell><cell></cell><cell>130</cell><cell>128?256</cell></row><row><cell>Average-pooling</cell><cell>2?2, stride 2</cell><cell></cell><cell>130</cell><cell>64?128</cell></row><row><cell>EDA module 2-1</cell><cell>dilation 2</cell><cell>40</cell><cell>170</cell><cell>64?128</cell></row><row><cell>EDA module 2-2</cell><cell>dilation 2</cell><cell>40</cell><cell>210</cell><cell>64?128</cell></row><row><cell>EDA module 2-3</cell><cell>dilation 4</cell><cell>40</cell><cell>250</cell><cell>64?128</cell></row><row><cell>EDA module 2-4</cell><cell>dilation 4</cell><cell>40</cell><cell>290</cell><cell>64?128</cell></row><row><cell>EDA module 2-5</cell><cell>dilation 8</cell><cell>40</cell><cell>330</cell><cell>64?128</cell></row><row><cell>EDA module 2-6</cell><cell>dilation 8</cell><cell>40</cell><cell>370</cell><cell>64?128</cell></row><row><cell>EDA module 2-7</cell><cell>dilation 16</cell><cell>40</cell><cell>410</cell><cell>64?128</cell></row><row><cell>EDA module 2-8</cell><cell>dilation 16</cell><cell>40</cell><cell>450</cell><cell>64?128</cell></row><row><cell>Projection layer</cell><cell>1?1 conv.</cell><cell></cell><cell># Classes</cell><cell>64?128</cell></row><row><cell>Bilinear interpolation</cell><cell>?8</cell><cell></cell><cell># Classes</cell><cell>512?1024</cell></row><row><cell>Bilinear interpolation (inference only)</cell><cell>?2</cell><cell></cell><cell># Classes</cell><cell>1024?2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 : IoU scores on Cityscapes test set.</head><label>11</label><figDesc></figDesc><table><row><cell>Class</cell><cell>IoU</cell></row><row><cell>Road</cell><cell>97.8</cell></row><row><cell>Sidewalk</cell><cell>80.6</cell></row><row><cell>Building</cell><cell>89.5</cell></row><row><cell>Wall</cell><cell>42.0</cell></row><row><cell>Fence</cell><cell>46.0</cell></row><row><cell>Pole</cell><cell>52.3</cell></row><row><cell>Traffic light</cell><cell>59.8</cell></row><row><cell>Traffic sign</cell><cell>65.0</cell></row><row><cell>Vegetation</cell><cell>91.4</cell></row><row><cell>Terrain</cell><cell>68.7</cell></row><row><cell>Sky</cell><cell>93.6</cell></row><row><cell>Person</cell><cell>75.7</cell></row><row><cell>Rider</cell><cell>54.3</cell></row><row><cell>Car</cell><cell>92.4</cell></row><row><cell>Truck</cell><cell>40.9</cell></row><row><cell>Bus</cell><cell>58.7</cell></row><row><cell>Train</cell><cell>56.0</cell></row><row><cell>Motorcycle</cell><cell>50.4</cell></row><row><cell>bicycle</cell><cell>64.0</cell></row><row><cell>Metric</cell><cell>Value</cell></row><row><cell>mIoU classes</cell><cell>67.3</cell></row><row><cell>mIoU categories</cell><cell>85.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 : IoU scores on CamVid test set.</head><label>12</label><figDesc></figDesc><table><row><cell>Class</cell><cell>IoU</cell></row><row><cell>Sky</cell><cell>90.8</cell></row><row><cell>Building</cell><cell>82.5</cell></row><row><cell>Pole</cell><cell>28.5</cell></row><row><cell>Road</cell><cell>93.3</cell></row><row><cell>Pavement</cell><cell>78.3</cell></row><row><cell>Tree</cell><cell>75.0</cell></row><row><cell>Sign symbol</cell><cell>43.7</cell></row><row><cell>Fence</cell><cell>44.4</cell></row><row><cell>Vehicle</cell><cell>81.0</cell></row><row><cell>Pedestrian</cell><cell>54.6</cell></row><row><cell>Bike</cell><cell>57.9</cell></row><row><cell>Metric</cell><cell>Value</cell></row><row><cell>mIoU</cell><cell>66.4</cell></row><row><cell>Class average acc.</cell><cell>76.7</cell></row><row><cell>Global acc.</cell><cell>90.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKOWLEDGMENTS</head><p>We would like to thank Ping-Rong Chen for his helpful discussions during the course of this project and Shang-Wei Hung for his drawing for this paper. This work was supported in part by the Mechanical and Mechatronics Systems Research Lab., ITRI, under Grant 3000547822.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic imae segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmenta-tion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04943</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">W</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02417</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ContextNet: Exploring context and detail for semantic segmentation in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P K</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic segmentation via structured patch prediction, context crf and guidance crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rtseg: Real-time semantic segmentation comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdel-Razek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schuberth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
