<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial Emotion Recognition: State of the Art Performance on FER2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousif</forename><surname>Khaireddin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofa</forename><surname>Chen</surname></persName>
							<email>zfchen@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial Emotion Recognition: State of the Art Performance on FER2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial emotion recognition (FER) is significant for human-computer interaction such as clinical practice and behavioral description. Accurate and robust FER by computer models remains challenging due to the heterogeneity of human faces and variations in images such as different facial pose and lighting. Among all techniques for FER, deep learning models, especially Convolutional Neural Networks (CNNs) have shown great potential due to their powerful automatic feature extraction and computational efficiency. In this work, we achieve the highest singlenetwork classification accuracy on the FER2013 dataset. We adopt the VGGNet architecture, rigorously fine-tune its hyperparameters, and experiment with various optimization methods. To our best knowledge, our model achieves state-of-theart single-network accuracy of 73.28 % on FER2013 without using extra training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Facial emotion recognition refers to identifying expressions that convey basic emotions such as fear, happiness, and disgust, etc. It plays an important role in human-computer interactions and can be applied to digital advertisement, online gaming, customer feedback assessment, and healthcare <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. With advancements in computer vision, high emotion recognition accuracy has been achieved in images captured under controlled conditions and consistent environments, rendering this a solved problem <ref type="bibr" target="#b3">[4]</ref>. Challenges persist in emotion recognition under naturalistic conditions due to high intra-class variation and low inter-class variation, e.g. changes in facial pose and subtle differences between expressions.</p><p>Developments in computer vision continuously aim to improve classification accuracy on such problems <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. In image classification, Convolutional Neural Networks (CNNs) have shown great potential due to their computational efficiency and feature extraction capability <ref type="bibr" target="#b7">[8]</ref>. They are the most widely used deep models for FER <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>.</p><p>One specific emotion recognition dataset that encompasses the difficult naturalistic conditions and challenges is FER2013. It was introduced at the International Conference on Machine Learning (ICML) in 2013 and became a benchmark in comparing model performance in emotion recognition. Human performance on this dataset is estimated to be 65.5 % <ref type="bibr" target="#b11">[12]</ref>. In comparing different methods and benchmarking our results, we are strictly concerned with previous work trained and evaluated on this dataset.</p><p>In this work, we aim to improve prediction accuracy on FER2013 using CNNs. We adopt the VGG network and construct various experiments to explore different optimization algorithms and learning rate schedulers. We thoroughly tune the model and training hyperparameters to achieve stateof-the-art results at a testing accuracy of 73.28 %. To our best knowledge, this is the highest single-network accuracy achieved on FER2013 without using any extra training data. We then construct several saliency maps to better understand the network's performance and decision-making process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Since being introduced in the late 1990s, CNNs have shown great potential in image processing <ref type="bibr" target="#b12">[13]</ref>. A typical CNN includes a convolutional layer, a pooling layer, and a fully connected layer. This makes it efficient in static image manipulation. However, at that time, the application of CNNs was limited due to the lack of training data and computing power. After the 2010s, the growth of computing power and the collection of larger datasets made CNNs a much more viable tool in feature extraction and image classification <ref type="bibr" target="#b7">[8]</ref>.</p><p>Various techniques have been proposed to even further improve performance. For instance, the Sigmoid activation function has been replaced by Rectified Linear Unit (ReLU) activation to avoid gradient dispersion problems and speed up training <ref type="bibr" target="#b13">[14]</ref>. Different pooling methods such as average pooling and max pooling are used to downsample the inputs and aid in generalization <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>. Dropout, regularization, and data augmentation are used to prevent overfitting. Batch normalization has been developed to help prevent gradient vanishing and exploding <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>.</p><p>A great deal of research has also been done in creating different optimization algorithms used in training. Though there is no systematic theoretical guideline on choosing an optimizer, empirical results show that a suitable optimization algorithm can effectively improve a model's performance <ref type="bibr" target="#b19">[19]</ref>. The most commonly used optimizer is Stochastic gradient descent (SGD). It is a simple technique that updates the parameters of a model based on the gradient of a single data point <ref type="bibr" target="#b19">[19]</ref>. Numerous variations of this algorithm have been proposed to speed up training. AdaGrad adaptively scales the learning rate for each dimension in the network <ref type="bibr" target="#b20">[20]</ref>. RMSProp radically diminishes the learning rate <ref type="bibr" target="#b21">[21]</ref>. Adam combines the advantages of AdaGrad and RMSProp by scaling the learning rate and introducing momentum of gradient, etc. <ref type="bibr" target="#b22">[22]</ref>.</p><p>Among many others, one significant factor that could impact performance is the learning rate. A large learning rate could lead to oscillations around the minima or divergence in the loss. A small learning rate would slow down the model's convergence significantly and could trap the model at a nonoptimal local minimum. A commonly used technique is to employ a learning rate scheduler that changes the learning rate during training <ref type="bibr" target="#b23">[23]</ref>. For instance, timebased decay reduces the learning rate either linearly or exponentially as the iteration number increases.</p><p>Step decay drops the learning rate by a factor after certain epochs. An adaptive learning rate schedule tries to automatically adjust the learning rate based on the local gradients during training. Cosine annealing resets the learning rate periodically and reuses "good weights" during the training process, etc. <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>.</p><p>With all the developments above and the extensive research in CNNs, they have become an extremely favorable tool when tackling tasks in image processing, pattern recognition, and feature extraction.  <ref type="bibr" target="#b34">[34]</ref>. However, in order to improve the ensemble performance even further, we aim to first optimize the building blocks of these ensembles, a single network. Other research work has tried to improve their model's performance on FER2013 by including auxiliary training data; however, that is out of the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset, Preprocessing, and Augmentation</head><p>In training on FER2013, we adhere to the official training, validation, and test sets as introduced by the ICML. FER2013 consists of 35888 images of 7 different emotions: anger, neutral, disgust, fear, happiness, sadness, and surprise. A Kaggle forum discussion held by the competition organizers places human accuracy on this dataset in the range of 65 -68 % <ref type="bibr" target="#b32">[32]</ref>.</p><p>To account for the variability in facial expression recognition, we apply a significant amount of data augmentation in training. This augmentation includes rescaling the images up to ? 20 % of its original scale, horizontally and vertically shifting the image by up to ? 20 % of its size, and rotating it up to ? 10 degrees. Each of the techniques is applied randomly and with a probability of 50 %. After this, the image is then tencropped to a size of 40?40, and random portions of each of the crops are erased with a probability of 50 %. Each crop is then normalized by dividing each pixel by 255.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training and Inference</head><p>We run all experiments for 300 epochs optimizing the cross-entropy loss. In the following sections, we vary the optimizer used as well as the learning rate schedulers and maintain other parameters constant. We use a fixed momentum of 0.9 and a weight decay of 0.0001. All experiments are run with gradient scaling to prevent gradient underflow. The models are evaluated using validation accuracy and tested using standard ten-crop averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. VGGNet Architecture</head><p>VGGNet is a classical convolutional neural network architecture used in large-scale image processing and pattern recognition <ref type="bibr" target="#b35">[35]</ref>. Our variant of VGGNet is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. The network consists of 4 convolutional stages and 3 fully connected layers. Each of the convolutional stages contains two convolutional blocks and a max-pooling layer. The convolution block consists of a convolutional layer, a ReLU activation, and a batch normalization layer. Batch normalization is used here to speed up the learning process, reduce the internal covariance shift, and prevent gradient vanishing or explosion <ref type="bibr" target="#b18">[18]</ref>. The first two fully connected layers are followed by a ReLU activation. The third fully connected layer is for classification. The convolutional stages are responsible for feature extraction, dimension reduction, and non-linearity. The fully connected layers are trained to classify the inputs as described by extracted features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Tuning</head><p>Initially, we tune our model architecture to maximize performance. All initial experiments were run using SGD. A grid search is performed to determine the optimal batch size and the best drop-out rate after our fully connected layers. Once the architecture has been optimized, we then explore the effects of different optimizers and learning rate schedulers on our model's performance. We then set up a final experiment to try and fine-tune the trained model's weights and improve its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Optimizer</head><p>Our first experiment intends to find the best optimizer in training our architecture. For this, we explore 6 different algorithms: SGD, SGD with Nesterov Momentum, Average SGD, Adam, Adam with AMSGrad, Adadelta, and Adagrad. Although many of these algorithms are very closely related, understanding how they perform differently in this optimization will help us understand the importance of their subtle differences.</p><p>We run 2 different variations of this experiment. In the first variation, we run all algorithms with a fixed learning rate of 0.001. This learning rate was determined using a grid search. In the second variation, we set up a simple learning rate scheduler with an initial learning rate of 0.01 and it is reduced by a factor of 0.75 if the validation accuracy plateaus for 5 epochs. The parameters of this scheduler were also determined using a grid search. All other parameters, such as weight decay, momentum, dropout, and batch size are maintained constant after the initial optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 LR Schedule</head><p>The next experiment we run is to find the optimal learning rate scheduler. In this section, we run the same architecture, using the optimal optimizer decided by the previous section with 5 different schedulers: Reduce Learning Rate on Plateau (RLRP), Cosine Annealing (Cosine), Cosine Annealing with Warm Restarts (CosineWR), One Cycle Learning Rate (OneCycleLR), and Step Learning Rate (StepLR). For a baseline, we also run a model using a constant learning rate that was determined using a grid search. All schedulers have an initial learning rate of 0.01 and their parameters are selected using a grid search. All other parameters are maintained constant.</p><p>Although these schedulers have their similarities, they perform the learning rate update differently. For example, StepLR and RLRP both have a similar assumption that the longer we train, the smaller our step sizes should be. However, StepLR systematically reduces the learning rate after a set number of epochs while RLRP monitors the model's current performance before making a learning rate update. Similarly, both Cosine Annealing and Cosine Annealing with Warm Restarts have a cosine-based learning rate update function. So, the learning rate gradually oscillates between two values. However, the key difference is that the latter one resets the model's parameters regularly to maintain good model weights before taking update steps. These subtle differences could drastically change the resulting performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D3. Fine Tuning</head><p>To further increase our model's accuracy, we then experimented with hyper-tuning the final weights of our model. We reload the parameters and train for a final 50 epochs using an initial learning rate of 0.0001. We set this learning rate to maintain the update steps small; thus, ensuring that our model's weights are not skewed far away, since this is an already trained model. This experiment is run using Cosine Annealing and Cosine Annealing with Warm Restarts since both of these schedulers slowly oscillate the learning rate back and forth thus not allowing for major weight changes. The second schedule is also favorable since its warm restarts would mean that it would regularly reset the model's weights back to some good location during its updates.</p><p>We then ran a second variation of this experiment in which we combined the validation set into training to allow for a larger dataset set when tuning. This larger dataset would allow the model to have more samples to learn from; thus, improving its performance. The test set is left un-altered and all other parameters are kept constant.</p><p>By running two variations of this experiment, we can verify two things. Using the first variation, we can confirm the effectiveness of the tuning. Using the second experiment, we can understand the benefits of added data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optimizer</head><p>We first study and compare the effect of optimizers on the performance of our model. <ref type="figure" target="#fig_2">Figure 2</ref> shows the validation accuracy attained by our model using the different optimizers. The yellow bars show the first variation of this experiment with a constant learning rate and the orange bars show the second variation using a decaying learning rate. Excluding Adadelta, all optimizers show a high validation accuracy, above 70 %. The model using the SGD with Nesterov momentum performs the best in both experiments attaining a validation accuracy of 73.2 % and 73.5 %. We also found that Adam and its AMSGrad variant perform better than Adadelta and Adagrad. This is because Adam optimization combines the advantage of AdaGrad and RMSProp by introducing momentum of the gradient. Finally, on this dataset, all variants of SGD are outperforming all other optimizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. LR Schedule</head><p>Next, we explore the effects of different learning rate schedulers on our model. <ref type="figure" target="#fig_3">Figure 3</ref> shows the validation and testing accuracies attained by our models. All runs here use the best performing optimizer based on the previous section, SGD with Nesterov momentum. The first thing to note is that Reducing Learning Rate on Plateau (RLRP) performs best. It achieves a validation accuracy of 73.59 % and a testing accuracy of 73.06 %. To our best knowledge, this is already surpassing the previous single-network state-of-the-art performance.</p><p>For the next set of comparisons, we will be focused on the validation accuracy, since the testing accuracies we are reporting are strictly for benchmarking publicly. The constant learning rate outperforms some of the other schedulers (OneCycleRL and StepLR). For OneCycleLR, this could be since it is usually intended for fast training with larger learning rates and this may not be applicable on FER2013 <ref type="bibr" target="#b36">[36]</ref>. Cosine Annealing and its counterpart Cosine Annealing with Warm Restart perform comparably. Comparing StepLR and RLRP, they both slowly reduce the learning rate to a minimum. RLRP performs better since it monitors the current performance before deciding when to drop the learning rate as opposed to systematically reducing the learning rate.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fine Tuning</head><p>To further increase our final performance, we then run an experiment in which we reload our best performing model thus far and tune it for 50 extra epochs using both cosine annealing schedulers at a small learning rate. We then also combine our training and validation data and perform this same tuning in a separate run. <ref type="table">Table 1</ref> shows the final testing accuracies achieved in this experiment benchmarked against the model which they are tuning. Cosine Annealing performs best here and improves the model by 0.05 %. Cosine Annealing with Warm Restarts, on the other hand negatively impacts performance in this tuning stage reducing the accuracy by 0.42 %. As expected both models perform better after training on the combined dataset resulting from the training and validation data. Our final best model achieves an accuracy of 73.28 %. Once again, the testing data is left unaltered at all stages. <ref type="table">Table 1</ref> Testing accuracy after extra tuning. <ref type="figure">Figure 4</ref> The confusion matrix of our final VGGNet on the FER2013 public test set. <ref type="figure">Figure 4</ref> shows the final model's confusion matrix on the FER2013 testing set. The model shows the best classification on the "happiness" and "surprise" emotions. On the other hand, it makes the most mistakes when classifying between "disgust" and "anger". Next, the low classification accuracy in "disgust" and "fear" can be attributed to the fact that they have a lower number of samples in the original training set. The misclassification between "fear" and "sadness" may be due to the inter-class similarities of the dataset <ref type="table" target="#tab_0">Table 2</ref> summarizes previous reported classification accuracies FER2013. Most reported methods perform better than the estimated human performance (~ 65.5 %). The previous best-reported single-network accuracy is 72.7 % <ref type="bibr" target="#b34">[34]</ref>. In this work, we achieve the state-of-the-art accuracy of 73.28 %.  One of the common visualization techniques in deep neural networks is called a saliency map <ref type="bibr" target="#b37">[37]</ref>. By propagating the loss back to the pixel values, a saliency map can highlight the pixels which have the most impact on the loss value. It highlights the visual features the CNN can capture from the input; thus, allowing us to better understand the importance of each feature in the original image on the final classification decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Confusion Matrix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance Comparison</head><p>We generate the saliency map using our best performing network to understand how it classifies each emotion in the FER2013 dataset. <ref type="figure" target="#fig_4">Figure 5</ref> shows a saliency map for each emotion before and after being superimposed on the image it is generated from. Judging by all the images, our CNN can effectively capture most of the critical regions for each emotion. The model is placing a large importance on almost all facial features of the person in each image. This is most clearly seen in (f) where the saliency map almost perfectly maps the entire face of the woman. Our model is also effectively dropping regions like the background in (a), (d) and (g), the hair in (a), (c), and (g), and the hand in (e), all of which are not very informative when it comes to describing someone's emotion.</p><p>There are some clear mistakes in the saliency maps, this can be seen in (b), (e), and the corner of (g) where the model highlights some of the background pixels. We think that a model that can more effectively identify the facial features in an image and drop all useless information will perform better on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper achieves single-network state-of-theart classification accuracy on FER2013 using a VGGNet. We thoroughly tune all hyperparameters towards an optimized model for facial emotion recognition. Different optimizers and learning rate schedulers are explored and the best initial testing classification accuracy achieved is 73.06 %, surpassing all single-network accuracies previously reported. We also carry out extra tuning on our model using Cosine Annealing and combine the training and validation datasets to further improve the classification accuracy to 73.28 %. For future work, we plan to explore different image processing techniques on FER2013 and investigate ensembles of different deep learning architectures to further improve our performance in facial emotion recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Once a large facial expression dataset, FER2013, was introduced at ICML in 2013, it became a benchmark in comparing model performance in emotion recognition. Many CNN variants have achieved remarkable results with a classification accuracy between 65 % and 72.7 % [26]-[34]. For instance, Liu et. al. trained the three separate CNNs and ensembled them to improve performance. Their best single-network accuracy is 62.44 % [26]. Minaee et. al used an attentional convolutional network in an end-to-end deep learning framework and achieved an accuracy of 70.02 % [27]. Tang et. al. replaced the softmax layer with a support vector machine in a deep neural network and achieved a classification accuracy of 71.2 % [32]. Shi et. al. proposed a novel amend representation module (ARM) to substitute the pooling layer and achieved a testing accuracy of 71.38 % [33]. Pramerdorfer et. al. compared the performance of three different architectures, VGG, Inception, and ResNet. Their results show that VGG performs best at an accuracy of 72.7 %, followed by ResNet at 72.4 %, and Inception at 71.6 % [34]. Ensembling multiple different models has been shown to improve performance. For instance, Liu et. al. ensembled of 3 CNNs and improved their accuracy by 2.6 % [26]. Pramerdorfer et. al. ensembled 8 CNNs and achieved a 2.5 % performance boost</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>VGGNet architecture. A face expression image is fed into the model. The four convolutional blocks (Conv) extract high-level features of the image and the fully-connected (FC) layers classify the emotion of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>VGGNet performance using different optimizers. The yellow bars show the results using a constant learning rate (LR). The orange bars show accuracies using decaying LR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>VGGNet performance using different LR schedulers. The green bars show the final validation accuracies and the blue ones show the corresponding testing accuracies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Original images, Saliency maps, and superimposition for all emotions in FER2013.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Fer2013 public testset benchmark. The 'black box' nature of deep learning models makes it difficult to understand how the model processes the input image to obtain the final result. Visualizing the information captured inside deep neural networks is important in evaluating the effectiveness of the model and understanding how it computes its prediction. Thus, generating an understandable visualization of our CNN can help describe how it differentiates between and captures different facial emotions.</figDesc><table><row><cell>F. Saliency map</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real Time Face Detection and Facial Expression Recognition: Development and Applications to Human Computer Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2003.10057</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human-computer interaction using emotion recognition from facial expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Abdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pruski</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMS.2011.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings -UKSim 5th European Modelling Symposium on Computer Modelling and Simulation</title>
		<meeting>-UKSim 5th European Modelling Symposium on Computer Modelling and Simulation<address><addrLine>EMS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic facial expression analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0031-3203(02)00052-3</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial affect: A survey of registration, representation, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2014.2366127</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facial emotion recognition using convolutional neural networks (FERC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mehendale</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42452-020-2234-1</idno>
	</analytic>
	<monogr>
		<title level="j">SN Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Facial emotion recognition on a dataset using Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>T?men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">F</forename><surname>S?ylemez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ergen</surname></persName>
		</author>
		<idno type="DOI">10.1109/IDAP.2017.8090281</idno>
	</analytic>
	<monogr>
		<title level="m">IDAP 2017 -International Artificial Intelligence and Data Processing Symposium</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extended deep neural network for facial emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shamsolmoali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sehdev</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2019.01.008</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automating facial emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gervasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Franzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riganelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tasso</surname></persName>
		</author>
		<idno type="DOI">10.3233/WEB-190397</idno>
	</analytic>
	<monogr>
		<title level="j">Web Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast Facial emotion recognition Using Convolutional Neural Networks and Gabor Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Imani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Majidi</surname></persName>
		</author>
		<idno type="DOI">10.1109/KBEI.2019.8734943</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 5th Conference on Knowledge Based Engineering and Innovation, KBEI 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Facial Emotion Recognition Using Deep Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pranav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Satheesh</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Supriya</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICACCS48705.2020.9074302</idno>
	</analytic>
	<monogr>
		<title level="m">2020 6th International Conference on Advanced Computing and Communication Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2014.09.005</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for LVCSR using rectified linear units and dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<idno type="DOI">10.1109/ICASSP.2013.6639346</idno>
	</analytic>
	<monogr>
		<title level="j">Acoustics, Speech and Signal Processing -Proceedings</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast image scanning with deep max-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2013.6738831</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICIP 2013 -Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding of a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Zawi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICEngTechnol.2017.8308186</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 International Conference on Engineering and Technology</title>
		<meeting>2017 International Conference on Engineering and Technology</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BranchOut: Regularization for online ensemble tracking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.63</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Optimization for deep learning: theory and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv. 2019</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adagrad-An Optimizer for Stochastic Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Lydia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Francis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A sufficient condition for convergences of adam and rmsprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A learning-rate schedule for stochastic gradient methods to matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-18038-0_35</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9077</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An Exponential Learning Rate Schedule For Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facial Expression Recognition with CNN Ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -2016 International Conference on Cyberworlds</title>
		<meeting>-2016 International Conference on Cyberworlds</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep-emotion: facial expression recognition using attentional convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolrashidi</surname></persName>
		</author>
		<idno type="DOI">10.3390/s21093046</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Local Learning to Improve Bag of Visual Words Model for Facial Expression Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Work. challenges Represent. Learn. ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local learning with deep and handcrafted features for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2917266</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning approaches for facial emotion recognition: A case study on FER-2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giannopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Perikos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hatzilygeroudis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66790-4_1</idno>
	</analytic>
	<monogr>
		<title level="m">Smart Innovation, Systems and Technologies</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">85</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper in facial expression recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2016.7477450</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<idno>1306.0239</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning to Amend Facial Expression Representation via De-albino and Affinity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Facial expression recognition using convolutional neural networks: state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Pramerdorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename></persName>
		</author>
		<idno>1612.02903</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2520589</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR 2014 -Workshop Track Proceedings</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
