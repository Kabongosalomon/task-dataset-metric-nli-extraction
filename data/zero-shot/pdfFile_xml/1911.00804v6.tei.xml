<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GENERALIZING TO UNSEEN DOMAINS VIA DISTRIBUTION MATCHING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabela</forename><surname>Albuquerque</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRS-EMT</orgName>
								<orgName type="institution" key="instit2">Universit? du Qu?bec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Monteiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRS-EMT</orgName>
								<orgName type="institution" key="instit2">Universit? du Qu?bec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Darvishi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Faubert Lab</orgName>
								<orgName type="institution" key="instit1">Universit? de Montr?al 3 Mila &amp; DIRO</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRS-EMT</orgName>
								<orgName type="institution" key="instit2">Universit? du Qu?bec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
						</author>
						<title level="a" type="main">GENERALIZING TO UNSEEN DOMAINS VIA DISTRIBUTION MATCHING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised learning results typically rely on assumptions of i.i.d. data. Unfortunately, those assumptions are commonly violated in practice. In this work, we tackle such problem by focusing on domain generalization: a formalization where the data generating process at test time may yield samples from never-before-seen domains (distributions). Our work relies on the following lemma: by minimizing a notion of discrepancy between all pairs from a set of given domains, we also minimize the discrepancy between any pairs of mixtures of domains. Using this result, we derive a generalization bound for our setting. We then show that low risk over unseen domains can be achieved by representing the data in a space where (i) the training distributions are indistinguishable, and (ii) relevant information for the task at hand is preserved. Minimizing the terms in our bound yields an adversarial formulation which estimates and minimizes pairwise discrepancies. We validate our proposed strategy on standard domain generalization benchmarks, outperforming a number of recently introduced methods. Notably, we tackle a real-world application where the underlying data corresponds to multi-channel electroencephalography time series from different subjects, each considered as a distinct domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main assumption within the empirical risk minimization framework is that all examples used for training and testing predictors are independently drawn from a fixed distribution, i.e. the i.i.d. assumption. A number of generalization guarantees were derived upon that assumption and those results induced several algorithms for the solution of supervised learning problems. However, important limitations in this setting can be highlighted: i) the i.i.d. property is unverifiable <ref type="bibr" target="#b0">[1]</ref> given that one doesn't have access to the data distribution, and ii) it doesn't account for distribution shifts which often occur in practice. Representative examples of these distribution shifts include changes in data acquisition conditions, such as illumination in images for object segmentation, or new data sources such as unseen speakers when performing speech recognition.</p><p>A number of alternative settings was then introduced in order to better cope with more realistic cases. Risk minimization under the domain adaptation setting, for instance, relaxes part of the i.i.d. assumption by allowing a source distribution (or domain) 2 as well as a different target distribution observed at test time. The domain adaptation results introduced in <ref type="bibr" target="#b1">[2]</ref> showed that the generalization gap in terms of risk difference across the two considered distributions for a fixed predictor is upper bounded by a notion of distance measured between the training and testing domains. While less restrictive than the previous setting, the domain adaptation case is still limited in that only pairs of distributions seen during training are expected to yield low risk, and shifts beyond those domains will likely induce poor performance. Moreover, algorithms devised for this setting rely on access at training time to an unlabeled sample from the target distribution so that representations can be learned inducing invariance across train and target domains <ref type="bibr" target="#b2">[3]</ref>. This is a limiting factor for practical applications where target domain data may be inaccessible; for example, a speech recognition service cannot be (re)trained on data obtained from every new speaker it observes.</p><p>A more general setting is often referred to as domain generalization <ref type="bibr" target="#b3">[4]</ref>. In this case, it is assumed that a set of distributions over the data is available at training time. At test time, however, both observed distributions as well as unseen novel domains might appear, and a low risk is expected regardless of the underlying domain. More importantly, unlike domain adaptation in which the goal is to find a representation that aligns training data distributions with a specific target domain, domain generalization strategies aim at finding a representation space that yields good performance on novel distributions, unknown at training time. Recent work on domain generalization has included the use of data augmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> at training time, meta-learning to simulate domain shift <ref type="bibr" target="#b6">[7]</ref>, adding a self-supervised task to encourage an encoder to learn robust representations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and learning domain-invariant representations <ref type="bibr" target="#b9">[10]</ref>, among other approaches.</p><p>In this paper, we propose an innovation within the domain generalization setting. We first argue and prove that, given a set of distributions over data, if the distances measured between any pair of such distributions is small, so is the distance between mixtures obtained from the same set. This leads to the development of a bound on the risk measured against any distribution, and further shows that generalization can be expected if one considers distributions on the neighborhood of the "convex hull" 3 defined by the set of domains accessible during training. Inspired by these findings, we define an approach so that an encoder is enforced to map the data to a space where domain-dependent cues are filtered away while relevant information to the task of interest is conserved. While doing so, unlike standard domain adaptation approaches, no data from test distributions is observed.</p><p>We summarize our contributions in the following:</p><p>1. We introduce assumptions on the data generating process tailored to the domain generalization setting, which we argue are more general than standard i.i.d. requirements and more likely to hold in practice. In other words, given a data sample, it is more likely that our assumptions will hold compared to the more restrictive i.i.d. property; 2. We prove a generalization bound for the risk over unseen domains and show that generalization can be expected for domains on the neighborhood of a notion of convex hull of distributions observed at training time; 3. Aiming to minimize the terms of the introduced bound, we devise an adversarial approach so that pairwise domain divergences are estimated and minimized. In order to do so, several practical improvements are proposed on top of previous approaches for domain adaption including the use of random projection layers prior to domain discriminators. 4. We provide evidence through empirical evaluation showing that the proposed approach yields improvements relative to alternative methods across scenarios where different assumptions over the observed domains hold, including realistic cases where the labeling functions might shift.</p><p>The remainder of this paper is organized as follows: In Section 2 we introduce the notation adopted within this work, and review related work and generalization guarantees. In section 3, we define the domain generalization setting and present our main results, as well as the resulting algorithm. Section 4 provides the experiment descriptions and their respective results. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>Let the data be represented by X ? R D , while Y denotes the label space. Examples correspond to a pair (x, y) : x ? X , y ? Y, such that y = f D (x), and f D : X ? Y is a deterministic labeling function.</p><p>A domain is defined as a tuple D, f D where D corresponds to a probability distribution over X . Moreover, we define a mapping h : X ? Y, such that h ? H, where H is a set of candidate hypothesis, and finally define the risk R associated with a given hypothesis h on domain D, f D as:</p><formula xml:id="formula_0">R[h] = E x?D [h(x), f D (x)],<label>(1)</label></formula><p>where the loss : Y ? Y ? R + quantifies how different h(x) is from the true labeling function y = f D (x) for a given instance (x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related work</head><p>A number of contributions under the domain generalization setting borrowed tools from causal inference to enforce the learned representations to be invariant across the different domains presented to the model at training time <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Other contributions such as <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref> proposed different strategies to leverage self-supervised tasks to improve the out-of-distribution performance of a given model. Inspired by the domain adaptation literature <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, previous work on domain generalization also proposed to add a regularization term based on the minimization of a notion of divergence between the source domains to the empirical loss computed on the training data. This is the case of CIDDG <ref type="bibr" target="#b9">[10]</ref>, where class-specific domain classifiers are employed to induce the encoder to learn representations where the mismatch between the labels conditional distributions is minimized. Moreover, <ref type="bibr" target="#b14">[15]</ref> proposed MMD-AAE, an approach that relies on an adversarial autoencoder used along with a maximum mean discrepancy penalty <ref type="bibr" target="#b15">[16]</ref> to remove domain-specific information.</p><p>Recent work has proposed settings where domain-shifts are simulated at training time by splitting the source domains into meta-train and meta-test sets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Strategies based on learning domain-invariant representations <ref type="bibr" target="#b3">[4]</ref>, data augmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>, and on decomposing the model's parameters into domain-agnostic and domain-specific components <ref type="bibr" target="#b19">[20]</ref> have also been introduced. Work on other settings with more restrictive assumptions than domain generalization are also related to our contribution. For example, recent work on multi-domain learning <ref type="bibr" target="#b20">[21]</ref>, a setting where multiple domains are available at training time and test data is drawn from the same distributions seen during training <ref type="bibr" target="#b21">[22]</ref>, also leveraged an adversarial approach to perform H-divergence minimization.</p><p>A straightforward approach to extend the empirical risk minimization (ERM) setting for domain generalization would be to learn h minimizing the empirical riskR[h] measured over all N S source domains and hope generalization would be achieved to the target data, i.e.:</p><formula xml:id="formula_1">h = arg minR = 1 N S N S j=1 1 M j Mj i=1 [h(x i ), f (x i )].<label>(2)</label></formula><p>In fact, as will be discussed in more detail in next sections, such a rather simplistic approach often yields strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generalization guarantees for domain adaptation</head><p>We now state results from the domain adaptation literature which are relevant in the context of this work. The discussion in <ref type="bibr" target="#b22">[23]</ref> established the theoretical foundations for studying cross-domain generalization properties for domain adaptation problems. They showed that, given a source domain D S and a target domain D T , the risk of a given hypothesis h ? H, h : X ? {0, 1}, on the target is bounded by:</p><formula xml:id="formula_2">R T [h] ? R S [h] + d H?H [D S , D T ] + ?,<label>(3)</label></formula><p>where ? corresponds to the minimal total risk over both domains which can be achieved within a given hypothesis class H. The term d H?H [D S , D T ] corresponds to the H-divergence introduced in <ref type="bibr" target="#b13">[14]</ref> for a hypothesis class H?H = {h(x) ? h (x)|h, h ? H}, where ? is the XOR function. The H-divergence between two distributions D S and D T is defined as:</p><formula xml:id="formula_3">d H [D S , D T ] = 2 sup ??H |Pr x?D S [?(x) = 1] ? Pr x?D T [?(x) = 1]|.<label>(4)</label></formula><p>As discussed in <ref type="bibr" target="#b1">[2]</ref>, an estimate of d H [D S , D T ] can be directly computed from the error of a binary classifier trained to distinguish domains.</p><p>3 Learning domain agnostic representations for domain generalization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formalizing domain generalization</head><p>We start by defining a set of assumptions over the data generating process considering the domain generalization case as well as the notion of risk we are concerned with. We then define D, referred to as meta-distribution, corresponding to a probability distribution over a countable set of possible domains. Under this view, a query for a data example consists of: i) sampling a domain from the meta-distribution, and ii) sampling a data point according to that particular domain. Such process is repeated m times so as to yield a training sample (x m ? D m , y m ). We remark the described model of data generating processes is sufficiently general so as to include the i.i.d. case (the meta-distribution yields a single domain) as well as the domain adaptation setting (if two domains are allowed), but further supports several other cases where multiple domains exist.</p><p>Once a finite train sample is collected, a set of N S domains is observed. Each distribution D i S , i ? [N S ], in such set will be referred to as source domain. At test time, however, drawing samples from D might yield data distributed according to new unseen domains. We then introduce extra notation and represent the set of possible domains unobserved while train data is acquired by D j U , j ? [N U ]. The labeling rules corresponding to each domain are denoted as f Si and f Uj , for the source and unseen domains, respectively. For the sake of clarity, we hereinafter omit the index from the notation corresponding to unseen domains whenever it can be inferred from the context.</p><p>We proceed and define a risk minimization framework similar to that corresponding to the i.i.d. setting: find the predictor h * ? H that minimizes the meta-risk R D [h] defined as follows:</p><formula xml:id="formula_4">h * ? argmin h?H R D [h], R D [h] = E D?D [E x?D [ (h(x), f D (x))]].<label>(5)</label></formula><p>However, within the domain generalization setting, no information regarding possible test distributions is available at training time, which renders estimating R D [h] uninformative for a practical number of source domains. Moreover, we argue that no-free-lunch type of impossibility results may be used to conclude that it is impossible to generalize to any possible unknown distribution 4 , so that one must assume something about the test domains in order to enable generalization. In the following results, we tackle this issue and introduce generalization guarantees for a particular set of domains lying close to the set of mixtures of source distributions, i.e., those observed once train data is collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Matching distributions in the convex hull</head><p>Let a set S of source domains such that |S| = N S be denoted by</p><formula xml:id="formula_5">D i S , i ? [N S ].</formula><p>The convex hull ? S of S is defined as the set of mixture distributions given by:</p><formula xml:id="formula_6">? S = {D :D(?) = N S i=1 ? i D i S (?), ? i ? ? N S ?1 }, where ? N S ?1 is the N S ? 1-th dimensional simplex.</formula><p>The following lemma shows that for any pair of domains such that D , D ? ? 2 S , the H-divergence between D and D is upper-bounded by the largest H-divergence measured between elements of S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 (Bounding the H-divergence between domains in the convex hull of the sources). Let</head><formula xml:id="formula_7">d H [D i S , D k S ] ? , ? i, k ? [N S ].</formula><p>The following inequality holds for the H-divergence between any pair of domains D ,</p><formula xml:id="formula_8">D ? ? 2 S : d H [D , D ] ? .<label>(6)</label></formula><p>Proof. C.f. Supplementary material.</p><p>We thus argue that if one minimizes the maximum pairwise H-divergence between source domains, which can be achieved by an encoding process that filters away domain discriminative cues, the H-divergence between any two domains in ? S also decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generalizing to unseen domains</head><p>Now we turn our attention to the set of unseen distributions D j U , j ? [N U ], i.e., those in the support of the metadistribution but not observed within the training sample. Given an unseen domain D U , we further introduceD U , the element within ? S which is closest to D U , i.e.,D U is given by argmin ?1,...,</p><formula xml:id="formula_9">? N S d H D U , N S i=1 ? i D i S .</formula><p>We now use Lemma 1 and previously proposed generalization bounds for the domain adaptation setting <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> to derive a generalization bound for the risk R U [h].</p><p>Theorem 1 (Upper-bounding the risk on unseen domains). Given the previous setup, let S be the set of source domains</p><formula xml:id="formula_10">and Y = [0, 1]. The risk R U [h], ?h ? H, for any unseen domain D U such that d H [D U , D U ] = ? is bounded as: R U [h] ? N S i=1 ? i R i S [h] + ? + + min{ED U [|f S? ? f U |], E D U [|f U ? f S? |]},<label>(7)</label></formula><p>where is the highest pairwiseH-divergence measured between pairs within S,</p><formula xml:id="formula_11">H = {sign(|h(x) ? h (x)| ? t)| h, h ? H, 0 ? t ? 1} and f S? (x) = N S i=i ? i f Si (x) is the labeling function for any x ? Supp(D U ) resulting from combining all f Si with weights ? i , i ? [N S ], determined byD U .</formula><p>Proof. C.f. Supplementary material.</p><p>Remark 1: Notice that the right-most term in Theorem 1 accounts for the mismatch between the labeling functions f S? and f U , which reduces to 0 in most adopted scenarios within domain adaption/generalization applications, since it is often considered that the covariate shift assumption holds <ref type="bibr" target="#b25">[26]</ref>. Under such setting, the labeling functions are the same across all domains in the support of D, i.e. f Si = f Uj = f for all i ? [N S ] and j ? [N U ]. Besides the covariate shift assumption, previous work on multi-source domain adaptation <ref type="bibr" target="#b26">[27]</ref> considered the case where the unseen domain D U can be represented as a mixture of the sources with weights ? i , i ? [N S ], i.e. D U =D U . When such assumption holds, the term indicated by ? in Theorem 1 will vanish. We thus re-state in Corollary 1 the previous result under such simplifying assumptions.</p><p>Corollary 1 (Generalization to unseen domains within ? S under the covariate shift assumption). Let all domains within the the support of the meta-distribution D have labeling function f . Let S be set of source domains and its convex-hull be denoted as ? S . The risk R U [h] of a hypothesis h on an unseen domain D U ? ? S , is upper-bounded by:</p><formula xml:id="formula_12">R U [h] ? N S i=1 ? i R i S [h] + .<label>(8)</label></formula><p>Proof. C.f. Supplementary material.</p><p>Remark 2: Based on the introduced results we define in the following an algorithm relying solely on source data, unlike domain adaptation approaches. While the total source risk can be minimized as usual, can be minimized by encoding source data to a space where source domains are hard to distinguish. We remark that we empirically found (c.f. Sections 4.1.1 and 4.2) the proposed algorithm was able to succeed even in scenarios where the considered assumptions are not likely to hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3:</head><p>We further highlight that the introduced results also provide insights regarding the importance of acquiring diverse datasets in practice when targeting domain generalization (and hint as to why data augmentation is often helpful). The more diverse a dataset is regarding the number of domains present at training time, more likely it is that an unseen distribution lies within the convex hull of the source domains (i.e. ? ? 0). Therefore, not only the amount of data is important to achieve better generalization on unseen domains, but also the diversity of the training data is crucial.</p><p>Remark 4: Another practical aspect worth remarking is that, even though our domain generalization setting is more general than ERM, Theorem 1 suggests that source domain labels should also be available, since they are required to estimate , which is not the case for ERM. However, collecting domain labels is inherent to the data acquisition procedure for several tasks and commonly available as meta-data in cases such as, speech recognition, where different speakers or recording devices can be viewed as different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Practical contributions</head><p>Motivated by the previous results, we propose to design algorithms that minimize the terms in the bound in <ref type="formula" target="#formula_0">(14)</ref> that can be estimated even if only source data is observed, i.e., as well as the risks over the train sample. We thus aim at learning an encoder E : X ? Z, where Z ? R d preserves information relevant for separating classes, while removing domain-specific cues in such a way that it is harder to distinguish examples from different domains in comparison to the original space X .</p><p>Efficiently estimating : Previous work on domain adaptation introduced strategies based on minimizing the empirical H-divergence between sources and a given target domain <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>. Instead, as per the discussion following Theorem 1, the domain generalization setting requires estimating pairwise H-divergences across all available sources, not considering target data of any sort. Naively extending previous methods to our case would require O(N 2 S ) estimators, which is unpractical given real-world cases where several source domains are available. We thus propose to use one-vs-all classifiers. In this case, there is one domain discriminator per source domain and the k-th discriminator estimates</p><formula xml:id="formula_13">l =k d H [D k S , D l S ]</formula><p>, and improves the method to a number of H-divergence estimators linear on N S . Training: The proposed approach contains three main modules, all parameterized by neural networks: an encoder E with parameters ?, a task classifier C with parameters ? C , and a set of H-divergence estimators D k with parameters ? k , k ? [N S ]. Intuitively, E attempts to minimize a classification loss L C (?; ? C ) (standard cross-entropy in our case) and empirical H-divergences, which is achieved through the maximization of domain discrimination losses, denominated L k . Each domain discriminator, on the other hand, aims at minimizing L k . The procedure for estimating ?, ? T , and all ? k 's can be thus formulated as the following multiplayer minimax game:</p><formula xml:id="formula_14">min ?,? C max ?1,...,? N S L C (C(E(x; ?); ? C ), y C ) ? N S k=1 L k (D k (E(x; ?); ? k ), y k ),<label>(9)</label></formula><p>where y C corresponds to the task label for the example x, and y k is equal to 1 in case x ? D k S , or 0 otherwise. Training is carried out with alternate updates. A pseudocode describing the training procedure is presented in Algorithm 1. To further illustrate our proposed approach, we provide in <ref type="figure" target="#fig_4">Figure 2</ref>  </p><formula xml:id="formula_15">(? D ), scaling (?), mini- batch size (m). 2: Initialize ?, ? C , ? 1 , . . . , ? N S as ? 0 , ? 0 C , ? 0 1 , . . . , ? 0 N S . 3: for t = 1, . . . , number of iterations do 4: Sample one mini-batch from each source domain {(x i 1 , y i C , y i 1 , . . . , y i N S )} m i=1 5:</formula><p># Update domain discriminators <ref type="bibr">6:</ref> for k = 1, . . . , N S do 7:</p><formula xml:id="formula_16">? t k ? ? t?1 k + ? D N S ?m N S ?m i=1 ? ? k L k (D k (E(x i ; ? t?1 ); ? t?1 k ), y i k ) 8:</formula><p>end for 9:</p><p># Update task classifier 10:</p><formula xml:id="formula_17">? t C ? ? t?1 C + ? C N S ?m N S ?m i=1 ? ? C L C (C(E(x i ; ? t?1 ); ? t?1 C ), y i C ) 11: # Update encoder 12: ? t ? ? t?1 + ? C N S ?m ( N S ?m i=1 ?? ? L C (C(E(x i ; ? t?1 ); ? t?1 C ), y i C ) 13: ?(1 ? ?)? ? k L k (D k (E(x i ; ? t?1 ); ? t k ), y i k )) 14: end for</formula><p>Improving training stability: Previous work on domain adaptation/generalization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> proposed solving the problem stated in (9) using a gradient reversal layer <ref type="bibr" target="#b27">[28]</ref>. We empirically observed such approach to be heavily dependent on the choice of hyperparameters in order for training to converge. We propose to augment the described adversarial approach using strategies originally utilized for stabilizing the training of generative adversarial networks with multiple discriminators <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. Namely, we include a random projection layer in the input of each domain discriminator with the goal of making examples from different distributions harder to be distinguished. In addition, we use the negative log-hypervolume instead of the summation in the game represented in <ref type="bibr" target="#b8">(9)</ref> in order to assign more preference to solutions which decrease all pairwise divergences uniformly even in cases where there is a trade-off in their minimization. We refer to the proposed approach as G2DM (Generalizing to unseen Domains via Distribution Matching).</p><p>Differences to multi-source domain adaptation: We further remark the differences between G2DM and previous adversarial approaches which are often employed in domain adaptation. Essentially, G2DM compares examples only from source domains to learn domain-agnostic representations, i.e., there is no notion of target distribution. Other settings such as <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> are more restricted in that a particular distribution is targeted and data from that distribution is required, besides the source data we use in our case. Moreover, those approaches do not aim at matching source distributions and only consider H-divergences computed between each source domain and the given target. In the case of G2DM, on the other hand, the goal is to match source domain distributions to decrease , and thus only pairwise discrepancies between training domains are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup and Results</head><p>We design our empirical evaluation to validate G2DM in conditions where different assumptions are satisfied. In the first scenario, we chose experimental conditions such that the covariate shift assumption holds. For that, we employ G2DM on object recognition tasks. In this case, we aim to answer the following research questions: i) Can G2DM perform better than standard ERM under i.i.d. assumptions by using information of source domains only? ii) Where does G2DM performance stand in comparison to previously proposed domain generalization strategies? iii) Is G2DM indeed enforcing distribution matching across source and unseen domains? And iv) What is the effect on the resulting performance given by different access models to test distributions during training?</p><p>We then evaluate whether G2DM is able to attain good out-of-domain performance even in the challenging scenario where the covariate shift assumption is likely to be violated. For that, we consider a real-world task that involves classifying electroencephalography (EEG) time series for affective state prediction, a burgeoning area within the human-machine systems field. In applications involving EEG data, subjects are often considered as distinct domains with different labeling functions <ref type="bibr" target="#b32">[33]</ref>. Such shift can be attributed to environmental and anatomic factors such as device placement and scalp characteristics <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation under covariate shift</head><p>The VLCS benchmark <ref type="bibr" target="#b35">[36]</ref> is composed by examples from five overlapping classes from the VOC2007 <ref type="bibr" target="#b36">[37]</ref>, LabelMe <ref type="bibr" target="#b37">[38]</ref>, Caltech-101 <ref type="bibr" target="#b38">[39]</ref>, and SUN <ref type="bibr" target="#b39">[40]</ref> datasets. PACS <ref type="bibr" target="#b19">[20]</ref>, in turn, consists of images distributed into seven classes from four different datasets: photo (P), art painting (A), cartoon (C), and sketch (S). We compare the performance of our proposed approach with a model trained with no mechanism to enforce domain generalization (referred to as ERM throughout this Section). Moreover, we consider the recently introduced invariant risk minimization (IRM) strategy <ref type="bibr" target="#b10">[11]</ref> and include results reported in the literature achieved by Epi-FCR <ref type="bibr" target="#b18">[19]</ref>, JiGen <ref type="bibr" target="#b7">[8]</ref> along with the ERM results they provided (referred to as ERM-JiGen), and MMD-AAE <ref type="bibr" target="#b14">[15]</ref>. Finally, the adaptation of DANN for domain generalization reported in <ref type="bibr" target="#b18">[19]</ref> was also considered. All such methods have as encoder the convolutional stack of AlexNet <ref type="bibr" target="#b40">[41]</ref>. Further implementation details can be found in the Supplementary material.</p><p>In <ref type="table" target="#tab_0">Tables 1 and 2</ref>, we report the average best accuracy across three runs with different random seeds on the test partition of the unseen domain under a leave-one-domain-out validation scheme. Results show that G2DM outperforms ERM in terms of average performance across the unseen domains for both benchmarks, and supports the claim that leveraging source domain information as done by G2DM provides an improvement on generalization to unseen distributions in comparison to simply considering the i.i.d. requirement is satisfied. G2DM further presented better average performance when compared to our implementation of IRM, as well as results from other methods previously reported in the literature. We finally highlight that G2DM showed an improvement in performance in more challenging domains <ref type="bibr" target="#b19">[20]</ref>, such as LabelMe and Sketch.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Checking H-divergences across sources and unseen domains</head><p>We now investigate whether cross-domain H-divergences are being in fact reduced by G2DM. We use ERM as a baseline as it does not include any mechanism to enforce distribution matching. We estimate H-divergences by computing the proxy pairwise A-distance <ref type="bibr" target="#b1">[2]</ref> for each pair of domains on the PACS benchmark. Classifiers are trained on top of the representations Z obtained with ERM and G2DM. We show in Figures 1 the differences in estimated discrepancies between ERM and G2DM for each unseen domain. Each entry corresponds to a pair of domains indicated in the row and the column and positive values indicate that G2DM decreased the corresponding pairwise A-distance in comparison to ERM. Notice that the diagonals are left blank as we do not compute the classification accuracy between the same domains.</p><p>We observe that, apart from the case where 'photo' is the test domain, G2DM was in fact able to better match most of the source distributions, thus yielding smaller which favours generalization as predicted by Theorem 1. Notably, we highlight that although our proposed approach has no access to data from the unseen domain at training time and, therefore, does not directly implement a strategy to decrease the divergence between the unseen domain and the convex hull of the sources (i.e. ?), the results presented in <ref type="figure" target="#fig_0">Figure 1</ref> show that the estimated pairwise H-divergence between the unseen domain and sources also decreased in most of the considered cases.</p><p>In fact, the only mechanism the encoder has in order to reduce corresponds to learning how to filter domain information from the data, in the sense that once samples from two distinct distributions are encoded, one cannot distinguish from which distribution each sample came from. Observed results thus suggest that such encoder also removes domain information from the unseen distributions observed at test time, preventing the learning algorithm to yield a high ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">The effect of different access methods to test data during training</head><p>Results of previous experiments correspond to an optimistic scenario where data from the unseen domain is made available for selecting the best performing model. This is not the case in practice since varying unseen distributions might appear. In <ref type="table" target="#tab_2">Table 3</ref>, we compare results obtained further considering different access methods to the test data. Namely, we consider the case where no access to the unseen distribution is allowed and only source data can be used  in order to define stopping criteria. In such cases, both validation accuracy and training loss computed on left-out in-domain data are employed (referred in <ref type="table" target="#tab_2">Table 3</ref> as source accuracy and source loss, respectively). Moreover, as a reference of performance, we further report the accuracy achieved assuming access to unseen domain data during training in order to select the best model (referred in <ref type="table" target="#tab_2">Table 3</ref> as unseen accuracy). For comparison, we further present the performance reported by <ref type="bibr" target="#b9">[10]</ref> for CIDDG, since a stopping criterion using solely data from source domains was employed in that case. We observe that, when using the training loss as stopping criterion, our strategy outperforms CIDDG for almost all domains, while the baseline performance severely degrades when 'sketch' is the unseen domain.</p><p>As an alternative to AlexNet, we further evaluate the performance of the proposed approach using the convolutional stack of a ResNet-18 <ref type="bibr" target="#b41">[42]</ref>, since it has shown promising results in recent work <ref type="bibr" target="#b7">[8]</ref>. We compare our approach with JiGen 5 adopting the same previously-discussed test data access methods for both approaches. We further report in <ref type="table" target="#tab_2">Table 3</ref> the performance obtained by JiGen as reported in <ref type="bibr" target="#b7">[8]</ref> although it is unclear which stopping criteria were adopted for that case. We observe that replacing AlexNet by ResNet-18 yields a more stable average performance across stopping criteria. Based on the results obtained with AlexNet, we remark that results might be too optimistic/pessimistic depending on the assumed access method to unseen distributions, and as such, in order to allow fair comparison between different approaches, the performance across different access methods should be reported. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation beyond the covariate shift assumption</head><p>We proceed to evaluate G2DM on a unfavorable scenario where the covariate shift is unlikely to hold. The goal of the selected task is to perform affective state estimation with three classes (positive, neutral, or negative) based on EEG signals from the SEED dataset <ref type="bibr" target="#b42">[43]</ref> collected from 15 subjects. We use the architecture described in <ref type="bibr" target="#b43">[44]</ref> for both G2DM and ERM. For each subject left out for testing, we use 10 out of the remaining 14 domains for training and use the other 4 as validation data.</p><p>We report in <ref type="table" target="#tab_3">Table 4</ref> the classification accuracy (%) averaged across all unseen subjects and three independent training runs. Under source data validation, the reported performance was computed on the epoch of highest accuracy on the  <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> 50.28 DANN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b45">46]</ref> 55.87 MDAN <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref> 56.65 MDMN <ref type="bibr" target="#b45">[46]</ref> 60.59 validation partition. The results under semi-privileged were obtained on the epoch of highest accuracy on the unseen subject data. The comparison between G2DM and ERM shows that even in this challenging case where the mismatch between labeling functions is not negligible, G2DM is able to successfully leverage the available domain information (which in this case comes with no additional effort at the data collection) and presents an improvement of more than 3.4% in accuracy in comparison to ERM in both considered scenarios for the DG setting.</p><p>Comparison with domain adaptation strategies: We further report in <ref type="table" target="#tab_3">Table 4</ref> results obtained by domain adaptation strategies (DA). Such methods, reported in <ref type="table" target="#tab_3">Table 4</ref> under privileged baselines, are privileged in the sense that unlabeled data belonging to the unseen domain (unknown in our case) is used to adapt representations at training time in order to yield subject-specific models. When comparing the DA strategies with our proposed domain generalization (DG) approach, we remark that DG strategies aim to obtain domain-agnostic models, as opposed to DA methods which target a specific distribution. As such, one would expect DA approaches to achieve better performance than DG. However, we observe G2DM's performance to be on par with, or even better than, some of the considered DA strategies. We conjecture a larger number of source domains available at training time would decrease the gap between DG and DA even further; i.e., it would be more likely that unseen domains are exactly represented in the convex hull of the sources yielding low ? (c.f. Theorem 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we tackled the domain generalization problem and showed that generalization can be achieved in the neighborhood of the set of mixtures of distributions observed during training. Based on this result, we introduced G2DM, an efficient approach in yielding invariant representations across unseen distributions. Our method employs multiple one-vs-all domain discriminators, such that pairwise divergences between source distributions are estimated and minimized at training time. We provide empirical evidence supporting the claim that making use of domain information improves performance relative to standard settings relying on i.i.d. requirements. Moreover, the introduced approach outperformed recent methods which also leverage domain labels. We further showed that our proposed method resulted in strong results on a realistic setting, with performance comparable to privileged systems tailored to test distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A Proof of Lemma 1</p><formula xml:id="formula_18">Lemma 1. Let d H [D i S , D k S ] ? , ? i, k ? [N S ].</formula><p>The following inequality holds for the H-divergence between any pair of domains D , D ? ? 2 S :</p><formula xml:id="formula_19">d H [D , D ] ? .<label>(10)</label></formula><p>Proof. Consider two unseen domains, D U and D U on the convex-hull ? S of N S source domains with support ?.</p><formula xml:id="formula_20">Consider also D U (?) = N S k=1 ? k D k S (?) and D U (?) = N S l=1 ? l D l S (?)</formula><p>The H-divergence between D U and D U can be written as:</p><formula xml:id="formula_21">d H [D U , D U ] =2 sup h?H |Pr x?D U [h(x) = 1] ? Pr x?D U [h(x) = 1]|, =2 sup h?H |E x?D U [I(h(x))] ? E x?D U [I(h(x))]|, =2 sup h?H ? D U (x)I(h(x))dx ? ? D U (x)I(h(x))dx , =2 sup h?H ? N S k=1 ? k D k S (x)I(h(x))dx ? ? N S l=1 ? l D l S (x)I(h(x))dx , =2 sup h?H ? N S l=1 N S k=1 ? l ? k D k S (x)I(h(x))dx ? ? N S l=1 N S k=1 ? l ? k D l S (x)I(h(x))dx , =2 sup h?H N S l=1 N S k=1 ? l ? k ? D k S (x)I(h(x))dx ? ? D l S (x)I(h(x))dx .<label>(11)</label></formula><p>Using the triangle inequality, we can write:</p><formula xml:id="formula_22">d H [D U , D U ] ? 2 sup h?H N S l=1 N S k=1 ? l ? k ? D k S (x)I(h(x))dx ? ? D l S (x)I(h(x))dx .<label>(12)</label></formula><p>Finally, using the sub-additivity of the sup: </p><formula xml:id="formula_23">d H [D U , D U ] ? N S l=1 N S k=1 ? l ? k 2 sup h?H ? D k S (x)I(h(x))dx ? ? D l S (x)I(h(x))dx , = N S l=1 N S k=1 ? l ? k d H [D k S , D l S ].<label>(13)</label></formula><formula xml:id="formula_24">Given d H [D k S , D l S ] ? ? k, l ? [N S ]: d H [D U , D U ] ? .</formula><formula xml:id="formula_25">R U [h] ? N S i=1 ? i R i S [h] + ? + + min{ED U [|f S? ? f U |], E D U [|f U ? f S? |]},<label>(14)</label></formula><p>where is the highest pairwiseH-divergence measured between pairs of domains within S,</p><formula xml:id="formula_26">H = {sign(|h(x) ? h (x)| ? t)|h, h ? H, 0 ? t ? 1} and f S? (x) = N S</formula><p>i=i ? i f Si (x) is the labeling function for any x ? Supp(D U ) resulting from combining all f Si with weights ? i , i ? [N S ], determined byD U .</p><p>Proof of Theorem 1. Let the source and target domains be D S , f S and D T , f T , respectively. For the single-source, single-target domain adaptation case, it was previously shown that the risk of any h ? H, h : X ? [0, 1] is bounded by <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_27">R T [h] ? R S [h] + dH[D S , D T ] + min{E D S [|f S ? f T |], E D T [|f T ? f S |]},<label>(15)</label></formula><formula xml:id="formula_28">whereH = {sign(|h(x) ? h (x)| ? t)|h, h ? H, 0 ? t ? 1}.</formula><p>In order to devise a generalization bound for the risk on any unseen domain in terms of quantities related to the distributions seen at training time, we start by writing <ref type="bibr" target="#b14">(15)</ref> considering D U and its "projection" onto the convexhull of the sourcesD U = argmin ?1,...,? N S d H D U ,</p><formula xml:id="formula_29">N S i=1 ? i D i S .</formula><p>For that, we introduce the labeling function</p><formula xml:id="formula_30">f S? (x) = N S i=i ? i f Si (x)</formula><p>, which is an ensemble of the respective labeling functions from each source domain weighted by the mixture coefficients that determineD U . R U [h] can thus be bounded as:</p><formula xml:id="formula_31">R U [h] ? R? [h] + dH[D U , D U ] + min{ED U [|f S? ? f U |], E D U [|f U ? f S? |]}.</formula><p>Similarly to the proof of our Lemma 1 for the case where D = D U and D =D U (and to <ref type="bibr" target="#b23">[24]</ref>), it follows that:</p><formula xml:id="formula_32">R U [h] ? N S i=1 ? i R i S [h] + N S i=1 ? i dH[D i S , D U ] + min{ED U [|f S? ? f U |], E D U [|f U ? f S? |]}.<label>(16)</label></formula><p>Using the triangle inequality for the H-divergence along with Lemma 1, we can bound theH-divergence between D U and any source domain D i S , dH[D U , D i S ], according to:</p><formula xml:id="formula_33">dH[D U , D i S ] ? dH[D U ,D U ] + dH[D U , D i S ], ? ? + , where ? = dH[D U ,D U ].</formula><p>Using this result, we can now upper-bound</p><formula xml:id="formula_34">N S i=1 ? i dH[D U , D i S ]</formula><p>by ? + and finally re-write (16) as: </p><formula xml:id="formula_35">R U [h] ? N S i=1 ? i R i S [h] + ? + + min{ED U [|f S? ? f U |], E D U [|f U ? f S? |]}.</formula><formula xml:id="formula_36">R U [h] ? N S i=1 ? i R i S [h] + .<label>(17)</label></formula><p>Proof of Corollary 1. The right-most term of <ref type="bibr" target="#b13">(14)</ref> accounts for the mismatch between the labeling functions of D U and D U . Since all domains within D have the same labeling function, this term is equal to 0. As D U ? ? S , D U =D U , which results in dH[D U ,D U ] = ? = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D One-vs-all H-divergence estimation</head><p>We illustrate the estimation of H-divergences using one-vs-all discriminators by considering an example in which 3 source domains are available. </p><formula xml:id="formula_37">L 1 = 1 3M 3M i=1 (D 1 (x i ), y 1 ), = 1 M M i=1 (D 1 (x i ), y 1 ) + 1 M 2M i=M +1 (D 1 (x i ), y 1 ) + 1 M 3M i=2M +1 (D 1 (x i ), y 1 ),<label>(18)</label></formula><p>where represents a loss function (e.g. 0-1 loss) and each term accounts for the loss provided by examples from one domain. Splitting the first term in two parts and replacing the domain labels y 1 by their corresponding values, we obtain:   In this experiment, we verify whether removing examples from one source domain impacts the performance on the target domain. We evaluate each target domain on models trained using all possible combinations of the remaining domains as sources. The ERM baseline is also included for reference. Results presented in <ref type="table" target="#tab_7">Table 5</ref> show that for all unseen domains, decreasing the number of source domains from 3 (see <ref type="table" target="#tab_0">Table 1</ref>) to 2 hurt the classification performance for almost all combinations of source domains. We notice that in some cases, excluding a particular source from the training severely decreases the target loss. As an example, for the Caltech-101, excluding from training examples from the VOC dataset decreased the accuracy in more than 10% for the proposed approach, as well as for ERM.</p><formula xml:id="formula_38">L 1 = 1 M M/2 i=1 (D 1 (x i ), 1) + 1 M 2M i=M +1 (D 1 (x i ), 0) + 1 M M i= M 2 +1 (D 1 (x i ), 1) + 1 M 3M i=2M +1 (D 1 (x i ), 0).<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Effect of random projection size</head><p>We further investigate the effectiveness on providing a more stable training of the random projection layer in the input of each discriminator. For that, we run experiments with 7 different projection sizes, as well as directly using the output of the feature extractor model. Besides the random projection size, we use the same hyperparameters values (the same used in the previous experiment) and initialization for all models. We report in <ref type="figure" target="#fig_6">Figure 3</ref> the best target accuracy achieved with all random projection sizes on the PACS benchmark considering the Sketch dataset as unseen domain. Overall, we observed that the random projection layer has indeed an impact on the generalization of the learned representation and that the best result was achieved with a size equal to 1000. Moreover, we notice that, in this case, having a smaller (500) random projection layer is less hurtful for the performance than using a larger one. We also found that removing the random projection layer did not allow the training to converge with this experimental setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Domain generalization benchmarks</head><p>The VLCS benchmark is composed by 4 datasets with 5 common classes, namely, bird, car, chair, dog, and person. The number of data points per dataset is detailed as follows. We split each dataset in 80%/20% train/test partitions.</p><p>? Pascal VOC2007: 3376;</p><p>? LabelMe: 2656;</p><p>? Caltech-101: 1415;</p><p>? SUN09: 3282.</p><p>The PACS benchmark is composed by 4 datasets with 7 common classes, namely, dog, elephant, giraffe, guitar, horse, house, and person. The number of data points per dataset is detailed as follows. We use the original train/validation partitions provided by the benchmark authors. In order to obtain a consistent comparison with the aforementioned baseline models, we follow previous work and employ the weights of a pre-trained AlexNet <ref type="bibr" target="#b40">[41]</ref> and ResNet-18 <ref type="bibr" target="#b41">[42]</ref> as the initialization for the feature extractor model on the experiments. The last layer is discarded and the representation of size 4096 for AlexNet and 512 for ResNet-18 is used as input for the task classifier and the domain discriminators. The domain discriminator architecture with AlexNet, consists of a four-layer fully-connected neural network of size 4096 ? random projection size ? 1024 ? 1 and five-layer fully connected network of size 512 ? random projection size ? 512 ? 256 ? 1 for ResNet-18. The random projection layer is implemented as a linear layer with weights normalized to have unitary L2-norm. The task classifier is a one-layer fully-connected network of size 4096 ? number of classes in the case of AlexNet and 512 ? number of classes in the case of ResNet. Following previous work on domain generalization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref>, we use models pre-trained on the ILSVRC dataset <ref type="bibr" target="#b46">[47]</ref> as initialization. For fair comparison, all models we implemented were given a budget of 200 epochs. We use label smoothing <ref type="bibr" target="#b47">[48]</ref> on the task classifier in order to prevent overfitting. Models were trained using SGD with Polyak's acceleration. One epoch corresponds to the length of the largest source domain training sample. The learning rate was "warmed-up" for a number of training iterations equal to nw. Hyperparameter tuning was performed through random search over a pre-defined grid so as to find the best values for the learning rate (lr), momentum, weight decay, label smoothing parameter ls, nw, random projection size 6 , learning rate reduction factor, and weighting (?). Each model was run with three different initializations (random seeds 1, 10, and 100 selected a priori) and the average best accuracy on the test partition of the target domain is reported. Details of the hyperparameters grid used in the search are provided in the Supplementary material. For our ERM we used the same hyperparameters as in <ref type="bibr" target="#b7">[8]</ref>, while for IRM we employed the same hyperparameter values reported in the authors implementation of the colored MNIST experiments.</p><p>The grids used on the hyperparameter search for each hyperparameter are presented in the following. A budget of 200 runs was considered and for each combination of hyperparameters each model was trained for 200 and 30 epochs in the case of AlexNet and ResNet-18, respectively. The best hyperparamters values for AlexNet on PACS and VLCS benchmarks are respectively denoted by * , ? . For the ResNet-18 experiments on PACS we indicate the hyperparameters by + . Moreover, in the case of ResNet-18, we aggregated the discriminators losses by computing the corresponding hypervolume as in <ref type="bibr" target="#b29">[30]</ref>, with a nadir slack equal to 2.5. All experiments were run considering a minibatch size of 64 (training each iteration took into account 64 examples from each source domain) on single GPU hardware (either an NVIDIA V100 or NVIDIA GeForce GTX 1080Ti).</p><p>? Learning rate for the task classifier and feature extractor: {0.01 * ,+ , 0.001 ? , 0.0005};</p><p>? Learning for the domain classifiers: {0.0005 * , 0.001, 0.005 ?,+ };</p><p>? Weight decay: {0.0005 * , 0.001, 0.005 ?+ }; ? Random projection size: {1000 * , 3000, 3500 ? , None + };</p><p>? Task classifier and feature extractor learning rate warm-up iterations: {1, 300 * , ? , 500 + };</p><p>? Warming-up threshold: {0.00001 * , 0.0001 ?,+ , 0.001};</p><p>? Learning rate schedule patience: {25 + , 60 ? , 80 * };</p><p>? Learning rate schedule decay factor: {0.1 + , 0.3 ? , 0.5 * }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Affective state prediction</head><p>We use SyncNet <ref type="bibr" target="#b43">[44]</ref> as the encoder for the experiments with the SEED dataset. We follow previous work and apply a simple pre-processing that consists of clipping artifacts with amplitude 5 times higher than the mean of the channel signal and windowing data with chunks of 60 seconds. Each window was normalized to have zero mean and unit variance. For the encoder network, we adopt an one layer parameterized convolutional filter with 2 filters (designed to extract synchrony coherence which interpretable features based on the previous neuroscience literature <ref type="bibr" target="#b43">[44]</ref>). We train all models for 100 epochs using SGD with Polyak's acceleration. The learning rate was "warmed-up" for a number of training iterations equal to 500.</p><p>The output of the encoder with size 602 is used as input for the task classifier and the domain discriminators. The domain discriminator architecture consists of a four-layer fully-connected neural network of size 602 ? random projection size ? 256 ? 128 ? 2. The random projection layer is implemented as a linear layer with weights normalized to have unitary L2-norm. The task classifier is a two-layer fully-connected network of size 602 ? 100 ? number of classes.</p><p>The summary of parameters is presented in the following.</p><p>? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 Proxy A-distance estimation</head><p>We implement the domain discriminators using tree ensemble classifiers with 100 estimators. We thus report the average classification accuracy using 5-fold cross-validation independently run for each domain pair. Each domain is represented by a random sample of size 500.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>(in the Supplementary material) a diagram showing the main components of the model in a case where three domains are available at training time. Generalizing to unseen Domains via Distribution Matching 1: Requires: classifier and encoder learning rate (? C ), domain discriminators learning rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Differences between estimated pairwise H-divergences under ERM and G2DM on PACS (captions denote unseen domains). Higher values indicate that G2DM better matched domains. Overall, G2DM is able to decrease pairwise discrepancies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>B Proof of Theorem 1 Theorem 1 .</head><label>11</label><figDesc>Let S be the set of source domains and Y = [0, 1]. The risk R U [h], ?h ? H, for any unseen domain D U such that d H [D U , D U ] = ?, is bounded as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Proposed approach illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F Extra experiments F. 1</head><label>experiments1</label><figDesc>Impact of source domains diversity on unseen domain accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy obtained on the PACS benchmark using Sketch as target domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>?</head><label></label><figDesc>Momentum: {0.5, 0.9 * , ?,+ } ? Label smoothing: {0.0 + , 0.1, 0.2 * , ? }; ? Losses weighting (?): {0.35, 0.8 * , ?,+ };</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy (%) on VLCS datasets for models trained with leave-one-domain-out validation.</figDesc><table><row><cell>Unseen domain (?)</cell><cell>V</cell><cell>L</cell><cell>C</cell><cell>S</cell><cell>Average</cell></row><row><cell>DANN</cell><cell cols="4">66.40 64.00 92.60 63.60</cell><cell>71.70</cell></row><row><cell>MMD-AAE</cell><cell cols="4">67.70 62.60 94.40 64.40</cell><cell>72.28</cell></row><row><cell>Epi-FCR</cell><cell cols="4">67.10 64.30 94.10 65.90</cell><cell>72.90</cell></row><row><cell>JiGen</cell><cell cols="4">70.62 60.90 96.93 64.30</cell><cell>73.19</cell></row><row><cell>ERM -JiGen</cell><cell cols="4">71.96 59.18 96.93 62.57</cell><cell>72.66</cell></row><row><cell>IRM</cell><cell cols="4">72.16 62.36 98.35 67.82</cell><cell>75.17</cell></row><row><cell>ERM</cell><cell cols="4">73.44 60.44 97.88 67.92</cell><cell>74.92</cell></row><row><cell>G2DM</cell><cell cols="4">71.14 67.63 95.52 69.37</cell><cell>75.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Unseen domain (?)</cell><cell>P</cell><cell>A</cell><cell>C</cell><cell>S</cell><cell>Average</cell></row><row><cell>DANN</cell><cell cols="4">88.10 63.20 67.50 57.00</cell><cell>69.00</cell></row><row><cell>Epi-FCR</cell><cell cols="4">86.10 64.70 72.30 65.00</cell><cell>72.00</cell></row><row><cell>JiGen</cell><cell cols="4">89.00 67.63 71.71 65.18</cell><cell>73.38</cell></row><row><cell>ERM -JiGen</cell><cell cols="4">89.98 66.68 69.41 60.02</cell><cell>71.52</cell></row><row><cell>IRM</cell><cell cols="4">89.97 64.84 71.16 63.63</cell><cell>72.39</cell></row><row><cell>ERM</cell><cell cols="4">90.02 64.86 70.18 61.40</cell><cell>71.61</cell></row><row><cell>G2DM</cell><cell cols="4">88.12 66.60 73.36 66.19</cell><cell>73.55</cell></row></table><note>Classification accuracy (%) on PACS datasets for models trained with leave-one-domain-out validation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%) on PACS with different stopping criteria.</figDesc><table><row><cell>Method</cell><cell>Criterion</cell><cell>P</cell><cell>A</cell><cell>C</cell><cell>S</cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell>AlexNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIDDG [10]</cell><cell>From [10]</cell><cell cols="4">78.65 62.70 69.73 64.45</cell><cell>68.88</cell></row><row><cell></cell><cell cols="5">Source acc. 85.33 57.76 69.71 49.45</cell><cell>65.56</cell></row><row><cell>G2DM</cell><cell cols="5">Source loss 87.37 66.70 70.26 50.98</cell><cell>68.82</cell></row><row><cell></cell><cell cols="5">Unseen acc. 88.80 66.70 73.29 65.03</cell><cell>73.45</cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet-18</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Source acc. 95.83 78.52 73.31 69.14</cell><cell>79.20</cell></row><row><cell>JiGen [8]</cell><cell cols="5">Source loss 95.83 78.89 73.32 70.73 Unseen acc. 96.11 79.56 74.25 71.00</cell><cell>79.69 80.23</cell></row><row><cell></cell><cell>From [8]</cell><cell cols="4">96.03 79.42 75.25 71.35</cell><cell>80.51</cell></row><row><cell></cell><cell cols="5">Source acc. 93.70 79.22 76.34 75.14</cell><cell>81.10</cell></row><row><cell>G2DM</cell><cell cols="5">Source loss 93.75 77.78 75.54 77.58</cell><cell>81.16</cell></row><row><cell></cell><cell cols="5">Unseen acc. 94.63 81.44 79.35 79.52</cell><cell>83.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Average accuracy (%) on the SEED dataset across 15 subjects. Semi-privileged approaches correspond to the best performing model under the domain generalization setting. Privileged baselines (domain adaptation setting) have access to unseen domain data at training time.</figDesc><table><row><cell>Setting</cell><cell>Method</cell><cell>Average accuracy (%)</cell></row><row><cell></cell><cell cols="2">Source data validation</cell></row><row><cell></cell><cell>ERM</cell><cell>51.98</cell></row><row><cell>Domain generalization</cell><cell>G2DM</cell><cell>55.77</cell></row><row><cell></cell><cell cols="2">Semi-privileged</cell></row><row><cell></cell><cell>ERM</cell><cell>56.82</cell></row><row><cell></cell><cell>G2DM</cell><cell>60.26</cell></row><row><cell></cell><cell>Privileged baselines</cell><cell></cell></row><row><cell></cell><cell>DAN</cell><cell></cell></row><row><cell>Domain adaptation</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Let all domains within the the support of the meta-distribution D have labeling function f . Let S be set of source domains and its convex-hull be denoted as ? S . The risk R U [h] of a hypothesis h on an unseen domain D U ? ? S , is upper-bounded by:</figDesc><table><row><cell>C Proof of Corollary 1</cell></row><row><cell>Corollary 1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Consider samples of size M from N S = 3 source domains which are available at training time. The loss L 1 for the domain discriminator D 1 accounting for estimating d H [D 1 , D 2 ] and d H [D 1 , D 3 ] can be written as:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The first two terms from Eq.19 account for d H [D 1 , D 2 ] and the last two terms account for d H [D 1 , D 3 ].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Impact of decreasing the number of source domains on VLCS. Rows represent the two source domains used.</figDesc><table><row><cell>Source</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">i.e., the set of all mixtures obtained from given distributions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For a fixed h, one can always define a distribution yielding high risk.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Results are generated using JiGen authors' source code (https://github.com/fmcarlucci/JigenDG).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The option of not having the random projection layer is included in the grid search.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tutorial on practical prediction theory for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="273" to="306" />
			<date type="published" when="2005-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Dx7fbCW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving out-of-distribution generalization via multi-task self-supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13525</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="624" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Domain generalization using causal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tople</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07500</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Invariant risk minimization games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhurandhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04692</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting change in data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth international conference on Very large data bases</title>
		<meeting>the Thirtieth international conference on Very large data bases</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using metaregularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6447" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00113</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-domain adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schoenauer-Sebag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schoenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Altschuler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09239</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-domain learning by confidence-weighted parameter combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="123" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8559" to="8570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On learning invariant representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Des Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7523" to="7532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Impossibility theorems for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>P?l</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Algorithms and theory for multiple-source adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8246" to="8256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Stabilizing gan training with multiple random projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07831</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-objective training of generative adversarial networks with multiple discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Considine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey of multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-subject statistical shift estimation for generalized electroencephalography-based mental workload assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rosanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Gagnon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3647" to="3653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reducing bci calibration effort in rsvp tasks using online weighted adaptation regularization with source domain selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Lawhern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Lance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Affective Computing and Intelligent Interaction (ACII)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A subject-transfer framework for obviating inter-and intra-subject variability in eeg-based drowsiness detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="407" to="419" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Labelme: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploiting hierarchical context on a large database of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="162" to="175" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Targeting eeg/lfp synchrony with neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dzirasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4620" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Extracting relationships by multi-domain matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6798" to="6809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
