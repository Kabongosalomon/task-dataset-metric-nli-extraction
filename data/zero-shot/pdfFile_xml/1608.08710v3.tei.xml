<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2017 PRUNING FILTERS FOR EFFICIENT CONVNETS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<email>haoli@cs.umd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
							<email>igord@nec-labs.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Labs America</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">NEC Labs America</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">NEC Labs America</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2017 PRUNING FILTERS FOR EFFICIENT CONVNETS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The ImageNet challenge has led to significant advancements in exploring various architectural choices in CNNs <ref type="bibr" target="#b23">(Russakovsky et al. (2015)</ref>; <ref type="bibr" target="#b13">Krizhevsky et al. (2012)</ref>; <ref type="bibr" target="#b24">Simonyan &amp; Zisserman (2015)</ref>; <ref type="bibr" target="#b27">Szegedy et al. (2015a)</ref>; <ref type="bibr" target="#b8">He et al. (2016)</ref>). The general trend since the past few years has been that the networks have grown deeper, with an overall increase in the number of parameters and convolution operations. These high capacity networks have significant inference costs especially when used with embedded sensors or mobile devices where computational and power resources may be limited. For these applications, in addition to accuracy, computational efficiency and small network sizes are crucial enabling factors <ref type="bibr" target="#b28">(Szegedy et al. (2015b)</ref>). In addition, for web services that provide image search and image classification APIs that operate on a time budget often serving hundreds of thousands of images per second, benefit significantly from lower inference times.</p><p>There has been a significant amount of work on reducing the storage and computation costs by model compression <ref type="bibr" target="#b15">(Le Cun et al. (1989)</ref>; <ref type="bibr" target="#b6">Hassibi &amp; Stork (1993)</ref>; <ref type="bibr" target="#b25">Srinivas &amp; Babu (2015)</ref>; <ref type="bibr" target="#b3">Han et al. (2015)</ref>; <ref type="bibr" target="#b19">Mariet &amp; Sra (2016)</ref>). Recently <ref type="bibr" target="#b3">Han et al. (2015;</ref><ref type="bibr" target="#b5">2016b</ref>) report impressive compression rates on AlexNet <ref type="bibr" target="#b13">(Krizhevsky et al. (2012)</ref>) and VGGNet <ref type="bibr" target="#b24">(Simonyan &amp; Zisserman (2015)</ref>) by pruning weights with small magnitudes and then retraining without hurting the overall accuracy. However, pruning parameters does not necessarily reduce the computation time since the majority of the parameters removed are from the fully connected layers where the computation cost is low, e.g., the fully connected layers of VGG-16 occupy 90% of the total parameters but only contribute less than 1% of the overall floating point operations <ref type="bibr">(FLOP)</ref>. They also demonstrate that the convolutional layers can be compressed and accelerated <ref type="bibr" target="#b9">(Iandola et al. (2016)</ref>), but additionally require sparse BLAS libraries or even specialized hardware <ref type="bibr" target="#b4">(Han et al. (2016a)</ref>). Modern libraries that provide speedup using sparse operations over CNNs are often limited <ref type="bibr" target="#b27">(Szegedy et al. (2015a)</ref>; <ref type="bibr" target="#b18">Liu et al. (2015)</ref>) and maintaining sparse data structures also creates an additional storage overhead which can be significant for low-precision weights.</p><p>Recent work on CNNs have yielded deep architectures with more efficient design <ref type="bibr" target="#b27">(Szegedy et al. (2015a;</ref><ref type="bibr">b)</ref>; <ref type="bibr" target="#b7">He &amp; Sun (2015)</ref>; <ref type="bibr" target="#b8">He et al. (2016)</ref>), in which the fully connected layers are replaced with average pooling layers <ref type="bibr" target="#b17">(Lin et al. (2013)</ref>; <ref type="bibr" target="#b8">He et al. (2016)</ref>), which reduces the number of parameters significantly. The computation cost is also reduced by downsampling the image at an early stage to reduce the size of feature maps <ref type="bibr" target="#b7">(He &amp; Sun (2015)</ref>). Nevertheless, as the networks continue to become deeper, the computation costs of convolutional layers continue to dominate.</p><p>CNNs with large capacity usually have significant redundancy among different filters and feature channels. In this work, we focus on reducing the computation cost of well-trained CNNs by pruning filters. Compared to pruning weights across the network, filter pruning is a naturally structured way of pruning without introducing sparsity and therefore does not require using sparse libraries or any specialized hardware. The number of pruned filters correlates directly with acceleration by reducing the number of matrix multiplications, which is easy to tune for a target speedup. In addition, instead of layer-wise iterative fine-tuning (retraining), we adopt a one-shot pruning and retraining strategy to save retraining time for pruning filters across multiple layers, which is critical for pruning very deep networks. Finally, we observe that even for ResNets, which have significantly fewer parameters and inference costs than AlexNet or VGGNet, still have about 30% of FLOP reduction without sacrificing too much accuracy. We conduct sensitivity analysis for convolutional layers in ResNets that improves the understanding of ResNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The early work by Le <ref type="bibr" target="#b15">Cun et al. (1989)</ref> introduces Optimal Brain Damage, which prunes weights with a theoretically justified saliency measure. Later, <ref type="bibr" target="#b6">Hassibi &amp; Stork (1993)</ref> propose Optimal Brain Surgeon to remove unimportant weights determined by the second-order derivative information. <ref type="bibr" target="#b19">Mariet &amp; Sra (2016)</ref> reduce the network redundancy by identifying a subset of diverse neurons that does not require retraining. However, this method only operates on the fully-connected layers and introduce sparse connections.</p><p>To reduce the computation costs of the convolutional layers, past work have proposed to approximate convolutional operations by representing the weight matrix as a low rank product of two smaller matrices without changing the original number of filters <ref type="bibr" target="#b2">(Denil et al. (2013)</ref>; <ref type="bibr" target="#b12">Jaderberg et al. (2014)</ref>; <ref type="bibr" target="#b34">Zhang et al. (2015b;</ref><ref type="bibr">a)</ref>; <ref type="bibr" target="#b29">Tai et al. (2016)</ref>; <ref type="bibr" target="#b10">Ioannou et al. (2016)</ref>). Other approaches to reduce the convolutional overheads include using FFT based convolutions <ref type="bibr" target="#b20">(Mathieu et al. (2013)</ref>) and fast convolution using the Winograd algorithm <ref type="bibr" target="#b14">(Lavin &amp; Gray (2016)</ref>). Additionally, quantization <ref type="bibr" target="#b5">(Han et al. (2016b)</ref>) and binarization <ref type="bibr" target="#b22">(Rastegari et al. (2016)</ref>; <ref type="bibr" target="#b1">Courbariaux &amp; Bengio (2016)</ref>) can be used to reduce the model size and lower the computation overheads. Our method can be used in addition to these techniques to reduce computation costs without incurring additional overheads.</p><p>Several work have studied removing redundant feature maps from a well trained network <ref type="bibr">(Anwar et al. (2015)</ref>; <ref type="bibr" target="#b21">Polyak &amp; Wolf (2015)</ref>). <ref type="bibr">Anwar et al. (2015)</ref> introduce a three-level pruning of the weights and locate the pruning candidates using particle filtering, which selects the best combination from a number of random generated masks. <ref type="bibr" target="#b21">Polyak &amp; Wolf (2015)</ref> detect the less frequently activated feature maps with sample input data for face detection applications. We choose to analyze the filter weights and prune filters with their corresponding feature maps using a simple magnitude based measure, without examining possible combinations. We also introduce network-wide holistic approaches to prune filters for simple and complex convolutional network architectures.</p><p>Concurrently with our work, there is a growing interest in training compact CNNs with sparse constraints <ref type="bibr" target="#b16">(Lebedev &amp; Lempitsky (2016)</ref>; <ref type="bibr" target="#b35">Zhou et al. (2016)</ref>; <ref type="bibr" target="#b30">Wen et al. (2016)</ref>). <ref type="bibr" target="#b16">Lebedev &amp; Lempitsky (2016)</ref> leverage group-sparsity on the convolutional filters to achieve structured brain damage, i.e., prune the entries of the convolution kernel in a group-wise fashion. <ref type="bibr" target="#b35">Zhou et al. (2016)</ref> add group-sparse regularization on neurons during training to learn compact CNNs with reduced filters. <ref type="bibr" target="#b30">Wen et al. (2016)</ref> add structured sparsity regularizer on each layer to reduce trivial filters, channels or even layers. In the filter-level pruning, all above work use 2,1 -norm as a regularizer.</p><p>Similar to the above work, we use 1 -norm to select unimportant filters and physically prune them. Our fine-tuning process is the same as the conventional training procedure, without introducing additional regularization. Our approach does not introduce extra layer-wise meta-parameters for the regularizer except for the percentage of filters to be pruned, which is directly related to the desired speedup. By employing stage-wise pruning, we can set a single pruning rate for all layers in one stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRUNING FILTERS AND FEATURE MAPS</head><p>Let n i denote the number of input channels for the ith convolutional layer and h i /w i be the height/width of the input feature maps. The convolutional layer transforms the input feature maps x i ? R ni?hi?wi into the output feature maps x i+1 ? R ni+1?hi+1?wi+1 , which are used as input feature maps for the next convolutional layer. This is achieved by applying n i+1 3D filters F i,j ? R ni?k?k on the n i input channels, in which one filter generates one feature map. Each filter is composed by n i 2D kernels K ? R k?k (e.g., 3 ? 3). All the filters, together, constitute the kernel matrix F i ? R ni?ni+1?k?k . The number of operations of the convolutional layer is n i+1 n i k 2 h i+1 w i+1 . As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, when a filter F i,j is pruned, its corresponding feature map x i+1,j is removed, which reduces n i k 2 h i+1 w i+1 operations. The kernels that apply on the removed feature maps from the filters of the next convolutional layer are also removed, which saves an additional n i+2 k 2 h i+2 w i+2 operations. Pruning m filters of layer i will reduce m/n i+1 of the computation cost for both layers i and i + 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DETERMINING WHICH FILTERS TO PRUNE WITHIN A SINGLE LAYER</head><p>Our method prunes the less useful filters from a well-trained model for computational efficiency while minimizing the accuracy drop. We measure the relative importance of a filter in each layer by calculating the sum of its absolute weights |F i,j |, i.e., its 1 -norm F i,j 1 . Since the number of input channels, n i , is the same across filters, |F i,j | also represents the average magnitude of its kernel weights. This value gives an expectation of the magnitude of the output feature map. Filters with smaller kernel weights tend to produce feature maps with weak activations as compared to the other filters in that layer. <ref type="figure">Figure 2</ref>(a) illustrates the distribution of filters' absolute weights sum for each convolutional layer in a VGG-16 network trained on the CIFAR-10 dataset, where the distribution varies significantly across layers. We find that pruning the smallest filters works better in comparison with pruning the same number of random or largest filters (Section 4.4). Compared to other criteria for activation-based feature map pruning (Section 4.5), we find 1 -norm is a good criterion for data-free filter selection.</p><p>The procedure of pruning m filters from the ith convolutional layer is as follows:</p><p>1. For each filter F i,j , calculate the sum of its absolute kernel weights s j = ni l=1 |K l |. 2. Sort the filters by s j . 3. Prune m filters with the smallest sum values and their corresponding feature maps. The kernels in the next convolutional layer corresponding to the pruned feature maps are also removed. 4. A new kernel matrix is created for both the ith and i + 1th layers, and the remaining kernel weights are copied to the new model. Relationship to pruning weights Pruning filters with low absolute weights sum is similar to pruning low magnitude weights <ref type="bibr" target="#b3">(Han et al. (2015)</ref>). Magnitude-based weight pruning may prune away whole filters when all the kernel weights of a filter are lower than a given threshold. However, it requires a careful tuning of the threshold and it is difficult to predict the exact number of filters that will eventually be pruned. Furthermore, it generates sparse convolutional kernels which can be hard to accelerate given the lack of efficient sparse libraries, especially for the case of low-sparsity.</p><p>Relationship to group-sparse regularization on filters Recent work <ref type="bibr" target="#b35">(Zhou et al. (2016)</ref>; Wen et al. <ref type="formula">(2016)</ref>) apply group-sparse regularization ( ni j=1 F i,j 2 or 2,1 -norm) on convolutional filters, which also favor to zero-out filters with small l 2 -norms, i.e. F i,j = 0. In practice, we do not observe noticeable difference between the 2 -norm and the 1 -norm for filter selection, as the important filters tend to have large values for both measures (Appendix 6.1). Zeroing out weights of multiple filters during training has a similar effect to pruning filters with the strategy of iterative pruning and retraining as introduced in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DETERMINING SINGLE LAYER'S SENSITIVITY TO PRUNING</head><p>To understand the sensitivity of each layer, we prune each layer independently and evaluate the resulting pruned network's accuracy on the validation set. <ref type="figure">Figure 2</ref>(b) shows that layers that maintain their accuracy as filters are pruned away correspond to layers with larger slopes in <ref type="figure">Figure 2</ref>(a). On the contrary, layers with relatively flat slopes are more sensitive to pruning. We empirically determine the number of filters to prune for each layer based on their sensitivity to pruning. For deep networks such as VGG-16 or ResNets, we observe that layers in the same stage (with the same feature map size) have a similar sensitivity to pruning. To avoid introducing layer-wise meta-parameters, we use the same pruning ratio for all layers in the same stage. For layers that are sensitive to pruning, we prune a smaller percentage of these layers or completely skip pruning them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PRUNING FILTERS ACROSS MULTIPLE LAYERS</head><p>We now discuss how to prune filters across the network. Previous work prunes the weights on a layer by layer basis, followed by iteratively retraining and compensating for any loss of accuracy <ref type="bibr" target="#b3">(Han et al. (2015)</ref>). However, understanding how to prune filters of multiple layers at once can be useful: 1) For deep networks, pruning and retraining on a layer by layer basis can be extremely time-consuming 2) Pruning layers across the network gives a holistic view of the robustness of the network resulting in a smaller network 3) For complex networks, a holistic approach may be necessary. For example, for the ResNet, pruning the identity feature maps or the second layer of each residual block results in additional pruning of other layers.</p><p>To prune filters across multiple layers, we consider two strategies for layer-wise filter selection:</p><p>? Independent pruning determines which filters should be pruned at each layer independent of other layers.</p><p>? Greedy pruning accounts for the filters that have been removed in the previous layers. This strategy does not consider the kernels for the previously pruned feature maps while calculating the sum of absolute weights. <ref type="figure">Figure 3</ref> illustrates the difference between two approaches in calculating the sum of absolute weights. The greedy approach, though not globally optimal, is holistic and results in pruned networks with higher accuracy especially when many filters are pruned. <ref type="figure">Figure 3</ref>: Pruning filters across consecutive layers. The independent pruning strategy calculates the filter sum (columns marked in green) without considering feature maps removed in previous layer (shown in blue), so the kernel weights marked in yellow are still included. The greedy pruning strategy does not count kernels for the already pruned feature maps. Both approaches result in a (n i+1 ? 1) ? (n i+2 ? 1) kernel matrix. For simpler CNNs like VGGNet or AlexNet, we can easily prune any of the filters in any convolutional layer. However, for complex network architectures such as Residual networks <ref type="bibr" target="#b8">(He et al. (2016)</ref>), pruning filters may not be straightforward. The architecture of ResNet imposes restrictions and the filters need to be pruned carefully. We show the filter pruning for residual blocks with projection mapping in <ref type="figure" target="#fig_2">Figure 4</ref>. Here, the filters of the first layer in the residual block can be arbitrarily pruned, as it does not change the number of output feature maps of the block. However, the correspondence between the output feature maps of the second convolutional layer and the identity feature maps makes it difficult to prune. Hence, to prune the second convolutional layer of the residual block, the corresponding projected feature maps must also be pruned. Since the identical feature maps are more important than the added residual maps, the feature maps to be pruned should be determined by the pruning results of the shortcut layer. To determine which identity feature maps are to be pruned, we use the same selection criterion based on the filters of the shortcut convolutional layers (with 1 ? 1 kernels). The second layer of the residual block is pruned with the same filter index as selected by the pruning of the shortcut layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">RETRAINING PRUNED NETWORKS TO REGAIN ACCURACY</head><p>After pruning the filters, the performance degradation should be compensated by retraining the network. There are two strategies to prune the filters across multiple layers:</p><p>1. Prune once and retrain: Prune filters of multiple layers at once and retrain them until the original accuracy is restored. 2. Prune and retrain iteratively: Prune filters layer by layer or filter by filter and then retrain iteratively. The model is retrained before pruning the next layer for the weights to adapt to the changes from the pruning process.</p><p>We find that for the layers that are resilient to pruning, the prune and retrain once strategy can be used to prune away significant portions of the network and any loss in accuracy can be regained by retraining for a short period of time (less than the original training time). However, when some filters from the sensitive layers are pruned away or large portions of the networks are pruned away, it may not be possible to recover the original accuracy. Iterative pruning and retraining may yield better results, but the iterative process requires many more epochs especially for very deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We prune two types of networks: simple CNNs (VGG-16 on CIFAR-10) and Residual networks (ResNet-56/110 on CIFAR-10 and ResNet-34 on ImageNet). Unlike AlexNet or VGG (on ImageNet) that are often used to demonstrate model compression, both VGG (on CIFAR-10) and Residual networks have fewer parameters in the fully connected layers. Hence, pruning a large percentage of parameters from these networks is challenging. We implement our filter pruning method in Torch7 <ref type="bibr" target="#b0">(Collobert et al. (2011)</ref>). When filters are pruned, a new model with fewer filters is created and the remaining parameters of the modified layers as well as the unaffected layers are copied into the new model. Furthermore, if a convolutional layer is pruned, the weights of the subsequent batch normalization layer are also removed. To get the baseline accuracies for each network, we train each model from scratch and follow the same pre-processing and hyper-parameters as ResNet <ref type="bibr" target="#b8">(He et al. (2016)</ref>). For retraining, we use a constant learning rate 0.001 and retrain 40 epochs for CIFAR-10 and 20 epochs for ImageNet, which represents one-fourth of the original training epochs. Past work has reported up to 3? original training times to retrain pruned networks <ref type="bibr" target="#b3">(Han et al. (2015)</ref>).  <ref type="formula">(2015)</ref>). Recently, <ref type="bibr" target="#b31">Zagoruyko (2015)</ref> applies a slightly modified version of the model on CIFAR-10 and achieves state of the art results. As shown in <ref type="table" target="#tab_1">Table 2</ref>, VGG-16 on CIFAR-10 consists of 13 convolutional layers and 2 fully connected layers, in which the fully connected layers do not occupy large portions of parameters due to the small input size and less hidden units. We use the model described in <ref type="bibr" target="#b31">Zagoruyko (2015)</ref> but add Batch Normalization <ref type="bibr" target="#b11">(Ioffe &amp; Szegedy (2015)</ref>) layer after each convolutional layer and the first linear layer, without using Dropout <ref type="bibr" target="#b26">(Srivastava et al. (2014)</ref>). Note that when the last convolutional layer is pruned, the input to the linear layer is changed and the connections are also removed.</p><p>As shown in <ref type="figure">Figure 2(b)</ref>, each of the convolutional layers with 512 feature maps can drop at least 60% of filters without affecting the accuracy. <ref type="figure">Figure 2(c)</ref> shows that with retraining, almost 90% of the filters of these layers can be safely removed. One possible explanation is that these filters operate on 4 ? 4 or 2 ? 2 feature maps, which may have no meaningful spatial connections in such small dimensions. For instance, ResNets for CIFAR-10 do not perform any convolutions for feature maps below 8 ? 8 dimensions. Unlike previous work (Zeiler &amp; Fergus (2014); <ref type="bibr" target="#b3">Han et al. (2015)</ref>), we observe that the first layer is robust to pruning as compared to the next few layers. This is possible for a simple dataset like CIFAR-10, on which the model does not learn as much useful filters as on ImageNet (as shown in <ref type="figure" target="#fig_3">Figure. 5</ref>). Even when 80% of the filters from the first layer are pruned, the number of remaining filters (12) is still larger than the number of raw input channels. However, when removing 80% filters from the second layer, the layer corresponds to a 64 to 12 mapping, which may lose significant information from previous layers, thereby hurting the accuracy. With 50% of the filters being pruned in layer 1 and from 8 to 13, we achieve 34% FLOP reduction for the same accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESNET-56/110 ON CIFAR-10</head><p>ResNets for CIFAR-10 have three stages of residual blocks for feature maps with sizes of 32 ? 32, 16 ? 16 and 8 ? 8. Each stage has the same number of residual blocks. When the number of feature maps increases, the shortcut layer provides an identity mapping with an additional zero padding for the increased dimensions. Since there is no projection mapping for choosing the identity feature maps, we only consider pruning the first layer of the residual block. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, most of the layers are robust to pruning. For ResNet-110, pruning some single layers without retraining even improves the performance. In addition, we find that layers that are sensitive to pruning (layers 20, 38 and 54 for ResNet-56, layer 36, 38 and 74 for ResNet-110) lie at the residual blocks close to the layers where the number of feature maps changes, e.g., the first and the last residual blocks for each stage. We believe this happens because the precise residual errors are necessary for the newly added empty feature maps.</p><p>The retraining performance can be improved by skipping these sensitive layers. As shown in <ref type="table" target="#tab_0">Table 1</ref>, ResNet-56-pruned-A improves the performance by pruning 10% filters while skipping the sensitive layers 16, 20, 38 and 54. In addition, we find that deeper layers are more sensitive to pruning than layers in the earlier stages of the network. Hence, we use a different pruning rate for each stage. We use p i to denote the pruning rate for layers in the ith stage. ResNet-56-pruned-B skips more layers <ref type="bibr">(16,</ref><ref type="bibr">18,</ref><ref type="bibr">20,</ref><ref type="bibr">34,</ref><ref type="bibr">38,</ref><ref type="bibr">54)</ref> and prunes layers with p 1 =60%, p 2 =30% and p 3 =10%. For ResNet-110, the first pruned model gets a slightly better result with p 1 =50% and layer 36 skipped. ResNet-110-pruned-B skips layers 36, 38, 74 and prunes with p 1 =50%, p 2 =40% and p 3 =30%. When there are more than two residual blocks at each stage, the middle residual blocks may be redundant and can be easily pruned. This might explain why ResNet-110 is easier to prune than ResNet-56.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RESNET-34 ON ILSVRC2012</head><p>ResNets for ImageNet have four stages of residual blocks for feature maps with sizes of 56 ? 56, 28 ? 28, 14 ? 14 and 7 ? 7. ResNet-34 uses the projection shortcut when the feature maps are down-sampled. We first prune the first layer of each residual block. <ref type="figure" target="#fig_5">Figure 7</ref> shows the sensitivity of the first layer of each residual block. Similar to ResNet-56/110, the first and the last residual blocks of each stage are more sensitive to pruning than the intermediate blocks <ref type="bibr">(i.e., layers 2, 8, 14, 16, 26, 28, 30, 32)</ref>. We skip those layers and prune the remaining layers at each stage equally. In <ref type="table" target="#tab_0">Table 1</ref> we compare two configurations of pruning percentages for the first three stages: (A) p 1 =30%, p 2 =30%, p 3 =30%; (B) p 1 =50%, p 2 =60%, p 3 =40%. Option-B provides 24% FLOP reduction with about 1% loss in accuracy. As seen in the pruning results for ResNet-50/110, we can predict that ResNet-34 is relatively more difficult to prune as compared to deeper ResNets.</p><p>We also prune the identity shortcuts and the second convolutional layer of the residual blocks. As these layers have the same number of filters, they are pruned equally. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>(b), these layers are more sensitive to pruning than the first layers. With retraining, ResNet-34-pruned-C prunes the third stage with p 3 =20% and results in 7.5% FLOP reduction with 0.75% loss in accuracy. Therefore, pruning the first layer of the residual block is more effective at reducing the overall FLOP than pruning the second layer. This finding also correlates with the bottleneck block design for deeper ResNets, which first reduces the dimension of input feature maps for the residual layer and then increases the dimension to match the identity mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">COMPARISON WITH PRUNING RANDOM FILTERS AND LARGEST FILTERS</head><p>We compare our approach with pruning random filters and largest filters. As shown in <ref type="figure">Figure 8</ref>, pruning the smallest filters outperforms pruning random filters for most of the layers at different pruning ratios. For example, smallest filter pruning has better accuracy than random filter pruning for all layers with the pruning ratio of 90%. The accuracy of pruning filters with the largest 1 -norms drops quickly as the pruning ratio increases, which indicates the importance of filters with larger 1 -norms. <ref type="figure">Figure 8</ref>: Comparison of three pruning methods for VGG-16 on CIFAR-10: pruning the smallest filters, pruning random filters and pruning the largest filters. In random filter pruning, the order of filters to be pruned is randomly permuted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">COMPARISON WITH ACTIVATION-BASED FEATURE MAP PRUNING</head><p>The activation-based feature map pruning method removes the feature maps with weak activation patterns and their corresponding filters and kernels <ref type="bibr" target="#b21">(Polyak &amp; Wolf (2015)</ref>), which needs sample data as input to determine which feature maps to prune. A feature map x i+1,j ? R wi+1?hi+1 is generated by applying filter F i,j ? R ni?k?k to feature maps of previous layer x i ? R ni?wi?hi , i.e., x i+1,j = F i,j * x i . Given N randomly selected images {x n 1 } N n=1 from the training set, the statistics of each feature map can be estimated with one epoch forward pass of the N sampled data. Note that we calculate statistics on the feature maps generated from the convolution operations before batch normalization or non-linear activation. We compare our 1 -norm based filter pruning with feature map pruning using the following criteria: <ref type="figure">Figure 9</ref>: Comparison of activation-based feature map pruning for VGG-16 on CIFAR-10.</p><formula xml:id="formula_0">? mean-mean (x i,j ) = 1 N N n=1 mean(x n i,j ), ? mean-std (x i,j ) = 1 N N n=1 std(x n i,j ), ? mean-1 (x i,j ) = 1 N N n=1 x n i,j 1 , ? mean-2 (x i,j ) = 1 N N n=1 x n i,j 2 and (a) Fi,j 1 (b) ?mean-mean (c) ?mean-std (d) ? mean-1 (e) ? mean-2 (f) ? var-2</formula><formula xml:id="formula_1">? var-2 (x i,j ) = var({ x n i,j 2 } N n=1 ),</formula><p>where mean, std and var are standard statistics (average, standard deviation and variance) of the input. Here, ? var-2 is the contribution variance of channel criterion proposed in <ref type="bibr" target="#b21">Polyak &amp; Wolf (2015)</ref>, which is motivated by the intuition that an unimportant feature map has almost similar outputs for the whole training data and acts like an additional bias.</p><p>The estimation of the criteria becomes more accurate when more sample data is used. Here we use the whole training set (N = 50, 000 for CIFAR-10) to compute the statistics. The performance of feature map pruning with above criteria for each layer is shown in <ref type="figure">Figure 9</ref>. Smallest filter pruning outperforms feature map pruning with the criteria ? mean-mean , ? mean-1 , ? mean-2 and ? var-2 . The ? mean-std criterion has better or similar performance to 1 -norm up to pruning ratio of 60%. However, its performance drops quickly after that especially for layers of conv 1, conv 2 and conv 3. We find 1 -norm is a good heuristic for filter selection considering that it is data free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Modern CNNs often have high capacity with large training and inference costs. In this paper we present a method to prune filters with relatively low weight magnitudes to produce CNNs with reduced computation costs without introducing irregular sparsity. It achieves about 30% reduction in FLOP for VGGNet (on CIFAR-10) and deep ResNets without significant loss in the original accuracy. Instead of pruning with specific layer-wise hayperparameters and time-consuming iterative retraining, we use the one-shot pruning and retraining strategy for simplicity and ease of implementation. By performing lesion studies on very deep CNNs, we identify layers that are robust or sensitive to pruning, which can be useful for further understanding and improving the architectures. 6 APPENDIX 6.1 COMPARISON WITH 2 -NORM BASED FILTER PRUNING We compare 1 -norm with 2 -norm for filter pruning. As shown in <ref type="figure" target="#fig_0">Figure 10</ref>, 1 -norm works slightly better than 2 -norm for layer conv 2. There is no significant difference between the two norms for other layers.</p><p>(a) Fi,j 1 (b) Fi,j 2 <ref type="figure" target="#fig_0">Figure 10</ref>: Comparison of 1 -norm and 2 -norm based filter pruning for VGG-16 on CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">FLOP AND WALL-CLOCK TIME</head><p>FLOP is a commonly used measure to compare the computation complexities of CNNs. It is easy to compute and can be done statically, which is independent of the underlying hardware and software implementations. Since we physically prune the filters by creating a smaller model and then copy the weights, there are no masks or sparsity introduced to the original dense BLAS operations. Therefore the FLOP and wall-clock time of the pruned model is the same as creating a model with smaller number of filters from scratch.</p><p>We report the inference time of the original model and the pruned model on the test set of CIFAR-10 and the validation set of ILSVRC 2012, which contains 10,000 32 ? 32 images and 50,000 224 ? 224 images respectively. The ILSVRC 2012 dataset is used only for ResNet-34. The evaluation is conducted in Torch7 with Titan X (Pascal) GPU and cuDNN v5.1, using a mini-batch size 128. As shown in <ref type="table" target="#tab_2">Table 3</ref>, the saved inference time is close to the FLOP reduction. Note that the FLOP number only considers the operations in the Conv and FC layers, while some calculations such as Batch Normalization and other overheads are not accounted. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Pruning a filter results in removal of its corresponding feature map and related kernels in the next layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Filters are ranked by sj (b) Prune the smallest filters (c) Prune and retrain Figure 2: (a) Sorting filters by absolute weights sum for each layer of VGG-16 on CIFAR-10. The x-axis is the filter index divided by the total number of filters. The y-axis is the filter weight sum divided by the max sum value among filters in that layer. (b) Pruning filters with the lowest absolute weights sum and their corresponding test accuracies on CIFAR-10. (c) Prune and retrain for each single layer of VGG-16 on CIFAR-10. Some layers are sensitive and it can be harder to recover accuracy after pruning them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Pruning residual blocks with the projection shortcut. The filters to be pruned for the second layer of the residual block (marked as green) are determined by the pruning result of the shortcut projection. The first layer of the residual block can be pruned without restrictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of filters in the first convolutional layer of VGG-16 trained on CIFAR-10. Filters are ranked by 1 -norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Sensitivity to pruning for the first layer of each residual block of ResNet-56/110.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>(a) Pruning the first layer of residual blocks (b) Pruning the second layer of residual blocks Sensitivity to pruning for the residual blocks of ResNet-34.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overall results. The best test/validation accuracy during the retraining process is reported. Training a pruned model from scratch performs worse than retraining a pruned model, which may indicate the difficulty of training a network with a small capacity.</figDesc><table><row><cell>Model</cell><cell cols="2">Error(%) FLOP</cell><cell cols="3">Pruned % Parameters Pruned %</cell></row><row><cell>VGG-16</cell><cell>6.75</cell><cell>3.13 ? 10 8</cell><cell></cell><cell>1.5 ? 10 7</cell></row><row><cell>VGG-16-pruned-A</cell><cell>6.60</cell><cell cols="2">2.06 ? 10 8 34.2%</cell><cell>5.4 ? 10 6</cell><cell>64.0%</cell></row><row><cell>VGG-16-pruned-A scratch-train</cell><cell>6.88</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-56</cell><cell>6.96</cell><cell>1.25 ? 10 8</cell><cell></cell><cell>8.5 ? 10 5</cell></row><row><cell>ResNet-56-pruned-A</cell><cell>6.90</cell><cell cols="2">1.12 ? 10 8 10.4%</cell><cell>7.7 ? 10 5</cell><cell>9.4%</cell></row><row><cell>ResNet-56-pruned-B</cell><cell>6.94</cell><cell cols="2">9.09 ? 10 7 27.6%</cell><cell>7.3 ? 10 5</cell><cell>13.7%</cell></row><row><cell>ResNet-56-pruned-B scratch-train</cell><cell>8.69</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-110</cell><cell>6.47</cell><cell>2.53 ? 10 8</cell><cell></cell><cell>1.72 ? 10 6</cell></row><row><cell>ResNet-110-pruned-A</cell><cell>6.45</cell><cell cols="2">2.13 ? 10 8 15.9%</cell><cell cols="2">1.68 ? 10 6 2.3%</cell></row><row><cell>ResNet-110-pruned-B</cell><cell>6.70</cell><cell cols="2">1.55 ? 10 8 38.6%</cell><cell cols="2">1.16 ? 10 6 32.4%</cell></row><row><cell cols="2">ResNet-110-pruned-B scratch-train 7.06</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-34</cell><cell>26.77</cell><cell>3.64 ? 10 9</cell><cell></cell><cell>2.16 ? 10 7</cell></row><row><cell>ResNet-34-pruned-A</cell><cell>27.44</cell><cell cols="2">3.08 ? 10 9 15.5%</cell><cell cols="2">1.99 ? 10 7 7.6%</cell></row><row><cell>ResNet-34-pruned-B</cell><cell>27.83</cell><cell cols="2">2.76 ? 10 9 24.2%</cell><cell cols="2">1.93 ? 10 7 10.8%</cell></row><row><cell>ResNet-34-pruned-C</cell><cell>27.52</cell><cell cols="2">3.37 ? 10 9 7.5%</cell><cell cols="2">2.01 ? 10 7 7.2%</cell></row><row><cell>4.1 VGG-16 ON CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>VGG-16 is a high-capacity network originally designed for the ImageNet dataset (Simonyan &amp; Zisserman</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>VGG-16 on CIFAR-10 and the pruned model. The last two columns show the number of feature maps and the reduced percentage of FLOP from the pruned model.</figDesc><table><row><cell cols="3">layer type wi ? hi #Maps</cell><cell cols="3">FLOP #Params #Maps FLOP%</cell></row><row><cell>Conv 1</cell><cell>32 ? 32</cell><cell cols="2">64 1.8E+06 1.7E+03</cell><cell>32</cell><cell>50%</cell></row><row><cell>Conv 2</cell><cell>32 ? 32</cell><cell cols="2">64 3.8E+07 3.7E+04</cell><cell>64</cell><cell>50%</cell></row><row><cell>Conv 3</cell><cell>16 ? 16</cell><cell cols="2">128 1.9E+07 7.4E+04</cell><cell>128</cell><cell>0%</cell></row><row><cell>Conv 4</cell><cell>16 ? 16</cell><cell cols="2">128 3.8E+07 1.5E+05</cell><cell>128</cell><cell>0%</cell></row><row><cell>Conv 5</cell><cell>8 ? 8</cell><cell cols="2">256 1.9E+07 2.9E+05</cell><cell>256</cell><cell>0%</cell></row><row><cell>Conv 6</cell><cell>8 ? 8</cell><cell cols="2">256 3.8E+07 5.9E+05</cell><cell>256</cell><cell>0%</cell></row><row><cell>Conv 7</cell><cell>8 ? 8</cell><cell cols="2">256 3.8E+07 5.9E+05</cell><cell>256</cell><cell>0%</cell></row><row><cell>Conv 8</cell><cell>4 ? 4</cell><cell cols="2">512 1.9E+07 1.2E+06</cell><cell>256</cell><cell>50%</cell></row><row><cell>Conv 9</cell><cell>4 ? 4</cell><cell cols="2">512 3.8E+07 2.4E+06</cell><cell>256</cell><cell>75%</cell></row><row><cell>Conv 10</cell><cell>4 ? 4</cell><cell cols="2">512 3.8E+07 2.4E+06</cell><cell>256</cell><cell>75%</cell></row><row><cell>Conv 11</cell><cell>2 ? 2</cell><cell cols="2">512 9.4E+06 2.4E+06</cell><cell>256</cell><cell>75%</cell></row><row><cell>Conv 12</cell><cell>2 ? 2</cell><cell cols="2">512 9.4E+06 2.4E+06</cell><cell>256</cell><cell>75%</cell></row><row><cell>Conv 13</cell><cell>2 ? 2</cell><cell cols="2">512 9.4E+06 2.4E+06</cell><cell>256</cell><cell>75%</cell></row><row><cell>Linear</cell><cell>1</cell><cell cols="2">512 2.6E+05 2.6E+05</cell><cell>512</cell><cell>50%</cell></row><row><cell>Linear</cell><cell>1</cell><cell cols="2">10 5.1E+03 5.1E+03</cell><cell>10</cell><cell>0%</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell>3.1E+08 1.5E+07</cell><cell></cell><cell>34%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The reduction of FLOP and wall-clock time for inference.</figDesc><table><row><cell>Model</cell><cell>FLOP</cell><cell cols="3">Pruned % Time (s) Saved %</cell></row><row><cell>VGG-16</cell><cell>3.13 ? 10 8</cell><cell></cell><cell>1.23</cell></row><row><cell>VGG-16-pruned-A</cell><cell cols="2">2.06 ? 10 8 34.2%</cell><cell>0.73</cell><cell>40.7%</cell></row><row><cell>ResNet-56</cell><cell>1.25 ? 10 8</cell><cell></cell><cell>1.31</cell></row><row><cell>ResNet-56-pruned-B</cell><cell cols="2">9.09 ? 10 7 27.6%</cell><cell>0.99</cell><cell>24.4%</cell></row><row><cell>ResNet-110</cell><cell>2.53 ? 10 8</cell><cell></cell><cell>2.38</cell></row><row><cell cols="3">ResNet-110-pruned-B 1.55 ? 10 8 38.6%</cell><cell>1.86</cell><cell>21.8%</cell></row><row><cell>ResNet-34</cell><cell>3.64 ? 10 9</cell><cell></cell><cell>36.02</cell></row><row><cell>ResNet-34-pruned-B</cell><cell cols="2">2.76 ? 10 9 24.2%</cell><cell>22.93</cell><cell>28.0%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the anonymous reviewers for their valuable feedback. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nando De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning both Weights and Connections for Efficient Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">EIE: Efficient Inference Engine on Compressed Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardavan</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Second Order Derivatives for Network Pruning: Optimal Brain Surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks at Constrained Time Cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalidand</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keutzer</forename><surname>Kurt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and ? 1MB model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training CNNs with Low-Rank Filters for Efficient Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yani</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast Algorithms for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimal Brain Damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast Convnets Using Group-wise Brain Damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
	</analytic>
	<monogr>
		<title level="j">Network in Network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Pensky</surname></persName>
		</author>
		<title level="m">Sparse Convolutional Neural Networks. In CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelda</forename><surname>Mariet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diversity Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5851</idno>
		<title level="m">Fast Training of Convolutional Networks through FFTs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Channel-Level Acceleration of Deep Face Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data-free Parameter Pruning for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional neural networks with low-rank regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Structured Sparsity in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<ptr target="http://torch.ch/blog/2015/07/30/cifar.html" />
		<title level="m">92.45% on CIFAR-10 in Torch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Accelerating Very Deep Convolutional Networks for Classification and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE T-PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. Efficient and accurate approximations of nonlinear convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Less Is More: Towards Compact CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
