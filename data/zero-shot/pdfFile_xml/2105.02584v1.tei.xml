<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TABBIE: Pretrained Representations of Tabular Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Iida</surname></persName>
							<email>hiroshi.iida@sony.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation ? UMass Amherst ? Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dung</forename><surname>Thai</surname></persName>
							<email>dthai@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation ? UMass Amherst ? Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation ? UMass Amherst ? Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
							<email>miyyer@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation ? UMass Amherst ? Adobe Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TABBIE: Pretrained Representations of Tabular Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing work on tabular representationlearning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of tablebased prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model's learned cell, column, and row representations shows that it understands complex table semantics and numerical trends. 2 2 trois pi??ces br??ves: andante brucknerhaus-edition: dai 3 3 trois pi??ces br??ves: assez lent brucknerhaus-edition: dai 4 4 sechs bagatellen: allegro con spirito brucknerhaus-edition: dai 5 5 sechs bagatellen: rubato. lamentoso brucknerhaus-edition: dai</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale self-supervised pretraining has substantially advanced the state-of-the-art in natural language processing <ref type="bibr" target="#b21">(Peters et al., 2018;</ref><ref type="bibr" target="#b5">Devlin et al., 2018;</ref><ref type="bibr" target="#b15">Liu et al., 2019)</ref>. More recently, these pretraining methods have been extended to jointly learn representations of tables as well as text <ref type="bibr" target="#b7">(Herzig et al., 2020;</ref><ref type="bibr" target="#b30">Yin et al., 2020)</ref>, which enables improved modeling of tasks such as question answering over tables. However, many practical problems involve semantic understanding of tabular data without additional text-based input, such as extracting tables from documents, retrieving similar columns or cells, and filling in missing information . In this work, we design a pretraining methodology specifically for tables (Tabular Information Embedding or TABBIE) that resembles several core tasks in table extraction and decomposition pipelines and  allows easy access to representations for different tabular substructures (cells, rows, and columns).</p><p>Existing table representation models such as TaBERT <ref type="bibr" target="#b30">(Yin et al., 2020)</ref> and TaPas <ref type="bibr" target="#b7">(Herzig et al., 2020)</ref> concatenate tabular data with an associated piece of text and then use BERT's masked language modeling objective for pretraining. These approaches are computationally expensive due to the long sequences that arise from concatenating text with linearized tables, which necessitates truncating the input sequences 1 to make training feasible. We show that TaBERT underperforms on downstream table-based applications that operate independent of external text (e.g., deciding whether cell text was corrupted while extracting a table from a PDF), which motivates us to investigate an approach that preserves the full table during pretraining.</p><p>Our TABBIE architecture relies on two Transformers that independently encode rows and columns, respectively; their representations are pooled at each layer. This setup reduces the sequence length of each Transformer's input, which cuts down on its complexity, while also allowing us to easily extract representations of cells, rows, and columns. Additionally, TABBIE uses a simplified training objective compared to masked language modeling: instead of predicting masked cells, we repurpose ELECTRA's objective function <ref type="bibr" target="#b1">(Clark et al., 2020)</ref> for tabular pretraining by asking the model to predict whether or not each cell in a table is real or corrupted. We emphasize that this pretraining objective is a fundamental task in table structure decomposition pipelines <ref type="bibr" target="#b19">(Nishida et al., 2017;</ref><ref type="bibr" target="#b25">Tensmeyer et al., 2019;</ref><ref type="bibr" target="#b23">Raja et al., 2020)</ref>, in which incorrectly predicting row/column separators or cell boundaries leads to corrupted cell text. Unlike <ref type="bibr" target="#b1">Clark et al. (2020)</ref>, we do not require a separate "generator" model that produces corrupted candidates, as we observe that simple corruption processes (e.g., sampling cells from other tables, swapping cells within the same column) yield powerful representations after pretraining.</p><p>In a controlled comparison to TaBERT (pretraining on the same number of tables and using a similarly-sized model), we evaluate TABBIE on three table-based benchmarks: column population, row population, and column type prediction. On most configurations of these tasks, TABBIE achieves state-of-the-art performance, outperforming TaBERT and other baselines, while in others it performs competitively with TaBERT. Additionally, TABBIE was trained on 8 V100 GPUs in just over a week, compared to the 128 V100 GPUs used to train TaBERT in six days. A qualitative nearest-neighbor analysis of embeddings derived from TABBIE confirms that it encodes complex semantic properties about textual and numeric cells and substructures. We release our pretrained models and code to support further advances on tablebased tasks. 2 2 Model TABBIE is a self-supervised pretraining approach trained exclusively on tables, unlike prior approaches <ref type="bibr" target="#b7">(Herzig et al., 2020;</ref><ref type="bibr" target="#b30">Yin et al., 2020)</ref> that jointly model tables and associated text snippets. At a high level, TABBIE encodes each cell of a table using two different Transformer models <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref>, one operating across the rows of the table and the other across columns. At each layer, the representations from the row and column Transformers are averaged and then passed as input to the next layer, which produces a contextualized representation of each cell within the table. We place a binary classifier over TABBIE's final-layer cell representations to predict whether or not it has been corrupted, or replaced by an intruder cell during preprocessing, inspired by the ELECTRA objective of <ref type="bibr" target="#b1">Clark et al. (2020)</ref>. In the remainder of this section, we formalize both TABBIE's model architecture and pretraining objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>TABBIE takes an M ?N table as input and produces embeddings x ij for each cell (where i and j are row and column indices, respectively), as well as embeddings for individual columns c j and rows r i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization:</head><p>We begin by initializing the cell embeddings x ij using a pretrained BERT model <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>. 3 Specifically, for each cell (i, j), we feed its contents into BERT and extract the 768-d [CLS] token representation. This step allows us to leverage the powerful semantic text encoder of BERT to compute representations of cells out-of-context, which is important because many tables contain cells with long-form text (e.g., Notes columns). Additionally, BERT has been shown to encode some degree of numeracy <ref type="bibr" target="#b28">(Wallace et al., 2019)</ref>, which helps represent cells with numerical content. We keep this BERT encoder fixed during training to reduce computational expense. Finally, we add learned positional embeddings to each of the [CLS] vectors to form the initialization of x ij . More specifically, we have two sets of positional embeddings, p Contextualizing the cell embeddings: The cell embeddings we get from BERT are uncontextualized: they are computed in isolation of all of the other cells in the table. While methods such as TaBERT and TaPaS contextualize cell embeddings by linearizing the table into a single long sequence, we take a different and more computationally manageable approach. We define a row Transformer, which encodes cells across each row of the table, as well as a column Transformer, which does the same across columns.</p><p>Concretely, assume row i contains cell embeddings x i,1 , x i,2 , . . . , x i,N . We pass this se-Column Transformer <ref type="bibr">[CLSCOL]</ref> [CLSCOL] <ref type="bibr">[CLSCOL]</ref> [CLSROW] <ref type="bibr">[CLSROW]</ref> [CLSROW]</p><p>[CLSROW] Step 1: compute column and row embeddings using two separate Transformers</p><p>Step 2: compute contextualized cell embeddings by averaging row/col embeddings x12 layers</p><p>Step 3: feed these contextualized cell embeddings as input to the next layer <ref type="bibr">[CLSCOL]</ref> [CLSCOL] <ref type="bibr">[CLSCOL]</ref> [CLSROW] <ref type="bibr">[CLSROW]</ref> [CLSROW]</p><p>[CLSROW] quence of embeddings into a row Transformer block, which uses self-attention to produce contextualized output representations r i,1 , r i,2 , . . . , r i,N . Similarly, assume column j contains cell embeddings x 1,j , x 2,j , . . . , x M,j ; the column Transformer produces contextualized representations c 1,j , c 2,j , . . . , c M,j . After running the two Transformers over all rows and columns, respectively, each cell (i, j) of a table is associated with a row embedding r i,j as well as a column embedding c i,j .</p><p>The final step of cell contextualization is to compose the row and column embeddings together before feeding the result to the next layer. Intuitively, if we do not aggregate the two sets of embeddings together, subsequent layers of the model will only have access to information from a specific row or column, which prevents contextualization across the whole table. We implement this aggregation through simple averaging: specifically, at layer L of TABBIE, we compute cell embeddings as:</p><formula xml:id="formula_0">x L+1 i,j = r L i,j + c L i,j 2<label>(1)</label></formula><p>The new cell representations x L+1 i,j are then fed to the row and column Transformers at the next layer L + 1.</p><p>Extracting representations of an entire row or column: The row and column Transformers defined above produce separate representations for every cell in a particular row or column. However, many table-related downstream tasks (e.g., retrieve similar columns from a huge dataset of tables to some query column) can benefit from embeddings that capture the contents of an entire row or column. To enable this functionality in TABBIE, we simply prepend [CLSROW] and [CLSCOL] tokens to the beginning of each row and column in an input table as a preprocessing step. After pretraining, we can extract the final-layer cell representations of these [CLS] tokens to use in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pretraining</head><p>Having described TABBIE's model architecture, we turn now to its training objective. We adapt the selfsupervised ELECTRA objective proposed by <ref type="bibr" target="#b1">Clark et al. (2020)</ref> for text representation learning, which places a binary classifier over each word in a piece of text and asks if the word either is part of the original text or has been corrupted. While this objective was originally motivated as enabling more efficient training compared to BERT's masked language modeling objective, it is especially suited for tabular data, as corrupt cell detection is actually a fundamental task in table structure decomposition pipelines such as <ref type="bibr" target="#b19">(Nishida et al., 2017;</ref><ref type="bibr" target="#b25">Tensmeyer et al., 2019;</ref><ref type="bibr" target="#b23">Raja et al., 2020)</ref>, in which incorrectly predicted row/column separators or cell boundaries can lead to corrupted cell text.</p><p>In our extension of ELECTRA to tables, a binary classifier takes a final-layer cell embedding as input to decide whether it has been corrupted. More concretely, for cell (i, j), we compute the corruption probability as</p><formula xml:id="formula_1">P corrupt (cell i,j ) = ?(w x L i,j )<label>(2)</label></formula><p>where L indexes TABBIE's final layer, ? is the sigmoid function, and w is a weight vector of the same dimensionality as the cell embedding. Our final loss function is the binary cross entropy loss of this classifier averaged across all cells in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cell corruption process</head><p>Our formulation diverges from <ref type="bibr" target="#b1">Clark et al. (2020)</ref> in how the corrupted cells are generated. In ELEC-TRA, a separate generator model is trained with BERT's masked language modeling objective to produce candidate corrupted tokens: for instance, given Jane went to the [MASK] to check on her experiments, the generator model might produce corrupted candidates such as lab or office. Simpler corruption strategies, such as randomly sampling words from the vocabulary, cannot induce powerful representations of text because local syntactic and semantic patterns are usually sufficient to detect obvious corruptions. For tabular data, however, we show that simple corruption strategies ( <ref type="figure" target="#fig_6">Figure 3</ref>) that take advantage of the intra-table structure actually do yield powerful representations without the need of a separate generator network. More specifically, we use two different corruption strategies:</p><p>? Frequency-based cell sampling: Our first strategy simply samples corrupt candidates from the training cell frequency distribution (i.e., more commonly-occurring cells are sampled more often than rare cells). One drawback of this method is that oftentimes it can result in samples that violate a particular column type (for instance, sampling a textual cell as a replacement for a cell in a numeric column). Despite its limitations, our analysis in Section 4 shows that this strategy alone results in strong performance on most downstream table-based tasks, although it does not result in a rich semantic understanding of intra-table semantics.</p><p>? Intra-table cell swapping: To encourage the model to learn fine-grained distinctions between topically-similar data, our second strategy produces corrupted candidates by swapping two cells in the same table <ref type="figure" target="#fig_6">(Figure 3c</ref>, d). This task is more challenging than the   Experimental settings: We train TABBIE for seven epochs for just over a week on 8 V100 GPUs using mixed precision. TABBIE has 12 layers and a hidden dimensionality of 768 for both row and column Transformers, in an effort to be comparablysized to the TaBERT-Base model. 5 Before computing the initial cell embeddings using BERT, we truncate each cell's contents to the first 300 characters, as some cells contain huge amounts of text. We also truncate tables to 30 rows and 20 columns to avoid memory issues (note that this is much larger than the three rows used by TaBERT), and <ref type="bibr">4</ref> The vast majority of text in these tables is in English. 5 TABBIE is slightly larger than TaBERT-Base (170M to 133M parameters) because its row and column Transformers are the same size, while TaBERT places a smaller "vertical" Transformer over the output of a fine-tuned BERT model. [CLS]</p><p>[CLS]</p><p>[CLS]</p><p>[CLS]</p><p>, a _di /'20M238e_4K' [CLS]</p><p>[CLS]</p><p>[CLS]</p><p>[CLS]</p><p>, a _di /'20M238e_4K' [CLS]</p><p>[CLS]</p><p>[CLS]</p><p>[CLS]  our maximum batch size is set at 4,800 cells (on average, 104 tables per batch). We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5. We compared two pretrained models trained with different cell corruption strategy for downstream tasks. The first strategy (FREQ) uses exclusively a frequency-based cell sampling. The second strategy is a 50/50 mixture (MIX) of frequencybased sampling and intra-table cell swapping, where we additionally specify that half of the intratable swaps must come from the same row or column to make the objective more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We validate TABBIE's table representation quality through its performance on three downstream tablecentric benchmarks (column population, row population, and column type prediction) that measure semantic table understanding. In most configurations of these tasks, TABBIE outperforms TaBERT and other baselines to set new state-of-the-art numbers. Note that we do not investigate TABBIE's performance on table-and-text tasks such as WikiTable-Questions <ref type="bibr" target="#b20">(Pasupat and Liang, 2015)</ref>, as our focus is not on integrating TABBIE into complex taskspecific pipelines <ref type="bibr" target="#b13">(Liang et al., 2018)</ref>, although this is an interesting avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fine-tuning TABBIE</head><p>In all of our downstream experiments, we apply essentially the same fine-tuning strategy to both TABBIE and TaBERT: we select a subset of its finallayer representations (i.e., cell or column representations) that correspond to the tabular substruc-  tures used in the downstream task, and we place a classifier over these representations to predict the training labels. We select task-specific hyperparameters based on the size of each dataset (full details in <ref type="table" target="#tab_4">Table 1</ref>) and report the test performance of the best-performing validation checkpoint. For both models, we backpropagate the downstream error signal into all of the model's parameters (i.e., we do not "freeze" our pretrained model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Column Population</head><p>In the column population task, which is useful for attribute discovery, tabular data augmentation, and table retrieval (Das <ref type="bibr" target="#b2">Sarma et al., 2012)</ref>, a model is given the first N columns of a "seed" table and asked to predict the remaining column headers. Zhang and Balog (2017) compile a dataset for this task comprising 1.6M tables from Wikipedia with a test set of 1,000 tables, formulated as a multi-label classification task with 127,656 possible header labels. Importantly, we remove all of the tables in the column population test set from our pretraining data to avoid inflating our results in case TABBIE memorizes the missing columns during pretraining. <ref type="bibr">6</ref> To fine-tune TABBIE on this task, we first concatenate the column [CLSCOL] embeddings of the seed table into a single vector and pass it through a single linear and softmax layer, training with a multi-label classification objective <ref type="bibr" target="#b18">(Mahajan et al., 2018)</ref>. Our baselines include the generative probabilistic model (GPM) of <ref type="bibr" target="#b32">Zhang and Balog (2017)</ref> as well as a word embedding-based extension called Table2VecH (TH) devised by <ref type="bibr" target="#b3">Deng et al. (2019)</ref>. As fine-tuning on the full dataset is extremely expensive for TABBIE and TaBERT, we fine-tune on a random subset of 100K training examples; as a further disadvantage to these, we do not use  <ref type="table">Table 2</ref>: TABBIE outperforms all methods on the column population task, with the biggest improvement coming with just a single seed column (N = 1). Despite its simplicity, the FREQ corruption strategy yields better results than MIX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>baselines, and TABBIE consistently outperforms</head><p>TaBERT regardless of how many seed columns are provided, especially with only one seed column. This result indicates that TABBIE encodes more semantics about headers and columns than TaBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Row Population</head><p>The row population task is more challenging than column population: given the first N rows of a table in which the first column contains entities (e.g., "Country"), models must predict the remaining entries of the first column. Making reasonable predictions of which entities best fill the column requires understanding the full context of the seed table. The <ref type="bibr" target="#b32">Zhang and Balog (2017)</ref> dataset also contains a split for row population, which we use to evaluate our models. Again, since the dataset is too large for our large embedding models, we sample a subset of tables for fine-tuning.  tions). When given only one seed row, TaBERT slightly outperforms TABBIE, but with more seed rows, TABBIE exhibits small improvements over TaBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Column Type Prediction</head><p>While the prior two tasks involve predicting missing elements of a table, the column type prediction task involves predicting a high-level type of a particular column (e.g., name, age, etc.) without access to its header. This task is useful when indexing tables with missing column names, which happens relatively often in practice, or for schema matching , and like the other tasks, requires understanding the surrounding context. We evaluate our models on the same subset of VizNet Web Tables  9 created by  to evaluate their column type predictor, SATO 10 . They formulate this task as a multi-class classification problem (with 78 classes), with a training set of 64,000 tables and a test set consisting of 16,000 tables. We set aside 6,400 training tables to form a validation for both TABBIE and TaBERT, and we fine-tune each of these models with small random subsets of the training data (1000 and 10000 labeled tables) in addition to the full training set to evaluate their performance in a simulated lowresource setting. Along with TaBERT, we compare with two recently-proposed column type prediction meth-  <ref type="table">Table 4</ref>: Support-weighted F1-score of different models on column type prediction. TaBERT and TABBIE perform similarly in low resource settings (n=1000) and when the full training data is used (n=all).</p><p>ods: Sherlock , which uses a multi-input neural network with hand-crafted features extracted from each column, and the aforementioned SATO , which improves Sherlock by incorporating table context, topic model outputs, and label co-occurrence information. <ref type="table">Table 4</ref> shows the support-weighted F1score for each method. Similar to the previous two tasks, <ref type="bibr">TABBIE</ref> and TaBERT significantly outperform the prior state-of-the-art (SATO). Here, there are no clear differences between the two models, but both reach higher F1 scores than the other baselines even when given only 1,000 training examples, which demonstrates the power of table-based pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>The results in the previous section show that TAB-BIE is a powerful table representation method, outperforming TaBERT in many downstream task configurations and remaining competitive in the rest. In this section, we dig deeper into TABBIE's representations by comparing them to TaBERT across a variety of quantitative and qualitative analysis tasks, including our own pretraining task of corrupt cell classification, as well as embedding clustering and nearest neighbors. Taken as a whole, the analysis suggests that TABBIE is able to better capture fine-grained table semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corrupt Cell Detection</head><p>We first examine how TaBERT performs on TABBIE's pretraining task of corrupt cell detection, which again is practically useful as a postprocessing step after table structure decomposition <ref type="bibr" target="#b25">(Tensmeyer et al., 2019;</ref><ref type="bibr" target="#b23">Raja et al., 2020)</ref> because mistakes in predicting row/column/cell boundaries (sometimes compounded by OCR errors) can lead to inaccurate extraction. We fine-tune TaBERT on 100K tables using the MIX corruption strategy for  ten epochs, and construct a test set of 10K tables that are unseen by both TaBERT and TABBIE during pretraining. While TABBIE of course sees an order of magnitude more tables for this task during pretraining, this is still a useful experiment to determine if TaBERT's pretraining objective enables it to easily detect corrupted cells. As shown in <ref type="table" target="#tab_10">Table 5</ref>, TaBERT performs significantly worse than TABBIE on all types of corrupt cells (both random corruption and intra-table swaps). Additionally, intra-column swaps are the most difficult for both models, as TABBIE achieves a 68.8 F1 on this subset compared to just 23.7 F1 by TaBERT. Interestingly, while the MIX strategy consistently performs worse than FREQ for the TABBIE models evaluated on the three downstream tasks in the previous section, it is substantially better at detecting more challenging corruptions, and is almost equivalent to detecting random cells sampled by FREQ. This result indicates that perhaps more complex table-based tasks are required to take advantage of representations derived using MIX corruption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Nearest neighbors</head><p>We now turn to a qualitative analysis of the representations learned by TABBIE. In <ref type="figure" target="#fig_12">Figure 6</ref> (top), we display the two nearest neighbor columns from our validation set to the date column marked by the red box. TABBIE is able to model the similarity of feb.   16 and saturday. february 5th despite the formatting difference, while TaBERT's neighbors more closely resemble the original column. <ref type="figure" target="#fig_12">Figure 6</ref> (bottom) shows that TABBIE's nearest neighbors are less reliant on matching headers than TaBERT, as the neighbors all have different headers (nom, nombre, name).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Clustering</head><p>Are the embeddings produced by TABBIE useful for clustering and data discovery? To find out, we perform clustering experiments on the FinTabNet dataset from <ref type="bibr" target="#b35">Zheng et al. (2021)</ref>. This dataset contains ?110K tables from financial reports of corporations in the S&amp;P-500. We use the [CLS] embedding at the (0, 0) position (i.e., the top left-most cell in the table), extracted from a TABBIE model trained with the FREQ strategy, as a representative embedding for each table in the dataset. Next, we perform k-means clustering on these embeddings using the FAISS library <ref type="bibr" target="#b10">(Johnson et al., 2017)</ref>, with k=1024 centroids. While the FinTabNet dataset is restricted to the homogenous domain of financial tables, these tables cluster into sub-types such as consolidated financial tables, jurisdiction tables, insurance tables, etc. We then examine the contents of these clusters <ref type="figure">(Figure 7)</ref> and observe that TABBIE embeddings can not only be clustered into these sub-types, but also that tables from reports of the same company, but from different financial years, are placed into the same cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Identifying numeric trends</head><p>Next, we analyze how well TABBIE understands trends in numerical columns by looking at specific examples of our corrupt cell detection task. The first column of the table in <ref type="figure" target="#fig_11">Figure 5</ref> contains jersey numbers sorted in ascending order. We swap two cells in this column, 16 and 18, which violates the increasing trend. Both TaBERT (fine-tuned for corrupt cell detection) and TABBIE FREQ struggle to identify this swap, while TABBIE MIX is almost certain that the two cells have been corrupted. This qualitative result is further evidence that the MIX model has potential for more complex table-based reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>The staggering amount of structured relational data in the form of tables on the Internet has attracted considerable attention from researchers over the past two decades <ref type="bibr" target="#b0">(Cafarella et al., 2008;</ref><ref type="bibr" target="#b14">Limaye et al., 2010;</ref><ref type="bibr" target="#b27">Venetis et al., 2011;</ref><ref type="bibr" target="#b24">Suchanek et al., 2007;</ref><ref type="bibr" target="#b6">Embley et al., 2006)</ref>, with applications including retrieval <ref type="bibr" target="#b2">(Das Sarma et al., 2012)</ref>, schemamatching <ref type="bibr" target="#b17">(Madhavan et al., 2001</ref><ref type="bibr" target="#b16">(Madhavan et al., , 2005</ref>, and entity linking . Similar to popular large-scale language models pretrained on tasks involving unstructured natural language <ref type="bibr" target="#b21">(Peters et al., 2018;</ref><ref type="bibr" target="#b5">Devlin et al., 2018;</ref><ref type="bibr" target="#b15">Liu et al., 2019)</ref>, our work is part of a recent trend of self-supervised models trained on structured tabular data. TaBERT <ref type="bibr" target="#b30">(Yin et al., 2020)</ref> and TaPaS <ref type="bibr" target="#b7">(Herzig et al., 2020)</ref>    <ref type="table" target="#tab_13">Table of</ref> Contents (first row), but it also places tables of the same type from the same company into the same cluster (second and third rows). We provide the source images of the corresponding tables in this figure.</p><p>with text (typically captions or questions), and are thus more suited for tasks like question answering <ref type="bibr" target="#b20">(Pasupat and Liang, 2015)</ref>. For pretraining, TaBERT attempts to recover the name and datatype of masked column headers (masked column prediction), in addition to contents of a particular cell (cell value recovery). The pretraining objectives of TaPaS, on the other hand, encourage tabular textual entailment. In a concurrent work, the TUTA model <ref type="bibr" target="#b29">(Wang et al., 2020)</ref> uses masked language modeling, cell-level cloze prediction, and tablecontext retrieval as pretraining objectives. Further, in addition to traditional position embeddings, this work accounts for the hierarchical nature of tabular data using tree-based positional embeddings. Similiarly, in <ref type="bibr" target="#b4">Deng et al. (2020)</ref>, the authors perform a variant of MLM called masked entity recovery. In contrast, TABBIE is pretrained strictly on tabular data and intended for more general-purpose tablebased tasks, and uses corrupt-cell classification as its pretraining task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed TABBIE, a selfsupervised pretraining method for tables without associated text. To reduce the computational cost of training our model, we repurpose the ELECTRA objective for corrupt cell detection, and we use two separate Transformers for rows and columns to minimize complexity associated with sequence length. On three downstream table-based tasks, TABBIE achieves competitive or better performance to existing methods such as TaBERT, and an analysis reveals that its representations include a deep semantic understanding of cells, rows, and columns. We publicly release our TABBIE pretrained models and code to facilitate future research on tabular representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ethics Statement</head><p>As with any research work that involves training large language models, we acknowledge that our work has a negative carbon impact on the environment. A cumulative of 1344 GPU-hours of computation was performed on Tesla V100 GPUs. Total emissions are estimated to be 149.19 kg of CO 2 per run of our model (in total, there were two runs). While this is a significant amount (equivalent to ? 17 gallons of fuel consumed by an average motor vehicle 11 ), it is lower than TaBERT's cost per run by more than a factor of 10 assuming a similar computing platform was used. Estimations were conducted using the Machine Learning Impact calculator presented in <ref type="bibr" target="#b12">Lacoste et al. (2019)</ref>. 11 https://www.epa.gov/greenvehicles/</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>TABBIE is a table embedding model trained to detect corrupted cells, inspired by the ELEC-TRA<ref type="bibr" target="#b1">(Clark et al., 2020)</ref> objective function. This simple pretraining objective results in powerful embeddings of cells, columns, and rows, and it yields stateof-the-art results on downstream table-based tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>j ? R H , which model the position of rows and columns, respectively, and are randomly initialized and fine-tuned via TABBIE's self-supervised objective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>TABBIE's computations at one layer. For a given table, the row Transformer contextualizes the representations of the cells in each row, while the column Transformer similarly contextualizes cells in each column. The final cell representation is an average of the row and column embeddings, which is passed as input to the next layer. [CLS] tokens are prepended to each row and column to facilitate downstream tasks operating on table substructures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>swap cells on the same column</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>The different cell corruption strategies used in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>The inputs and outputs for each of our</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 :</head><label>5</label><figDesc>In this figure, (b) and (c) contain the predicted corruption probability of each cell in (a). Only TABBIE MIX is able to reliably identify violations of numerical trends in columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 :</head><label>6</label><figDesc>Nearest neighbors of the date and nom columns from the tables on the left, from both TAB-BIE and TaBERT. TABBIE's nearest neighbors exhibit more diverse formatting and less reliance on the header, which is an example of its semantic representation capability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We aim for as controlled of a comparison with TaBERT<ref type="bibr" target="#b30">(Yin et al., 2020)</ref> as possible, as its performance on table QA tasks indicate the strength of its table encoder. TaBERT's pretraining data was not publicly released at the time of our work, but their dataset consists of 26.6M tables from Wikipedia and the Common Crawl. We thus form a pretraining dataset of equivalent size by combining 1.8M Wikipedia tables with 24.8M preprocessed Common Crawl tables from Viznet. 4</figDesc><table><row><cell>frequency-based sampling strategy above, es-</cell></row><row><cell>pecially when the swapped cells occur within</cell></row><row><cell>the same column. While it underperforms</cell></row><row><cell>frequency-based sampling on downstream</cell></row><row><cell>tasks, it qualitatively results in more semantic</cell></row><row><cell>similarity among nearest neighbors of column</cell></row><row><cell>and row embeddings.</cell></row><row><cell>2.4 Pretraining details</cell></row><row><cell>Data:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>tablebased prediction tasks. Column type prediction does not include headers as part of the table.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Fine-tuning hyperparameters of each downstream task for TABBIE and TaBERT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table captions</head><label>captions</label><figDesc></figDesc><table><row><cell>N</cell><cell>Method</cell><cell cols="4">MAP MRR Ndcg-10 Ndcg-20</cell></row><row><cell></cell><cell>GPM</cell><cell>25.1</cell><cell>37.5</cell><cell>-</cell><cell>-</cell></row><row><cell>1</cell><cell>GPM+TH TaBERT</cell><cell cols="2">25.5 0.38.0 33.1 41.3</cell><cell>27.1 35.1</cell><cell>31.5 38.1</cell></row><row><cell></cell><cell cols="2">TABBIE (FREQ) 37.9</cell><cell>49.1</cell><cell>41.2</cell><cell>43.8</cell></row><row><cell></cell><cell>TABBIE (MIX)</cell><cell>37.1</cell><cell>48.7</cell><cell>40.4</cell><cell>43.1</cell></row><row><cell></cell><cell>GPM</cell><cell>28.5</cell><cell>40.4</cell><cell>-</cell><cell>-</cell></row><row><cell>2</cell><cell>GPM+TH TaBERT</cell><cell>33.2 51.1</cell><cell>44.0 60.1</cell><cell>36.1 54.7</cell><cell>41.3 56.6</cell></row><row><cell></cell><cell cols="2">TABBIE (FREQ) 52.0</cell><cell>62.8</cell><cell>55.8</cell><cell>57.6</cell></row><row><cell></cell><cell>TABBIE (MIX)</cell><cell>51.7</cell><cell>62.3</cell><cell>55.6</cell><cell>57.2</cell></row><row><cell></cell><cell>GPM</cell><cell>28.5</cell><cell>35.5</cell><cell>-</cell><cell>-</cell></row><row><cell>3</cell><cell>GPM+TH TaBERT</cell><cell>40.0 53.3</cell><cell>50.8 60.9</cell><cell>45.2 56.9</cell><cell>48.5 57.9</cell></row><row><cell></cell><cell cols="2">TABBIE (FREQ) 54.5</cell><cell>63.3</cell><cell>57.9</cell><cell>58.9</cell></row><row><cell></cell><cell>TABBIE (MIX)</cell><cell>54.1</cell><cell>62.3</cell><cell>57.4</cell><cell>58.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(unlike GPM and GPM+TH)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>during training. Nevertheless, as Table 2 shows,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABBIE and TaBERT substantially outperform both</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>We sample all tables that have at least five entries in the left-most column, which results in roughly 200K tables. Due to the large number of labels, we resort to negative sampling during training instead of the full softmax to cut down on fine-tuning time. Negative samples are formed by uniform random sampling on the label space.</figDesc><table><row><cell>N</cell><cell>Method</cell><cell cols="4">MAP MRR Ndcg-10 Ndcg-20</cell></row><row><cell></cell><cell>Entitables</cell><cell>36.8</cell><cell>45.2</cell><cell>-</cell><cell>-</cell></row><row><cell>1</cell><cell cols="2">TaBERT TABBIE (FREQ) 42.8 43.2</cell><cell>55.7 54.2</cell><cell>45.6 44.8</cell><cell>47.7 46.9</cell></row><row><cell></cell><cell>TABBIE (MIX)</cell><cell>42.6</cell><cell>54.7</cell><cell>45.1</cell><cell>46.8</cell></row><row><cell></cell><cell>Entitables</cell><cell>37.2</cell><cell>45.1</cell><cell>-</cell><cell>-</cell></row><row><cell>2</cell><cell cols="2">TaBERT TABBIE (FREQ) 44.4 43.8</cell><cell>56.0 57.2</cell><cell>46.4 47.1</cell><cell>48.8 49.5</cell></row><row><cell></cell><cell>TABBIE (MIX)</cell><cell>43.7</cell><cell>55.7</cell><cell>46.2</cell><cell>48.6</cell></row><row><cell></cell><cell>Entitables</cell><cell>37.1</cell><cell>44.6</cell><cell>-</cell><cell>-</cell></row><row><cell>3</cell><cell cols="2">TaBERT TABBIE (FREQ) 43.4 42.9</cell><cell>55.1 56.5</cell><cell>45.6 46.6</cell><cell>48.5 49.0</cell></row><row><cell></cell><cell>TABBIE (MIX)</cell><cell>42.9</cell><cell>55.5</cell><cell>45.9</cell><cell>48.3</cell></row><row><cell>7 Our label space</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>consists of 300K entities that occur at least twice</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>in Wikipedia tables, and we again formulate this</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>problem as multi-label classification, this time on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>top of the first column's [CLSCOL] representation. 8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>On this task, TaBERT and TABBIE again outper-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>form the baseline Entitables model (which uses</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>external information in the form of table cap-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>78</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>TABBIE outperforms baselines on row population when provided with more seed rows N , although TaBERT is superior given just a single seed row. Again, the FREQ strategy produces better results than MIX.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: A fine-grained comparison of different models</cell></row><row><cell>on corrupt cell detection, with different types of corrup-</cell></row><row><cell>tion. TaBERT struggles on this task, especially in the</cell></row><row><cell>challenging setting of intra-column swaps. Unlike our</cell></row><row><cell>downstream tasks, the MIX strategy is far superior to</cell></row><row><cell>FREQ here.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>table 16</head><label>16</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">(b) TABBIE (MIX)</cell><cell cols="2">(c) TaBERT</cell><cell></cell></row><row><cell></cell><cell></cell><cell>#</cell><cell>name</cell><cell>year</cell><cell>#</cell><cell>name</cell><cell>year</cell></row><row><cell></cell><cell></cell><cell>0.0%</cell><cell>0.1%</cell><cell>0.0%</cell><cell>2.6%</cell><cell>1.6%</cell><cell>8.9%</cell></row><row><cell></cell><cell></cell><cell>100%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>3.2%</cell><cell>2.6%</cell><cell>1.9%</cell></row><row><cell></cell><cell></cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>4.3%</cell><cell>7.6%</cell><cell>5.2%</cell></row><row><cell></cell><cell>hydn sophomore</cell><cell>99.9%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>2.2%</cell><cell>0.3%</cell><cell>0.5%</cell></row><row><cell>19</cell><cell>hayley sophomore</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>3.3%</cell><cell>3.3%</cell><cell>1.5%</cell></row><row><cell cols="2">20 michelle graduate</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>4.0%</cell><cell>6.6%</cell><cell>2.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>jointly model tables Semantic type Sample Tables Centroid No.</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table of</head><label>of</label><figDesc>Sample tables from clusters obtained by running k-means on TABBIE's [CLS] embeddings on the FinTab-Net dataset. TABBIE not only clusters embeddings into reasonable semantic types, such as</figDesc><table><row><cell>contents</cell><cell>23</cell></row><row><cell>Investment income</cell><cell></cell></row><row><cell>table for Everest Re</cell><cell>190</cell></row><row><cell>Group</cell><cell></cell></row><row><cell>Market share table</cell><cell></cell></row><row><cell>for Phillip Morris</cell><cell>295</cell></row><row><cell>International</cell><cell></cell></row><row><cell>Figure 7:</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"><ref type="bibr" target="#b7">Herzig et al. (2020)</ref> use a fixed limit of 128 tokens for both text and table, while Yin et al. (2020) drop all but three rows of the table during pretraining.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/SFIG611/tabbie</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use the BERT-base-uncased model in all experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Note that TaBERT's pretraining data likely includes the test set tables, which may give it an advantage in our comparisons.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Again, we ensure that none of the test tables in this dataset occur in TABBIE's pretraining data. 10 https://github.com/megagonlabs/sato</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="0">date</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their useful comments. We thank Christopher Tensmeyer for helpful comments and pointing us to relevant datasets for some of our experiments. We also thank the UMass NLP group for feedback during the paper writing process. This work was made possible by research awards from Sony Corp. and Adobe Inc. MI is also partially supported by award IIS-1955567 from the National Science Foundation (NSF).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Webtables: Exploring the power of tables on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.14778/1453856.1453916</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="538" to="549" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding related tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish Das</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujun</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongrae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reynold</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2213836.2213962</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;12</title>
		<meeting>the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="817" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ta-ble2vec: Neural word and entity embeddings for table population and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SI-GIR</title>
		<meeting>SI-GIR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Turl: Table understanding through representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Lees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="307" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Table-processing paradigms: a research survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Embley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="66" to="86" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tapas: Weakly supervised table parsing via pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Viznet: Towards A large-scale visualization learning and benchmarking repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Zeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snehalkumar (</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><forename type="middle">S</forename><surname>Gaikwad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelon</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?sar</forename><forename type="middle">A</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?agatay</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demiralp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI 2019</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems, CHI 2019<address><addrLine>Glasgow, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sherlock: A deep learning approach to semantic data type detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demiralp</forename><surname>Ccaugatay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C&amp;apos;esar A</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with gpus</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Quantifying the carbon emissions of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dandres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09700</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Memory augmented policy optimization for program synthesis and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Annotating and searching web tables using entities, types and relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girija</forename><surname>Limaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="DOI">10.14778/1920841.1921005</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1338" to="1347" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Corpus-based schema matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE.2005.39</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Data Engineering, ICDE &apos;05</title>
		<meeting>the 21st International Conference on Data Engineering, ICDE &apos;05<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generic schema matching with cupid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Rahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Very Large Data Bases, VLDB &apos;01</title>
		<meeting>the 27th International Conference on Very Large Data Bases, VLDB &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding the semantic structures of tables with a hybrid deep neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kugatsu</forename><surname>Sadamitsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichiro</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey of approaches to automatic schema matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Rahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="334" to="350" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Table structure recognition using top-down and bottom-up cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajoy</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Yago: A core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
		<meeting>the 16th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep splitting and merging for table structure decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2019.00027</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recovering semantics of tables on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Venetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pa?ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengxin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.14778/2002938.2002939</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="528" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do NLP models know numbers? probing numeracy in embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structureaware pre-training for table understanding with treebased transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2010.12537</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TaBERT: Pretraining for joint understanding of textual and tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sato: Contextual semantic type detection in tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelon</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?agatay</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Entitables: Smart assistance for entity-focused tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Web table extraction, retrieval, and augmentation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Novel entity discovery from web tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ridho</forename><surname>Reinanda</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380205</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020, WWW &apos;20</title>
		<meeting>The Web Conference 2020, WWW &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1298" to="1308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy Xin Ru</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
