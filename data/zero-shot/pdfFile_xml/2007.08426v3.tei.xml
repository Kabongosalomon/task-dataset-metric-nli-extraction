<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Investigating Pretrained Language Models for Graph-to-Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing (CIS)</orgName>
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab</orgName>
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">LMU Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmitt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing (CIS)</orgName>
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab</orgName>
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">LMU Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing (CIS)</orgName>
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab</orgName>
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">LMU Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing (CIS)</orgName>
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab</orgName>
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">LMU Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Investigating Pretrained Language Models for Graph-to-Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recent pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that approaches based on PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. We report new state-of-the-art BLEU scores of 49.72 on AMR-LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets -a relative improvement of 31.8%, 4.5%, and 42.4%, respectively, with our models generating significantly more fluent texts than human references. In an extensive analysis, we identify possible reasons for the PLMs' success on graph-totext tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are important data structures in NLP as they represent complex relations within a set of objects. For example, semantic and syntactic structures of sentences can be represented using different graph representations (e.g., <ref type="bibr">AMRs, Banarescu et al., 2013;</ref><ref type="bibr">semantic-role labeling, Surdeanu et al., 2008;</ref><ref type="bibr">syntactic and semantic graphs, Belz et al., 2011)</ref> and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities <ref type="bibr">(Gardent et al., 2017;</ref><ref type="bibr" target="#b33">Vougiouklis et al., 2018;</ref><ref type="bibr" target="#b7">Koncel-Kedziorski et al., 2019)</ref>.</p><p>Graph-to-text generation, a subtask of data-totext generation <ref type="bibr">(Gatt and Krahmer, 2018)</ref>, aims to create fluent natural language text to describe an input graph (see <ref type="figure" target="#fig_1">Figure 1</ref>). This task is important for NLP applications such as dialogue generation <ref type="bibr" target="#b14">(Moon et al., 2019)</ref> and question answering <ref type="bibr">(Duan et al., 2017)</ref>. Recently, it has been shown that structured meaning representation, such as AMR or KG, can store the internal state of a dialog system, providing core semantic knowledge <ref type="bibr">(Bonial et al., 2020;</ref><ref type="bibr">Bai et al., 2021)</ref> or can be the result of a database query for conversational QA <ref type="bibr" target="#b41">(Yu et al., 2019)</ref>. Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators <ref type="bibr">(Cheng et al., 2020)</ref>.</p><p>Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> have considerably outperformed prior state of the art in various downstream tasks <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr" target="#b38">Yang et al., 2019a;</ref><ref type="bibr" target="#b20">Radford et al., 2019)</ref>.</p><p>In this paper, we analyze the applicability of two recent text-to-text pretrained language models (PLMs), BART  and T5 <ref type="bibr" target="#b21">(Raffel et al., 2019)</ref>, for graph-to-text generation. We choose these models because of their encoderdecoder architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We further introduce task-adaptive graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin.</p><p>While recent works have shown the benefit of explicitly encoding the graph structure in graph-totext generation <ref type="bibr" target="#b29">(Song et al., 2018;</ref><ref type="bibr" target="#b22">Ribeiro et al., 2019</ref><ref type="bibr" target="#b24">Ribeiro et al., , 2020</ref><ref type="bibr">Schmitt et al., 2020;</ref><ref type="bibr" target="#b43">Zhao et al., 2020a,</ref> to name a few), our approaches based on PLMs consistently outperform these models, even though PLMs -as sequence models -do not exhibit any Linearized representation: &lt;H&gt; Apollo 12 &lt;R&gt; backup pilot &lt;T&gt; Alfred Worden &lt;H&gt; Alan Bean &lt;R&gt; was a crew member of &lt;T&gt; Apollo 12 &lt;H&gt; Apollo 12 &lt;R&gt; operator &lt;T&gt; NASA &lt;H&gt; Alan Bean &lt;R&gt; occupation &lt;T&gt; Test pilot &lt;H&gt; Apollo 12 &lt;R&gt; commander &lt;T&gt; David Scott &lt;H&gt; Alan Bean &lt;R&gt; was selected by NASA &lt;T&gt; 1963 &lt;H&gt; Alan Bean &lt;R&gt; alma Mater &lt;T&gt; <ref type="bibr">UT Austin B.S. 1955</ref>  Text: Alan Bean graduated from UT Austin in 1955 with a Bachelor of Science degree. He was hired by NASA in 1963 and served as a test pilot. Apollo 12's backup pilot was Alfred Worden and was commanded by David Scott.</p><p>Text: As his children, we feel very terrible now. graph-specific structural bias. 2 Simply representing the graph as a linear traversal (see <ref type="figure" target="#fig_1">Figure 1</ref>) leads to remarkable generation performance in the presence of a strong language model. In our analysis we investigate to what extent fine-tuned PLMs make use of the graph structure represented in the graph linearization. We notably observe that PLMs achieve high performance on two popular KG-totext benchmarks even when the KG is reduced to a mere bag of node and edge labels.</p><p>Our contributions are the following:</p><p>? We investigate and compare two PLMs, BART and T5, for graph-to-text generation, exploring language model adaptation (LMA) and supervised task adaptation (STA) pretraining, employing additional task-specific data. ? Our approaches consistently outperform the state of the art by significant margins, ranging from 2.6 to 12.0 BLEU points, on three established graph-to-text benchmarks from different domains, exceeding specialized graph architectures (e.g., Graph Neural Networks, GNNs, Kipf and Welling, 2017). ? In a crowdsourcing experiment, we demonstrate that our methods generate texts with significantly better fluency than existing works and the human references. ? We discover that PLMs perform well even when trained on a shuffled linearized graph representation without any information about connectivity (bag of node and edge labels), which is surprising since prior studies showed that explicitly encoding the graph structure improves models trained from scratch (e.g.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph-to-text Learning. Various neural models have been proposed to generate sentences from graphs from different domains. <ref type="bibr" target="#b8">Konstas et al. (2017)</ref> propose the first neural approach for AMRto-text generation that uses a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input <ref type="bibr" target="#b31">(Trisedya et al., 2018;</ref><ref type="bibr" target="#b15">Moryossef et al., 2019;</ref><ref type="bibr">Castro Ferreira et al., 2019;</ref><ref type="bibr" target="#b23">Ribeiro et al., 2021a)</ref>. Recent approaches <ref type="bibr" target="#b13">(Marcheggiani and Perez Beltrachini, 2018;</ref><ref type="bibr" target="#b29">Song et al., 2018;</ref><ref type="bibr">Beck et al., 2018;</ref><ref type="bibr">Damonte and Cohen, 2019;</ref><ref type="bibr" target="#b22">Ribeiro et al., 2019;</ref><ref type="bibr" target="#b43">Zhao et al., 2020a;</ref><ref type="bibr" target="#b26">Schmitt et al., 2021;</ref><ref type="bibr" target="#b25">Ribeiro et al., 2021b)</ref> propose architectures based on GNNs to directly encode the graph structure, whereas other efforts <ref type="bibr" target="#b24">(Ribeiro et al., 2020;</ref><ref type="bibr">Schmitt et al., 2020;</ref><ref type="bibr" target="#b40">Yao et al., 2020;</ref> inject the graph structure information into Transformer-based architectures. The success of those approaches suggests that imposing a strong relational inductive bias into the graph-to-text model can assist the generation.</p><p>Pretrained Language Models. Pretrained Transformer-based models, such as <ref type="bibr">BERT (Devlin et al., 2019)</ref>, XLNet <ref type="bibr" target="#b39">(Yang et al., 2019b)</ref>, or RoBERTa , have established a qualitatively new level of baseline performance for many widely used natural language understanding (NLU) benchmarks.</p><p>Generative pretrained Transformer-based methods, such as GPT-2 <ref type="bibr" target="#b20">(Radford et al., 2019)</ref>, BART , and T5 <ref type="bibr" target="#b21">(Raffel et al., 2019)</ref>, are employed in many natural language generation (NLG) tasks. <ref type="bibr" target="#b12">Mager et al. (2020)</ref> were the first to employ GPT-2, a decoder-only PLM, for AMR-to-text generation and use cycle consistency to improve the adequacy. In contrast, we are the first to investigate BART and T5 models, which have both a Transformer-based encoder and decoder, in AMRto-text generation. Recently, <ref type="bibr" target="#b2">Harkous et al. (2020)</ref> and <ref type="bibr" target="#b4">Kale (2020)</ref> demonstrate state-of-the-art results in different data-to-text datasets, employing GPT-2 and T5 models respectively. <ref type="bibr" target="#b19">Radev et al. (2020)</ref> propose DART, a new data-to-text dataset, and train a BART model gradually augmenting the WebNLG training data with DART data. <ref type="bibr" target="#b3">Hoyle et al. (2021)</ref> explore scaffolding objectives in PLMs and show gains in low-resource graph-to-text settings. Different from the above works, we focus on a general transfer learning strategies for graph-to-text generation, investigating task-adaptive pretraining approaches, employing additional collected task-specific data for different PLMs (BART and T5) and benchmarks. In addition, we provide a detailed analysis aimed at explaining the good performance of PLMs on KGto-text tasks.</p><p>Recently, Gururangan et al. (2020) explored taskadaptive pretraining strategies for text classification. While our LMA (see ?3) is related to their DAPT as both use a self-supervised objective on a domainspecific corpus, they notably differ in that DAPT operates on the model input while LMA models the output. We are the first to show the benefits of additional task-specific pretraining in PLMs for graph-to-text tasks.</p><p>3 PLMs for Graph-to-Text Generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Models in this Study</head><p>We investigate BART  and T5 <ref type="bibr" target="#b21">(Raffel et al., 2019)</ref>, two PLMs based on the Transformer encoder-decoder architecture <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref>, for graph-to-text generation. They mainly differ in how they are pretrained and the input corpora used for pretraining. We experiment with different T5 (small -60M parameters, base -220M, and large -770M) and BART (base -140M and large -400M) capacity models.</p><p>We fine-tune both PLMs for a few epochs on the supervised downstream graph-to-text datasets. For T5, in the supervised setup, we add a prefix "translate from Graph to Text:" before the graph input. We add this prefix to imitate the T5 setup, when translating between different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task-specific Adaptation</head><p>Inspired by previous work <ref type="bibr" target="#b8">(Konstas et al., 2017;</ref><ref type="bibr">Gururangan et al., 2020)</ref>, we investigate whether leveraging additional task-specific data can improve the PLMs' performance on graph-to-text generation. Task-specific data refers to a pretraining corpus that is more task-relevant and usually smaller than the text corpora used for taskindependent pretraining. In order to leverage the task-specific data, we add an intermediate adaptive pretraining step between the original pretraining and fine-tuning phases for graph-to-text generation.</p><p>More precisely, we first continue pretraining BART and T5 using language model adaptation (LMA) or supervised task adaptation (STA) training. In the supervised approach, we use pairs of graphs and corresponding texts collected from the same or similar domain as the target task. In the LMA approach, we follow BART and T5 pretraining strategies for language modeling, using the reference texts that describe the graphs. Note that we do not use the graphs in the LMA pretraining, but only the target text of our task-specific data collections. The goal is to adapt the decoder to the domain of the final task <ref type="bibr">(Gururangan et al., 2020)</ref>. In particular, we randomly mask text spans, replacing 15% of the tokens. 3 Before evaluation, we finally fine-tune the models using the original training set as usual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>We evaluate the text-to-text PLMs on three graph-to-text benchmarks: AMR (LDC2017T10), <ref type="bibr">WebNLG (Gardent et al., 2017)</ref>, and AGENDA <ref type="bibr" target="#b7">(Koncel-Kedziorski et al., 2019)</ref>. We chose those datasets because they comprise different domains and are widely used in prior work. <ref type="table" target="#tab_20">Table 10</ref> in Appendix shows statistics for each dataset.</p><p>AMR. Abstract meaning representation (AMR) is a semantic formalism that represents the meaning of a sentence as a rooted directed graph expressing "who is doing what to whom" <ref type="bibr">(Banarescu et al., 2013)</ref>. In an AMR graph, nodes represent concepts and edges represent semantic relations. An instance in LDC2017T10 consists of a sentence annotated with its corresponding AMR graph. Following <ref type="bibr" target="#b12">Mager et al. (2020)</ref>, we linearize the AMR graphs using the PENMAN notation (see <ref type="figure" target="#fig_1">Figure 1a</ref>). 4</p><p>WebNLG. Each instance of WebNLG contains a KG from DBPedia <ref type="bibr">(Auer et al., 2007)</ref> and a target text with one or multiple sentences that describe the graph. The test set is divided into two partitions: seen, which contains only DBPedia categories present in the training set, and unseen, which covers categories never seen during training. Their union is called all. Following previous work <ref type="bibr" target="#b2">(Harkous et al., 2020)</ref>, we prepend H , R , and T tokens before the head entity, the relation and tail entity of a triple (see <ref type="figure" target="#fig_1">Figure 1b</ref>).</p><p>AGENDA. In this dataset, KGs are paired with scientific abstracts extracted from proceedings of AI conferences. Each sample contains the paper title, a KG, and the corresponding abstract. The KG contains entities corresponding to scientific terms and the edges represent relations between these entities. This dataset has loose alignments between the graph and the corresponding text as the graphs were automatically generated. The input for the models is a text containing the title, a sequence of all KG entities, and the triples. The target text is the paper abstract. We add special tokens into the triples in the same way as for WebNLG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Additional Task-specific Data</head><p>In order to evaluate the proposed task-adaptive pretraining strategies for graph-to-text generation, we collect task-specific data for two graph domains: meaning representations (like AMR) and scientific data (like AGENDA). We did not attempt collecting additional data like WebNLG because the texts in this benchmark do not stem from a corpus but were specifically written by annotators.</p><p>AMR Silver Data. In order to generate additional data for AMR, we sample two sentence collections of size 200K and 2M from the Gigaword 5 corpus and use a state-of-the-art AMR parser (Cai and Lam, 2020a) to parse them into AMR graphs. <ref type="bibr">6</ref> For supervised pretraining, we condition a model on the AMR silver graphs to generate the corresponding sentences before fine-tuning it on gold AMR graphs. For self-supervised pretraining, we only use the sentences. 7 Semantic Scholar AI Data. We collect titles and abstracts of around 190K scientific papers from the Semantic Scholar <ref type="bibr" target="#b0">(Ammar et al., 2018)</ref> taken from the proceedings of 36 top Computer Science/AI conferences. We construct KGs from the paper abstracts employing DyGIE++ <ref type="bibr" target="#b34">(Wadden et al., 2019)</ref>, an information extraction system for scientific texts. Note that the AGENDA dataset was constructed using the older SciIE system <ref type="bibr" target="#b11">(Luan et al., 2018)</ref>, which also extracts KGs from AI scientific papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>A second difference is that in our new dataset, the domain is broader as we collected data from 36 conferences compared to 12 from AGENDA. Furthermore, to prevent data leakage, all AGENDA samples used for performance evaluation are removed from our dataset. We will call the new dataset KGAIA (KGs from AI Abstracts). 8 <ref type="table" target="#tab_21">Table 11</ref> in Appendix shows relevant dataset statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We modify the BART and T5 implementations released by Hugging Face <ref type="bibr" target="#b37">(Wolf et al., 2019)</ref> in order to adapt them to graph-to-text generation. For the KG datasets, we add the H , R , and T tokens to the models' vocabulary. We add all edge labels seen in the training set to the vocabulary of the <ref type="bibr">8</ref> We will release the collected additional task-specific data.  with an initial learning rate of 3 ? 10 ?5 . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from {2,4,8} and {1,3,5}, respectively, based on the respective development set. Dev BLEU is used for model selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU</head><p>Following previous works, we evaluate the results with BLEU <ref type="bibr" target="#b16">(Papineni et al., 2002)</ref>, ME-TEOR (Denkowski and Lavie, 2014), and chrF++ <ref type="bibr" target="#b18">(Popovi?, 2015)</ref> metrics. We also use Mover-Score <ref type="bibr" target="#b44">(Zhao et al., 2019)</ref>, BERTScore , and BLEURT <ref type="bibr" target="#b28">(Sellam et al., 2020)</ref> metrics, as they employ contextual and semantic knowledge and thus depend less on the surface symbols. Additionally, we perform a human evaluation (cf. ?5.4) quantifying the fluency, semantic adequacy and meaning similarity of the generated texts. <ref type="table">Table 1</ref> shows our results for the setting without additional pretraining, with additional self-supervised task-adaptive pretraining solely using the collected Gigaword sentences (LMA), and with additional supervised task adaptation (STA), before fine-tuning. We also report several recent results on the AMR test set. <ref type="bibr" target="#b12">Mager et al. (2020)</ref> and <ref type="bibr" target="#b2">Harkous et al. (2020)</ref> employ GPT-2 in their approaches. Note that GPT-2 only consists of a Transformer-based decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on AMR-to-Text</head><p>Only considering approaches without task adaptation, BART large already achieves a considerable improvement of 5.77 BLEU and 3.98 METEOR scores over the previous state of the art. With a BLEU score of 45.80, T5 large performs best. The other metrics follow similar trends. See <ref type="table" target="#tab_4">Table 13</ref> in Appendix for evaluation with more metrics. The strong performance of both BART and T5 in the AMR dataset suggests that PLMs can infer the AMR structure by a simple linear sequence of the graph, in contrast to GNN-based models that explicitly consider the graph structure using messagepassing between adjacent nodes <ref type="bibr">(Beck et al., 2018)</ref>.</p><p>Task-specific Adaptation. LMA already brings some gains with T5 benefitting more than BART in most metrics. It still helps less than STA even though we only have automatically generated annotations. This suggests that the performance increases with STA do not only come from additional exposure to task-specific target texts and that the models learn how to handle graphs and the graphtext correspondence even with automatically generated AMRs. After STA, T5 achieves 49.72 BLEU points, the new state of the art for AMR-to-text generation. Interestingly, gains from STA with 2M over 200K are larger in BART than in T5, suggesting that large amounts of silver data may not be required for a good performance with T5.</p><p>In general, models pretrained on the STA setup converge faster than without task-specific adaptation. For example, T5 large without additional pretraining converges after 5 epochs of fine-tuning whereas T5 large with STA already converges after 2 epochs.  hand, fully end-to-end models <ref type="bibr" target="#b24">(Ribeiro et al., 2020;</ref><ref type="bibr">Schmitt et al., 2020)</ref> have strong performance on the seen dataset and usually perform poorly in unseen data. Models that explicitly encode the graph structure <ref type="bibr" target="#b24">(Ribeiro et al., 2020;</ref><ref type="bibr" target="#b43">Zhao et al., 2020a)</ref> achieve the best performance among approaches that do not employ PLMs. Note that T5 is also used in <ref type="bibr" target="#b4">Kale (2020)</ref>. Differences in our T5 setup include a modified model vocabulary, the use of beam search, the learning rate schedule and the prefix before the input graph. Our T5 approach achieves 59.70, 65.05 and 54.69 BLEU points on all, seen and unseen sets, the new state of the art. We conjecture that the performance gap between seen and unseen sets stems from the advantage obtained by a model seeing examples of relation-text pairs during fine-tuning. For example, the relation party (political party) was never seen during training and the model is required to generate a text that verbalizes the tuple: Abdul Taib Mahmud, party, Parti Bumiputera Sarawak . Interestingly, BART performs much worse than T5 on this benchmark, especially in the unseen partition with 9.7 BLEU points lower compared to T5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on WebNLG</head><p>For lack of a suitable data source (cf. ?4), we did not explore our LMA or STA approaches for WebNLG. However, we additionally discuss crossdomain STA in Appendix B.  dataset. We believe that their capacity to generate fluent text helps when generating paper abstracts, even though they were not pretrained in the scientific domain. BART large shows an impressive performance with a BLEU score of 23.65, which is 5.6 points higher than the previous state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on AGENDA</head><p>Task-specific Adaptation. On AGENDA, BART benefits more from our task-adaptive pretraining, achieving the new state of the art of 25.66 BLEU points, a further gain of 2 BLEU points compared to its performance without task adaptation. The improvements from task-adaptive pretraining are not as large as for AMR. We hypothesize that this is due to the fact that the graphs do not completely cover the target text <ref type="bibr" target="#b7">(Koncel-Kedziorski et al., 2019)</ref>, making this dataset more challenging. See <ref type="table" target="#tab_2">Table 12</ref> in Appendix for more automatic metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Human Evaluation</head><p>To further assess the quality of the generated text, we conduct a human evaluation on AMR and WebNLG via crowd sourcing on Amazon Mechanical Turk. 9 Following previous works <ref type="bibr">(Gardent et al., 2017;</ref><ref type="bibr">Castro Ferreira et al., 2019)</ref>, we assess three quality criteria: (i) Fluency (i.e., does the text flow in a natural, easy-to-read manner?), for AMR and WebNLG; Arrabbiata sauce can be found in Italy where Sergio Mattarella is the leader and the capital city is Rome. Italians are the people who live there and the language spoken is Italian.</p><p>Italians live in Italy where the capital is Rome and the language is Italian. Sergio Mattarella is the leader of the country and arrabbiata sauce can be found there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5</head><p>Reference: Arrabbiata sauce is from Italy where the capital is Rome, Italian is the language spoken and Sergio Mattarella is a leader. close in meaning is the generated text to the reference sentence?) for AMR; (ii) Semantic Adequacy (i.e., does the text clearly express the data?) for WebNLG. We randomly select 100 generated texts of each model, which the annotators then rate on a 1-7 Likert scale. For each text, we collect scores from 3 annotators and average them. 10 <ref type="table" target="#tab_6">Table 4</ref> shows the results. Our approaches improve the fluency, meaning similarity, and semantic adequacy on both datasets compared to other stateof-the-art approaches with statistically significant margins (p&lt;0.05). Interestingly, the highest fluency improvement (+0.97) is on AMR, where our approach also has the largest BLEU improvement (+8.10) over <ref type="bibr" target="#b2">Harkous et al. (2020)</ref>. Finally, our models score higher than the references in fluency with statistically significant margins, highlighting their strong language generation abilities. 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Limiting the Training Data</head><p>In <ref type="figure">Figure 3,</ref>   fine-tuning. We find that, when fine-tuned with only 40% of the data, both BART and T5 already greatly improve the performance compared to using the entire training data in all three benchmarks. For example, BART fine-tuned on 40% of AMR training data achieves 91% of the BLEU score when fine-tuned on full data. Note that in a low-resource scenario in AMR and WebNLG, T5 considerably outperforms BART. In particular, with only 1% of training examples, the difference between T5 and BART is 7.51 and 5.64 BLEU points for AMR and WebNLG, respectively. This suggests that T5 is more data efficient when adapting to the new task, likewise our findings in AMR-STA (cf. ?5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Influence of the Graph Structure</head><p>We conduct further experiments to examine how much the PLMs consider the graph structure. To this end, we remove parentheses in AMRs and replace H , R , and T tokens with neutral separator tokens, denoted ?, for KGs, such that the graph structure is only defined by the order of node and edge labels. If we shuffle such a sequence, the graph structure is thus completely obscured and the input effectively becomes a bag of node and edge labels. See <ref type="figure">Figure 2</ref> for an example of both a correctly ordered and a shuffled triple sequence. Antwerp International Airport serves the city of Antwerp in Belgium where the German language is spoken and Charles Michel is the leader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Quantitative Analysis</head><p>(</p><formula xml:id="formula_1">2) T ? California ? is Part Of ? US ? California ? capital ? Sacramento</formula><p>California is part of the United States and its capital is Sacramento.</p><p>California is part of the United States and its capital is Sacramento.  vs. shuffled ones (T5 shuf ) for both training and evaluation. We first observe that T5 order only has marginally lower performance (around 2-4%) with the neutral separators than with the H / R / T tags or parentheses. <ref type="bibr">12</ref> We see that as evidence that the graph structure is similarly well captured by T5 order . Without the graph structure (T5 shuf ), AMR-to-text performance drops significantly. Possible explanations of this drop are: (i) the relative ordering of the AMR graph is known to correlate with the target sentence order <ref type="bibr" target="#b8">(Konstas et al., 2017)</ref>; (ii) in contrast to WebNLG that contains common knowledge, the AMR dataset contains very specific sentences with higher surprisal; 13 (iii) AMRs are much more complex graph structures than the KGs from WebNLG and AGENDA. 14 On the other hand, KG-to-text performance is not much lower, indicating that most of the PLMs' success in this task stems from their language modeling rather than their graph encoding capabilities. We hypothesize that a PLM can match the entities in a shuffled input with sentences mentioning these entities from the pretraining or fine-tuning phase. It has recently been argued that large PLMs can recall certain common knowledge facts from pretraining <ref type="bibr" target="#b17">(Petroni et al., 2019;</ref><ref type="bibr">Bosselut et al., 2019)</ref>.</p><formula xml:id="formula_2">(3) F ? US ? is Part Of ? California ? California ? capital ? Sacramento California'</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Qualitative Analysis</head><p>The example in <ref type="figure">Figure 2</ref> confirms our impression. T5 shuf produces a text with the same content as 12 See a more fine-grained comparison in Appendix C. <ref type="bibr">13</ref> Perplexities estimated on the dev sets of AMR and WebNLG datasets, with GPT-2 fine-tuned on the corresponding training set, are 20.9 and 7.8, respectively. 14 In Appendix D, we present the graph properties of the datasets and discuss the differences. T5 order but does not need the correct triple structure to do so. Example (1) in <ref type="table" target="#tab_12">Table 6</ref> shows the output of both models with shuffled input. Interestingly, even T5 order produces a reasonable and truthful text. This suggests that previously seen facts serve as a strong guide during text generation, even for models that were fine-tuned with a clearly marked graph structure, suggesting that T5 order also relies more on language modeling than the graph structure. It does have more difficulties covering the whole input graph though. The fact that Antwerp is located in Belgium is missing from its output.</p><p>To further test our hypothesis that PLMs make use of previously seen facts during KG-to-text generation, we generate example true facts, corrupt them in a controlled setting, and feed them to both T5 order and T5 shuf to observe their output (examples (2)-(5) in <ref type="table" target="#tab_12">Table 6</ref>). The model trained on correctly ordered input has learned a bit more to rely on the input graph structure. The false fact in example <ref type="formula" target="#formula_0">(3)</ref> with two triples is reliably transferred to the text by T5 order but not by T5 shuf , which silently corrects it. Also note that, in example (5), both models refuse to generate an incorrect fact. More examples can be found in <ref type="table" target="#tab_6">Table 14</ref> in the Appendix.</p><p>Our qualitative analysis illustrates that state-ofthe-art PLMs, despite their fluency capacities (cf. ?5.4), bear the risk of parroting back training sentences while ignoring the input structure. This issue can limit the practical usage of those models as, in many cases, it is important for a generation model to stay true to its input <ref type="bibr" target="#b36">(Wiseman et al., 2017;</ref><ref type="bibr">Falke et al., 2019)</ref>.</p><p>We investigated two pretrained language models (PLMs) for graph-to-text generation and show that the pretraining strategies, language model adaptation (LMA) and supervised task adaptation (STA), can lead to notable improvements. Our approaches outperform the state of the art by a substantial margin on three graph-to-text benchmarks. Moreover, in a human evaluation our generated texts are perceived significantly more fluent than human references. Examining the influence of the graph structure on the text generation process, we find that PLMs may not always follow the graph structure and instead use memorized facts to guide the generation. A promising direction for future work is to explore ways of injecting a stronger graphstructural bias into PLMs, thus possibly leveraging their strong language modeling capabilities and keeping the output faithful to the input graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>In this supplementary material, we provide: (i) additional information about the data used in the experiments, and (ii) results that we could not fit into the main body of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A AMR Input Representation</head><p>We test three variants for the representation of the input AMR graph. Following previous work <ref type="bibr" target="#b8">(Konstas et al., 2017;</ref><ref type="bibr" target="#b12">Mager et al., 2020)</ref>, we evaluate (i) only node representation, where the edge information is removed from the linearization; (ii) depth-first search (DFS) through the graph and the (iii) PENMAN representation. An example for each representation is illustrated below:</p><p>only nodes value interrogative commodity true DFS value :mode interrogative :ARG1 commodity :ARG1-of true PENMAN ( value :mode interrogative :ARG1 ( commodity ) :ARG1-of ( true ) )</p><p>In this experiment we employ T5 small .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Cross-domain Adaptation</head><p>For a given task, it is not always possible to collect closely related data -as we saw, e.g., for WebNLG. We therefore report STA in a cross-domain setting for the different KG-to-text benchmarks. <ref type="table" target="#tab_17">Table 8</ref> shows the results using BART base and T5 base . While the texts in KGAIA and AGENDA share the domain of scientific abstracts, texts in WebNLG are more general. Also note that WebNLG graphs do not share any relations with the other KGs. For BART base , STA increases the performance in the cross-domain setting in most of the cases. For T5 base , STA in KGAIA improves the performance on WebNLG.</p><p>In general, we find that exploring additional adaptive pretraining for graph-to-text generation can improve the performance even if the data do not come from the same domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STA on</head><p>Fine  C Input Graph Size <ref type="figure" target="#fig_4">Figure 4</ref> visualizes T5 small 's performance with respect to the number of input graph triples in WebNLG dataset. We observe that T5 order and T5 shuf perform similarly for inputs with only one triple but that the gap between the models increases with larger graphs. While it is obviously more difficult to reconstruct a larger graph than a smaller one, this also suggests that the graph structure is more taken into account for graphs with more than 2 triples. For the unseen setting, the performance gap for these graphs is even larger, suggesting that the PLM can make more use of the graph structure when it has to.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Graph Statistics</head><p>In <ref type="table" target="#tab_19">Table 9</ref>, we present the graph properties of the three datasets. All statistics are calculated using AMR WebNLG AGENDA min, avg and max number of nodes 2 28.6 335 2 6.8 15 2 10.5 80 min, avg and max node degrees 1 2.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Model Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMR</head><p>Reference I had to deal with verbal abuse from my dad for a long 8 years before I came to uni and honestly, the only reason why I'm here is because it was the only way out. T5 I had to deal with 8 years of verbal abuse from my dad before coming to university and honestly the only reason I'm here is because it's the only way out. BART I had to deal with my dad's verbal abuse for 8 years long before coming to uni and honestly the only reason I'm here is because it's the only way out. <ref type="bibr" target="#b12">Mager et al. (2020)</ref> i've had to deal with verbal abuse from my dad for 8 years (before i came to uni i was honestly the only reason i was here) and it's only because of the way it is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WebNLG</head><p>Reference Aaron Turner is an electric guitar player who has played with the black metal band Twilight and with Old Man Gloom. Death metal is a musical fusion of black metal. T5</p><p>Aaron Turner plays the electric guitar and is associated with the band Twilight. He is also a member of the Old Man Gloom band. Black metal and death metal are both genres of music. BART</p><p>The black metal genre is a variation of the death metal genre. It is also where the band, Twilight, and the alternative rock band, Old Man Gloom, are from. One of the members of the band is Aaron Turner, who plays the electric guitar. <ref type="bibr" target="#b2">Harkous et al. (2020)</ref> Aaron Turner, a.k.a. Black Metal, is a member of the <ref type="bibr">Twilight (band)</ref> and Old Man Gloom bands. He also plays electric guitar and has a strong connection with the Death Metal genre. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Examples</head><p>Reference in this paper, a new array signal processing technique by using particle swarm optimization is proposed to identify multipath channel parameters. the proposed array signal processing technique provides estimates to the channel parameters by finding a global minimum of an optimization problem. since the optimization problem is formulated in the cross-ambiguity function (caf) domain of the transmitted signal and the received array outputs, the proposed array signal processing technique is called as particle swarm optimization. the performance of the particle swarm optimization is compared with the space alternating generalized expectation maximization technique and with another recently proposed pso based technique for various snr values. simulation results indicate the superior performance of the pso based technique over mentioned techniques for all snr values.</p><p>T5 this paper, we propose an array signal processing technique based on particle swarm optimization to estimate the multipath channel parameters from the received and the transmitted signal in a cross-ambiguity domain. the proposed array signal processing technique is formulated as an optimization problem and particle swarm optimization is compared with the generalized expectation maximization technique. simulation results show that the proposed particle swarm optimization outperforms the pso based technique for a wide range of snr values. in addition, the proposed particle swarm optimization is computationally more efficient than the gem based technique for small snr values.</p><p>BART in this paper, a new array signal processing technique based on particle swarm optimization is proposed. the proposed array signal processing technique is used to estimate the multipath channel parameters from the transmitted signal. the proposed array signal processing technique is formulated as an optimization problem in the cross-ambiguity domain. the particle swarm optimization is compared with the generalized expectation maximization technique. simulation results show that the proposed particle swarm optimization outperforms the pso based technique for all snr values. furthermore, the proposed particle swarm optimization is able to estimate the channel parameters more accurately than the generalized expectation maximization technique. <ref type="bibr" target="#b24">Ribeiro et al. (2020)</ref> in this paper, a novel array signal processing technique based on particle swarm optimization is proposed to estimate the multipath channel parameters from the transmitted signal. the proposed array signal processing technique uses particle swarm optimization to estimate the multipath channel parameters. the proposed array signal processing technique is formulated as an optimization problem. simulation results show that the proposed array signal processing technique outperforms the conventional generalized expectation maximization technique and the pso based technique is robust to the snr values. <ref type="table" target="#tab_12">Table 16</ref>: Examples of text generated by the different models trained on the AGENDA dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Linearized representation: ( feel :ARG0 ( we ) :ARG1 ( terrible :degree ( very ) ) :time ( now ) :ARG1-of ( cause :ARG0 ( have-rel-role :ARG0 we :ARG1 ( he ) Examples of (a) AMR and (b) WebNLG graphs, the input for the models and the reference texts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Example graph with 5 triples, from WebNLG dev linearized with the neutral separator tag, denoted ?, (top left), its shuffled version (top right), texts generated with two fine-tuned versions of T5 small and a gold reference (bottom). Note that T5 can produce a reasonable text even when the input triples are shuffled randomly. Performance of BART base and T5 base in the dev set when experimenting with different amounts of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>chrF++ scores with respect to the number of triples for WebNLG seen and unseen test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Castro Ferreira et al. (2019) 51.68 56.35 38.92 32.00 41.00 21.00 ---Moryossef et al. (2019) 47.24 53.30 34.41 39.00 44.00 37.00 62.74 41.53 40.18 44.45 35.36 70.02 76.68 62.76 BARTlarge 54.72 63.45 43.97 42.23 45.49 38.61 72.29 77.57 66.53 T5small 56.34 65.05 45.37 42.78 45.94 39.29 73.31 78.46 67.69 T5base 59.17 64.64 52.55 43.19 46.02 41.49 74.82 78.40 70.92 T5large 59.70 64.71 53.67 44.18 45.85 42.26 75.40 78.29 72.25</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>METEOR</cell><cell></cell><cell></cell><cell>chrF++</cell><cell></cell></row><row><cell>Model</cell><cell>A</cell><cell>S</cell><cell>U</cell><cell>A</cell><cell>S</cell><cell>U</cell><cell>A</cell><cell>S</cell><cell>U</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Schmitt et al. (2020)</cell><cell>-</cell><cell>59.39</cell><cell>-</cell><cell>-</cell><cell>42.83</cell><cell>-</cell><cell>-</cell><cell>74.68</cell><cell>-</cell></row><row><cell>Ribeiro et al. (2020)</cell><cell>-</cell><cell>63.69</cell><cell>-</cell><cell>-</cell><cell>44.47</cell><cell>-</cell><cell>-</cell><cell>76.66</cell><cell>-</cell></row><row><cell>Zhao et al. (2020a)</cell><cell cols="6">52.78 64.42 38.23 41.00 46.00 37.00</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>based on PLMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Harkous et al. (2020)</cell><cell>52.90</cell><cell>-</cell><cell>-</cell><cell>42.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Kale (2020)</cell><cell cols="6">57.10 63.90 52.80 44.00 46.00 41.00</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Radev et al. (2020)</cell><cell cols="6">45.89 52.86 37.85 40.00 42.00 37.00</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BARTbase</cell><cell>53.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on WebNLG. A, S and U stand for all, seen, and unseen partitions of the test set, respectively.</figDesc><table /><note>models for AMR. Following Wolf et al. (2019), we use the Adam optimizer (Kingma and Ba, 2015)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>shows the results for the WebNLG test set. Neural pipeline models<ref type="bibr" target="#b15">(Moryossef et al., 2019;</ref> Castro Ferreira et al., 2019)  achieve strong performance in the unseen dataset. On the other</figDesc><table><row><cell>Model</cell><cell>BLEU</cell><cell>M</cell><cell>BT</cell></row><row><cell>Koncel et al. 2019</cell><cell cols="2">14.30 18.80</cell><cell>-</cell></row><row><cell>An (2019)</cell><cell cols="2">15.10 19.50</cell><cell>-</cell></row><row><cell cols="3">Schmitt et al. (2020) 17.33 21.43</cell><cell>-</cell></row><row><cell cols="3">Ribeiro et al. (2020) 18.01 22.23</cell><cell>-</cell></row><row><cell>BART base</cell><cell cols="3">22.01 23.54 -13.02</cell></row><row><cell>BART large</cell><cell cols="3">23.65 25.19 -10.93</cell></row><row><cell>T5 small</cell><cell cols="3">20.22 21.62 -24.10</cell></row><row><cell>T5 base</cell><cell cols="3">20.73 21.88 -21.03</cell></row><row><cell>T5 large</cell><cell cols="3">22.15 23.73 -13.96</cell></row><row><cell cols="2">with task-adaptive pretraining</cell><cell></cell><cell></cell></row><row><cell>BART large + LMA</cell><cell cols="3">25.30 25.54 -08.79</cell></row><row><cell>T5 large + LMA</cell><cell cols="3">22.92 24.40 -10.39</cell></row><row><cell>BART large + STA</cell><cell cols="3">25.66 25.74 -08.97</cell></row><row><cell>T5 large + STA</cell><cell cols="3">23.69 24.92 -08.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Results on AGENDA test set. Bold (Italic) indicates best scores without (with) task-adaptive pre- training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>lists the results for the AGENDA test set. The models also show strong performance on this</figDesc><table><row><cell>Model</cell><cell cols="2">AMR</cell></row><row><cell></cell><cell>F</cell><cell>MS</cell></row><row><cell>Mager et al. (2020)</cell><cell>5.69 A</cell><cell>5.08 A</cell></row><row><cell>Harkous et al. (2020)</cell><cell>5.78 A</cell><cell>5.47 AB</cell></row><row><cell>T5 large</cell><cell>6.55 B</cell><cell>6.44 C</cell></row><row><cell>BART large</cell><cell>6.70 B</cell><cell>5.72 BC</cell></row><row><cell>Reference</cell><cell>5.91 A</cell><cell>-</cell></row><row><cell>Model</cell><cell cols="2">WebNLG</cell></row><row><cell></cell><cell>F</cell><cell>SA</cell></row><row><cell cols="2">Castro Ferreira et al. (2019) 5.52 A</cell><cell>4.77 A</cell></row><row><cell>Harkous et al. (2020)</cell><cell cols="2">5.74 AB 6.21 B</cell></row><row><cell>T5 large</cell><cell>6.71 C</cell><cell>6.63 B</cell></row><row><cell>BART large</cell><cell>6.53 C</cell><cell>6.50 B</cell></row><row><cell>Reference</cell><cell>5.89 B</cell><cell>6.47 B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Fluency (F), Meaning Similarity (MS) and Se- mantic Adequacy (SA) obtained in the human evalua- tion. Differences between models which have a letter in common are not statistically significant and were deter- mined by pairwise Mann-Whitney tests with p &lt; 0.05.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>we investigate the PLMs' performance, measured with BLEU score, while varying (from 1% to 100%) the amount of training data used for 10 Inter-annotator agreement for the three criteria ranged from 0.40 to 0.79, with an average Krippendorff's ? of 0.56.</figDesc><table><row><cell cols="4">Model AMR WebNLG AGENDA</cell></row><row><cell cols="2">T5 order 36.83</cell><cell>63.41</cell><cell>19.86</cell></row><row><cell>T5 shuf</cell><cell>15.56</cell><cell>61.54</cell><cell>19.08</cell></row></table><note>11 Examples of fluent generations can be found in the Ta- bles 15 and 16 in Appendix.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Impact (measured with BLEU) of using a bag of entities and relations (shuf ) as input for T5 small .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc>shows the effect on T5's performance when its input contains correctly ordered triples (T5 order ) German language ? Antwerp ? Antwerp ? Antwerp International Airport ? Belgium ? Belgium ? Charles Michel ? city Served ? leader Name ? Belgium ? language ? country Antwerp International Airport serves the city of Antwerp. German is the language spoken in Belgium where Charles Michel is the leader.</figDesc><table><row><cell>T/F</cell><cell>Input Fact</cell><cell>T5 order</cell><cell>T5 shuf</cell></row><row><cell>(1) S ?</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Example generations from shuffled (S), true (T), and corrupted (F) triple facts by T5 small , fine-tuned on</cell></row><row><cell>correctly ordered triples (order) and randomly shuffled input (shuf ).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>S?ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In Proceedings of the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference, ISWC'07/ASWC'07, page 722-735, Berlin, Heidelberg. Springer-Verlag.</figDesc><table><row><cell>Deng Cai and Wai Lam. 2020b. Graph transformer for</cell><cell>Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie</cell></row><row><cell>graph-to-sequence learning. In The Thirty-Fourth</cell><cell>Utama, Ido Dagan, and Iryna Gurevych. 2019.</cell></row><row><cell>AAAI Conference on Artificial Intelligence, AAAI</cell><cell>Ranking generated summaries by correctness: An in-</cell></row><row><cell>2020, The Thirty-Second Innovative Applications of</cell><cell>teresting but challenging application for natural lan-</cell></row><row><cell>Artificial Intelligence Conference, IAAI 2020, The</cell><cell>guage inference. In Proceedings of the 57th Annual</cell></row><row><cell>Tenth AAAI Symposium on Educational Advances</cell><cell>Meeting of the Association for Computational Lin-</cell></row><row><cell>in Artificial Intelligence, EAAI 2020, New York, NY,</cell><cell>guistics, pages 2214-2220, Florence, Italy. Associa-</cell></row><row><cell>USA, February 7-12, 2020, pages 7464-7471. AAAI</cell><cell>tion for Computational Linguistics.</cell></row><row><cell>Press.</cell><cell>Xuefeng Bai, Yulong Chen, Linfeng Song, and Yue Zhang. 2021. Semantic representation for dialogue Claire Gardent, Anastasia Shimorina, Shashi Narayan,</cell></row><row><cell>Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neu-ral data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceed-</cell><cell>modeling. In Proceedings of the 59th Annual Meet-ing of the Association for Computational Linguistics and the 11th International Joint Conference on Nat-ural Language Processing (Volume 1: Long Papers), and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Pro-ceedings of the 10th International Conference on Natural Language Generation, pages 124-133, San-</cell></row><row><cell>ings of the 2019 Conference on Empirical Methods</cell><cell>pages 4430-4445, Online. Association for Computa-tiago de Compostela, Spain. Association for Compu-</cell></row><row><cell>in Natural Language Processing and the 9th Interna-</cell><cell>tional Linguistics. tational Linguistics.</cell></row><row><cell>tional Joint Conference on Natural Language Pro-cessing (EMNLP-IJCNLP), pages 552-562, Hong Kong, China. Association for Computational Lin-guistics.</cell><cell>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of Artifi-cial Intelligence Research, 61(1):65-170.</cell></row><row><cell>Jianpeng Cheng, Mart?nez Alonso, Shruti Bhargava, Joris Driesen, Devang Agrawal, H?ctor Federico Flego, Dain Kaplan, Dimitri Kartsaklis, Lin Li, Dhivya Piraviperumal, Jason D. Williams, Hong Yu, Diarmuid ? S?aghdha, and Anders Johannsen. 2020. Conversational semantic parsing for dialog state tracking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8107-8117, Online. Association for Computational Linguistics. Marco Damonte and Shay B. Cohen. 2019. Structural</cell><cell>for sembanking. In Proceedings of the 7th Linguis-tic Annotation Workshop and Interoperability with Discourse, pages 178-186, Sofia, Bulgaria. Associa-tion for Computational Linguistics. Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transac-tions of the Association for Computational Linguis-tics, 7:297-312. Graph-to-sequence learning using gated Suchin Gururangan, Ana Marasovi?, Swabha graph neural networks. In Proceedings of the Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, 56th Annual Meeting of the Association for Com-and Noah A. Smith. 2020. Don't stop pretraining: putational Linguistics (Volume 1: Long Papers), Adapt language models to domains and tasks. In pages 273-283, Melbourne, Australia. Association Proceedings of the 58th Annual Meeting of the for Computational Linguistics. Association for Computational Linguistics, pages</cell></row><row><cell>neural encoders for AMR-to-text generation. In Pro-tional Linguistics. Minneapolis, Minnesota. Association for Computa-ume 1 (Long and Short Papers), pages 3649-3658, Linguistics: Human Language Technologies, Vol-ican Chapter of the Association for Computational ceedings of the 2019 Conference of the North Amer-</cell><cell>8342-8360, Online. Association for Computational Anja Belz, Michael White, Dominic Espinosa, Eric Computational Linguistics. tion, pages 217-226, Nancy, France. Association for European Workshop on Natural Language Genera-and evaluation results. In Proceedings of the 13th The first surface realisation shared task: Overview Kow, Deirdre Hogan, and Amanda Stent. 2011. Linguistics.</cell></row><row><cell>Michael Denkowski and Alon Lavie. 2014. Meteor uni-versal: Language specific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376-380, Baltimore, Maryland, USA. Association for Computational Linguistics.</cell><cell>Claire Bonial, Lucia Donatelli, Mitchell Abrams, Stephanie M. Lukin, Stephen Tratz, Matthew Marge, Ron Artstein, David Traum, and Clare Voss. 2020. Dialogue-AMR: Abstract Meaning Representation for dialogue. In Proceedings of the 12th Lan-guage Resources and Evaluation Conference, pages</cell></row><row><cell>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of</cell><cell>684-695, Marseille, France. European Language Re-sources Association.</cell></row><row><cell>deep bidirectional transformers for language under-</cell><cell>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-</cell></row><row><cell>standing. In Proceedings of the 2019 Conference</cell><cell>tanya Malaviya, Asli Celikyilmaz, and Yejin Choi.</cell></row><row><cell>of the North American Chapter of the Association</cell><cell>2019. COMET: Commonsense transformers for au-</cell></row><row><cell>for Computational Linguistics: Human Language</cell><cell>tomatic knowledge graph construction. In Proceed-</cell></row><row><cell>Technologies, Volume 1 (Long and Short Papers),</cell><cell>ings of the 57th Annual Meeting of the Association</cell></row><row><cell>pages 4171-4186, Minneapolis, Minnesota. Associ-</cell><cell>for Computational Linguistics, pages 4762-4779,</cell></row><row><cell>ation for Computational Linguistics.</cell><cell>Florence, Italy. Association for Computational Lin-</cell></row><row><cell></cell><cell>guistics.</cell></row><row><cell>Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou.</cell><cell></cell></row><row><cell>2017. Question generation for question answering.</cell><cell>Deng Cai and Wai Lam. 2020a. AMR parsing via</cell></row><row><cell>In Proceedings of the 2017 Conference on Empiri-</cell><cell>graph-sequence iterative inference. In Proceedings</cell></row><row><cell>cal Methods in Natural Language Processing, pages</cell><cell>of the 58th Annual Meeting of the Association for</cell></row><row><cell>866-874, Copenhagen, Denmark. Association for</cell><cell>Computational Linguistics, pages 1290-1301, On-</cell></row><row><cell>Computational Linguistics.</cell><cell>line. Association for Computational Linguistics.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7</head><label>7</label><figDesc>shows the results on the AMR development set. The PENMAN representation leads to best results. Therefore, this representation is used in the rest of the experiments.</figDesc><table><row><cell>Input</cell><cell>BLEU</cell></row><row><cell cols="2">only nodes 28.22</cell></row><row><cell>DFS</cell><cell>34.94</cell></row><row><cell>PENMAN</cell><cell>38.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Results on the AMR dev set using T5 small for different AMR linearizations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Effect (measured with BLEU score) of cross-</cell></row><row><cell>domain STA.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 9 :</head><label>9</label><figDesc>Graph statistics of AMR, WebNLG and AGENDA datasets. The values are calculated using the training data. Note that AMR graphs contain a more complex structure than WebNLG and AGENDA graphs.</figDesc><table><row><cell cols="4">the Levi transformation (Beck et al., 2018) of the</cell></row><row><cell cols="4">undirected version of the graphs, where edges are</cell></row><row><cell cols="4">also considered nodes in the graph. WebNLG and</cell></row><row><cell cols="4">AGENDA datasets contain disconnected graphs,</cell></row><row><cell cols="4">and we use the largest subgraph to calculate the</cell></row><row><cell cols="4">diameter. Note that AMR graphs have a much</cell></row><row><cell cols="4">more complex structure: (i) they have more nodes</cell></row><row><cell cols="4">and edges than WebNLG and AGENDA graphs;</cell></row><row><cell cols="4">(ii) the average graph diameter and the average</cell></row><row><cell cols="4">shortest path between nodes in AMRs are at least</cell></row><row><cell cols="4">three times larger than in WebNLG and AGENDA</cell></row><row><cell cols="4">graphs; (iii) nodes in AMRs have larger degrees</cell></row><row><cell cols="4">than nodes in WebNLG and AGENDA graphs.</cell></row><row><cell></cell><cell cols="3">AMR17 WebNLG AGENDA</cell></row><row><cell>#Train</cell><cell>36,521</cell><cell>18,102</cell><cell>38,720</cell></row><row><cell>#Dev</cell><cell>1,368</cell><cell>872</cell><cell>1,000</cell></row><row><cell>#Test</cell><cell>1,371</cell><cell>1,862</cell><cell>1,000</cell></row><row><cell>#Relations</cell><cell>155</cell><cell>373</cell><cell>7</cell></row><row><cell>Avg #Tokens</cell><cell>16.1</cell><cell>31.5</cell><cell>157.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 10 :</head><label>10</label><figDesc>Statistics for the graph-to-text benchmarks.</figDesc><table><row><cell></cell><cell>Title</cell><cell>Abstract</cell><cell>KG</cell></row><row><cell>Vocab</cell><cell>48K</cell><cell>173K</cell><cell>113K</cell></row><row><cell>Tokens</cell><cell>2.1M</cell><cell>31.7M</cell><cell>9.6M</cell></row><row><cell>Entities</cell><cell>-</cell><cell>-</cell><cell>3.7M</cell></row><row><cell>Avg Length</cell><cell>11.1</cell><cell>167.1</cell><cell>-</cell></row><row><cell>Avg #Nodes</cell><cell>-</cell><cell>-</cell><cell>19.9</cell></row><row><cell>Avg #Edges</cell><cell>-</cell><cell>-</cell><cell>9.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>Statistics for the KGAIA dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">chrF++ BS (F1) MS</cell></row><row><cell cols="2">Schmitt et al. (2020) 44.53</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ribeiro et al. (2020) 46.37</cell><cell>-</cell><cell>-</cell></row><row><cell>BART base BART large</cell><cell>48.02 50.44</cell><cell>89.36 88.74</cell><cell>34.33 32.24</cell></row><row><cell>T5 small T5 base T5 large</cell><cell>44.91 48.14 48.14</cell><cell>88.56 88.81 89.60</cell><cell>30.25 31.33 35.23</cell></row><row><cell cols="2">with task-adaptive pretraining</cell><cell></cell><cell></cell></row><row><cell>BART large + LMA</cell><cell>51.33</cell><cell>89.12</cell><cell>33.42</cell></row><row><cell>T5 large + LMA</cell><cell>49.37</cell><cell>89.75</cell><cell>36.13</cell></row><row><cell>BART large + STA T5 large + STA</cell><cell>51.63 50.27</cell><cell>89.27 89.93</cell><cell>34.28 36.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :</head><label>12</label><figDesc>Results of the chrF++, BertScore (BS) and MoverScore (MS) scores for AGENDA test set.</figDesc><table><row><cell cols="4">Bold (Italic) indicates best scores without (with) task-</cell></row><row><cell>adaptive pretraining.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">chrF++ BS (F1) MS</cell></row><row><cell>Guo et al. (2019)</cell><cell>57.30</cell><cell>-</cell><cell>-</cell></row><row><cell>Zhu et al. (2019)</cell><cell>64.05</cell><cell>-</cell><cell>-</cell></row><row><cell>Cai and Lam (2020b)</cell><cell>59.40</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang et al. (2020)</cell><cell>65.80</cell><cell>-</cell><cell>-</cell></row><row><cell>Yao et al. (2020)</cell><cell>65.60</cell><cell>-</cell><cell>-</cell></row><row><cell>based on PLMs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mager et al. (2020)</cell><cell>63.89</cell><cell>-</cell><cell>-</cell></row><row><cell>BART base BART large</cell><cell cols="3">66.65 95.22 60.78 71.06 96.08 65.74</cell></row><row><cell>T5 small T5 base T5 large</cell><cell cols="3">68.78 95.62 63.70 70.81 95.99 65.63 72.57 96.27 67.37</cell></row><row><cell cols="2">with task-adaptive pretraining</cell><cell></cell><cell></cell></row><row><cell>BART large + LMA</cell><cell cols="3">71.14 95.94 64.75</cell></row><row><cell>T5 large + LMA</cell><cell cols="3">72.83 96.32 67.44</cell></row><row><cell cols="4">BART large + STA (200K) 72.26 96.21 66.75</cell></row><row><cell>BART large + STA (2M)</cell><cell cols="3">73.58 96.43 68.14</cell></row><row><cell>T5 large + STA (200K) T5 large + STA (2M)</cell><cell cols="3">74.09 96.51 68.86 74.79 96.59 69.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 13 :</head><label>13</label><figDesc>Results of the chrF++, BertScore (BS) and MoverScore (MS) scores for the LDC2017T10 test set.? capital ? leader Name ? London ? Pound sterling ? United Kingdom ? leader Name ? United Kingdom ? Elizabeth II ? United Kingdom ? Boris Johnson ? London ? currencyThe capital city is London, the currency is the Pound sterling and the leader is Elizabeth II. Boris Johnson is also a leader in the UK.The capital of the United Kingdom is London, the currency is the Pound sterling and the country is lead by Elizabeth II and Boris Johnson.</figDesc><table><row><cell>Bold (Italic) indicates the best score without (with)</cell></row><row><cell>task-adaptive pretraining.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 :</head><label>14</label><figDesc>Example generations from shuffled (S), true (T), and corrupted (F) triple facts by T5 small , fine-tuned on correctly ordered triples (order) and randomly shuffled input (shuf ).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 15 :</head><label>15</label><figDesc>Examples of text generated by the different models. D refers to the dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/UKPLab/plms-graph2text.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The model architecture does not explicitly encode the graph structure, i.e., which entities are connected to each other, but has to retrieve it from a sequence that tries to encode this information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Please, refer to and<ref type="bibr" target="#b21">Raffel et al. (2019)</ref> for details about the self-supervised pretraining strategies.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Details of the preprocessing procedure of AMRs are provided in Appendix A. 5 https://catalog.ldc.upenn.edu/LDC2003T05 6 We filter out sentences that do not yield well-formed AMR graphs. 7 Gigaword and AMR datasets share similar data sources.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We exclude AGENDA because its texts are scientific in nature and annotators are not necessarily AI experts.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank our anonymous reviewers for their thoughtful feedback. Leonardo F. R. Ribeiro is supported by the German Research Foundation (DFG) as part of the Research Training Group "Adaptive Preparation of Information form Heterogeneous Sources" (AIPHES, GRK 1994/1) and as part of the DFG funded project UKP-SQuARE with the number GU 798/29-1. Martin Schmitt is supported by the BMBF as part of the project MLWin (01IS18050) and by the German Academic Scholarship Foundation (Studienstiftung des deutschen Volkes).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Construction of the literature graph in semantic scholar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dunkelberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Kohlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu-Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Skjonsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-3011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans -Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Repulsive bayesian sampling for diversified attention modeling</title>
	</analytic>
	<monogr>
		<title level="m">4th workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Bang An</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamza</forename><surname>Harkous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Saffari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.218</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2410" to="2424" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Promoting graph awareness in linearized graph-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander Miserlis</forename><surname>Hoyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.82</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="944" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Text-to-text pre-training for data-totext tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text Generation from Knowledge Graphs with Graph Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1238</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2284" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GPT-too: A language-model-first approach for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Mager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram?n</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arafat</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Suk</forename><surname>Sultan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1846" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep graph convolutional encoders for structured data to text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">Perez</forename><surname>Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg University</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>The Netherlands</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pararth</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1081</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Step-by-step: Separating planning from realization in neural data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Moryossef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1236</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2267" to="2277" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
	<note>Language models as knowledge bases?</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">chrF: character n-gram F-score for automatic MT evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popovi?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-3049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrit</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinand</forename><surname>Sivaprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiachun</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazneen</forename><surname>Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadit</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxiaokang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Irwanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faiaz</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murori</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Tarabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<title level="m">Dart: Open-domain structured data record to text generation</title>
		<editor>Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, and Richard Socher</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>text transformer. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhancing AMR-to-text generation with dual graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1314</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3183" to="3194" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smelting gold and silver for improved multilingual amr-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-11-07" />
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling global and local node contexts for text generation from knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00332</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="589" to="604" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structural adapters in pretrained language models for amr-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-11-07" />
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling graph structure via relative position for text generation from knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15)</title>
		<meeting>the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15)<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
		</author>
		<title level="m">Philipp Dufter, Iryna Gurevych, and Hinrich Sch?tze. 2020. Modeling graph structure via relative position for better text generation from knowledge graphs</title>
		<imprint/>
	</monogr>
	<note>arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BLEURT: Learning robust metrics for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.704</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7881" to="7892" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A graph-to-sequence model for AMRto-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1616" to="1626" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>M?rquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning</title>
		<meeting><address><addrLine>Manchester, England. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="159" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GTR-LSTM: A triple encoder for sentence generation from RDF data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Bayu Distiawan Trisedya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1151</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia. As</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1627" to="1637" />
		</imprint>
	</monogr>
	<note>sociation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural wikipedian: Generating textual summaries from knowledge base triples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucie-Aim?e</forename><surname>Kaffee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?rique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.websem.2018.07.002</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="page" from="52" to="53" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Amr-to-text generation with graph transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00297</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="19" to="33" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Morgan Funtowicz, and Jamie Brew</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer for graphto-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.640</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7145" to="7154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CoSQL: A conversational text-to-SQL challenge towards crossdomain natural language interfaces to databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyang</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrok</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifan</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1204</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1962" to="1979" />
		</imprint>
	</monogr>
	<note>Walter Lasecki, and Dragomir Radev. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bridging the structural gap between encoding and decoding for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.224</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2481" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1053</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="563" to="578" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Line graph enhanced AMR-to-text generation with mix-order graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.67</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="732" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling graph structure in transformer for better AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1548</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5459" to="5468" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
