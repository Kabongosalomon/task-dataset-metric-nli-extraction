<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global-to-Local Neural Networks for Document-Level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">National Institute of Healthcare Data Science</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermei</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Sun</surname></persName>
							<email>sunweijian@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Technologies Co., Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Global-to-Local Neural Networks for Document-Level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions. * Corresponding author</p><p>[S1] Pacific Fair is a major shopping centre in Broadbeach Waters on the Gold Coast, Queensland, Australia.</p><p>[S11] Pacific Fair fronts Little Tallebudgera Creek and is the southern end of the Surfers Riverwalk.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) aims to identify the semantic relations between named entities in text. While previous work <ref type="bibr" target="#b37">(Zeng et al., 2014;</ref><ref type="bibr" target="#b39">Zhang et al., 2015</ref> focuses on extracting relations within a sentence, a.k.a. sentence-level RE, recent studies <ref type="bibr" target="#b31">(Verga et al., 2018;</ref><ref type="bibr" target="#b24">Sahu et al., 2019;</ref><ref type="bibr" target="#b34">Yao et al., 2019)</ref> have escalated it to the document level, since a large amount of relations between entities usually span across multiple sentences in the real world. According to an analysis on Wikipedia corpus <ref type="bibr" target="#b34">(Yao et al., 2019)</ref>, at least 40.7% of relations can only be extracted on the document level.</p><p>Compared with sentence-level RE, documentlevel RE requires more complex reasoning, such ? ? ? coreference <ref type="figure" target="#fig_4">Figure 1</ref>: An example of document-level RE excerpted from the DocRED dataset <ref type="bibr" target="#b34">(Yao et al., 2019)</ref>. Arrows denote intra/inter-sentential relations.</p><p>as logical reasoning, coreference reasoning and common-sense reasoning. A document often contains many entities, and some entities have multiple mentions under the same phrase of alias. To identify the relations between entities appearing in different sentences, document-level RE models must be capable of modeling the complex interactions between multiple entities and synthesizing the context information of multiple mentions. <ref type="figure" target="#fig_4">Figure 1</ref> shows an example of document-level RE. Assume that one wants to extract the relation between "Surfers Riverwalk" in S11 and "Queensland" in S1. One has to find that "Surfers Riverwalk" contains "Pacific Fair" (from S11), and "Pacific Fair" (coreference) is located in "Queensland" (from S1). This chain of interactions helps infer the inter-sentential relation "located in" between "Surfers Riverwalk" and "Queensland".</p><p>State-of-the-art. Early studies <ref type="bibr" target="#b19">(Peng et al., 2017;</ref> confined document-level RE to short text spans (e.g., within three sentences). Some other studies <ref type="bibr" target="#b17">(Nguyen and Verspoor, 2018;</ref><ref type="bibr" target="#b7">Gupta et al., 2019)</ref> were restricted to handle two entity mentions in a document. We argue that they are incapable of dealing with the example in <ref type="figure" target="#fig_4">Figure 1</ref>, which needs to consider multiple mentions of entities integrally. To encode the semantic interactions of multiple entities in long distance, recent work defined document-level graphs and proposed graph-based neural network models. For example, <ref type="bibr" target="#b24">Sahu et al. (2019)</ref>; <ref type="bibr" target="#b7">Gupta et al. (2019)</ref> interpreted words as nodes and constructed edges according to syntactic dependencies and sequential information. However, there is yet a big gap between word representations and relation prediction.  introduced the notion of document graphs with three types of nodes (mentions, entities and sentences), and proposed an edge-oriented graph neural model for RE. However, it indiscriminately integrated various information throughout the whole document, thus irrelevant information would be involved as noise and damages the prediction accuracy.</p><p>Our approach and contributions. To cope with the above limitations, we propose a novel graphbased neural network model for document-level RE. Our key idea is to make full use of document semantics and predict relations by learning the representations of involved entities from both coarsegrained and fine-grained perspectives as well as other context relations. Towards this goal, we address three challenges below:</p><p>First, how to model the complex semantics of a document? We use the pre-trained language model BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> to capture semantic features and common-sense knowledge, and build a heterogeneous graph with heuristic rules to model the complex interactions between all mentions, entities and sentences in the document.</p><p>Second, how to learn entity representations effectively? We design a global-to-local neural network to encode coarse-grained and fine-grained semantic information of entities. Specifically, we learn entity global representations by employing R-GCN <ref type="bibr" target="#b25">(Schlichtkrull et al., 2018)</ref> on the created heterogeneous graph, and entity local representations by aggregating multiple mentions of specific entities with multi-head attention <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref>.</p><p>Third, how to leverage the influence from other relations? In addition to target relation representations, other relations imply the topic information of a document. We learn context relation representations with self-attention <ref type="bibr" target="#b28">(Sorokin and Gurevych, 2017)</ref> to make final relation prediction.</p><p>In summary, our main contribution is twofold: </p><formula xml:id="formula_0">? We</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>RE has been intensively studied in a long history.</p><p>In this section, we review closely-related work.</p><p>Sentence-level RE. Conventional work addressed sentence-level RE by using carefully-designed patterns <ref type="bibr" target="#b26">(Soderland et al., 1995)</ref>, features <ref type="bibr" target="#b8">(Kambhatla, 2004)</ref> and kernels <ref type="bibr" target="#b4">(Culotta and Sorensen, 2004)</ref>.</p><p>Recently, deep learning-based work has advanced the state-of-the-art without heavy feature engineering. Various neural networks have been exploited, e.g., CNN <ref type="bibr" target="#b37">(Zeng et al., 2014)</ref>, RNN <ref type="bibr" target="#b39">(Zhang et al., 2015;</ref><ref type="bibr" target="#b1">Cai et al., 2016)</ref> and GNN . Furthermore, to cope with the wrong labeling problem caused by distant supervision, <ref type="bibr" target="#b36">Zeng et al. (2015)</ref> adopted Piecewise CNN (PCNN), <ref type="bibr" target="#b14">Lin et al. (2016)</ref>; <ref type="bibr" target="#b41">Zhang et al. (2017)</ref> employed attention mechanisms, and ; <ref type="bibr" target="#b22">Qu et al. (2019)</ref> leveraged knowledge graphs as external resources. All these models are limited to extracting intra-sentential relations. They also ignore the interactions of entities outside a target entity pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-level RE.</head><p>As documents often provide richer information than sentences, there has been an increasing interest in document-level RE. <ref type="bibr" target="#b6">Gu et al. (2017)</ref>; Nguyen and Verspoor (2018); <ref type="bibr" target="#b7">Gupta et al. (2019)</ref>;  extended the sentencelevel RE models to the document level. <ref type="bibr" target="#b35">Ye et al. (2020)</ref> explicitly incorporated coreference information into language representation models (e.g., BERT). <ref type="bibr" target="#b42">Zheng et al. (2018)</ref>; <ref type="bibr" target="#b29">Tang et al. (2020)</ref> proposed hierarchical networks to aggregate information from the word, sentence and document levels.  proposed the notion of document-level graphs, where nodes denote words and edges incorporate both syntactic dependencies and discourse relations. Following this, <ref type="bibr" target="#b19">Peng et al. (2017)</ref> first splitted a document-level graph into two directed acyclic graphs (DAGs), then used a graph LSTM for each DAG to learn the contextual representation of each word, which was concate- Besides, a few models <ref type="bibr" target="#b12">(Levy et al., 2017;</ref><ref type="bibr" target="#b21">Qiu et al., 2018)</ref> borrowed the reading comprehension techniques to document-level RE. However, they require domain knowledge to design question templates, and may perform poorly in zero-answer and multi-answers scenarios , which are very common for RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>We model document-level RE as a classification problem. Given a document annotated with enti-ties and their corresponding textual mentions, the objective of document-level RE is to identify the relations of all entity pairs in the document. <ref type="figure" target="#fig_0">Figure 2</ref> depicts the architecture of our model, named GLRE. It receives an entire document with annotations as input. First, in (a) encoding layer, it uses a pre-trained language model such as BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> to encode the document. Then, in (b) global representation layer, it constructs a global heterogeneous graph with different types of nodes and edges, and encodes the graph using a stacked R-GCN <ref type="bibr" target="#b25">(Schlichtkrull et al., 2018)</ref> to capture entity global representations. Next, in (c) local representation layer, it aggregates multiple mentions of specific entities using multi-head attention <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> to obtain entity local representations. Finally, in (d) classifier layer, it combines the context relation representations obtained with self-attention <ref type="bibr" target="#b28">(Sorokin and Gurevych, 2017)</ref> to make final relation prediction. Please see the rest of this section for technical details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Layer</head><formula xml:id="formula_1">Let D = [w 1 , w 2 , . . . , w k ] be an input document, where w j (1 ? j ? k)</formula><p>is the j th word in it. We use BERT to encode D as follows:</p><formula xml:id="formula_2">H = [h 1 , h 2 , . . . , h k ] = BERT([w 1 , w 2 , . . . , w k ]),<label>(1)</label></formula><p>where h j ? R dw is a sequence of hidden states at the output of the last layer of BERT. Limited by the input length of BERT, we encode a long document sequentially in form of short paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Global Representation Layer</head><p>Based on H, we construct a global heterogeneous graph, with different types of nodes and edges to capture different dependencies (e.g., co-occurrence dependencies, coreference dependencies and order dependencies), inspired by . Specifically, there are three types of nodes:</p><p>? Mention nodes, which model different mentions of entities in D. The representation of a mention node m i is defined by averaging the representations of contained words. To distinguish node types, we concatenate a node type representation t m ? R dt . Thus, the represen-</p><formula xml:id="formula_3">tation of m i is n m i = [avg w j ?m i (h j ); t m ], where [ ; ]</formula><p>is the concatenation operator. ? Entity nodes, which represent entities in D.</p><p>The representation of an entity node e i is defined by averaging the representations of the mention nodes to which they refer, together with a node type representation t e ? R dt . Therefore, the representation of e i is n e i = [avg m j ?e i (n m j ); t e ]. ? Sentence nodes, which encode sentences in D.</p><p>Similar to mention nodes, the representation of a sentence node s i is formalized as</p><formula xml:id="formula_4">n s i = [avg w j ?s i (h j ); t s ], where t s ? R dt .</formula><p>Then, we define five types of edges to model the interactions between the nodes:</p><p>? Mention-mention edges. We add an edge for any two mention nodes in the same sentence. ? Mention-entity edges. We add an edge between a mention node and an entity node if the mention refers to the entity. ? Mention-sentence edges. We add an edge between a mention node and a sentence node if the mention appears in the sentence. ? Entity-sentence edges. We create an edge between an entity node and a sentence node if at least one mention of the entity appears in the sentence. ? Sentence-sentence edges. We connect all sentence nodes to model the non-sequential information (i.e., break the sentence order). Note that there are no entity-entity edges, because they form the relations to be predicted.</p><p>Finally, we employ an L-layer stacked R-GCN <ref type="bibr" target="#b25">(Schlichtkrull et al., 2018)</ref> to convolute the global heterogeneous graph. Different from GCN, R-GCN considers various types of edges and can better model multi-relational graphs. Specifically, its node forward-pass update for the (l + 1) th layer is defined as follows:</p><formula xml:id="formula_5">n l+1 i = ? x?X j?N x i 1 |N x i | W l x n l j + W l 0 n l i ,<label>(2)</label></formula><p>where ?(?) is the activation function. N x i denotes the set of neighbors of node i linked with edge x, and X denotes the set of edge types. W l x , W l 0 ? R dn?dn are trainable parameter matrices (d n is the dimension of node representations). We refer to the representations of entity nodes after graph convolution as entity global representations, which encode the semantic information of entities throughout the whole document. We denote an entity global representation by e glo i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Local Representation Layer</head><p>We learn entity local representations for specific entity pairs by aggregating the associated mention representations with multi-head attention <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref>. The "local" can be understood from two angles: (i) It aggregates the original mention information from the encoding layer. (ii) For different entity pairs, each entity would have multiple different local representations w.r.t. the counterpart entity. However, there is only one entity global representation.</p><p>Multi-head attention enables a RE model to jointly attend to the information of an entity composed of multiple mentions from different representation subspaces. Its calculation involves the sets of queries Q and key-value pairs (K, V):</p><formula xml:id="formula_6">MHead(Q, K, V) = [head 1 ; . . . ; head z ]W out , (3) head i = softmax QW Q i (KW K i ) ? d v VW V i , (4) where W out ? R dn?dn and W Q i , W K i , W V i ? R dn?dv are trainable parameter matrices. z is the number of heads satisfying that z ? d v = d n .</formula><p>In this paper, Q is related to the entity global representations, K is related to the initial sentence node representations before graph convolution (i.e., the input features of sentence nodes in R-GCN), and V is related to the initial mention node representations. Specifically, given an entity pair (e a , e b ), we define their local representations as follows:</p><formula xml:id="formula_7">e loc a = LN MHead 0 (e glo b , {n s i } s i ?Sa , {n m j } m j ?Ma ) , e loc b = LN MHead 1 (e glo a , {n s i } s i ?S b , {n m j } m j ?M b ) ,<label>(5)</label></formula><p>where LN(?) denotes layer normalization <ref type="bibr">(Ba et al., 2016)</ref>. M a is the corresponding mention node set of e a , and S a is the corresponding sentence node set in which each mention node in M a is located. M b and S b are similarly defined for e b . Note that MHead 0 and MHead 1 learn independent model parameters for entity local representations. Intuitively, if a sentence contains two mentions m a , m b corresponding to e a , e b , respectively, then the mention node representations n ma , n m b should contribute more to predicting the relation of (e a , e b ) and the attention weights should be greater in getting e loc a , e loc b . More generally, a higher semantic similarity between the node representation of a sentence containing m a and e glo b indicates that this sentence and m b are more semantically related, and n ma should get a higher attention weight to e loc a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classifier Layer</head><p>To classify the target relation r for an entity pair (e a , e b ), we firstly concatenate entity global representations, entity local representations and relative distance representations to generate entity final representations:?</p><formula xml:id="formula_8">a = [e glo a ; e loc a ; ?(? ab )], e b = [e glo b ; e loc b ; ?(? ba )],<label>(6)</label></formula><p>where ? ab denotes the relative distance from the first mention of e a to that of e b in the document. ? ba is similarly defined. The relative distance is first divided into several bins {1, 2, . . . , 2 b }. Then, each bin is associated with a trainable distance embedding. ?(?) associates each ? to a bin. Then, we concatenate the final representations of e a , e b to form the target relation representation</p><formula xml:id="formula_9">o r = [? a ;? b ].</formula><p>Furthermore, all relations in a document implicitly indicate the topic information of the document, such as "director" and "character" often appear in movies. In turn, the topic information implies possible relations. Some relations under similar topics are likely to co-occur, while others under different topics are not. Thus, we use self-attention <ref type="bibr" target="#b28">(Sorokin and Gurevych, 2017)</ref> to capture context relation representations, which reveal the topic information of the document:</p><formula xml:id="formula_10">o c = p i=0 ? i o i = p i=0 exp(o i Wo r ) p j=0 exp(o j Wo r ) o i ,<label>(7)</label></formula><p>where W ? R dr?dr is a trainable parameter matrix. d r is the dimension of target relation representations. o i (o j ) is the relation representation of the i th (j th ) entity pair. ? i is the attention weight for o i . p is the number of entity pairs.</p><p>Finally, we use a feed-forward neural network (FFNN) over the target relation representation o r and the context relation representation o c to make the prediction. Besides, considering that an entity pair may hold several relations, we transform the multi-classification problem into multiple binary classification problems. The predicted probability distribution of r over the set R of all relations is defined as follows:</p><formula xml:id="formula_11">y r = sigmoid(FFNN([o r ; o c ])),<label>(8)</label></formula><p>where y r ? R |R| . We define the loss function as follows:</p><formula xml:id="formula_12">L = ? r?R y * r log(y r ) + (1 ? y * r ) log(1 ? y r ) ,<label>(9)</label></formula><p>where y * r ? {0, 1} denotes the true label of r. We employ Adam optimizer (Kingma and <ref type="bibr" target="#b9">Ba, 2015)</ref> to optimize this loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We implemented our GLRE with PyTorch 1.5. The source code and datasets are available online. <ref type="bibr" target="#b44">1</ref> In this section, we report our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluated GLRE on two public document-level RE datasets. <ref type="table">Table 1</ref> lists their statistical data:</p><p>? The Chemical-Disease Relations (CDR) data set <ref type="bibr" target="#b13">(Li et al., 2016)</ref> was built for the BioCreative V challenge and annotated with one relation "chemical-induced disease" manually. ? The DocRED dataset <ref type="bibr" target="#b34">(Yao et al., 2019)</ref> was built from Wikipedia and Wikidata, covering various relations related to science, art, personal life, etc. Both manually-annotated and distantly-supervised data are offered. We only used the manually-annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparative Models</head><p>First, we compared GLRE with five sentence-level RE models adapted to the document level: ? <ref type="bibr" target="#b34">Yao et al. (2019)</ref> proposed four baseline models. The first three ones are based on CNN, LSTM and BiLSTM, respectively. The fourth context-aware model incorporates the attention mechanism into LSTM. We also compared GLRE with nine documentlevel RE models:</p><p>? It also leveraged BERT and designed a hierarchical inference network to aggregate inference information from entity level to sentence level, then to document level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Setup</head><p>Due to the small size of CDR, some work <ref type="bibr" target="#b43">(Zhou et al., 2016;</ref><ref type="bibr" target="#b31">Verga et al., 2018;</ref><ref type="bibr" target="#b42">Zheng et al., 2018;</ref>) created a new split by unionizing the training and development sets, denoted by "train + dev". Under this setting, a model was trained on the train + dev set, while the best epoch was found on the development set. To make a comprehensive comparison, we also measured the corresponding precision, recall and F1 scores. For consistency, we used the same experiment setting on DocRED. Additionally, the gold standard of the test set of DocRED is unknown, and only F1 scores can be obtained via an online interface. Besides, it was noted that some relation instances are present in both training and development/test sets <ref type="bibr" target="#b34">(Yao et al., 2019)</ref>. We also measured F1 scores ignoring those duplicates, denoted by Ign F1.   For GLRE and , we used different BERT models in the experiments. For CDR, we chose BioBERT-Base v1.1 , which re-trained the BERT-Base-cased model on biomedical corpora. For DocRED, we picked up the BERT-Base-uncased model. For the comparative models without using BERT, we selected the PubMed pre-trained word embeddings <ref type="bibr" target="#b2">(Chiu et al., 2016)</ref> for CDR and GloVe <ref type="bibr" target="#b20">(Pennington et al., 2014)</ref> for DocRED. For the models with source code, we used our best efforts to tune the hyperparameters. Limited by the space, we refer interested readers to the appendix for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>Tables 2 and 3 list the results of the comparative models and GLRE on CDR and DocRED, respectively. We have four findings below: (1) The sentence-level RE models <ref type="bibr" target="#b34">Yao et al., 2019)</ref> obtained medium performance. They still fell behind a few document-level models, indicating the difficulty of directly applying them to the document level. (2) The graph-based RE models <ref type="bibr" target="#b18">(Panyam et al., 2018;</ref><ref type="bibr" target="#b31">Verga et al., 2018;</ref> and the non-graph models <ref type="bibr" target="#b43">(Zhou et al., 2016;</ref><ref type="bibr" target="#b6">Gu et al., 2017</ref>  2018; <ref type="bibr" target="#b42">Zheng et al., 2018)</ref> achieved comparable results, while the best graph-based model  outperformed the best non-graph <ref type="bibr" target="#b17">(Nguyen and Verspoor, 2018)</ref>. We attribute it to the document graph on the entity level, which can better model the semantic information in a document. (3) From the results of ; <ref type="bibr" target="#b29">Tang et al. (2020)</ref>, the BERT-based models showed stronger prediction power for document-level RE. They outperformed the other comparative models on both CDR and DocRED. (4) GLRE achieved the best results among all the models. We owe it to entity global and local representations. Furthermore, BERT and context relation representations also boosted the performance. See our analysis below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Detailed Analysis</head><p>Entity distance. We examined the performance of the open-source models in terms of entity distance, which is defined as the shortest sentence distance between all mentions of two entities. <ref type="figure" target="#fig_3">Figure 3</ref> depicts the comparison results on CDR and DocRED using the training set only. We observe that:</p><p>(1) GLRE achieved significant improvement in extracting the relations between entities of long distance, especially when distance ? 3. This is because the global heterogeneous graph can effectively model the interactions of semantic information of different nodes (i.e., mentions, entities and sentences) in a document. Furthermore, entity local representations can reduce the influence of noisy context of multiple mentions of entities in long distance. (2) According to the results on CDR, the graphbased model  performed better than the sentence-level model  and the BERT-based model  in extracting intersentential relations. The main reason is that it leveraged heuristic rules to construct the document graph at the entity level, which can bet-ter model the semantic information across sentences and avoid error accumulation involved by NLP tools, e.g., the dependency parser used in . <ref type="formula">(3)</ref> On DocRED, the models  outperformed the model , due to the power of BERT and the increasing accuracy of dependency parsing in the general domain.</p><p>Number of entity mentions. To assess the effectiveness of GLRE in aggregating the information of multiple entity mentions, we measured the performance in terms of the average number of mentions for each entity pair. Similar to the previous analysis, <ref type="figure">Figure 4</ref> shows the results on CDR and DocRED using the training set only. We see that:</p><p>(1) GLRE achieved great improvement in extracting the relations with average number of mentions ? 2, especially ? 4. The major reason is that entity local representations aggregate the contextual information of multiple mentions selectively. As an exception, when the average number of mentions was in <ref type="bibr" target="#b44">[1,</ref><ref type="bibr" target="#b45">2)</ref>, the performance of GLRE was slightly lower than  on CDR. This is because both GLRE and  relied on modeling the interactions between entities in the document, which made them indistinguishable under this case. In fact, the performance of all the models decreased when the average number of mentions was small, because less relevant information was provided in the document, which made relations harder to be predicted. We will consider external knowledge in our future work. (2) As compared with  and , the BERT-based model  performed better in general, except for one interval. When the average number of mentions was in [1, 2) on CDR, its performance was significantly lower than other models. The reason is twofold. On one hand, it is more difficult to capture the latent knowledge in the biomedical field. On the other hand, the model  only relied on the semantic information of the mentions of target entity pairs to predict the relations. When the average number was small, the prediction became more difficult. Furthermore, when the average number was large, its performance increase was not significant. The  main reason is that, although BERT brought rich knowledge, the model  indiscriminately aggregated the information of multiple mentions and introduced much noisy context, which limited its performance.</p><p>Ablation study. To investigate the effectiveness of each layer in GLRE, we conducted an ablation study using the training set only. <ref type="table" target="#tab_7">Table 4</ref> shows the comparison results. We find that: (1) BERT had a greater influence on DocRED than CDR. This is mainly because BERT introduced valuable linguistic knowledge and common-sense knowledge to RE, but it was hard to capture latent knowledge in the biomedical field. (2) F1 scores dropped when we removed entity global representations, entity local representations or context relation representations, which verified their usefulness in documentlevel RE. <ref type="formula">(3)</ref> Particularly, when we removed entity local representations, F1 scores dropped more dramatically. We found that more than 54% and 19% of entities on CDR and DocRED, respectively, have multiple mentions in different sentences. The local representation layer, which uses multi-head attention to selectively aggregate multiple mentions, can reduce much noisy context.</p><p>Pre-trained language models. To analyze the impacts of pre-trained language models on GLRE and also its performance upper bound, we replaced BERT-Base with BERT-Large, XLNet-Large <ref type="bibr" target="#b33">(Yang et al., 2019)</ref> or ALBERT-xxLarge <ref type="bibr" target="#b10">(Lan et al., 2020)</ref>. <ref type="table" target="#tab_9">Table 5</ref> shows the comparison results using the training set only, from which we observe that larger models boosted the performance of GLRE to some extent. When the "train + dev" setting was used  on DocRED, the Ign F1 and F1 scores of XLNet-Large even reached to 58.5 and 60.5, respectively. However, due to the lack of biomedical versions, XLNet-Large and ALBERT-xxLarge did not bring improvement on CDR. We argue that selecting the best pre-trained models is not our primary goal.</p><p>Case study. To help understanding, we list a few examples from the CDR test set in <ref type="table" target="#tab_10">Table 6</ref>. See Appendix for more cases from DocRED.</p><p>(1) From Case 1, we find that logical reasoning is necessary. Predicting the relation between "rofecoxib" and "GI bleeding" depends on the bridge entity "non-users of aspirin". GLRE used R-GCN to model the document information based on the global heterogeneous graph, thus it dealt with complex inter-sentential reasoning better. (2) From Case 2, we observe that, when a sentence contained multiple entities connected by conjunctions (such as "and"), the model  might miss some associations between them. GLRE solved this issue by building the global heterogeneous graph and considering the context relation information, which broke the word sequence. (3) Prior knowledge is required in Case 3. One must know that "fatigue" belongs to "adverse effects" ahead of time. Then, the relation between "bepridil" and "dizziness" can be identified correctly. Unfortunately, both GLRE and  lacked the knowledge, and we leave it as our future work. We analyzed all 132 inter-sentential relation instances in the CDR test set that were incorrectly predicted by GLRE. Four major error types are as follows: (1) Logical reasoning errors, which occurred when GLRE could not correctly identify the relations established indirectly by the bridge entities, account for 40.9%. (2) Component missing errors, which happened when some component of a sentence (e.g., subject) was missing, account for 28.8%. In this case, GLRE needed the whole document information to infer the lost component and ... [S8] Among non-users of aspirin, the adjusted hazard ratios were: rofecoxib 1.27, naproxen 1.59, diclofenac 1.17 and ibuprofen 1.05. ... [S10] CONCLUSION: Among non-users of aspirin, naproxen seemed to carry the highest risk for AMI / GI bleeding. ...  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed GLRE, a global-to-local neural network for document-level RE. Entity global representations model the semantic information of an entire document with R-GCN, and entity local representations aggregate the contextual information of mentions selectively using multi-head attention. Moreover, context relation representations encode the topic information of other relations using self-attention. Our experiments demonstrated the superiority of GLRE over many comparative models, especially the big leads in extracting relations between entities of long distance and with multiple mentions. In future work, we plan to integrate knowledge graphs and explore other document graph modeling ways (e.g., hierarchical graphs) to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Notations</head><p>To help understanding, <ref type="table" target="#tab_12">Table 7</ref> summarizes the key notations used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Availability</head><p>The CDR dataset <ref type="bibr" target="#b13">(Li et al., 2016)</ref> is available at https://biocreative.bioinformatics.udel. edu/media/store/files/2016/CDR_Data.zip.</p><p>The DocRED dataset <ref type="bibr" target="#b34">(Yao et al., 2019)</ref> is available at https://github.com/thunlp/DocRED. Note that, the gold standard of the test set of DocRED is unknown, and only F1 scores can be obtained via an online interface at https://competitions. codalab.org/competitions/20717.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Setup</head><p>In this section, we provide more details of our experiments. We implemented GLRE with PyTorch 1.5 and trained it on a server with an Intel Xeon   uated on the development set, in order to find the best epoch. Then, we re-ran on the union of the training and development sets until the best epoch and evaluated on the test set. For both cases, we employed dropout and layer normalization <ref type="bibr">(Ba et al., 2016)</ref> to prevent model overfitting.</p><p>The parameters of GLRE were initialized with a Gaussian distribution (mean = 0 and SD = 1.0) using a fixed initialization seed. We trained GLRE by Adam optimizer (Kingma and <ref type="bibr" target="#b9">Ba, 2015)</ref> with mini-batches. The hidden size of BERT was set to 768. A transformation layer was used to project the BERT output into a low-dimensional space of size 256. All hyperparameter values used in the experiments are shown in <ref type="table" target="#tab_13">Table 8</ref>. ier to be predicted. In contrast, GLRE without context relation representations imprecisely predicts it as "creator" (for general work).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Case Study on DocRED</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of the proposed model. nated and finally fed to the relation classifier. Differently,<ref type="bibr" target="#b27">Song et al. (2018)</ref> kept the original graph structure and directly modeled the whole documentlevel graph using graph-state LSTM. These models only predict the relation of a single mention pair in a document at a time, and ignore multiple mentions of a target entity pair as well as other entities.Several models predict the relation of a target entity pair by aggregating the scores of all mention pairs with multi-instance learning.<ref type="bibr" target="#b31">Verga et al. (2018)</ref> proposed a Transformer-based model. Later, Sahu et al. (2019) switched Transformer to GCN. The two models only consider one target entity pair per document, and construct the documentlevel graphs relying on external syntactic analysis tools. Christopoulou et al. (2019) built a document graph with heterogeneous types of nodes and edges, and proposed an edge-oriented model to obtain global representations for relation classification. Our model differs in further learning entity local representations to reduce the influence of irrelevant information and considering other relations in the document to refine the prediction. Recently, Nan et al. (2020) defined a document graph as a latent variable and induced it based on the structured attention. Unlike our work, it improves the performance of document-level RE models by optimizing the structure of the document graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc> employed GCN over pruned dependency trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b43">Zhou et al. (2016)</ref> combined feature-, tree kernel-and neural network-based models.? Gu et al. (2017) leveraged CNN and maximum entropy. ? Nguyen and Verspoor (2018) integrated character-based word representations in CNN. ? Panyam et al. (2018) exploited graph kernels. ? Verga et al. (2018) proposed a bi-affine network with Transformer. ? Zheng et al. (2018) designed a hierarchical network using multiple BiLSTMs. ? Christopoulou et al. (2019) put forward an edge-oriented graph neural model with multiinstance learning. ? Wang et al. (2019) applied BERT to encode documents, and used a bilinear layer to predict entity relations. It improved performance by two phases. First, it predicted whether a relation exists between two entities. Then, it predicted the type of the relation. ? Tang et al. (2020) is a sequence-based model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Results w.r.t. entity distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Case 1</head><label>1</label><figDesc>Label: CID GLRE: CID Wang et al.: N/A ... [S2] S-53482 and S-23121 are N-phenylimide herbicides and produced embryolethality, teratogenicity. ... Case 2 Label: CID GLRE: CID Wang et al.: N/A [S1] Clinical evaluation of adverse effects during bepridil administration for atrial fibrillation and flutter. ... [S8] There was marked QT prolongation greater than 0.55 s in 13 patients ... and general fatigue in 1 patient each. ... Case 3 Label: CID GLRE: N/A Wang et al.: N/A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>For details, please see Section 3.? We conducted extensive experiments on two public document-level RE datasets. Our results demonstrated the superiority of GLRE compared with many state-of-the-art competitors. Our detailed analysis further showed its advantage in extracting relations between entities of long distance and having multiple mentions. For details, please see Section 4.</figDesc><table /><note>propose a novel model, called GLRE, for document-level RE. To predict relations between entities, GLRE synthesizes entity global representations, entity local represen- tations and context relation representations integrally.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>denotes that we performed hyperparameter tuning. For others, we reused the reported results due to the lack of source code.</figDesc><table><row><cell>Models</cell><cell>P</cell><cell>Train R</cell><cell>F1</cell><cell>P</cell><cell cols="2">Train + Dev R F1</cell></row><row><cell>Zhang et al.  ?</cell><cell cols="6">52.3 72.0 60.6 58.1 74.6 65.3</cell></row><row><cell>Zhou et al.</cell><cell cols="6">64.9 49.3 56.0 55.6 68.4 61.3</cell></row><row><cell>Gu et al.</cell><cell cols="3">55.7 68.1 61.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Nguyen and Verspoor 57.0 68.6 62.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Panyam et al.</cell><cell cols="3">55.6 68.4 61.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Verga et al.</cell><cell cols="6">55.6 70.8 62.1 63.3 67.1 65.1</cell></row><row><cell>Zheng et al.</cell><cell cols="6">45.2 68.1 54.3 56.2 68.0 61.5</cell></row><row><cell cols="7">Christopoulou et al.  ? 62.7 66.3 64.5 61.5 73.6 67.0</cell></row><row><cell>Wang et al.  ?</cell><cell cols="6">61.9 68.7 65.1 66.0 68.3 67.1</cell></row><row><cell>GLRE (ours)</cell><cell cols="6">65.1 72.2 68.5 70.5 74.5 72.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Result comparison on CDR.</figDesc><table><row><cell>Models</cell><cell cols="4">Train Ign F1 F1 Ign F1 F1 Train + Dev</cell></row><row><cell>Zhang et al.  ?</cell><cell>49.9</cell><cell>52.1</cell><cell>52.5</cell><cell>54.6</cell></row><row><cell>Yao et al. (CNN)</cell><cell>40.3</cell><cell>42.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Yao et al. (LSTM)</cell><cell>47.7</cell><cell>50.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Yao et al. (BiLSTM)</cell><cell>48.8</cell><cell>51.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Yao et al. (Context-aware)</cell><cell>48.4</cell><cell>50.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Christopoulou et al.  ?</cell><cell>49.1</cell><cell>50.9</cell><cell>48.3</cell><cell>50.4</cell></row><row><cell>Wang et al.  ?</cell><cell>53.1</cell><cell>55.4</cell><cell>54.5</cell><cell>56.5</cell></row><row><cell>Tang et al.</cell><cell>53.7</cell><cell>55.6</cell><cell>-</cell><cell>-</cell></row><row><cell>GLRE (ours)</cell><cell>55.4</cell><cell>57.4</cell><cell>56.7</cell><cell>58.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Result comparison on DocRED.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results of ablation study.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results w.r.t. different pre-training models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Case study on the CDR test set. CID is short for the "chemical-induced disease" relation. Target entities and related entities are colored accordingly. predict the relation, which was not always accurate.(3)Prior knowledge missing errors account for 13.6%. (4) Coreference reasoning errors, which were caused by pronouns that could not be understood correctly, account for 12.9%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Symbols Descriptions D, k a document, the document length w, h a word, the hidden states of a word H the output of BERT m, e, s a mention, an entity, a sentence n m , t m a mention's node rep. &amp; type rep. n e , t e an entity's node rep. &amp; type rep. n s , t s a sentence's node rep. &amp; type rep. L the number of R-GCN layers x, X an edge type, the set of edge types N a node's neighbors linked by an edge Q, K, V queries, keys, values of multi-head attn. z the number of attention heads M, S the sets of mention &amp; sentence nodes e glo , e loc an entity global rep. &amp; local rep.</figDesc><table><row><cell>?, ?</cell><cell>a distance, the distance rep. matrix</cell></row><row><cell>e</cell><cell>an entity final rep.</cell></row><row><cell>r, R</cell><cell>a relation, the set of relations</cell></row><row><cell>o r</cell><cell>a target relation rep.</cell></row><row><cell>o c</cell><cell>a context relation rep.</cell></row><row><cell>?</cell><cell>the attn. weight for a relation rep.</cell></row><row><cell>y</cell><cell>the probability distribution of a relation</cell></row><row><cell>y</cell><cell></cell></row></table><note>* the true label of a relation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Notations in the paper.Gold 5117 CPU, 120 GB memory, two NVIDIA Tesla V100 GPU cards and Ubuntu 18.04.Analogous to, we pre-processed the CDR dataset, including sentence splitting, word tokenization and hypernym filtering.When using the training set only, we trained a model on the training set, searched the best epoch in terms of F1 scores on the development set, and tested on the test set. Under the "train + dev" setting, we first trained on the training set and eval-</figDesc><table><row><cell>Hyperparameters</cell><cell>Values</cell></row><row><cell>Batch size</cell><cell>8</cell></row><row><cell>Learning rate</cell><cell>0.0005</cell></row><row><cell>Gradient clipping</cell><cell>10</cell></row><row><cell>Early stop patience</cell><cell>15</cell></row><row><cell>Regularization</cell><cell>10 ?4</cell></row><row><cell>Dropout ratio</cell><cell>0.2 or 0.5</cell></row><row><cell>Dimension of words</cell><cell>768</cell></row><row><cell>Dimension of nodes</cell><cell>256</cell></row><row><cell>Dimension of node types</cell><cell>20</cell></row><row><cell cols="2">Number of R-GCN layers CDR = 3, DocRED = 2</cell></row><row><cell cols="2">Number of attention heads CDR = 4, DocRED = 2</cell></row><row><cell>Dimension of distance</cell><cell>20</cell></row><row><cell cols="2">Final dimension of entities 532 (= 256 ? 2 + 20)</cell></row><row><cell>Dimension of relations</cell><cell>1064 (= 532 + 532)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters in the experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Case study on the DocRED development set. Target entities and related entities are colored.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/nju-websoft/GLRE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Hinton. 2016. Layer normalization</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How to train good word embeddings for biomedical NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamal</forename><surname>Crichton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BioNLP</title>
		<meeting><address><addrLine>Berlin, Germany. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="166" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<meeting><address><addrLine>Hong Kong. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4925" to="4936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dependency tree kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Barcelona, Spain. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<meeting><address><addrLine>Minneapolis, MN, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Chemical-induced disease relation extraction via convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural relation extraction within and across sentence boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subburam</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Runkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6513" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2004</title>
		<meeting><address><addrLine>Barcelona, Spain. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<meeting><address><addrLine>San Diego, CA, USA. OpenReview</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, Addis Ababa</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BioBERT: A pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page">682</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<meeting><address><addrLine>Vancouver, Canada. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">BioCreative V CDR task corpus: A resource for chemical disease relation extraction. Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">68</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural machine reading comprehension: Methods and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">3698</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Online. ACL</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for chemical-disease relation extraction are improved with character-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BioNLP</title>
		<meeting><address><addrLine>Melbourne, Australia. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting graph kernels for high performance biomedical relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Panyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotagiri</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramamohanarao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Semantics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LSTMs. TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Doha, Qatar. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">QA4IE: A question answering based framework for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suoheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<meeting><address><addrLine>Monterey, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="198" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A fine-grained and noise-aware method for neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dantong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="659" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<meeting><address><addrLine>Valencia, Spain. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inter-sentence relation extraction with document-level graph convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Sunil Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Florence, Italy. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4309" to="4316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CRYSTAL: Inducing a conceptual dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Aseltine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><surname>Lehnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<meeting><address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1314" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">N-ary relation extraction using graphstate LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Brussels, Belgium. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Contextaware representations for knowledge base relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Copenhagen, Denmark. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1784" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">HIN: Hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<meeting><address><addrLine>New Orleans, LA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="872" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fine-tune Bert for DocRED with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Florence, Italy. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Coreferential reasoning learning for language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<editor>EMNLP, Online. ACL</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Lisbon, Portugal. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<meeting><address><addrLine>Dublin, Ireland. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Long-tail relation extraction via knowledge graph embeddings and graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<meeting><address><addrLine>Minneapolis, MN, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3016" to="3025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACLIC</title>
		<meeting><address><addrLine>Shanghai, China. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Brussels, Belgium. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An effective neural model extracting document level chemical-induced disease relations from biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploiting syntactic and semantics information for chemicaldisease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Logical reasoning is needed in Case 1. In order to identify the relational fact, one needs to use two mentions of &quot;Conrad Johnson</title>
	</analytic>
	<monogr>
		<title level="m">S1 and S2, respectively. Specifically, one first identifies the fact that &quot;Conrad Johnson&quot; was born in &quot;Texas&quot; from S2, and then infers the fact that</title>
		<imprint/>
	</monogr>
	<note>Conrad Oberon Johnson&quot; (coreference) is an &quot;American&quot; educator from S1</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Coreference reasoning is needed in Case 2. In order to recognize the relation between &quot;The Hungry Ghosts&quot; and &quot;Michael Imperioli</title>
		<imprint/>
	</monogr>
	<note>one has to infer that &quot;He&quot; refers to &quot;Michael Imperioli&quot; in S5</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Prior knowledge is needed in Case 3. Through some external prior knowledge, one can know that</title>
	</analytic>
	<monogr>
		<title level="m">We also compare GLRE against the model without entity local representations and the model without context relation representations</title>
		<imprint/>
	</monogr>
	<note>California&quot; is a state, which are the valuable information to help judge their relation</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Kunar&quot; and &quot;Afghanistan&quot;, GLRE attends more to &quot;Afghanistan&quot; in [S3] by entity local representations, and correctly identifies the relation. However, GLRE without entity local representations outputs</title>
		<imprint/>
	</monogr>
	<note>In order to predict the relation between</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Breaking Dawn&quot; and &quot;Stephenie Meyer</title>
	</analytic>
	<monogr>
		<title level="m">GLRE relies on the context relation &quot;author&quot; between &quot;Eclipse&quot; and &quot;Stephenie Meyer</title>
		<imprint/>
	</monogr>
	<note>To predict the relation between. which is eas</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">) was an American music educator, long associated with the city of Houston, who was inducted into the Texas Bandmasters Hall of Fame</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename><surname>Conrad Oberon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1915-11-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">was nine when his family moved to Houston</title>
		<imprint>
			<pubPlace>Victoria, Texas, Conrad Johnson</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al</surname></persName>
			<affiliation>
				<orgName type="collaboration">N/A</orgName>
			</affiliation>
		</author>
		<title level="m">Case 1 Label: country GLRE: country Wang et</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">) is an American actor, writer and director best known for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Imperioli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966-03-26" />
		</imprint>
	</monogr>
	<note>S4] He was starring as Detective Louis Fitch in the ABC police drama Detroit 1-8-7 ... [S5] He wrote and directed his first feature film, The Hungry Ghosts</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Case 2 Label: director GLRE: director</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">cast</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The Pleistocene coyote (Canis latrans orcutti), also known as the Ice Age coyote, is an extinct subspecies of coyote that lived in western North America during the Late Pleistocene era. [S2] Most remains of the subspecies were found in southern California, though at least one was discovered in Idaho</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Case 3 Label: continent GLRE: continent Wang et al.: country</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Operation Unified Resolve is an air and ground operation to flush out and trap al -Qaeda fighters hiding in the eastern Afghanistan provinces</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Operation Unified Resolve is a joint operation between Pakistan, United States, and Afghanistan. [S3] Over 500 troops, mostly from the U.S. 82nd Airborne Division, began hunting the Taliban and al -Qaeda fighters in the provinces of Nangarhar and Kunar on Afghanistans eastern border</title>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Case 4 Label: country GLRE: country w/o local rep</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Eclipse is the third novel in the Twilight Saga by Stephenie Meyer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<title level="m">Eclipse is preceded by New Moon and followed by Breaking Dawn. ... [S6] Eclipse was the fourth bestselling book of 2008, only behind Twilight, New Moon, and Breaking Dawn</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Case 5 Label: author GLRE: author w/o context rel</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
