<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Sung</forename><surname>Chuang</surname></persName>
							<email>yungsung@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumen</forename><surname>Dangovski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Solja?i?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Wen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santa</forename><surname>Barbara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ? Meta AI MIT-IBM Watson AI Lab ? UC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning <ref type="bibr" target="#b9">(Dangovski et al., 2021)</ref>, which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other "harmful" types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE 1 by 2.3 absolute points on semantic textual similarity tasks. 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning "universal" sentence representations that capture rich semantic information and are at the same time performant across a wide range of downstream NLP tasks without task-specific finetuning is an important open issue in the field <ref type="bibr" target="#b8">(Conneau et al., 2017;</ref><ref type="bibr" target="#b3">Cer et al., 2018;</ref><ref type="bibr" target="#b20">Kiros et al., 2015;</ref><ref type="bibr" target="#b23">Logeswaran and Lee, 2018;</ref><ref type="bibr" target="#b13">Giorgi et al., 2020;</ref><ref type="bibr" target="#b37">Yan et al., 2021;</ref><ref type="bibr" target="#b12">Gao et al., 2021)</ref>. Recent work has shown that finetuning pretrained language models with contrastive learning makes it possible to learn good sentence embeddings without any labeled data <ref type="bibr" target="#b13">(Giorgi et al., 2020;</ref><ref type="bibr" target="#b37">Yan et al., 2021;</ref><ref type="bibr" target="#b12">Gao et al., 2021)</ref>. Contrastive learning uses multiple augmentations on a single datum to construct positive pairs whose representations are trained to be more similar to one another than negative pairs. While different data augmentations (random cropping, color jitter, rotations, etc.) have been found to be crucial for pretraining vision models <ref type="bibr" target="#b4">(Chen et al., 2020)</ref>, such augmentations have generally been unsuccessful when applied to contrastive learning of sentence embeddings. Indeed, <ref type="bibr" target="#b12">Gao et al. (2021)</ref> find that constructing positive pairs via a simple dropout-based augmentation works much better than more complex augmentations such as word deletions or replacements based on synonyms or masked language models. This is perhaps unsurprising in hindsight; while the training objective in contrastive learning encourages representations to be invariant to augmentation transformations, direct augmentations on the input (e.g., deletion, replacement) often change the meaning of the sentence. That is, ideal sentence embeddings should not be invariant to such transformations.</p><p>We propose to learn sentence representations that are aware of, but not necessarily invariant to, such direct surface-level augmentations. This is an instance of equivariant contrastive learning <ref type="bibr" target="#b9">(Dangovski et al., 2021)</ref>, which improves vision representation learning by using a contrastive loss on insensitive image transformations (e.g., grayscale) and a prediction loss on sensitive image transformations (e.g., rotations). We operationalize equivariant contrastive learning on sentences by using dropout-based augmentation as the insensitive transformation (as in SimCSE <ref type="bibr" target="#b12">(Gao et al., 2021)</ref>) and MLM-based word replacement as the sensitive transformation. This results in an additional crossentropy loss based on the difference between the original and the transformed sentence.</p><p>We conduct experiments on 7 semantic textual similarity tasks (STS) and 7 transfer tasks from Sen-tEval <ref type="bibr" target="#b7">(Conneau and Kiela, 2018)</ref> and find that this difference-based learning greatly improves over standard contrastive learning. Our DiffCSE approach can achieve around 2.3% absolute improve-"You <ref type="bibr">[MASK]</ref> know what you're gonna <ref type="bibr">[MASK]</ref> ." "You never know what you're gonna get ."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Encoder</head><p>Generator <ref type="formula">(</ref> On the left-hand side is a standard SimCSE model trained with regular contrastive loss on dropout transformations. On the right hand side is a conditional difference prediction model which takes the sentence vector h as input and predict the difference between x and x . During testing we discard the discriminator and only use h as the sentence embedding. ment on STS datasets over SimCSE, the previous state-of-the-art model. We also conduct a set of ablation studies to justify our designed architecture. Qualitative study and analysis are also included to look into the embedding space of DiffCSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning Sentence Embeddings</head><p>Learning universal sentence embeddings has been studied extensively in prior work, including unsupervised approaches such as Skip-Thought <ref type="bibr" target="#b20">(Kiros et al., 2015)</ref>, Quick-Thought <ref type="bibr" target="#b23">(Logeswaran and Lee, 2018)</ref> and FastSent <ref type="bibr" target="#b15">(Hill et al., 2016)</ref>, or supervised methods such as InferSent <ref type="bibr" target="#b8">(Conneau et al., 2017)</ref>, Universal Sentence Encoder <ref type="bibr" target="#b3">(Cer et al., 2018)</ref> and Sentence-BERT <ref type="bibr" target="#b29">(Reimers and Gurevych, 2019)</ref>. Recently, researchers have focused on (unsupervised) contrastive learning approaches such as Sim-CLR <ref type="bibr" target="#b4">(Chen et al., 2020)</ref> to learn sentence embeddings. SimCLR <ref type="bibr" target="#b4">(Chen et al., 2020)</ref> learns image representations by creating semantically close augmentations for the same images and then pulling these representations to be closer than representations of random negative examples. The same framework can be adapted to learning sentence embeddings by designing good augmentation methods for natural language. ConSERT <ref type="bibr" target="#b37">(Yan et al., 2021)</ref> uses a combination of four data augmentation strategies: adversarial attack, token shuffling, cut-off, and dropout. DeCLUTR <ref type="bibr" target="#b13">(Giorgi et al., 2020)</ref> uses overlapped spans as positive ex-amples and distant spans as negative examples for learning contrastive span representations. Finally, SimCSE <ref type="bibr" target="#b12">(Gao et al., 2021)</ref> proposes an extremely simple augmentation strategy by just switching dropout masks. While simple, sentence embeddings learned in this manner have been shown to be better than other more complicated augmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Equivariant Contrastive Learning</head><p>DiffCSE is inspired by a recent generalization of contrastive learning in computer vision (CV) called equivariant contrastive learning <ref type="bibr" target="#b9">(Dangovski et al., 2021)</ref>. We now explain how this CV technique can be adapted to natural language.</p><p>Understanding the role of input transformations is crucial for successful contrastive learning. Past empirical studies have revealed useful transformations for contrastive learning, such as random resized cropping and color jitter for computer vision <ref type="bibr" target="#b4">(Chen et al., 2020)</ref> and dropout for NLP <ref type="bibr" target="#b12">(Gao et al., 2021)</ref>. Contrastive learning encourages representations to be insensitive to these transformations, i.e. the encoder is trained to be invariant to a set of manually chosen transformations. The above studies in CV and NLP have also revealed transformations that are harmful for contrastive learning. For example, <ref type="bibr" target="#b4">Chen et al. (2020)</ref> showed that making the representations insensitive to rotations decreases the ImageNet linear probe accuracy, and <ref type="bibr" target="#b12">Gao et al. (2021)</ref> showed that using an MLM to replace 15% of the words drastically reduces performance on STS-B. While previous works simply omit these transformations from contrastive pre-training, here we argue that we should still make use of these transformations by learning representations that are sensitive (but not necessarily invariant) to such transformations.</p><p>The notion of (in)sensitivity can be captured by the more general property of equivariance in mathematics. Let T be a transformation from a group G and let T (x) denote the transformation of a sentence x. Equivariance is the property that there is an induced group transformation T on the output features <ref type="bibr" target="#b9">(Dangovski et al., 2021)</ref>:</p><formula xml:id="formula_0">f (T (x)) = T (f (x)).</formula><p>In the special case of contrastive learning, T 's target is the identity transformation, and we say that f is trained to be "invariant to T ." However, invariance is just a trivial case of equivariance, and we can design training objectives where T is not the identity for some transformations (such as MLM), while it is the identity for others (such as dropout). <ref type="bibr" target="#b9">Dangovski et al. (2021)</ref> show that generalizing contrastive learning to equivariance in this way improves the semantic quality of features in CV, and here we show that the complementary nature of invariance and equivariance extends to the NLP domain. The key observation is that the encoder should be equivariant to MLM-based augmentation instead of being invariant. We can operationalize this by using a conditional discriminator that combines the sentence representation with an edited sentence, and then predicts the difference between the original and edited sentences. This is essentially a conditional version of the ELEC-TRA model <ref type="bibr" target="#b6">(Clark et al., 2020)</ref>, which makes the encoder equivariant to MLM by using a binary discriminator which detects whether a token is from the original sentence or from a generator. We hypothesize that conditioning the ELECTRA model with the representation from our sentence encoder is a useful objective for encouraging f to be "equivariant to MLM."</p><p>To the best of our knowledge, we are the first to observe and highlight the above parallel between CV and NLP. In particular, we show that equivariant contrastive learning extends beyond CV, and that it works for transformations even without algebraic structures, such as diff operations on sentences. Further, insofar as the canonical set of useful transformations is less established in NLP than is in CV, DiffCSE can serve as a diagnostic tool for NLP researchers to discover useful transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Difference-based Contrastive Learning</head><p>Our approach is straightforward and can be seen as combining the standard contrastive learning objective from SimCSE <ref type="figure" target="#fig_0">(Figure 1</ref>, left) with a difference prediction objective which conditions on the sentence embedding <ref type="figure" target="#fig_0">(Figure 1, right)</ref>.</p><p>Given an unlabeled input sentence x, SimCSE creates a positive example x + for it by applying different dropout masks. By using the BERT base encoder f , we can obtain the sentence embedding h = f (x) for x (see section 4 for how h is obtained). The training objective for SimCSE is:</p><formula xml:id="formula_1">L contrast = ? log e sim(h i ,h + i )/? N j=1 e sim(h i ,h + j )/? , where N is the batch size for the input batch {x i } N i=1</formula><p>as we are using in-batch negative examples, sim(?, ?) is the cosine similarity function, and ? is a temperature hyperparameter.</p><p>On the right-hand side of <ref type="figure" target="#fig_0">Figure 1</ref> is a conditional version of the difference prediction objective used in ELECTRA <ref type="bibr" target="#b6">(Clark et al., 2020)</ref>, which contains a generator and a discriminator.</p><p>Given a sentence of length T , x = [x (1) , x (2) , ..., x (T ) ], we first apply a random mask m = [m (1) , m (2) , ..., m (T ) ], m (t) ? [0, 1] on x to obtain x = m ? x. We use another pretrained MLM as the generator G to perform masked language modeling to recover randomly masked tokens in x to obtain the edited sentence x = G(x ). Then, we use a discriminator D to perform the Replaced Token Detection (RTD) task. For each token in the sentence, the model needs to predict whether it has been replaced or not. The cross-entropy loss for a single sentence x is:</p><formula xml:id="formula_2">L x RTD = T t=1 ?1 x (t) = x (t) log D x , h, t ? 1 x (t) = x (t) log 1 ? D x , h, t</formula><p>And the training objective for a batch is</p><formula xml:id="formula_3">L RTD = N i=1 L x i RTD .</formula><p>Finally we optimize these two losses together with a weighting coefficient ?:</p><formula xml:id="formula_4">L = L contrast + ? ? L RTD</formula><p>The difference between our model and ELECTRA is that our discriminator D is conditional, so it can use the information of x compressed in a fixeddimension vector h = f (x). The gradient of D can be backward-propagated into f through h. By doing so, f will be encouraged to make h informative enough to cover the full meaning of x, so that D can distinguish the tiny difference between x and x . This approach essentially makes the conditional discriminator perform a "diff operation", hence the name DiffCSE.</p><p>When we train our DiffCSE model, we fix the generator G, and only the sentence encoder f and the discriminator D are optimized. After training, we discard D and only use f (which remains fixed) to extract sentence embeddings to evaluate on the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>In our experiment, we follow the setting of unsupervised SimCSE <ref type="bibr" target="#b12">(Gao et al., 2021)</ref> and build our model based on their PyTorch implementation. <ref type="bibr">3</ref> We also use the checkpoints of BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b22">(Liu et al., 2019)</ref> as the initialization of our sentence encoder f . We add an MLP layer with Batch Normalization (Ioffe and Szegedy, 2015) (BatchNorm) on top of the [CLS] representation as the sentence embedding. We will compare the model with/without BatchNorm in section 5. For the discriminator D, we use the same model as the sentence encoder f (BERT/RoBERTa). For the generator G, we use the smaller DistilBERT and DistilRoBERTa <ref type="bibr" target="#b30">(Sanh et al., 2019)</ref> for efficiency. Note that the generator is fixed during training unlike the ELECTRA paper <ref type="bibr" target="#b6">(Clark et al., 2020)</ref>. We will compare the results of using different size model for the generator in section 5. More training details are shown in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data</head><p>For unsupervised pretraining, we use the same 10 6 randomly sampled sentences from English Wikipedia that are provided by the source code of SimCSE. <ref type="bibr">3</ref> We evaluate our model on 7 semantic textual similarity (STS) and 7 transfer tasks in SentEval. 4 STS tasks includes STS 2012-2016 <ref type="bibr" target="#b0">(Agirre et al., 2016)</ref>, STS Benchmark <ref type="bibr" target="#b2">(Cer et al., 2017)</ref> and SICK-Relatedness <ref type="bibr" target="#b24">(Marelli et al., 2014)</ref>. All the STS experiments are fully unsupervised, which means no STS training datasets are used and all embeddings are fixed once they are trained. The transfer tasks are various sentence classification tasks, including MR <ref type="bibr" target="#b27">(Pang and Lee, 2005)</ref>, CR <ref type="bibr" target="#b16">(Hu and Liu, 2004)</ref>, SUBJ <ref type="bibr" target="#b26">(Pang and Lee, 2004)</ref>, MPQA <ref type="bibr" target="#b36">(Wiebe et al., 2005)</ref>, SST-2 <ref type="bibr" target="#b31">(Socher et al., 2013)</ref>, <ref type="bibr">TREC (Voorhees and Tice, 2000)</ref> and <ref type="bibr">MRPC (Dolan and Brockett, 2005)</ref>. In these transfer tasks, we will use a logistic regression classifier trained on top of the frozen sentence embeddings, following the standard setup (Conneau and Kiela, 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Baselines We compare our model with many strong unsupervised baselines including Sim-CSE <ref type="bibr" target="#b12">(Gao et al., 2021)</ref>, IS-BERT <ref type="bibr" target="#b39">(Zhang et al., 2020)</ref>, CMLM , <ref type="bibr">De-CLUTR (Giorgi et al., 2020)</ref>, CT-BERT <ref type="bibr">(Carlsson et al., 2021)</ref>, SG-OPT <ref type="bibr" target="#b19">(Kim et al., 2021)</ref> and some post-processing methods like BERT-flow <ref type="bibr" target="#b21">(Li et al., 2020)</ref> and BERT-whitening <ref type="bibr" target="#b32">(Su et al., 2021)</ref> along with some naive baselines like averaged GloVe embeddings <ref type="bibr" target="#b28">(Pennington et al., 2014)</ref> and averaged first and last layer BERT embeddings.</p><p>Semantic Textual Similarity (STS) We show the results of STS tasks in <ref type="table" target="#tab_1">Table 1</ref> including BERT base (upper part) and RoBERTa base (lower part). We also reproduce the previous state-ofthe-art SimCSE <ref type="bibr" target="#b12">(Gao et al., 2021</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies</head><p>In the following sections, we perform an extensive series of ablation studies that support our model design. We use BERT base model to evaluate on the development set of STS-B and transfer tasks.</p><p>Removing Contrastive Loss In our model, both the contrastive loss and the RTD loss are crucial because they maintain what should be sensitive and what should be insensitive respectively. If we remove the RTD loss, the model becomes a SimCSE model; if we remove the contrastive loss, the performance of STS-B drops significantly by 30%, while the average score of transfer tasks also drops by 2% (see <ref type="table" target="#tab_4">Table 3</ref>). This result shows that it is important to have insensitive and sensitive attributes that exist together in the representation space.</p><p>Next Sentence vs. Same Sentence Some methods for unsupervised sentence embeddings like Quick-Thoughts (Logeswaran and Lee, 2018) and CMLM  predict the next sentence as the training objective. We also experiment with a variant of DiffCSE by conditioning the ELECTRA loss based on the next sentence.</p><p>Note that this kind of model is not doing a "diff operation" between two similar sentences, and is not an instance of equivariant contrastive learning. As shown in <ref type="table" target="#tab_4">Table 3</ref> (use next sent. for x ), the score of STS-B decreases significantly compared to DiffCSE while transfer performance remains similar. We also tried using the same sentence and the next sentence at the same time for conditioning the ELECTRA objective (use same+next sent. for x ), and did not observe improvements.</p><p>Other Conditional Pretraining Tasks Instead of a conditional binary difference prediction loss, we can also consider other conditional pretraining tasks such as a conditional MLM objective proposed by , or corrective language modeling, 5 proposed by COCO-LM <ref type="bibr" target="#b25">(Meng et al., 2021)</ref>. We experiment with these objectives instead of the difference prediction objective in <ref type="table" target="#tab_4">Table 3</ref>. We observe that conditional MLM on the same sentence does not improve the performance either on STS-B or transfer tasks compared with DiffCSE. Conditional MLM on the next sentence performs even worse for STS-B, but slightly better than using the same sentence on transfer tasks. Using both the same and the next sentence also does not improve the performance compared with DiffCSE. For the corrective LM objective, the performance of STS-B decreases significantly compared with DiffCSE.     randomly insert mask tokens to the sentence, and then use a generator to convert mask tokens into real tokens. The number of inserted masked tokens is 15% of the sentence length. The task is to predict whether a token is an inserted token or the original token. For deletion, we randomly delete 15% tokens in the sentence, and the task is to predict for each token whether a token preceding it has been deleted or not. The results are shown in <ref type="table" target="#tab_5">Table 4</ref>. We can see that using either insertion or deletion achieves a slightly worse STS-B performance than using MLM replacement. For transfer tasks, their results are similar. Finally, we find that combining all three augmentations in the training process does not improve the MLM replacement strategy.</p><p>Pooler Choice In SimCSE, the authors use the pooler in BERT's original implementation (one linear layer with tanh activation function) as the final layer to extract features for computing contrastive loss. In our implementation (see details in Appendix A), we find that it is better to use a two-layer pooler with Batch Normalization (Batch-Norm) <ref type="bibr" target="#b18">(Ioffe and Szegedy, 2015)</ref>, which is commonly used in contrastive learning framework in computer vision <ref type="bibr" target="#b4">(Chen et al., 2020;</ref><ref type="bibr" target="#b14">Grill et al., 2020;</ref><ref type="bibr" target="#b5">Chen and He, 2021;</ref><ref type="bibr" target="#b17">Hua et al., 2021)</ref>. We show the ablation results in   BERT's performance due to knowledge distillation.</p><p>We show our results in <ref type="table" target="#tab_8">Table 6</ref>, we can see the performance of transfer tasks does not change much with different generators. However, the score of STS-B decreases as we switch from BERTmedium to BERT-tiny. This finding is not the same as ELECTRA, which works best with generators 1/4-1/2 the size of the discriminator. Because our discriminator is conditional on sentence vectors, it will be easier for the discriminator to perform the RTD task. As a result, using stronger generators (BERT base , DistilBERT base ) to increase the difficulty of RTD would help the discriminator learn better. However, when using a large model like BERT large , it may be a too-challenging task for the discriminator. In our experiment, using DistilBERT base , which has the ability close to but slightly worse than BERT base , gives us the best performance.</p><p>Masking Ratio In our conditional ELECTRA task, we can mask the original sentence in different ratios for the generator to produce MLM-based augmentations. A higher masking ratio will make more perturbations to the sentence. Our empirical result in <ref type="table" target="#tab_9">Table 7</ref> shows that the difference between difference masking ratios is small (in 15%-40% ), and a masking ratio of around 30% can give us the best performance.</p><p>Coefficient ? In Section 3, we use the ? coefficient to weight the ELECTRA loss and then add it with contrastive loss. Because the contrastive learning objective is a relatively easier task, the scale of contrastive loss will be 100 to 1000 smaller than  ELECTRA loss. As a result, we need a smaller ? to balance these two loss terms. In the <ref type="table" target="#tab_11">Table 8</ref> we show the STS-B result under different ? values. Note that when ? goes to zero, the model becomes a SimCSE model. We find that using ? = 0.005 can give us the best performance.</p><p>6 Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Qualitative Study</head><p>A very common application for sentence embeddings is the retrieval task. Here we show some retrieval examples to qualitatively explain why Dif-fCSE can perform better than SimCSE. In this study, we use the 2758 sentences from STS-B testing set as the corpus, and then use sentence query to retrieve the nearest neighbors in the sentence embedding space by computing cosine similarities. We show the retrieved top-3 examples in <ref type="table" target="#tab_13">Table 9</ref>. The first query sentence is "you can do it, too.". The SimCSE model retrieves a very similar sentence but has a slightly different meaning ("you can use it, too.") as the rank-1 answer. In contrast, DiffCSE can distinguish the tiny difference, so it retrieves the ground truth answer as the rank-1 answer. The second query sentence is "this is not a problem". SimCSE retrieves a sentence with opposite meaning but very similar wording, while DiffCSE can retrieve the correct answer with less similar wording. We also provide a third example where both SimCSE and DiffCSE fail to retrieve the correct answer for a query sentence using double negation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Retrieval Task</head><p>Besides the qualitative study, we also show the quantitative result of the retrieval task. Here we also use all the 2758 sentences in the testing set of STS-B as the corpus. There are 97 positive pairs in this corpus (with 5 out of 5 semantic similarity scores from human annotation). For each positive pair, we use one sentence to retrieve the other one, and see whether the other sentence is in the top-1/5/10 ranking. The recall@1/5/10 of the retrieval task are shown in 3) can you do it?</p><p>Query: this is not a problem.</p><p>1) this is a big problem. 1) i don 't see why this could be a problem. 2) you have a problem.</p><p>2) i don 't see why that should be a problem. 3) i don 't see why that should be a problem.</p><p>3) this is a big problem.</p><p>Query: i think that is not a bad idea. 1) i do not think it's a good idea.</p><p>1) i do not think it's a good idea . 2) it's not a good idea .</p><p>2) it is not a good idea. 3) it is not a good idea .</p><p>3) but it is not a good idea.   recall@1/5/10, showing the effectiveness of using DiffCSE for the retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Distribution of Sentence Embeddings</head><p>To look into the representation space of DiffCSE, we plot the cosine similarity distribution of sentence pairs from STS-B test set for both SimCSE and DiffCSE in <ref type="figure" target="#fig_1">Figure 2</ref>. We observe that both SimCSE and DiffCSE can assign cosine similarities consistent with human ratings. However, we also observe that under the same human rating, DiffCSE assigns slightly higher cosine similarities compared with SimCSE. This phenomenon  may be caused by the fact that ELECTRA and other Transformer-based pretrained LMs have the problem of squeezing the representation space, as mentioned by <ref type="bibr" target="#b25">Meng et al. (2021)</ref>. As we use the sentence embeddings as the input of ELECTRA to perform conditional ELECTRA training, the sentence embedding will be inevitably squeezed to fit the input distribution of ELECTRA. We follow prior studies <ref type="bibr" target="#b35">(Wang and Isola, 2020;</ref><ref type="bibr" target="#b12">Gao et al., 2021)</ref> to use uniformity and alignment (details in Appendix C) to measure the quality of representation space for DiffCSE and SimCSE in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we present DiffCSE, a new unsupervised sentence embedding framework that is aware of, but not invariant to, MLM-based word replacement. Empirical results on semantic textual similarity tasks and transfer tasks both show the effectiveness of DiffCSE compared to current stateof-the-art sentence embedding methods. We also conduct extensive ablation studies to demonstrate the different modeling choices in DiffCSE. Qualitative study and the retrieval results also show that DiffCSE can produce a better embedding space for sentence retrieval. One limitation of our work is that we do not explore the supervised setting that uses human-labeled NLI datasets to further boost the performance. We leave this topic for future work. We believe that our work can provide researchers in the NLP community a new way to utilize augmentations for natural language and thus produce better sentence embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head><p>We use a single NVIDIA 2080Ti GPU for each experiment. The averaged running time for DiffCSE is 3-6 hours. We use gridsearch of batch size ? {64, 128} learning rate ? {2e-6, 3e-6, 5e-6, 7e-6, 1e-5} and masking ratio ? {0.15, 0.20, 0.30, 0.40} and ? ? {0.1, 0.05, 0.01, 0.005, 0.001}. The temperature ? in SimCSE is set to 0.05 for all the experiments. During the training process, we save the checkpoint with the highest score on the STS-B development set. And then we use STS-B development set to find the best hyperparameters (listed in <ref type="table" target="#tab_1">Table 12)</ref> for STS task; we use the averaged score of the development sets of 7 transfer tasks to find the best hyperparameters (listed in <ref type="table" target="#tab_1">Table 13</ref>) for transfer tasks. All numbers in <ref type="table" target="#tab_1">Table 1 and Table 2</ref>      encoder, so the model size is the same as the Sim-CSE model.</p><p>Projector with BatchNorm In Section 5, we mention that we use a projector with BatchNorm as the final layer of our model. Here we provided the PyTorch code for its structure: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Using Augmentations as Positive/Negative Examples</head><p>In Section 5, we try to use different augmentations (e.g. insertion, deletion, replacement) for learning equivariance. In <ref type="table" target="#tab_1">Table 15</ref> we provide the results of using these augmentations as additional positive or negative examples along with the SimCSE training paradigm. We can observe that using these augmentations as additional positives only decreases the performance. The only method that can improve the performance a little bit is to use MLM 15% replaced examples as additional negative examples. Overall, none of these results can perform better than our proposed method, e.g. using these augmentations to learn equivariance.</p><p>C Uniformity and Alignment <ref type="bibr" target="#b35">Wang and Isola (2020)</ref>  The smaller the values of uniformity and alignment, the better the quality of the representation space is indicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Source Code</head><p>We build our model using the PyTorch implementation of SimCSE 7 <ref type="bibr" target="#b12">Gao et al. (2021)</ref>, which is based on the HuggingFace's Transformers package. <ref type="bibr">8</ref> We also upload our code 9 and pretrained models (links in README.md). Please follow the instructions in README.md to reproduce the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Potential Risks</head><p>On the risk side, insofar as our method utilizes pretrained language models, it may inherit and propagate some of the harmful biases present in such models. Besides that, we do not see any other potential risks in our paper. 7 https://github.com/princeton-nlp/ SimCSE 8 https://github.com/huggingface/ transformers 9 https://github.com/voidism/DiffCSE</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of DiffCSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The distribution of cosine similarities from SimCSE/DiffCSE for STS-B test set. Along the y-axis are 5 groups of data splits based on human ratings. The x-axis is the cosine similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>propose to use two properties, alignment and uniformity, to measure the quality of representations. Given a distribution of positive pairs p pos and the distribution of the whole dataset p data , alignment computes the expected distance between normalized embeddings of the paired sentences: align E (x,x + )?ppos f (x) ? f x + 2 . Uniformity measures how well the embeddings are uniformly distributed in the representation space: uniform log E x,y i.i.d. ? p data e ?2 f (x)?f (y) 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). DiffCSE-BERT base can significantly outperform SimCSE-BERT base and raise the averaged Spearman's correlation from 76.25% to 78.49%. For the RoBERTa model, DiffCSE-RoBERTa base can also improve upon SimCSE-RoBERTa base from 76.57% to 77.80%. Tasks We show the results of transfer tasks in Table 2. Compared with SimCSE-BERT base , DiffCSE-BERT base can improve the averaged scores from 85.56% to 86.86%. When applying it to the RoBERTa model, DiffCSE-RoBERTa base also improves upon SimCSE-RoBERTa base from 84.84% to 87.04%. Note that the CMLM-BERT base can achieve even better performance than DiffCSE. However, they use 1TB of the training data from Common Crawl dumps while our model only use 115MB of the Wikipedia data for pretraining. We put their scores inTable 2for reference. In Sim-CSE, the authors propose to use MLM as an auxiliary task for the sentence encoder to further boost the performance of transfer tasks. Compared with</figDesc><table><row><cell>STS12 STS13 STS14 STS15 STS16 STS-B SICK-R 55.14 70.66 59.73 68.25 63.66 58.02 53.76 39.70 59.38 49.67 66.03 66.19 53.87 62.06 58.40 67.10 60.85 75.16 71.22 68.66 64.47 57.83 66.90 60.90 75.08 71.31 68.24 63.73 56.77 69.24 61.21 75.23 70.16 69.21 64.25 58.20 61.07 61.67 73.32 74.88 76.60 64.80 61.63 76.80 68.47 77.50 76.48 74.31 69.19 66.84 80.13 71.23 81.56 77.17 77.23 68.16 68.40 82.41 74.38 80.91 78.56 76.85 72.23 70.82 82.24 73.25 81.38 77.06 77.24 71.16 72.28 84.43 76.47 83.90 80.54 80.59 71.23 40.88 58.74 49.07 65.63 61.48 58.55 61.63 46.99 63.24 57.23 71.36 68.99 61.36 62.91 52.41 75.19 65.52 77.12 78.63 72.41 68.62 70.16 81.77 73.24 81.36 80.65 80.22 68.56 68.60 81.36 73.16 81.61 80.76 80.58 68.83 Transfer Model GloVe embeddings (avg.) ? BERTbase (first-last avg.) ? BERTbase-flow ? BERTbase-whitening ? IS-BERTbase ? CMLM-BERTbase ? (1TB data) CT-BERTbase ? SG-OPT-BERTbase  ? SimCSE-BERTbase ?  *  SimCSE-BERTbase(reproduce)  *  DiffCSE-BERTbase RoBERTabase (first-last avg.) ? RoBERTabase-whitening ? DeCLUTR-RoBERTabase ? SimCSE-RoBERTabase ?  *  SimCSE-RoBERTabase(reproduce)  *  DiffCSE-RoBERTabase 70.05 83.43 75.49 82.81 82.12 82.38 71.19</cell><cell>Avg. 61.32 56.70 66.55 66.28 66.58 67.22 72.05 74.62 76.25 76.16 78.49 56.57 61.73 69.99 76.57 76.41 78.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The performance on STS tasks (Spearman's correlation) for different sentence embedding models. ?: results from Reimers and Gurevych (2019); ?: results from Zhang et al. (2020); ?: results from Gao et al. (2021); ?: results from Yang et al. (2020); ?: results from Kim et al. (2021); * : results from our experiments.the results of SimCSE with MLM, DiffCSE still can have a little improvement around 0.2%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Transfer task results of different sentence embedding models (measured as accuracy). ?: results from<ref type="bibr" target="#b29">Reimers and Gurevych (2019)</ref>; ?: results from<ref type="bibr" target="#b39">Zhang et al. (2020)</ref>; ?: results from<ref type="bibr" target="#b12">Gao et al. (2021)</ref>.</figDesc><table><row><cell></cell><cell cols="2">STS-B Avg. transfer</cell></row><row><cell>SimCSE</cell><cell>81.47</cell><cell>83.91</cell></row><row><cell>DiffCSE</cell><cell>84.56</cell><cell>85.95</cell></row><row><cell>w/o contrastive loss</cell><cell>54.48</cell><cell>83.46</cell></row><row><cell>use next sent. for x</cell><cell>82.91</cell><cell>85.83</cell></row><row><cell>use same+next sent. for x</cell><cell>83.41</cell><cell>85.82</cell></row><row><cell>Conditional MLM</cell><cell></cell><cell></cell></row><row><cell>for same sent.</cell><cell>83.08</cell><cell>84.43</cell></row><row><cell>for next sent.</cell><cell>75.82</cell><cell>85.68</cell></row><row><cell>for same+next sent.</cell><cell>82.88</cell><cell>84.82</cell></row><row><cell>Conditional Corrective LM</cell><cell>79.79</cell><cell>85.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Development set results of STS-B and transfer tasks for DiffCSE model variants, where we vary the objective and the use of same or next sentence.</figDesc><table><row><cell>Augmentation</cell><cell cols="2">STS-B Avg. transfer</cell></row><row><cell>MLM 15%</cell><cell>84.48</cell><cell>85.95</cell></row><row><cell cols="2">randomly insert 15% 82.20</cell><cell>85.96</cell></row><row><cell cols="2">randomly delete 15% 82.59</cell><cell>85.97</cell></row><row><cell>combining all</cell><cell>82.80</cell><cell>85.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Development set results of STS-B and transfer tasks with different augmentation methods for learning equivariance.</figDesc><table><row><cell></cell><cell cols="2">STS-B Avg. transfer</cell></row><row><cell>DiffCSE</cell><cell></cell><cell></cell></row><row><cell>w/ BatchNorm</cell><cell>84.56</cell><cell>85.95</cell></row><row><cell>w/o BatchNorm</cell><cell>83.23</cell><cell>85.24</cell></row><row><cell>SimCSE</cell><cell></cell><cell></cell></row><row><cell>w/ BatchNorm</cell><cell>82.22</cell><cell>85.66</cell></row><row><cell>w/o BatchNorm</cell><cell>81.47</cell><cell>83.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Development set results of STS-B and transfer tasks for DiffCSE and SimCSE with and without BatchNorm.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>We can observe that adding BatchNorm is beneficial for either DiffCSE or SimCSE to get better performance on STS-B and transfer tasks.Size of the Generator In our DiffCSE model, the generator can be in different model size from BERT large , BERT base<ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, DistilBERT base<ref type="bibr" target="#b30">(Sanh et al., 2019)</ref>, BERT medium , BERT</figDesc><table><row><cell></cell><cell cols="2">STS-B Avg. transfer</cell></row><row><cell>SimCSE</cell><cell>81.47</cell><cell>83.91</cell></row><row><cell>DiffCSE w/ generator:</cell><cell></cell><cell></cell></row><row><cell>BERTlarge (L=24, H=1024)</cell><cell>82.93</cell><cell>85.88</cell></row><row><cell>BERTbase (L=12, H=768)</cell><cell>83.63</cell><cell>85.85</cell></row><row><cell>DistilBERTbase (L=6, H=768)</cell><cell>84.56</cell><cell>85.95</cell></row><row><cell>BERTmedium (L=8, H=512)</cell><cell>82.25</cell><cell>85.80</cell></row><row><cell>BERTsmall (L=4, H=512)</cell><cell>82.64</cell><cell>85.66</cell></row><row><cell>BERTmini (L=4, H=256)</cell><cell>82.12</cell><cell>85.90</cell></row><row><cell>BERTtiny (L=2, H=128)</cell><cell>81.40</cell><cell>85.23</cell></row></table><note>small , BERT mini , BERT tiny (Turc et al., 2019). Their exact sizes are shown in Table 6 (L: number of layers, H: hidden dimension). Notice that although DistilBERT base has only half the number of layers of BERT, it can retain 97% of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Development set results of STS-B and transfer tasks with different generators.</figDesc><table><row><cell>Ratio</cell><cell>15% 20% 25% 30% 40% 50%</cell></row><row><cell cols="2">STS-B 84.48 84.04 84.49 84.56 84.48 83.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Development set results of STS-B under different masking ratio for augmentations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Development set results of STS-B under different ?.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>. We can</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="4">: Retrieved top-3 examples by SimCSE and Dif-</cell></row><row><cell>fCSE from STS-B test set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model/Recall</cell><cell>@1</cell><cell>@5</cell><cell>@10</cell></row><row><cell cols="4">SimCSE-BERTbase 77.84 92.78 95.88</cell></row><row><cell cols="4">DiffCSE-BERTbase 78.87 95.36 97.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>The retrieval results for SimCSE and Dif-fCSE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Alignment and Uniformity<ref type="bibr" target="#b35">(Wang and Isola, 2020</ref>) measured on STS-B test set for SimCSE and Dif-fCSE. The smaller the number is better. We also show the averaged STS score in the right-most column.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>The main hyperparameters in STS tasks.</figDesc><table><row><cell cols="3">hyperparam BERTbase RoBERTabase</cell></row><row><cell>learning rate</cell><cell>2e-6</cell><cell>3e-6</cell></row><row><cell>masking ratio</cell><cell>0.15</cell><cell>0.15</cell></row><row><cell>?</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>training epochs</cell><cell>2</cell><cell>2</cell></row><row><cell>batch size</cell><cell>64</cell><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>The main hyperparameters in transfer tasks.</figDesc><table><row><cell>Method</cell><cell cols="2">BERTbase RoBERTabase</cell></row><row><cell>SimCSE</cell><cell>110M</cell><cell>125M</cell></row><row><cell>DiffCSE (train)</cell><cell>220M</cell><cell>250M</cell></row><row><cell>DiffCSE (test)</cell><cell>110M</cell><cell>125M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14 :</head><label>14</label><figDesc>The number of parameters used in our models.During testing, we follow SimCSE to discard the MLP projector and only use the [CLS] output to extract the sentence embeddings.The numbers of model parameters for BERT base and RoBERTa base are listed inTable 14. Note that in training time DiffCSE needs two BERT models to work together (sentence encoder + discriminator), but in testing time we only need the sentence</figDesc><table><row><cell>Method</cell><cell cols="2">STS-B Avg. transfer</cell></row><row><cell>SimCSE</cell><cell>81.47</cell><cell>83.91</cell></row><row><cell>+ Additional positives</cell><cell></cell><cell></cell></row><row><cell>MLM 15%</cell><cell>73.59</cell><cell>83.33</cell></row><row><cell>random insert 15%</cell><cell>80.39</cell><cell>83.92</cell></row><row><cell>random delete 15%</cell><cell>78.58</cell><cell>81.80</cell></row><row><cell>+ Additional negatives</cell><cell></cell><cell></cell></row><row><cell>MLM 15%</cell><cell>83.02</cell><cell>84.49</cell></row><row><cell>random insert 15%</cell><cell>55.65</cell><cell>79.86</cell></row><row><cell>random delete 15%</cell><cell>55.13</cell><cell>82.56</cell></row><row><cell>+ Equivariance (Ours)</cell><cell></cell><cell></cell></row><row><cell>MLM 15%</cell><cell>84.48</cell><cell>85.95</cell></row><row><cell>randomly insert 15%</cell><cell>82.20</cell><cell>85.96</cell></row><row><cell>randomly delete 15%</cell><cell>82.59</cell><cell>85.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 15 :</head><label>15</label><figDesc>Development set results of STS-B and transfer tasks for using three types of augmentations (replace, insert, delete) in different ways.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">SimCSE has two settings: unsupervised and supervised. In this paper, we focus on the unsupervised setting. Unless otherwise stated, in this paper we use SimCSE to refer to unsupervised SimCSE.2 Pretrained models and code are available at https:// github.com/voidism/DiffCSE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/princeton-nlp/SimCSE 4 https://github.com/facebookresearch/SentEval</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This task is similar to ELECTRA. However, instead of a binary classifier for replaced token detection, corrective LM uses a vocabulary-size classifier with the copy mechanism to recover the replaced tokens.6  Edit distance operators include insert, delete and replace.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was partially supported by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Fund (InnoHK).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1081</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelia</forename><surname>Amaru Cuba Gyllensten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gogoulou</surname></persName>
		</author>
		<title level="m">Erik Ylip?? Hellqvist, and Magnus Sahlgren. 2021. Semantic re-tuning with contrastive tension</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Universal sentence encoder for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
			<date type="published" when="2020" />
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">SentEval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pulkit Agrawal, and Marin Solja?i?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumen</forename><surname>Dangovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwook</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00899</idno>
	</analytic>
	<monogr>
		<title level="m">Equivariant contrastive learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
		<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvald</forename><surname>John M Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03659</idno>
		<title level="m">Declutr: Deep contrastive learning for unsupervised textual representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03483</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On feature decorrelation in self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sucheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9598" to="9608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07345</idno>
		<title level="m">Self-guided contrastive learning for bert sentence representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.733</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
		<meeting><address><addrLine>Reykjavik</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08473</idno>
		<title level="m">Coco-lm: Correcting and contrasting text sequences for language model pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno>cs/0409058</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyiwen</forename><surname>Ou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15316</idno>
		<title level="m">Whitening sentence representations for better semantics and faster retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Well-read students learn better: On the importance of pre-training compact models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Building a question answering test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="200" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Consert: A contrastive framework for self-supervised sentence representation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanmeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11741</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Universal sentence representation learning with conditional masked language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jax</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Darve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14388</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An unsupervised sentence embedding method by mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
