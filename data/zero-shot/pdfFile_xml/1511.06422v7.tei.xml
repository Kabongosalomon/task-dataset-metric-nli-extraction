<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ALL YOU NEED IS A GOOD INIT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
							<email>mishkdmy@cmp.felk.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Perception</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
							<email>matas@cmp.felk.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Perception</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ALL YOU NEED IS A GOOD INIT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Layer-sequential unit-variance (LSUV) initialization -a simple method for weight initialization for deep net learning -is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets <ref type="bibr" target="#b20">(Romero et al. (2015)</ref>) and Highway <ref type="bibr" target="#b26">(Srivastava et al. (2015)</ref>). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets. 1  The code allowing to reproduce the experiments is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep nets have demonstrated impressive results on a number of computer vision and natural language processing problems. At present, state-of-the-art results in image classification <ref type="bibr" target="#b24">(Simonyan &amp; Zisserman (2015)</ref>; <ref type="bibr" target="#b28">Szegedy et al. (2015)</ref>) and speech recognition <ref type="bibr" target="#b23">(Sercu et al. (2015)</ref>), etc., have been achieved with very deep (? 16 layer) CNNs. Thin deep nets are of particular interest, since they are accurate and at the same inference-time efficient <ref type="bibr" target="#b20">(Romero et al. (2015)</ref>).</p><p>One of the main obstacles preventing the wide adoption of very deep nets is the absence of a general, repeatable and efficient procedure for their end-to-end training. For example, VGGNet <ref type="bibr" target="#b24">(Simonyan &amp; Zisserman (2015)</ref>) was optimized by a four stage procedure that started by training a network with moderate depth, adding progressively more layers. <ref type="bibr" target="#b20">Romero et al. (2015)</ref> stated that deep and thin networks are very hard to train by backpropagation if deeper than five layers, especially with uniform initialization.</p><p>On the other hand,  showed that it is possible to train the VGGNet in a single optimization run if the network weights are initialized with a specific ReLU-aware initialization. The  procedure generalizes to the ReLU non-linearity the idea of filter-size dependent initialization, introduced for the linear case by <ref type="bibr" target="#b3">(Glorot &amp; Bengio (2010)</ref>). Batch normalization <ref type="bibr" target="#b12">(Ioffe &amp; Szegedy (2015)</ref>), a technique that inserts layers into the the deep net that transform the output for the batch to be zero mean unit variance, has successfully facilitated training of the twenty-two layer GoogLeNet ). However, batch normalization adds a 30% computational overhead to each iteration.</p><p>The main contribution of the paper is a proposal of a simple initialization procedure that, in connection with standard stochastic gradient descent (SGD), leads to state-of-the-art thin and very deep neural nets 1 . The result highlights the importance of initialization in very deep nets. We review the history of CNN initialization in Section 2, which is followed by a detailed description of the novel initialization method in Section 3. The method is experimentally validated in Section 4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INITIALIZATION IN NEURAL NETWORKS</head><p>After the success of CNNs in IVSRC 2012 <ref type="bibr">(Krizhevsky et al. (2012)</ref>), initialization with Gaussian noise with mean equal to zero and standard deviation set to 0.01 and adding bias equal to one for some layers become very popular. But, as mentioned before, it is not possible to train very deep network from scratch with it <ref type="bibr" target="#b24">(Simonyan &amp; Zisserman (2015)</ref>). The problem is caused by the activation (and/or) gradient magnitude in final layers ). If each layer, not properly initialized, scales input by k, the final scale would be k L , where L is a number of layers. Values of k &gt; 1 lead to extremely large values of output layers, k &lt; 1 leads to a diminishing signal and gradient. <ref type="bibr" target="#b3">Glorot &amp; Bengio (2010)</ref> proposed a formula for estimating the standard deviation on the basis of the number of input and output channels of the layers under assumption of no non-linearity between layers. Despite invalidity of the assumption, Glorot initialization works well in many applications.  extended this formula to the ReLU <ref type="bibr" target="#b4">(Glorot et al. (2011)</ref>) non-linearity and showed its superior performance for ReLU-based nets. <ref type="figure">Figure 1</ref> shows why scaling is important. Large weights lead to divergence via updates larger than the initial values, small initial weights do not allow the network to learn since the updates are of the order of 0.0001% per iteration. The optimal scaling for ReLU-net is around 1.4, which is in line with the theoretically derived ? 2 by . <ref type="bibr" target="#b27">Sussillo &amp; Abbott (2014)</ref> proposed the so called Random walk initialization, RWI, which keeps constant the log of the norms of the backpropagated errors. In our experiments, we have not been able to obtain good results with our implementation of RWI, that is why this method is not evaluated in experimental section. <ref type="bibr" target="#b11">Hinton et al. (2014)</ref> and <ref type="bibr" target="#b20">Romero et al. (2015)</ref> take another approach to initialization and formulate training as mimicking teacher network predictions (so called knowledge distillation) and internal representations (so called Hints initialization) rather than minimizing the softmax loss. <ref type="bibr" target="#b26">Srivastava et al. (2015)</ref> proposed a LSTM-inspired gating scheme to control information and gradient flow through the network. They trained a 1000-layers MLP network on MNIST. Basically, this kind of networks implicitly learns the depth needed for the given task.</p><p>Independently, <ref type="bibr" target="#b22">Saxe et al. (2014)</ref> showed that orthonormal matrix initialization works much better for linear networks than Gaussian noise, which is only approximate orthogonal. It also work for networks with non-linearities.</p><p>The approach of layer-wise pre-training <ref type="bibr" target="#b0">(Bengio et al. (2007)</ref>) which is still useful for multi-layerperceptron, is not popular for training discriminative convolution networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LAYER-SEQUENTIAL UNIT-VARIANCE INITIALIZATION</head><p>To the best of our knowledge, there have been no attempts to generalize <ref type="bibr" target="#b3">Glorot &amp; Bengio (2010)</ref> formulas to non-linearities other than ReLU, such as tanh, maxout, etc. Also, the formula does not cover max-pooling, local normalization layers <ref type="bibr">Krizhevsky et al. (2012)</ref> and other types of layers which influences activations variance. Instead of theoretical derivation for all possible layer types, or doing extensive parameters search as in <ref type="figure">Figure 1</ref>, we propose a data-driven weights initialization.</p><p>We thus extend the orthonormal initialization <ref type="bibr" target="#b22">Saxe et al. (2014)</ref> to an iterative procedure, described in Algorithm 1. <ref type="bibr" target="#b22">Saxe et al. (2014)</ref> could be implemented in two steps. First, fill the weights with Gaussian noise with unit variance. Second, decompose them to orthonormal basis with QR or SVDdecomposition and replace weights with one of the components.</p><p>The LSUV process then estimates output variance of each convolution and inner product layer and scales the weight to make variance equal to one. The influence of selected mini-batch size on estimated variance is negligible in wide margins, see Appendix.</p><p>The proposed scheme can be viewed as an orthonormal initialization combined with batch normalization performed only on the first mini-batch. The similarity to batch normalization is the unit variance normalization procedure, while initial ortho-normalization of weights matrices efficiently de-correlates layer activations, which is not done in <ref type="bibr" target="#b12">Ioffe &amp; Szegedy (2015)</ref>. Experiments show that such normalization is sufficient and computationally highly efficient in comparison with full batch normalization.</p><p>The LSUV algorithm is summarized in Algorithm 1. The single parameter Tol var influences convergence of the initialization procedure, not the properties of the trained network. Its value does not noticeably influence the performance in a broad range of 0.01 to 0.1. Because of data variations, it is often not possible to normalize variance with the desired precision. To eliminate the possibility of Algorithm 1 Layer-sequential unit-variance orthogonal initialization. L -convolution or fullconnected layer, W L -its weights, B L -its output blob., Tol var -variance tolerance, T i -current trial, T max -max number of trials.</p><p>Pre-initialize network with orthonormal matrices as in <ref type="bibr" target="#b22">Saxe et al. (2014)</ref> for each layer L do while |Var(BL) ? 1.0| ? Tolvar and (Ti &lt; Tmax) do do Forward pass with a mini-batch calculate Var(BL) WL = WL / Var(BL) end while end for an infinite loop, we restricted number of trials to T max . However, in experiments described in paper, the T max was never reached. The desired variance was achieved in 1-5 iterations.</p><p>We tested a variant LSUV initialization which was normalizing input activations of the each layer instead of output ones. Normalizing the input or output is identical for standard feed-forward nets, but normalizing input is much more complicated for networks with maxout <ref type="bibr" target="#b5">(Goodfellow et al. (2013)</ref>) or for networks like GoogLeNet ) which use the output of multiple layers as input. Input normalization brought no improvement of results when tested against the LSUV Algorithm 1, LSUV was also tested with pre-initialization of weights with Gaussian noise instead of orthonormal matrices. The Gaussian initialization led to small, but consistent, decrease in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL VALIDATION</head><p>Here we show that very deep and thin nets could be trained in a single stage. Network architectures are exactly as proposed by <ref type="bibr" target="#b20">Romero et al. (2015)</ref>. The architectures are presented in <ref type="table" target="#tab_1">Table 1</ref>.  <ref type="formula">(100)</ref> softmax-10(100) softmax-10 4.1 MNIST First, as a "sanity check", we performed an experiment on the MNIST dataset <ref type="bibr" target="#b17">(Lecun et al. (1998)</ref>). It consists of 60,000 28x28 grayscale images of handwritten digits 0 to 9. We selected the FitNet-MNIST architecture (see <ref type="table" target="#tab_1">Table 1</ref>) of <ref type="bibr" target="#b20">Romero et al. (2015)</ref> and trained it with the proposed initialization strategy, without data augmentation. Recognition results are shown in <ref type="table" target="#tab_2">Table 2</ref>, right block. LSUV outperforms orthonormal initialization and both LSUV and orthonormal outperform Hints initialization <ref type="bibr" target="#b20">Romero et al. (2015)</ref>. The error rates of the Deeply-Supervised Nets (DSN, <ref type="bibr" target="#b18">Lee et al. (2015)</ref>) and maxout networks <ref type="bibr" target="#b5">Goodfellow et al. (2013)</ref>, the current state-of-art, are provided for reference.</p><p>Since the widely cited DSN error rate of 0.39%, the state-of-the-art (until recently) was obtained after replacing the softmax classifier with SVM, we do the same and also observe improved results (line FitNet-LSUV-SVM in <ref type="table" target="#tab_2">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CIFAR-10/100</head><p>We validated the proposed initialization LSUV strategy on the CIFAR-10/100 (Krizhevsky <ref type="formula">(2009))</ref> dataset. It contains 60,000 32x32 RGB images, which are divided into 10 and 100 classes, respectively.</p><p>The FitNets are trained with the stochastic gradient descent with momentum set to 0.9, the initial learning rate set to 0.01 and reduced by a factor of 10 after the 100th, 150th and 200th epoch, finishing at 230th epoch. <ref type="bibr" target="#b26">Srivastava et al. (2015)</ref> and <ref type="bibr" target="#b20">Romero et al. (2015)</ref> trained their networks for 500 epochs. Of course, training time is a trade-off dependent on the desired accuracy; one could train a slightly less accurate network much faster.</p><p>Like in the MNIST experiment, LSUV and orthonormal initialized nets outperformed Hints-trained Fitnets, leading to the new state-of-art when using commonly used augmentation -mirroring and random shifts. The gain on the fine-grained CIFAR-100 is much larger than on CIFAR-10. Also, note that FitNets with LSUV initialization outperform even much larger networks like Large-All- <ref type="bibr">CNN Springenberg et al. (2014)</ref> and Fractional Max-pooling Graham (2014a) trained with affine and color dataset augmentation on CIFAR-100. The results of LSUV are virtually identical to the orthonormal initialization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS OF EMPIRICAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">INITIALIZATION STRATEGIES AND NON-LINEARITIES</head><p>For the FitNet-1 architecture, we have not experienced any difficulties training the network with any of the activation functions (ReLU, maxout, tanh), optimizers (SGD, RMSProp) or initialization (Xavier, MSRA, Ortho, LSUV), unlike the uniform initialization used in <ref type="bibr" target="#b20">Romero et al. (2015)</ref>. The most probable cause is that CNNs tolerate a wide variety of mediocre initialization, only the learning time increases. The differences in the final accuracy between the different initialization methods for the FitNet-1 architecture is rather small and are therefore not presented here.</p><p>The FitNet-4 architecture is much more difficult to optimize and thus we focus on it in the experiments presented in this section.</p><p>We have explored the initializations with different activation functions in very deep networks. More specifically, ReLU, hyperbolic tangent, sigmoid, maxout and the VLReLU -very leaky ReLU (Graham (2014c)) -a variant of leaky ReLU <ref type="bibr" target="#b19">( Maas et al. (2013)</ref>, with a large value of the negative slope <ref type="table">Table  3:  The  compatibility  of  activation  functions  and</ref> initialization. Dataset: CIFAR-10. Architecture: FitNet4, 2.5M params for maxout net, 1.2M for the rest, 17 layers. The n/c symbol stands for "failed to converge"; n/c ? -after extensive trials, we managed to train a maxout-net with MSRA initialization with very small learning rate and gradient clipping, see <ref type="figure" target="#fig_1">Figure 2</ref>. The experiment is marked n/c as training time was excessive and parameters non-standard.  <ref type="formula">(2014b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Init</head><p>Testing was performed on CIFAR-10 and results are in <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_1">Figure 2</ref>. Performance of orthonormal-based methods is superior to the scaled Gaussian-noise approaches for all tested types of activation functions, except tanh. Proposed LSUV strategy outperforms orthonormal initialization by smaller margin, but still consistently (see <ref type="table">Table 3</ref>). All the methods failed to train sigmoid-based very deep network. <ref type="figure" target="#fig_1">Figure 2</ref> shows that LSUV method not only leads to better generalization error, but also converges faster for all tested activation functions, except tanh.</p><p>We have also tested how the different initializations work "out-of-the-box" with the Residual net training ; a residual net won the ILSVRC-2015 challenge. The original paper proposed different implementations of residual learning. We adopted the simplest one, showed in Table 1, FitResNet-4. The output of each even convolutional layer is summed with the output of the previous non-linearity layer and then fed into the next non-linearity. Results are shown in Table 4. LSUV is the only initialization algorithm which leads nets to convergence with all tested non-linearities without any additional tuning, except, again, sigmoid. It is worth nothing that the residual training improves results for ReLU and maxout, but does not help tanh-based network. LSUV procedure could be viewed as batch normalization of layer output done only before the start of training. Therefore, it is natural to compare LSUV against a batch-normalized network, initialized with the standard method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">WHERE TO PUT BN -BEFORE OR AFTER NON-LINEARITY?</head><p>It is not clear from the paper <ref type="bibr" target="#b12">Ioffe &amp; Szegedy (2015)</ref> where to put the batch-normalization layerbefore input of each layer as stated in Section 3.1, or before non-linearity, as stated in section 3.2, so we have conducted an experiment with FitNet4 on CIFAR-10 to clarify this.</p><p>Results are shown in <ref type="table" target="#tab_6">Table 5</ref>. Exact numbers vary from run to run, but in the most cases, batch normalization put after non-linearity performs better.</p><p>In the next experiment we compare BN-FitNet4, initialized with Xavier and LSUV-initialized Fit-Net4. Batch-normalization reduces training time in terms of needed number of iterations, but each it-   eration becomes slower because of extra computations. The accuracy versus wall-clock-time graphs are shown in <ref type="figure">Figure 3</ref>. LSUV-initialized network is as good as batch-normalized one.</p><p>However, we are not claiming that batch normalization can always be replaced by proper initialization, especially in large datasets like ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">IMAGENET TRAINING</head><p>We trained CaffeNet <ref type="bibr" target="#b13">(Jia et al. (2014)</ref>) and GoogLeNet ) on the ImageNet-1000 dataset <ref type="bibr" target="#b21">( Russakovsky et al. (2015)</ref>) with the original initialization and LSUV. CaffeNet is a variant of AlexNet with the nearly identical performance, where the order of pooling and normalization layers is switched to reduce the memory footprint.</p><p>LSUV initialization reduces the starting flat-loss time from 0.5 epochs to 0.05 for CaffeNet, and starts to converge faster, but it is overtaken by a standard CaffeNet at the 30-th epoch (see <ref type="figure">Figure 4</ref>) and its final precision is 1.3% lower. We have no explanation for this empirical phenomenon.</p><p>On the contrary, the LSUV-initialized GoogLeNet learns faster than hen then original one and shows better test accuracy all the time -see <ref type="figure">Figure 5</ref>. The final accuracy is 0.680 vs. 0.672 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">TIMINGS</head><p>A significant part of LSUV initialization is SVD-decomposition of the weight matrices, e.g. for the fc6 layer of CaffeNet, an SVD of a 9216x4096 matrix is required. The computational overhead on top of generating almost instantly the scaled random Gaussian samples is shown in <ref type="table" target="#tab_7">Table 6</ref>. In the slowest case -CaffeNet -LSUV initialization takes 3.5 minutes, which is negligible in comparison the training time.  6 CONCLUSIONS LSUV, layer sequential uniform variance, a simple strategy for weight initialization for deep net learning, is proposed. We have showed that the LSUV initialization, described fully in six lines of pseudocode, is as good as complex learning schemes which need, for instance, auxiliary nets.</p><p>The LSUV initialization allows learning of very deep nets via standard SGD, is fast, and leads to (near) state-of-the-art results on MNIST, CIFAR, ImageNet datasets, outperforming the sophisticated systems designed specifically for very deep nets such as FitNets <ref type="bibr" target="#b20">( Romero et al. (2015)</ref>) and Highway <ref type="bibr" target="#b26">( Srivastava et al. (2015)</ref>). The proposed initialization works well with different activation functions.</p><p>Our experiments confirm the finding of <ref type="bibr" target="#b20">Romero et al. (2015)</ref> that very thin, thus fast and low in parameters, but deep networks obtain comparable or even better performance than wider, but shallower ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TECHNICAL DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 INFLUENCE OF MINI-BATCH SIZE TO LSUV INITIALIZATION</head><p>We have selected tanh activation as one, where LSUV initialization shows the worst performance and tested the influence of mini-batch size to training process. Note, that training mini-batch is the same for all initializations, the only difference is mini-batch used for variance estimation. One can see from <ref type="table" target="#tab_8">Table 7</ref> that there is no difference between small or large mini-batch, except extreme cases, where only two sample are used.  <ref type="table" target="#tab_9">Tables 8 and 9</ref> show the standard deviations of the filter weights, found by the LSUV procedure and by other initialization schemes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 GRADIENTS</head><p>To check how the activation variance normalization influences the variance of the gradient, we measure the average variance of the gradient at all layers after 10 mini-batches. The variance is close to 10 ?9 for all convolutional layers. It is much more stable than for the reference methods, except MSRA; see <ref type="table" target="#tab_1">Table 10</ref>.  3.21E-10 7.03E-09 2.95E-14 1.35E-13 conv15</p><p>3.85E-10 6.57E-09 6.71E-14 3.10E-13 conv21</p><p>1.25E-09 9.11E-09 1.95E-13 8.00E-13 conv22</p><p>1.15E-09 9.73E-09 3.79E-13 1.56E-12 conv23 1.19E-09 1.07E-08 8.18E-13 3.28E-12 conv24 9.12E-10 1.07E-08 1.79E-12 6.69E-12 conv25 7.45E-10 1.09E-08 4.04E-12 1.36E-11 conv26 8.21E-10 1.15E-08 8.36E-12 2.99E-11 conv31</p><p>3.06E-09 1.92E-08 2.65E-11 1.05E-10 conv32 2.57E-09 2.01E-08 5.95E-11 2.28E-10 conv33 2.40E-09 1.99E-08 1.21E-10 4.69E-10 conv34 2.19E-09 2.25E-08 2.64E-10 1.01E-09 conv35</p><p>1.94E-09 2.57E-08 5.89E-10 2.27E-09 conv36 2.31E-09 2.97E-08 1.32E-09 5.57E-09 ip1 1.24E-07 1.95E-07 6.91E-08 7.31E-08 var(ip1)/var(conv11) 255 20 12198922 3176821</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>CIFAR-10 accuracy of FitNet-4 with different activation functions. Note that graphs are cropped at 0.4 accuracy. Highway19 is the network from<ref type="bibr" target="#b26">Srivastava et al. (2015)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>CIFAR-10 accuracy of FitNet-4 LSUV and batch normalized (BN) networks as function of wall-clock time. BN-half stands for half the number of iterations in each step. CaffeNet training on ILSVRC-2012 dataset with LSUV and original Krizhevsky et al. (2012) initialization. Training loss (left) and validation accuracy (right). Top -first epoch, middlefirst 10 epochs, bottom -full training. GoogLeNet training on ILSVRC-2012 dataset with LSUV and reference Jia et al. (2014) BVLC initializations. Training loss (left) and validation accuracy (right). Top -first epoch, middle -first ten epochs, bottom -full training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>FitNets Romero et al. (2015)  network architecture used in experiments. Non-linearity: Maxout with 2 linear pieces in convolution layers, Maxout with 5 linear pieces in fully-connected.</figDesc><table><row><cell>FitNet-1</cell><cell>FitNet-4</cell><cell>FitResNet-4</cell><cell>FitNet-MNIST</cell></row><row><cell>250K param</cell><cell>2.5M param</cell><cell>2.5M param</cell><cell>30K param</cell></row><row><cell>conv 3x3x16</cell><cell>conv 3x3x32</cell><cell>conv 3x3x32</cell><cell>conv 3x3x16</cell></row><row><cell>conv 3x3x16</cell><cell>conv 3x3x32</cell><cell>conv 3x3x32 ?sum</cell><cell>conv 3x3x16</cell></row><row><cell>conv 3x3x16</cell><cell>conv 3x3x32</cell><cell>conv 3x3x48</cell><cell></cell></row><row><cell></cell><cell>conv 3x3x48</cell><cell>conv 3x3x48 ?ssum</cell><cell></cell></row><row><cell></cell><cell>conv 3x3x48</cell><cell>conv 3x3x48</cell><cell></cell></row><row><cell>pool 2x2</cell><cell>pool 2x2</cell><cell>pool 2x2</cell><cell>pool 4x4, stride2</cell></row><row><cell>conv 3x3x32</cell><cell>conv 3x3x80</cell><cell>conv 3x3x80</cell><cell>conv 3x3x16</cell></row><row><cell>conv 3x3x32</cell><cell>conv 3x3x80</cell><cell>conv 3x3x80 ?sum</cell><cell>conv 3x3x16</cell></row><row><cell>conv 3x3x32</cell><cell>conv 3x3x80</cell><cell>conv 3x3x80</cell><cell></cell></row><row><cell></cell><cell>conv 3x3x80</cell><cell>conv 3x3x80?sum</cell><cell></cell></row><row><cell></cell><cell>conv 3x3x80</cell><cell>conv 3x3x80</cell><cell></cell></row><row><cell>pool 2x2</cell><cell>pool 2x2</cell><cell>pool 2x2</cell><cell>pool 4x4, stride2</cell></row><row><cell>conv 3x3x48</cell><cell>conv 3x3x128</cell><cell>conv 3x3x128</cell><cell>conv 3x3x12</cell></row><row><cell>conv 3x3x48</cell><cell cols="3">conv 3x3x128 conv 3x3x128 ?sum conv 3x3x12</cell></row><row><cell>conv 3x3x64</cell><cell>conv 3x3x128</cell><cell>conv 3x3x128</cell><cell></cell></row><row><cell></cell><cell cols="2">conv 3x3x128 conv 3x3x128 ?sum</cell><cell></cell></row><row><cell></cell><cell>conv 3x3x128</cell><cell>conv 3x3x128</cell><cell></cell></row><row><cell cols="3">pool 8x8 (global) pool 8x8 (global) pool 8x8 (global)</cell><cell>pool 2x2</cell></row><row><cell>fc-500</cell><cell>fc-500</cell><cell>fc-500</cell><cell></cell></row><row><cell cols="2">softmax-10(100) softmax-10</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Accuracy on CIFAR-10/100, with data augmentation</cell><cell cols="4">Error on MNIST w/o data augmentation</cell></row><row><cell>Network</cell><cell cols="2">CIFAR-10, [%] CIFAR-100,[%]</cell><cell>Network</cell><cell cols="3">layers params Error, %</cell></row><row><cell>Fitnet4-LSUV</cell><cell>93.94</cell><cell>70.04 (72.34 ?)</cell><cell cols="3">FitNet-like networks</cell><cell></cell></row><row><cell>Fitnet4-OrthoInit</cell><cell>93.78</cell><cell>70.44 (72.30 ?)</cell><cell>HighWay-16</cell><cell>10</cell><cell>39K</cell><cell>0.57</cell></row><row><cell>Fitnet4-Hints</cell><cell>91.61</cell><cell>64.96</cell><cell>FitNet-Hints</cell><cell>6</cell><cell>30K</cell><cell>0.51</cell></row><row><cell>Fitnet4-Highway</cell><cell>92.46</cell><cell>68.09</cell><cell>FitNet-Ortho</cell><cell>6</cell><cell>30K</cell><cell>0.48</cell></row><row><cell>ALL-CNN</cell><cell>92.75</cell><cell>66.29</cell><cell>FitNet-LSUV</cell><cell>6</cell><cell>30K</cell><cell>0.48</cell></row><row><cell>DSN</cell><cell>92.03</cell><cell>65.43</cell><cell cols="2">FitNet-Ortho-SVM 6</cell><cell>30K</cell><cell>0.43</cell></row><row><cell>NiN</cell><cell>91.19</cell><cell>64.32</cell><cell cols="2">FitNet-LSUV-SVM 6</cell><cell>30K</cell><cell>0.38</cell></row><row><cell>maxout</cell><cell>90.62</cell><cell>65.46</cell><cell cols="3">State-of-art-networks</cell><cell></cell></row><row><cell>MIN</cell><cell>93.25</cell><cell>71.14</cell><cell>DSN-Softmax</cell><cell>3</cell><cell cols="2">350K 0.51</cell></row><row><cell cols="3">Extreme data augmentation</cell><cell>DSN-SVM</cell><cell>3</cell><cell cols="2">350K 0.39</cell></row><row><cell>Large ALL-CNN</cell><cell>95.59</cell><cell>n/a</cell><cell>HighWay-32</cell><cell>10</cell><cell cols="2">151K 0.45</cell></row><row><cell cols="2">Fractional MP (1 test) 95.50</cell><cell>68.55</cell><cell>maxout</cell><cell>3</cell><cell cols="2">420K 0.45</cell></row><row><cell cols="2">Fractional MP (12 tests) 96.53</cell><cell>73.61</cell><cell>MIN 2</cell><cell>9</cell><cell cols="2">447K 0.24</cell></row></table><note>Network performance comparison on the MNIST and CIFAR-10/100 datasets. Results marked ' ?' were obtained with the RMSProp optimizer Tieleman &amp; Hinton (2012).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The performance of activation functions and initialization in the Residual learning setup He</figDesc><table><row><cell cols="6">et al. (2015), FitResNet-4 from Table 1.The n/c symbol stands for "failed to converge";</cell></row><row><cell cols="5">Init method maxout ReLU VLReLU tanh</cell><cell>Sigmoid</cell></row><row><cell>LSUV</cell><cell>94.16</cell><cell cols="2">92.82 93.36</cell><cell cols="2">89.17 n/c</cell></row><row><cell cols="2">OrthoNorm n/c</cell><cell cols="2">91.42 n/c</cell><cell cols="2">89.31 n/c</cell></row><row><cell>Xavier</cell><cell>n/c</cell><cell cols="2">92.48 93.34</cell><cell cols="2">89.62 n/c</cell></row><row><cell>MSRA</cell><cell>n/c</cell><cell>n/c</cell><cell>n/c</cell><cell cols="2">88.59 n/c</cell></row><row><cell cols="4">5.2 COMPARISON TO BATCH NORMALIZATION (BN)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>CIFAR-10 accuracy of batch-normalized FitNet4. Comparison of batch normalization put before and after non-linearity.</figDesc><table><row><cell cols="3">Non-linearity Where to put BN</cell></row><row><cell></cell><cell>Before</cell><cell>After</cell></row><row><cell>TanH</cell><cell>88.10</cell><cell>89.22</cell></row><row><cell>ReLU</cell><cell>92.60</cell><cell>92.58</cell></row><row><cell>Maxout</cell><cell>92.30</cell><cell>92.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Time needed for network initialization on top of random Gaussian (seconds).</figDesc><table><row><cell>Network</cell><cell>Init</cell><cell></cell></row><row><cell></cell><cell cols="2">OrthoNorm LSUV</cell></row><row><cell>FitNet4</cell><cell>1</cell><cell>4</cell></row><row><cell>CaffeNet</cell><cell>188</cell><cell>210</cell></row><row><cell>GoogLeNet</cell><cell>24</cell><cell>60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>FitNet4 TanH final performance on CIFAR-10. Dependence on LSUV mini-batch size</figDesc><table><row><cell cols="2">Batch size for LSUV 2</cell><cell>16</cell><cell>32</cell><cell>128</cell><cell>1024</cell></row><row><cell>Final accuracy, [%]</cell><cell cols="5">89.27 89.30 89.30 89.28 89.31</cell></row><row><cell cols="6">A.2 LSUV WEIGHT STANDARD DEVIATIONS IN DIFFERENT NETWORKS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="5">: Standard deviations of the weights per layer for different initializations, FitNet4, CIFAR10,</cell></row><row><cell>ReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layer</cell><cell cols="4">LSUV OrthoNorm MSRA Xavier</cell></row><row><cell cols="2">conv11 0.383</cell><cell>0.175</cell><cell>0.265</cell><cell>0.191</cell></row><row><cell cols="2">conv12 0.091</cell><cell>0.058</cell><cell>0.082</cell><cell>0.059</cell></row><row><cell cols="2">conv13 0.083</cell><cell>0.058</cell><cell>0.083</cell><cell>0.059</cell></row><row><cell cols="2">conv14 0.076</cell><cell>0.058</cell><cell>0.083</cell><cell>0.059</cell></row><row><cell cols="2">conv15 0.068</cell><cell>0.048</cell><cell>0.060</cell><cell>0.048</cell></row><row><cell cols="2">conv21 0.036</cell><cell>0.048</cell><cell>0.052</cell><cell>0.037</cell></row><row><cell cols="2">conv22 0.048</cell><cell>0.037</cell><cell>0.052</cell><cell>0.037</cell></row><row><cell cols="2">conv23 0.061</cell><cell>0.037</cell><cell>0.052</cell><cell>0.037</cell></row><row><cell cols="2">conv24 0.052</cell><cell>0.037</cell><cell>0.052</cell><cell>0.037</cell></row><row><cell cols="2">conv25 0.067</cell><cell>0.037</cell><cell>0.052</cell><cell>0.037</cell></row><row><cell cols="2">conv26 0.055</cell><cell>0.037</cell><cell>0.052</cell><cell>0.037</cell></row><row><cell cols="2">conv31 0.034</cell><cell>0.037</cell><cell>0.052</cell><cell>0.037</cell></row><row><cell cols="2">conv32 0.044</cell><cell>0.029</cell><cell>0.041</cell><cell>0.029</cell></row><row><cell cols="2">conv33 0.042</cell><cell>0.029</cell><cell>0.041</cell><cell>0.029</cell></row><row><cell cols="2">conv34 0.041</cell><cell>0.029</cell><cell>0.041</cell><cell>0.029</cell></row><row><cell cols="2">conv35 0.040</cell><cell>0.029</cell><cell>0.041</cell><cell>0.029</cell></row><row><cell cols="2">conv36 0.043</cell><cell>0.029</cell><cell>0.041</cell><cell>0.029</cell></row><row><cell>ip1</cell><cell>0.048</cell><cell>0.044</cell><cell>0.124</cell><cell>0.088</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Standard deviations of the weights per layer for different non-linearities, found by LSUV,</figDesc><table><row><cell>FitNet4, CIFAR10</cell><cell></cell><cell></cell></row><row><cell>Layer</cell><cell cols="2">TanH ReLU VLReLU Maxout</cell></row><row><cell cols="2">conv11 0.386 0.388 0.384</cell><cell>0.383</cell></row><row><cell cols="2">conv12 0.118 0.083 0.084</cell><cell>0.058</cell></row><row><cell cols="2">conv13 0.102 0.096 0.075</cell><cell>0.063</cell></row><row><cell cols="2">conv14 0.101 0.082 0.080</cell><cell>0.065</cell></row><row><cell cols="2">conv15 0.081 0.064 0.065</cell><cell>0.044</cell></row><row><cell cols="2">conv21 0.065 0.044 0.037</cell><cell>0.034</cell></row><row><cell cols="2">conv22 0.064 0.055 0.047</cell><cell>0.040</cell></row><row><cell cols="2">conv23 0.060 0.055 0.049</cell><cell>0.032</cell></row><row><cell cols="2">conv24 0.058 0.064 0.049</cell><cell>0.041</cell></row><row><cell cols="2">conv25 0.061 0.061 0.043</cell><cell>0.040</cell></row><row><cell cols="2">conv26 0.063 0.049 0.052</cell><cell>0.037</cell></row><row><cell cols="2">conv31 0.054 0.032 0.037</cell><cell>0.027</cell></row><row><cell cols="2">conv32 0.052 0.049 0.037</cell><cell>0.031</cell></row><row><cell cols="2">conv33 0.051 0.048 0.042</cell><cell>0.033</cell></row><row><cell cols="2">conv34 0.050 0.047 0.038</cell><cell>0.028</cell></row><row><cell cols="2">conv35 0.051 0.047 0.039</cell><cell>0.030</cell></row><row><cell cols="2">conv36 0.051 0.040 0.037</cell><cell>0.033</cell></row><row><cell>ip1</cell><cell>0.084 0.044 0.044</cell><cell>0.038</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Variance of the initial gradients per layer, different initializations, FitNet4, ReLU</figDesc><table><row><cell>Layer</cell><cell>LSUV</cell><cell>MSRA</cell><cell cols="2">OrthoInit Xavier</cell></row><row><cell>conv11</cell><cell cols="3">4.87E-10 9.42E-09 5.67E-15</cell><cell>2.30E-14</cell></row><row><cell>conv12</cell><cell cols="3">5.07E-10 9.62E-09 1.17E-14</cell><cell>4.85E-14</cell></row><row><cell>conv13</cell><cell cols="3">4.36E-10 1.07E-08 2.30E-14</cell><cell>9.94E-14</cell></row><row><cell>conv14</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">When preparing this submission we have found recent unreviewed paperMIN Chang &amp; Chen (2015)   paper, which uses a sophisticated combination of batch normalization, maxout and network-in-network nonlinearities and establishes a new state-of-art on MNIST.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors were supported by The Czech Science Foundation Project GACR P103/12/G084 and CTU student grant SGS15/155/OHK3/2T/13.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Sch?lkopf, B., Platt, J.C., and Hoffman, T.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.02583" />
		<title level="m">Batch-normalized Maxout Network in Network. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Classifying plankton with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<ptr target="http://benanne.github.io/2015/03/17/plankton.html" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</title>
		<editor>Gordon, Geoffrey J. and Dunson, David B.</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>Journal of Machine Learning Research -Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maxout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Networks</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v28/goodfellow13.html" />
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fractional Max-Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6071" />
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Train you very own deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/cifar-10/forums/t/10493/train-you-very-own-deep-convolutional-network" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Spatially-sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1512/03385" />
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1502.01852" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning and Representation Learning Workshop: NIPS 2014</title>
		<meeting>Deep Learning and Representation Learning Workshop: NIPS 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<editor>Blei, David and Bach, Francis</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/?{}kriz/learning-features-2009-TR.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>Pereira, F., Burges, C.J.C., Bottou, L., and</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<ptr target="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" />
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saining</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v38/lee15a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6550" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhiheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Surya</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6120" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very Deep Multilingual Convolutional Neural Networks for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1509/08967" />
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Striving for Simplicity: The All Convolutional Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6806" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<meeting>ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training Very Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1507.06228" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Random Walk Initialization for Training Very Deep Feedforward Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbott</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6558" />
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.4842" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RMSProp -Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
