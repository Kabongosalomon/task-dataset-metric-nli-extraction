<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Video Domain Adaptation: A Disentanglement Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Wei</surname></persName>
							<email>pengfei.wei@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingdong</forename><surname>Kong</surname></persName>
							<email>lingdong@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghua</forename><surname>Qu</surname></persName>
							<email>xinghua.qu@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Xu</surname></persName>
							<email>zhiqiang.xu@mbzuai.ac.ae</email>
							<affiliation key="aff2">
								<orgName type="institution">MBZUAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jing.jiang@uts.edu.au</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejun</forename><surname>Ma</surname></persName>
							<email>mazejun@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Video Domain Adaptation: A Disentanglement Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to disentangle the domain-related information from the data during the adaptation process. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static domain-related information and another encoding the temporal and semantic-related information. A Transfer Sequential VAE (TranSVAE) framework is then developed to model such generation. To better serve for adaptation, we further propose several objectives to constrain the latent factors in TranSVAE. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE compared with several state-of-the-art methods. Code is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past decades, unsupervised domain adaptation (UDA) has attracted extensive research attention <ref type="bibr" target="#b34">(Wilson and Cook 2020)</ref>. Numerous UDA methods have been proposed and successfully applied to various real-world applications, e.g., object recognition <ref type="bibr" target="#b31">(Tzeng et al. 2017;</ref><ref type="bibr" target="#b35">Xiao and Zhang 2021;</ref><ref type="bibr" target="#b39">Zhang et al. 2022)</ref>, semantic segmentation <ref type="bibr" target="#b42">(Zou et al. 2018;</ref><ref type="bibr" target="#b17">Kong et al. 2021;</ref><ref type="bibr" target="#b28">Saporta et al. 2022)</ref>, and object detection <ref type="bibr" target="#b2">(Cai et al. 2019a;</ref><ref type="bibr" target="#b13">Guan et al. 2021;</ref><ref type="bibr" target="#b37">Yu et al. 2022)</ref>. However, most of these methods and their applications are limited to the image domain, while much less attention has been devoted to video-based UDA, where the latter is undoubtedly more challenging.</p><p>Compared with image-based UDA, the source and target domains also differ temporally in video-based UDA. Images are spatially well-structured data, while videos are sequences of image frames with both spatial and temporal relations. Existing image-based UDA methods can hardly achieve satisfactory performance on the video-based UDA tasks as they fail to consider the temporal dependency of the video frames in handling the domain gaps. For instance, in video-based cross-domain action recognition tasks, domain gaps are presented by not only the actions of different per-Copyright ? 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1 https://github.com/ldkong1205/TranSVAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compression View</head><p>Source Info Target Info Semantic-Related Info Source Domain Target Domain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disentanglement View</head><p>Figure 1: Conceptual comparisons between the traditional compression view and the proposed disentanglement view.</p><p>Prior arts compress implicit domain information to obtain domain-indistinguishable representations; while in this work, we pursue explicit decouplings of domain-specific information from other information via generative modeling.</p><p>sons in different scenarios but also the actions that appear at different timestamps or last at different time lengths.</p><p>Recently, few works have been proposed for video-based UDA. The key idea of these methods is to achieve temporal alignment by aligning both frame-and video-level features through adversarial learning <ref type="bibr" target="#b6">(Chen et al. 2019;</ref><ref type="bibr" target="#b22">Luo et al. 2020)</ref>, contrastive learning <ref type="bibr" target="#b30">(Turrisi et al. 2022;</ref><ref type="bibr" target="#b26">Sahoo et al. 2021)</ref>, attention <ref type="bibr" target="#b9">(Choi et al. 2020)</ref>, or combining some of these mechanisms, e.g., adversarial learning with attention <ref type="bibr" target="#b25">(Pan et al. 2020;</ref><ref type="bibr" target="#b7">Chen, Gao, and Ma 2022)</ref>. Though they have advanced video-based UDA, there is still room for improvement. Cross-domain videos are highly complex data, containing diverse information like domain-related information, semantic information, temporal information, etc. Existing works usually conduct feature alignments with diverse information mixed up, and thus may not guarantee that the source and target domains are sufficiently aligned.</p><p>Generally, prior methods follow a compression way to achieve adaptations <ref type="figure">(Fig. 1, )</ref>. With specifically-designed constraints, the domain-related information is highly compressed, making domains indistinguishable from the new representation, while the temporal and semantic-related information is adequately compressed so as to achieve good predictions using such new representation. However, as diverse information is mixed up, information confusion and loss are unavoidable during the compression, which may highly jeopardize the final adaptation performance. This motivates us to handle the video-based UDA from a disentanglement perspective. We aim at disentangling the domain in- formation from the other information during the adaptation process ( <ref type="figure">Fig. 1, )</ref>, such that the effect of domain discrepancy on the prediction task can be largely eliminated.</p><formula xml:id="formula_0">! ! $ % &amp; $ % &amp; ? ? ? &amp; $ % (a) Generative model ? ? ? &amp; $ ! ! $ % &amp; $ % &amp; % (b) Inference model</formula><p>To achieve this goal, we first consider the generation process of cross-domain videos. We assume that a video sequence is generated from two sets of latent factors: one set consists of a sequence of random variables, which are dynamic over time and encode the semantic information that is related to the prediction task; the other set is static and introduces the domain-related information to the generated video. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates the cross-domain video generation process. The blue/red nodes are the observed source/target videos x S /x T , respectively, over t timestamps. The static variables z S d and z T d follow a joint distribution and are domain-specific. Combining either of them with the dynamic variable z t at each timestamp, we can construct one frame data of a domain. Note that the sequences of the dynamic variables are shared across domains and are domaininvariant. They are also used for the final prediction task.</p><p>With the above generative model, we develop a Transfer Sequential Variational AutoEncoder (TranSVAE) for videobased UDA. We design this novel VAE structure to model the cross-domain video generation process, and then leverage appropriate components to ensure that the disentanglement indeed serves the adaptation purpose. Firstly, we enable a good decoupling of the two sets of latent factors by minimizing their mutual dependence. This encourages the information in these two factor sets to be mutually exclusive. We then consider constraining each latent factor set. For sequences z t that are expected to be domain-invariant, we propose to align them across domains at both frame and video levels through adversarial learning. Meanwhile, we also add the task-specific supervision on z t extracted from source data (w/ ground-truth). Regarding z D d with D ? {S, T } that are expected to be static and domain-specific, we propose a contrastive triplet loss on them. Note that z D d should be naturally domain-specific as it is mutually independent with z t , which are constrained to be domain-invariant.</p><p>To the best of our knowledge, this is the first work that tackles the challenging video-based UDA from a domain disentanglement perspective. We conduct extensive experiments on popular benchmarks (UCF-HMDB, Jester, Epic-Kitchens, Sprites) and the results show that our TranSVAE framework consistently outperforms previous state-of-theart (SoTA) methods by large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised Video Domain Adaptation. Despite the great progress in image-based UDA, only a few methods have recently attempted video-based UDA. In <ref type="bibr" target="#b6">(Chen et al. 2019</ref>), a temporal attentive adversarial adaptation network (TA 3 N) is proposed to integrate a temporal relation module for temporal alignment. <ref type="bibr" target="#b9">Choi et al. (2020)</ref> proposed a SAVA method using self-supervised clip order prediction and clip attention-based alignment. Based on a cross-domain co-attention mechanism, the temporal co-attention network TCoN <ref type="bibr" target="#b25">(Pan et al. 2020</ref>) focuses on common key frames across domains for better alignment. <ref type="bibr" target="#b22">Luo et al. (2020)</ref> pay more attention to the domain-agnostic classifier by using a network topology of the bipartite graph to model the crossdomain correlations. Instead of using adversarial learning, <ref type="bibr" target="#b26">Sahoo et al. (2021)</ref> develop an end-to-end temporal contrastive learning framework named CoMix with background mixing and target pseudo-labels. Recently, <ref type="bibr" target="#b7">Chen, Gao, and Ma (2022)</ref> learn multiple domain discriminators for multilevel temporal attentive features to achieve better alignment, while Turrisi et al. (2022) exploit two-headed deep architecture to learn a more robust target classifier by the combination of cross-entropy and contrastive losses. Although these methods have advanced video-based UDA, they align features with diverse information mixed up from a compression perspective, which leaves room for further improvement. Multi-Modal Video Adaptation. Most recently, there are also a few works integrating multiple modality data for video-based UDA. Although we only use the single modality RGB features, we still discuss this multi-modal research line for a complete literature review. The very first work exploring the multi-modal nature of videos for UDA is MM-SADA <ref type="bibr" target="#b24">(Munro and Damen 2020)</ref>, where the correspondence of multiple modalities is exploited as a selfsupervised alignment in addition to adversarial alignment. A later work, spatial-temporal contrastive domain adaptation (STCDA) <ref type="bibr" target="#b29">(Song et al. 2021)</ref>, utilizes a video-based contrastive alignment as the multi-modal domain metric to measure the video-level discrepancy across domains. <ref type="bibr" target="#b16">Kim et al. (2021)</ref> propose cross-modal and cross-domain contrastive losses to handle feature spaces across modalities and domains. <ref type="bibr" target="#b36">Yang et al. (2022)</ref> leverage both cross-modal complementarity and cross-modal consensus to learn the most transferable features through a CIA framework. It is worth noting that the proposed TranSVAE -although only uses single modality RGB features -surprisingly achieves better UDA performance compared to the current state-of-the-art multi-modal method CIA, which highlights our superiority. Disentanglement. Feature disentanglement is a wide and hot research topic. We only focus on works that are closely related to ours. In the image domain, some works consider adaptation from a generative view. <ref type="bibr" target="#b3">Cai et al. (2019b)</ref> learn a disentangled semantic representation across domains. A similar idea is then applied to graph domain adaptation <ref type="bibr" target="#b4">(Cai et al. 2021</ref>) and domain generalization <ref type="bibr" target="#b14">(Ilse et al. 2020)</ref>. <ref type="bibr" target="#b11">(Deng et al. 2021</ref>) propose a novel informative feature disentanglement, equipped with the adversarial network or the metric discrepancy model. Another disentanglement-related topic is sequential data generation. To generate videos, exist-ing works <ref type="bibr" target="#b19">(Li and Mandt 2018;</ref><ref type="bibr" target="#b41">Zhu et al. 2020;</ref><ref type="bibr" target="#b0">Bai, Wang, and Gomes 2021)</ref> extend VAE to a recurrent form with different recursive structures. In this paper, we present a VAEbased structure to generate cross-domain videos. We aim at tackling the challenging video-based UDA from a new perspective: sequential domain disentanglement and transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Technical Approach</head><p>Formally, for a typical video-based UDA problem, we have two domains, namely source domain S and target domain T . Domain S contains sufficient labeled video sequences, de-</p><formula xml:id="formula_1">noted as {(V S i , y S i )} N S i=1 , where V S i is a video sequence and y S i is the class label. Domain T consists of unlabeled video sequences, denoted as {V T i } N T i=1 . For a sequence V D i from domain D ? {S, T }, it contains T frames {x D i 1 , ..., x D i T } in total 2 . We further denote N = N S + N T . The domains S and T are of different distributions but are sharing the same label space. The objective is to utilize both {(V S i , y S i )} N S i=1 and {V T i } N T i=1</formula><p>to train a good classifier for domain T . Framework Overview. We adopt a VAE-based structure to model the cross-domain video generation process as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, and propose four constraints to regulate the two sets of latent factors. The reconstruction process is based on these two latent factors, i.e., z D 1 , ..., z D T and z D d that are sampled from the posteriors q(z D t |x D &lt;t ) and q(z D d |x D 1:T ), respectively. The overall architecture consists of five segments including the encoder, LSTM, latent spaces, sampling, and decoder, as shown in <ref type="figure">Fig. 3</ref>. To better disentangle the domain information and facilitate the adaptation task, we propose four objective losses, namely mutual information loss, domain adversarial loss, task supervision loss, and contrastive triplet loss. The mutual information loss works on q(z D t |x D &lt;t ) and q(z D d |x D 1:T ) to maximize the independence of the two latent factor sets. The domain adversarial loss constrains the dynamic latent factors to be domain-invariant, i.e., one cannot distinguish domains using z D 1 , ..., z D T . With the two losses, the static latent factors z D d naturally encode the domain-related information. To further enhance the domain specificity, we put a contrastive triplet loss on z D d . By doing so, we also guarantee that z D d are time-invariant. At last, as domain S has labels, we build a prediction network using z S 1 , ..., z S T . In the adaptation phase, we extract the dynamic representations of the target test video sequences, and then pass them to the prediction network. Next, we elaborate the technical details of each component in TranSVAE. Video Sequence Reconstruction. The overall architecture of TranSVAE follows a VAE-based structure <ref type="bibr" target="#b19">(Li and Mandt 2018;</ref><ref type="bibr" target="#b41">Zhu et al. 2020;</ref><ref type="bibr" target="#b0">Bai, Wang, and Gomes 2021)</ref> with two sets of latent factors z D 1 , ..., z D T and z D d . The generative and inference graphical models are presented in <ref type="figure" target="#fig_0">Fig. 2</ref>(a) and 2(b), respectively. Similar to the conventional VAE, we use a standard Gaussian distribution for static latent factors. For dynamic ones, we use a sequential prior z D t |z D &lt;t ? N (? t , diag(? 2 t )), that is, the prior distribution of the current 2 Without confusion, we omit the subscript i to represent a video sequence V D and the corresponding notations, e.g.,</p><formula xml:id="formula_2">{x D 1 , ..., x D T }.</formula><p>dynamic factor is conditioned on the historical dynamic factors. The distribution parameters can be re-parameterized as a recurrent network, e.g., LSTM, with all previous dynamic latent factors as the input. Denoting Z D = {z D 1 , ..., z D T } for simplification, we then get the prior as follows:</p><formula xml:id="formula_3">p(z D d , Z D ) = p(z D d ) T t=1 p(z D t |z D &lt;t ).<label>(1)</label></formula><p>Following <ref type="figure" target="#fig_0">Fig. 2</ref></p><formula xml:id="formula_4">(a), x D t is generated from z D d and z D t , and we thus model p(x D t |z D d , z D t ) = N (? t , diag(? t 2 )</formula><p>). The distribution parameters are re-parameterized by the decoder which can be flexible networks like the deconvolutional neural network.</p><formula xml:id="formula_5">Using V D = {x D 1 , ..., x D T },</formula><p>the generation can be formulated as:</p><formula xml:id="formula_6">p(V D ) = p(z D d ) T t=1 p(x D t |z D d , z D t )p(z D t |z D &lt;t ).<label>(2)</label></formula><p>Following <ref type="figure" target="#fig_0">Fig. 2(b)</ref>, we model the posterior distributions of the latent factors as another two Gaussian distributions,</p><formula xml:id="formula_7">i.e., q(z D d |V D ) = N (? d , diag(? 2 d )) and q(z D t |x D &lt;t ) = N (? t , diag(? t 2 )</formula><p>). The parameters of these two distributions are re-parameterized by the encoder, which can be a convolutional or LSTM module. However, the network of the static latent factors uses the whole sequence as the input while that of the dynamic latent factors only uses previous frames. Then the inference can be factorized as:</p><formula xml:id="formula_8">q(z D d , Z D |V D ) = q(z D d |V D ) T t=1 q(z D t |x D &lt;t ).<label>(3)</label></formula><p>Combining the above generation and inference process, we obtain the sequential VAE related objective function as: <ref type="figure">Figure 3</ref>: TranSVAE overview. The input videos are fed into an encoder to extract the visual features, followed by an LSTM to explore the temporal information. Two groups of mean and variance networks are then applied to model the posterior of the latent factors, i.e., q(z D t |x D &lt;t ) and q(z D d |x D 1:T ). The new representations z D 1 , ..., z D T and z D d are sampled, and then concatenated and passed to a decoder for reconstruction. Four constraints are proposed to regulate the latent factors for adaptation purposes.</p><formula xml:id="formula_9">L svae = E q(z D d ,Z D |V D ) [? ! , # , ? , $ ! , # , ? , $ Input Encoder Bi-LSTM Latent Space Sample &amp; Predict ! , ? , $ ! , ? , $ % ! , % # , ? , % $ % ! , % # , ? , % $ Decoder Reconstruct ? ? ? mean var Domain-Static Factor ( &amp; | !:$ ) mean var Domain-Invariant Sequential Factor ( ) | *) ) &amp; &amp; ? Classify / : Channel Concat ? Domain Adversarial Eq. (10) ? Task Supervision Eq. (13) ? Contrastive Triplet Eq. (11) ? Mutual Independent Eq. (5)</formula><p>bution of each variable. Thus, we obtain:</p><formula xml:id="formula_10">L mi (z D d , Z D ) = T t=1 KL q(z D d , z D t )||q(z D d )q(z D t ) = T t=1 [H(z D d ) + H(z D t ) ? H(z D d , z D t )].<label>(5)</label></formula><p>To calculate Eq. (5), we need to estimate the densities of</p><formula xml:id="formula_11">z D d , z D t and (z D d , z D t ).</formula><p>Based on <ref type="bibr" target="#b8">(Chen et al. 2018)</ref>, we use the mini-batch weighted sampling as follows:</p><formula xml:id="formula_12">H(z) = ?E q(z) [log q(z)] ? ? log 1 M M i=1 [log 1 M N M j=1 q (z(x i )|x j )],<label>(6)</label></formula><p>where</p><formula xml:id="formula_13">z is z D d , z D t or (z D d , z D t )</formula><p>, N denotes the data size and M is the mini-batch size. As discussed in <ref type="bibr" target="#b8">(Chen et al. 2018)</ref>, such an estimator is biased, but computing it does not require any additional hyperparameters. Domain Temporal Alignment <ref type="figure">(Fig. 3, )</ref>. We now consider enforcing the dynamic latent factors to be domaininvariant. Similar to the image-based UDA, we propose to reduce the domain gaps between the cross-domain dynamic latent factors. There are several ways to achieve this, and in this paper, we take advantage of the most popular adversarial-based idea <ref type="bibr" target="#b12">(Ganin et al. 2016)</ref>. Specifically, we build a domain classifier to discriminate whether the data is from the source domain or the target domain. When backpropagating the gradients, a gradient reversal layer (GRL) is adopted to invert the gradients. Like existing video-based UDA methods, we also conduct both frame-level and videolevel alignments. As TA 3 N (Chen et al. 2019) does, we exploit the temporal relation network (TRN) <ref type="bibr" target="#b40">(Zhou et al. 2018)</ref> to discover the temporal relations among different frames, and then aggregate all the temporal relation features into the final video-level features. This enables another level of alignment on the temporal relation features. Thus, we have:</p><formula xml:id="formula_14">L f = 1 N N i=1 1 T T t=1 CE G f (z D i t ), d i ,<label>(7)</label></formula><formula xml:id="formula_15">L r = 1 N N i=1 1 T ? 1 T n=2 CE G r T rN n (Z D i ) , d i , (8) L v = 1 N N i=1 CE G v 1 T ? 1 T n=2 T rN n (Z D i ) , d i ,<label>(9)</label></formula><p>where d i is the domain label, CE denotes the cross-entropy loss function,</p><formula xml:id="formula_16">Z D i = {z D i 1 , ..., z D i T },</formula><p>T rN i is the n-frame temporal relation network, G f , G r , and G v are the frame feature level, the temporal relation feature level, and the video feature level domain classifiers, respectively. Note that the other ways of integrating frame-level features to videolevel features, e.g., using a Graph Convolutional Network (GCN) (Wang and Gupta 2018) as <ref type="bibr" target="#b26">(Sahoo et al. 2021</ref>) do, can be straightforwardly applied here. To this end, we obtain the domain adversarial loss by summing up Eqs. (7-9):</p><formula xml:id="formula_17">L adv = L f + L r + L v .<label>(10)</label></formula><p>We assign equal importance for these three levels of losses to reduce the overhead of the hyperparameter search.</p><p>Domain Specificity &amp; Static Consistency <ref type="figure">(Fig. 3, )</ref>. With L mi and L adv , the dynamic latent factors are enforced to be domain-invariant, and independent of the static latent factors. Naturally, the static latent factors should encode the domain-specific information. However, we further notice that the characteristics of the domain specificity should be statically consistent over dynamic frames. Thus, we hope that z D d does not change a lot when z D t varies over time. To achieve this, given a sequence, we randomly shuffle the temporal order of frames to form a new sequence. The static latent factors disentangled from the original sequence and the shuffled sequence should be ideally equal or be very close at least. This motivates us to minimize the distance between these two static factors. Meanwhile, to further enhance the domain specificity, we enforce the dynamic latent factors from different domains to have a large distance. To this end, we propose the following contrastive triplet loss:</p><formula xml:id="formula_18">L ctc = max D(z D + d , z D + d ) ? D(z D + d , z D ? d ) + m, 0 ,<label>(11)</label></formula><p>where D(?, ?) denotes the Euclidean distance, m is the margin set to 1 in the experiments, z D + d , z D + d , and z D ? d are static latent factors of the anchor sequence from domain D + , the shuffled sequence, and a randomly selected sequence from domain D ? , respectively. Note that D + and D ? represent two different domains. Task-Specific Supervision <ref type="figure">(Fig. 3, )</ref>. We further encourage the dynamic latent factors to carry the semantic information. Considering that the source domain has sufficient labels, we accordingly design the task supervision as the regularization imposed on z S t . This gives us:</p><formula xml:id="formula_19">L cls = 1 N S N S i=1 L F(Z S i ), y S i ,<label>(12)</label></formula><p>where F(?) is the feature transformer mapping the framelevel features to video-level features, specifically TRN used in this paper, and L(?, ?) can be either cross-entropy loss or mean squared error loss according to the task. Although the dynamic latent factors are constrained to be domain-invariant, we do not completely rely on source semantics to learn features discriminative for the target domain. We propose to incorporate target pseudo-labels in task specific supervision. During the training, we use the prediction network obtained in the previous epoch to generate the target pseudo-labels of the unlabelled target training data for the current epoch. However, to increase the reliability of target pseudo-labels, we let the prediction network be trained only on the source supervision for several epochs, and then integrate the target pseudo-labels in the following training epochs. Meanwhile, a confidence threshold is set to determine whether to use the target pseudo-labels or not. Thus, we have the final task-specific supervision as follows:</p><formula xml:id="formula_20">L cls = 1 N ? ? N S i=1 L(F(Z S i ), y S i ) + N T i=1 L(F(Z T i ), y T i ) ? ? ,<label>(13)</label></formula><formula xml:id="formula_21">where y T i is the pseudo-label of Z T i .</formula><p>Summary. To this end, we reach the final objective function of our TranSVAE framework as follows:</p><formula xml:id="formula_22">L = L svae + ? 1 L mi + ? 2 L adv + ? 3 L ctc + ? 4 L cls ,<label>(14)</label></formula><p>where ? i , i = 1, 2, 3, 4 denotes the loss balancing weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct extensive experimental studies on popular video-based UDA benchmarks to verify the effectiveness of the proposed TranSVAE framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>UCF-HMDB is constructed by collecting the relevant and overlapping action classes from UCF 101 <ref type="bibr" target="#b30">(Soomro, Zamir, and Shah 2012)</ref> and HMDB 51 <ref type="bibr" target="#b18">(Kuehne et al. 2011</ref>  <ref type="bibr" target="#b24">Munro and Damen (2020)</ref> construct three domains across the eight largest actions. They are D 1 , D 2 , and D 3 corresponding to P08, P01, and P22 kitchens of the full dataset, resulting in six cross-domain tasks in-between them. Sprites (Li and Mandt 2018) contains sequences of animated cartoon characters with 15 action categories. The appearances of characters are fully controlled by four attributes, i.e., body, top, bottom, and hair. We construct two domains, P 1 and P 2 . P 1 uses the human body with attributes randomly selected from 3 tops, 4 bottoms, and 5 hairs, while P 2 uses the alien body with attributes randomly selected from 4 tops, 3 bottoms, and 5 hairs. The attribute pools are non-overlapping across domains, resulting in completely heterogeneous P 1 and P 2 . Each domain has 900 video sequences and each sequence contains 8 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Architecture. Following the latest works <ref type="bibr" target="#b26">(Sahoo et al. 2021;</ref><ref type="bibr" target="#b30">Turrisi et al. 2022)</ref>, we use I3D (Carreira and Zisserman 2017) as the backbone. However, different from CoMix which jointly trains the backbone, we simply use the pretrained I3D model on Kinetics <ref type="bibr" target="#b15">(Kay et al. 2017)</ref>, provided by <ref type="bibr" target="#b5">(Carreira and Zisserman 2017)</ref>, to extract RGB features. For the first three benchmarks, RGB features are used as the input of TranSVAE. For Sprites, we use the original image as the input, for the purpose of visualizing the reconstruction and disentanglement results. We use the shared encoder and decoder structures across the source and target domains. For RGB feature inputs, the encoder and decoder are fullyconnected layers. For original image inputs, the encoder and decoder are the convolution and deconvolution layers (from DCGAN <ref type="bibr" target="#b38">(Yu et al. 2017)</ref>), respectively. For the TRN model, we directly use the one provided by <ref type="bibr" target="#b6">(Chen et al. 2019)</ref>. Other details on this aspect are placed in Appendix.</p><p>Configurations. Our TranSVAE is implemented with Py-Torch. We use Adam with a weight decay of 1e ?4 as the optimizer. The learning rate is initially set to be 1e ?3 and follows a commonly used decreasing strategy in <ref type="bibr" target="#b12">(Ganin et al. 2016)</ref>. The batch size and the learning epoch is uniformly set to be 128 and 1,000, respectively, for all the experiments. We uniformly set 100 epochs of training under only source supervisions and involve the target pseudo-labels afterward. Following the common protocol in video-based UDA (Turrisi et al. 2022), we perform hyperparameter selection on the validation set. NVIDIA A100 GPUs are used for all experiments. Kindly refer to our Appendix for all other details.</p><p>Competitors. We compare methods from three lines. We first consider the source-only (S only ) and supervised-target (T sup ) which uses only labeled source data and only labeled target data, respectively. These two baselines serve as the lower and upper bounds for our tasks. Secondly, we consider five popular image-based UDA methods by simply ignoring temporal information, namely DANN <ref type="formula">(</ref> All these methods use the single modality RGB features. We directly quote numbers reported in published papers whenever possible. There exist recent works conducting videobased UDA using multi-modal data, e.g., RGB + Flow. Although TranSVAE solely uses RGB features, we still take this set of methods into account. Specifically, we consider MM-SADA (Munro and Damen 2020), STCDA <ref type="bibr" target="#b29">(Song et al. 2021)</ref>, CMCD <ref type="bibr" target="#b16">(Kim et al. 2021)</ref>, and CIA <ref type="bibr" target="#b36">(Yang et al. 2022</ref>).    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Property Analysis</head><p>Disentanglement Analysis. We analyze the disentanglement effect of TranSVAE on Sprites and show results in <ref type="figure" target="#fig_2">Fig. 4</ref>. The left subfigure shows the original sequences of the two domains. The "Human" and "Alien" are with completely different appearances and the former is casting spells while the latter is slashing. The middle subfigure shows the sequences reconstructed only using {z D 1 , ..., z D T }. It can be clearly seen that the two sequences keep the same action as the corresponding original ones. However, if we only focus on the appearance characteristics, it is difficult to distinguish the domain to which the sequences belong. This indicates that {z D 1 , ..., z D T } are indeed domain-invariant and well encode the semantic information. The right subfigure shows the sequences reconstructed by exchanging z D d , which results in two sequences with the same actions but exchanged appearance. This verifies that z D d is representing the appearance information, which is actually the domain-related information in this example. This property study 3 sufficiently supports that TranSVAE can successfully disentangle the domain information from other information, with the former embedded in z D d and the latter embedded in {z D 1 , ..., z D T }. Ablation Studies. We now analyze the effectiveness of each loss term in Eq. (14). We compare with four variants of TranSVAE, each removing one loss term by equivalently setting the weight ? to 0. The ablation results on UCF-HMDB, Jester, and Epic-Kitchens are shown in Tab. 4. As can be seen, removing L cls significantly reduces the transfer performance in all the tasks. This is reasonable as L cls is used 3 Try a live demo for domain disentanglement and transfer in TranSVAE at: https://huggingface.co/spaces/ldkong/TranSVAE. to discover the discriminative features. Removing any of L adv , L mi , and L ctc leads to an inferior result than the full TranSVAE setup. This demonstrates that every proposed loss matters in our framework. We further conduct another ablation study by sequentially integrating L cls , L adv , L mi , and L ctc into our sequential VAE structure. We follow the above integration order based on the average positive improvement that a loss brings to TranSVAE as shown in Tab. 4. We further use t-SNE (Van der Maaten and Hinton 2008) to visualize the features learned by these different variants. We plot two sets of t-SNE figures, one using the class-wise label and another using the domain label. <ref type="figure" target="#fig_3">Fig. 5 and Fig. 6</ref> show the visualization and the quantitative results. As can be seen from the t-SNE feature visualizations, adding a new component improves both the domain and semantic alignments, and the best alignment is achieved when all the components are considered. The quantitative results further show that the transfer performance gradually increases with the sequential integration of the four components, which again verifies the effectiveness of each component in TranSVAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a TranSVAE framework for the video-based UDA tasks. Our key idea is to explicitly disentangle the domain information from other information during the adaptation. We developed a novel sequential VAE structure with two sets of latent factors and proposed four constraints to regulate these factors for adaptation purposes. Extensive empirical studies clearly verify that TranSVAE consistently offers performance improvements compared with existing state-of-the-art methods. Comprehensive property analysis further demonstrates that TranSVAE is an effective and promising method for video-based UDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Public Resources Used</head><p>We acknowledge the use of the following public resources, during the course of this work:</p><p>? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">.</head><p>(+ . )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>93.</head><p>(+ . )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>93.</head><p>(+ . )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>94.</head><p>(+ . )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>98.</head><p>(+ . ) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Graphical illustrations of the proposed generative and inference models for video domain disentanglement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Ganin et al. 2016), JAN (Long et al. 2017), ADDA (Tzeng et al. 2017), AdaBN (Li et al. 2018), and MCD (Saito et al. 2018). Lastly and most importantly, we compare several recent SoTA videobased UDA methods, including TA 3 N (Chen et al. 2019), SAVA (Choi et al. 2020), TCoN (Pan et al. 2020), ABG (Luo et al. 2020), CoMix (Sahoo et al. 2021), CO 2 A (Turrisi et al. 2022), and MA 2 L-TD (Chen, Gao, and Ma 2022).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Domain disentanglement and transfer examples. Left: Video sequence inputs for D = P 1 ("Human", ) and D = P 2 ("Alien", ). Middle: Reconstructed sequences ( ) with z D 1 , ..., z D T . Right: Domain transferred sequences with exchanged z D d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>#Figure 5 :</head><label>5</label><figDesc>Sprites 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . CC-BY-SA-3.0 ? !"#$ ? %&amp;! ? #'" ? () ? %*% PL Accuracy (%) Loss integration studies on U ? H. Left: The t-SNE plots for class-wise (top row) and domain (bottom row, red source &amp; blue target) features. Right: Ablation results (%) by adding each loss sequentially, i.e., row (a) -row (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>#Figure 6 :</head><label>6</label><figDesc>? !"#$ ? %&amp;! ? #'" ? () ? %*% PL Accuracy Loss integration studies on H ? U. Left: The t-SNE plots for class-wise (top row) and domain (bottom row, red source &amp; blue target) features. Right: Ablation results (%) by adding each loss sequentially, i.e., row (a) -row (e). ? UCF 101 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown ? HMDB 51 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . CC BY 4.0 ? Jester 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown ? Epic-Kitchens 8 . . . . . . . . . . . . . . . . . . . . . . . CC BY-NC 4.0 ? I3D 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 ? TRN 10 . . . . . . . . . . . . . . . . . . . . . . . . BSD 2-Clause License ? Netron 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>UDA performance comparisons on UCF-HMDB.</figDesc><table><row><cell>Method &amp; Year</cell><cell>Backbone</cell><cell cols="3">U ? H H ? U Average ?</cell></row><row><cell cols="2">DANN (JMLR'16) ResNet-101</cell><cell>75.28</cell><cell>76.36</cell><cell>75.82</cell></row><row><cell cols="2">JAN (ICML'17) ResNet-101</cell><cell>74.72</cell><cell>76.69</cell><cell>75.71</cell></row><row><cell cols="2">AdaBN (PR'18) ResNet-101</cell><cell>72.22</cell><cell>77.41</cell><cell>74.82</cell></row><row><cell cols="2">MCD (CVPR'18) ResNet-101</cell><cell>73.89</cell><cell>79.34</cell><cell>76.62</cell></row><row><cell cols="2">TA 3 N (ICCV'19) ResNet-101</cell><cell>78.33</cell><cell>81.79</cell><cell>80.06</cell></row><row><cell cols="2">ABG (MM'20) ResNet-101</cell><cell>79.17</cell><cell>85.11</cell><cell>82.14</cell></row><row><cell cols="2">TCoN (AAAI'20) ResNet-101</cell><cell>87.22</cell><cell>89.14</cell><cell>88.18</cell></row><row><cell cols="2">MA 2 L-TD (WACV'22) ResNet-101</cell><cell>85.00</cell><cell>86.59</cell><cell>85.80</cell></row><row><cell>Source-only (S only )</cell><cell>I3D</cell><cell>80.27</cell><cell>88.79</cell><cell>84.53</cell></row><row><cell>DANN (JMLR'16)</cell><cell>I3D</cell><cell>80.83</cell><cell>88.09</cell><cell>84.46</cell></row><row><cell>ADDA (CVPR'17)</cell><cell>I3D</cell><cell>79.17</cell><cell>88.44</cell><cell>83.81</cell></row><row><cell>TA 3 N (ICCV'19)</cell><cell>I3D</cell><cell>81.38</cell><cell>90.54</cell><cell>85.96</cell></row><row><cell>SAVA (ECCV'20)</cell><cell>I3D</cell><cell>82.22</cell><cell>91.24</cell><cell>86.73</cell></row><row><cell>CoMix (NeurIPS'21)</cell><cell>I3D</cell><cell>86.66</cell><cell>93.87</cell><cell>90.22</cell></row><row><cell>CO 2 A (WACV'22)</cell><cell>I3D</cell><cell>87.78</cell><cell>95.79</cell><cell>91.79</cell></row><row><cell>TranSVAE (Ours)</cell><cell>I3D</cell><cell>87.78</cell><cell>98.95</cell><cell>93.37</cell></row><row><cell>Supervised-target (T sup )</cell><cell>I3D</cell><cell>95.00</cell><cell>96.85</cell><cell>95.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison results on Jester and Epic-Kitchens. S only DANN ADDA TA 3 N CoMix TranSVAE T sup</figDesc><table><row><cell>Task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>J S ? J T</cell><cell>51.5</cell><cell>55.4</cell><cell>52.3</cell><cell>55.5</cell><cell>64.7</cell><cell>66.1 (+14.6) 95.6</cell></row><row><cell cols="2">D 1 ? D 2 32.8</cell><cell>37.7</cell><cell>35.4</cell><cell>34.2</cell><cell>42.9</cell><cell>50.5 (+17.7) 64.0</cell></row><row><cell cols="2">D 1 ? D 3 34.1</cell><cell>36.6</cell><cell>34.9</cell><cell>37.4</cell><cell>40.9</cell><cell>50.3 (+16.2) 63.7</cell></row><row><cell cols="2">D 2 ? D 1 35.4</cell><cell>38.3</cell><cell>36.3</cell><cell>40.9</cell><cell>38.6</cell><cell>50.3 (+14.9) 57.0</cell></row><row><cell cols="2">D 2 ? D 3 39.1</cell><cell>41.9</cell><cell>40.8</cell><cell>42.8</cell><cell>45.2</cell><cell>58.6 (+19.5) 63.7</cell></row><row><cell cols="2">D 3 ? D 1 34.6</cell><cell>38.8</cell><cell>36.1</cell><cell>39.9</cell><cell>42.3</cell><cell>48.0 (+13.4) 57.0</cell></row><row><cell cols="2">D 3 ? D 2 35.8</cell><cell>42.1</cell><cell>41.4</cell><cell>44.2</cell><cell>49.2</cell><cell>58.0 (+22.2) 64.0</cell></row><row><cell cols="2">Average ? 35.3</cell><cell>39.2</cell><cell>37.4</cell><cell>39.9</cell><cell>43.2</cell><cell>52.6 (+17.3) 61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with methods using multi-modal data. This is because that the H ? U task already has a good performance without adaptation, i.e., 88.79% for the sourceonly (S only ) baseline, and thus the target pseudo-labels used in TranSVAE are almost correct. By further aligning domains and equivalently augmenting training data, TranSVAE outperforms T sup which is only trained with target data. Results on Jester &amp; Epic-Kitchens. Tab. 2 shows the comparison results on the Jester and Epic-Kitchens benchmarks. We can see that our TranSVAE is the clear winner among all the methods on all the tasks. Specifically, TranSVAE achieves a 1.4% improvement and a 9.4% average improvement over the runner-up baseline CoMix<ref type="bibr" target="#b26">(Sahoo et al. 2021)</ref> on Jester and Epic-Kitchens, respectively. This verifies the superiority of TranSVAE over others in handling videobased UDA. However, we also notice that the accuracy gap between CoMix and T sup is still significant on Jester. This is because the large-scale Jester dataset contains highly heterogeneous data across domains, e.g., the source domain contains videos of the rolling hand forward, while the target domain only consists of videos of the rolling hand backward. This leaves much room for improvement in the future.</figDesc><table><row><cell>Task</cell><cell cols="5">S only MM-SADA STCDA CMCD CIA TranSVAE</cell></row><row><cell>U ? H</cell><cell>86.1</cell><cell>84.2</cell><cell>83.1</cell><cell>84.7</cell><cell>88.3 87.8 (+1.7)</cell></row><row><cell>H ? U</cell><cell>92.5</cell><cell>91.1</cell><cell>92.1</cell><cell>92.8</cell><cell>94.1 99.0 (+6.5)</cell></row><row><cell cols="2">Average ? 89.3</cell><cell>87.7</cell><cell>87.6</cell><cell>88.8</cell><cell>91.2 93.4 (+4.1)</cell></row><row><cell cols="2">D 1 ? D 2 43.2</cell><cell>49.5</cell><cell>52.0</cell><cell>50.3</cell><cell>52.5 50.5 (+7.3)</cell></row><row><cell cols="2">D 1 ? D 3 42.5</cell><cell>44.1</cell><cell>45.5</cell><cell>46.3</cell><cell>47.8 50.3 (+7.8)</cell></row><row><cell cols="2">D 2 ? D 1 43.0</cell><cell>48.2</cell><cell>49.0</cell><cell>49.5</cell><cell>49.8 50.3 (+7.3)</cell></row><row><cell cols="2">D 2 ? D 3 48.0</cell><cell>52.7</cell><cell>52.5</cell><cell>52.0</cell><cell>53.2 58.6 (+10.6)</cell></row><row><cell cols="2">D 3 ? D 1 43.0</cell><cell>50.9</cell><cell>52.6</cell><cell>51.5</cell><cell>52.2 48.0 (+5.0)</cell></row><row><cell cols="2">D 3 ? D 2 55.5</cell><cell>56.1</cell><cell>55.6</cell><cell>56.3</cell><cell>57.6 58.0 (+2.5)</cell></row><row><cell cols="2">Average ? 45.9</cell><cell>50.3</cell><cell>51.2</cell><cell>51.0</cell><cell>52.2 52.6 (+6.7)</cell></row><row><cell cols="4">4.3 Comparative Studies</cell><cell></cell><cell></cell></row><row><cell cols="6">Results on UCF-HMDB. Tab. 1 shows the comparison re-</cell></row><row><cell cols="6">sults of TranSVAE with baselines and SoTA methods on</cell></row><row><cell cols="6">UCF-HMDB. The best result among all the baselines is</cell></row><row><cell cols="6">highlighted using bold. Overall, methods using I3D back-</cell></row><row><cell cols="6">bone achieve better results than those using ResNet-101.</cell></row><row><cell cols="6">Our TranSVAE consistently outperforms all previous meth-</cell></row><row><cell cols="6">ods. In particular, TranSVAE achieves 93.37% average ac-</cell></row><row><cell cols="6">curacy, improving the best competitor CO 2 A (Turrisi et al.</cell></row><row><cell cols="6">2022), with the same I3D backbone, by 1.38%. Surprisingly,</cell></row><row><cell cols="6">we observe that TranSVAE even yields better results (by a</cell></row><row><cell cols="6">2.1% improvement) than the supervised-target (T sup ) base-</cell></row><row><cell>line.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Comparisons with Multi-Modal Methods. We further</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Loss separation studies on different video-based UDA tasks by dropping each loss sequentially.Lcls Ladv Lmi Lctc PL U ? H H ? U JS ? JT D1 ? D2 D1 ? D3 D2 ? D1 D2 ? D3 D3 ? D1 D3 ? D2Avg.</figDesc><table><row><cell>Lsvae 18.61</cell><cell>26.62</cell><cell>22.92</cell><cell>34.00</cell><cell>30.29</cell><cell>33.79</cell><cell>30.49</cell><cell>28.51</cell><cell>34.27</cell><cell>28.83</cell></row><row><cell>83.06</cell><cell>93.52</cell><cell>48.07</cell><cell>40.93</cell><cell>43.33</cell><cell>43.91</cell><cell>51.13</cell><cell>41.84</cell><cell>52.67</cell><cell>55.38</cell></row><row><cell>85.83</cell><cell>93.52</cell><cell>65.12</cell><cell>46.67</cell><cell>48.56</cell><cell>49.43</cell><cell>55.34</cell><cell>45.52</cell><cell>54.53</cell><cell>60.60</cell></row><row><cell>83.89</cell><cell>95.80</cell><cell>64.89</cell><cell>48.53</cell><cell>48.25</cell><cell>48.96</cell><cell>54.21</cell><cell>45.52</cell><cell>55.73</cell><cell>60.64</cell></row><row><cell>87.22</cell><cell>94.40</cell><cell>64.64</cell><cell>49.87</cell><cell>48.25</cell><cell>49.66</cell><cell>56.47</cell><cell>47.59</cell><cell>55.07</cell><cell>61.46</cell></row><row><cell>87.78</cell><cell>98.95</cell><cell>66.10</cell><cell>50.53</cell><cell>50.31</cell><cell>50.34</cell><cell>58.62</cell><cell>48.04</cell><cell>58.00</cell><cell>63.19</cell></row><row><cell cols="3">compare four very recent video-based UDA methods that</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">use multiple modalities, i.e., RGB features and optical flows,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">although TranSVAE only uses RGB features. Surprisingly,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">TranSVAE is the best in 4 of 6 tasks and achieves better re-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">sults than the latest SoTA method CIA (Yang et al. 2022).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Considering TranSVAE only uses single-modal data, we are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">confident that there exists great potential for further im-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">provements with multi-modal data taken into account.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">T t=1 log p(x D t |z D d , z D t )]+ KL(q(z D d |V D )||p(z D d )) + T t=1 KL(q(z D t |x D &lt;t )||p(z D t |z D &lt;t )),(4)which is a frame-wise negative variational lower bound. Mutual Dependence Minimization(Fig. 3, ). Only using the above disentanglement model cannot guarantee that the static latent factors are encoding the domain-related information and the dynamic latent factors are encoding the temporal and semantic-related information. Considering that the static latent factors are expected to be domain-specific while the dynamic latent factors are in the opposite, we first enforce the two sets of latent factors to be mutually independent. To do so, we introduce the mutual information loss L mi to regulate the two sets of latent factors. Mutual information<ref type="bibr" target="#b1">(Batina et al. 2011</ref>) is formally defined as the KL divergence of the joint distribution to the product of the marginal distri-</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/YingzhenLi/Sprites</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contrastively disentangled sequential variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10105" to="10118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mutual information analysis: a comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Batina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gierlichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Prouff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-X</forename><surname>Standaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Veyrat-Charvillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="291" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring object relation in mean teacher for crossdomain detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11457" to="11466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning disentangled semantic representation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07482</idno>
		<title level="m">Graph domain adaptation: A generative view</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal attentive alignment for large-scale video domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6321" to="6330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-level Attentive Adversarial Learning with Temporal Dilation for Unsupervised Video Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1259" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shuffle and attend: Video domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="678" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Informative feature disentanglement for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2407" to="2421" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uncertainty-aware unsupervised domain adaptation in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2502" to="2514" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diva: Domain invariant variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging with Deep Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="322" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning crossmodal contrastive features for video domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13618" to="13627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">ConDA: Unsupervised domain adaptation for LiDAR segmentation via regularized domain concatenation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15242</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Disentangled sequential autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5670" to="5679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial bipartite graph learning for video domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The jester dataset: A large-scale video dataset of human gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-modal domain adaptation for fine-grained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial cross-domain action recognition with co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11815" to="11822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contrast and mix: Temporal contrastive video domain adaptation with background mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="23386" to="23400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saporta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="3751" to="3760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatio-temporal contrastive domain adaptation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9787" to="9795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Turrisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2234" to="2243" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Dual-Head Contrastive Domain Adaptation for Video Action Recognition</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey of unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic weighted learning for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15242" to="15251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interact Before Align: Leveraging Cross-Modal Knowledge for Domain Adaptive Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="14722" to="14732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SC-UDA: Style and Content Gaps aware Unsupervised Domain Adaptation for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karianakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional neural network for remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image and Graphics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="97" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spectral unsupervised domain adaptation for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="9829" to="9840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">S3vae: Self-supervised sequential vae for representation disentanglement and data generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6538" to="6547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
