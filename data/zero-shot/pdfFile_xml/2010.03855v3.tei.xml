<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Relational Image Captioning via Multi-task Triple-Stream Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Members, IEEE</roleName><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
						</author>
						<title level="a" type="main">Dense Relational Image Captioning via Multi-task Triple-Stream Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Dense captioning</term>
					<term>image captioning</term>
					<term>visual relationship</term>
					<term>relational analysis</term>
					<term>scene graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce dense relational captioning, a novel image captioning task which aims to generate multiple captions with respect to relational information between objects in a visual scene. Relational captioning provides explicit descriptions for each relationship between object combinations. This framework is advantageous in both diversity and amount of information, leading to a comprehensive image understanding based on relationships, e.g., relational proposal generation. For relational understanding between objects, the part-of-speech (POS; i.e., subject-object-predicate categories) can be a valuable prior information to guide the causal sequence of words in a caption. We enforce our framework to learn not only to generate captions but also to understand the POS of each word. To this end, we propose the multi-task triple-stream network (MTTSNet) which consists of three recurrent units responsible for each POS which is trained by jointly predicting the correct captions and POS for each word. In addition, we found that the performance of MTTSNet can be improved by modulating the object embeddings with an explicit relational module. We demonstrate that our proposed model can generate more diverse and richer captions, via extensive experimental analysis on large scale datasets and several metrics. Then, we present applications of our framework to holistic image captioning, scene graph generation, and retrieval tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The human visual system has the capability to effectively and instantly collect the holistic understanding of contextual associations among objects in a scene <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b49">[49]</ref> by densely and adaptively skimming the visual scene through the eyes, i.e., the saccadic eye movement. Such rich information instantly extracted from the scene allows humans to understand even subtle relationships among objects. Motivated by such human ability, in this work, we present a new concept of scene understanding, called dense relational captioning that provides dense and relational captions.</p><p>Rich representation of an image often leads to performance improvements of computer vision algorithms; e.g., contexts surrounding objects of a scene <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b49">[49]</ref>. To achieve richer objectcentric understanding, Johnson et al. <ref type="bibr" target="#b24">[25]</ref> proposed the DenseCap framework that generates captions for each of densely sampled local image regions. These regional descriptions facilitate both rich and dense semantic understanding of a scene in the form of interpretable language. In contrast, the information that we want to acquire includes not only that of the objects itself but also the interaction among other surrounding objects or the environment.</p><p>As an alternative way of representing an image, we focus on dense relationships between objects. In the context of human cognition, there has been a general consensus that objects and particular environments near the target object affect search and recognition efficiency. Understanding the relationships between objects clearly reveal object interactions and object-attribute combinations <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b45">[45]</ref>. In another view, interestingly, we observe that the human annotations on various computer vision datasets predominantly have relational forms. In the Visual Genome <ref type="bibr" target="#b34">[35]</ref> and MS COCO <ref type="bibr" target="#b42">[42]</ref> caption datasets, most of the labels take the format of subjectpredicate-object more so than subject-predicate. Moreover, the UCF101 <ref type="bibr" target="#b60">[60]</ref> action recognition dataset contains 85 actions out of 101 (84.2%) that are described in terms of human interactions with other objects or surroundings. These aspects tell us that understanding interaction and relationships between objects facilitate a major component in object-centric visual event understanding.</p><p>In this regard, we introduce a novel captioning framework relational captioning that can provide diverse and dense representations from a visual scene. In this task, we first exploit the relational context between two objects as a representation unit. This allows generating a combinatorial number of localized regional information. Secondly, we make use of captioning and its ability to express significantly richer concepts beyond the limited label space of object classes used in object detection tasks. Due to these aspects, our relational captioning expands the regime further along the label space both in terms of density and complexity, and provides richer representation for an image.</p><p>Our main contributions are summarized as follows. <ref type="bibr" target="#b0">(1)</ref> We introduce relational captioning, a new captioning task that generates captions with respect to relational information between objects in an image. <ref type="bibr" target="#b1">(2)</ref> In order to efficiently train the relational caption information, we propose the multi-task triple-stream network (MTTSNet) that consists of three recurrent units trained via multitask learning with the part-of-speech prediction. <ref type="bibr" target="#b2">(3)</ref> We show that our proposed method is able to generate denser and more diverse captions by evaluating on our relational captioning dataset augmented from the Visual Genome (VG) <ref type="bibr" target="#b34">[35]</ref> dataset as well as other relevant tasks and datasets. <ref type="bibr" target="#b3">(4)</ref> We demonstrate several use cases of our framework, including "caption graphs" which contain richer information than conventional scene graphs.</p><p>This work is the extension of our previous conference version <ref type="bibr" target="#b26">[27]</ref>. We extend it in several aspects: We extend our architecture by adding a relational embedding module (REM) motivated by the non-local networks <ref type="bibr" target="#b68">[68]</ref> to explicitly augment semantic meanings of surrounding objects. Also, we show that the REM further enhances MTTSNet in all the application scenarios we demonstrate. In addition, we expand our experimental results and analyses to show multiple aspects of our proposed method's algorithmic behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work mainly relates to two topics: image captioning and relationship detection. In this section, we review related work on these categorized topics. Image captioning. By virtue of deep learning and the use of recurrent neural network (e.g., LSTM <ref type="bibr" target="#b18">[19]</ref>) based decoders, image captioning <ref type="bibr" target="#b50">[50]</ref> techniques have been extensively explored <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b65">[65]</ref>, <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b76">[76]</ref>, <ref type="bibr" target="#b78">[78]</ref>. One of the research issues in captioning is the generation of diverse and informative captions. Thus, learning to generate diverse captions has been extensively studied recently <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b66">[66]</ref>. As one of the solutions, the dense captioning (DenseCap) task <ref type="bibr" target="#b24">[25]</ref> was proposed which uses diverse region proposals to generate localized descriptions, extending the conventional holistic image captioning to diverse captioning that can describe local contexts. Since DenseCap generates each caption per bounding box by only relying on an internal region of the bounding box, Yang et al. <ref type="bibr" target="#b73">[73]</ref> improves the DenseCap model by incorporating a global image feature as a context cue as well as a region feature of the desired objects with late fusion. Motivated by this, in order to learn dependencies of subject, object and union representations, we incorporate a triple-stream LSTM for our captioning module and further enhance the relational embedding by a non-local layer <ref type="bibr" target="#b68">[68]</ref>. Our triple-stream LSTM has analogies with the neural module networks which have been used in various language-related tasks such as visual question answering <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, visual dialog <ref type="bibr" target="#b32">[33]</ref>, visual grounding <ref type="bibr" target="#b43">[43]</ref>, captioning <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr" target="#b75">[75]</ref>, and symbolic reasoning <ref type="bibr" target="#b16">[17]</ref>. Our triple-stream LSTM can be seen as a simplified version of a neural module network with subject, predicate, and object modules specifically designed for our relational captioning task. Moreover, our relational captioning is able to generate even more diverse caption proposals than dense captioning by considering relations between objects.</p><p>Visual relationship detection and scene graph generation. Understanding visual relationships between objects have been an important concept in various tasks. Conventional visual relationship detection (VRD) typically deals with predicting the subjectpredicate-object (in short, subj-pred-obj). A pioneering work by Lu et al. <ref type="bibr" target="#b45">[45]</ref> formalized the VRD task and provides a dataset, while addressing the subject (or object) and predicate classification models separately. Their VRD dataset has also led to extensive studies on visual relationship understanding <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b74">[74]</ref>, <ref type="bibr" target="#b77">[77]</ref>, <ref type="bibr" target="#b79">[79]</ref>, <ref type="bibr" target="#b81">[81]</ref>, <ref type="bibr" target="#b83">[83]</ref>, <ref type="bibr" target="#b84">[84]</ref>. On the other hand, similar to the VRD task, scene graph generation has started to be explored <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b67">[67]</ref>, <ref type="bibr" target="#b69">[69]</ref>, <ref type="bibr" target="#b70">[70]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b80">[80]</ref>, where the task is to generate a structured graph that expresses the context relationships of a scene and provides a compact and interpretable representation of scenes. Moreover, human-object interaction detection task has also started to be explored recently <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p><p>Although the VRD dataset is larger (100 object classes and 70 predicates) than Visual Phrases <ref type="bibr" target="#b57">[57]</ref> dataset, it is still inadequate to handle real world scale. The Visual Genome (VG) dataset <ref type="bibr" target="#b34">[35]</ref> for relationship detection consists of 31, 000 predicate types and 64, 000 object types, which provides the combinatorial relationship triplets that are too diverse for the VRD models to comply with. This is because, in the VRD task, each object label should be assigned to each of the various adjective and noun combinations, e.g., respective different labels for "little boy" and "small boy." As a result, only the simplified version of VG relationship dataset has been studied <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b36">[37]</ref>. In contrast, our method is able to represent extensive natural language of relations by tokenizing the whole relational expressions into words and learning from them directly.</p><p>While the recent state-of-the-art VRD <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b77">[77]</ref>, <ref type="bibr" target="#b79">[79]</ref> or scene graph generation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b69">[69]</ref>, <ref type="bibr" target="#b70">[70]</ref>, <ref type="bibr" target="#b80">[80]</ref> methods attempted to use language priors to detect relationships, we directly learn the relationship in a descriptive language form. In addition, the expressions of the scene graph generation or the VRD tasks are restricted to subj-pred-obj triplets, whereas our proposed relational captioning task can provide additional information such as attributes or noun modifiers by adopting free-form natural language expressions. Thereby, we present an extended scene graph representation, called caption graph.</p><p>In summary, dense captioning facilitates a natural language interpretation of regions in an image, while VRD can predict relational information between objects within a restricted set. Our work combines both axes, resulting in much denser and more diverse captions than DenseCap. That is, given B number of region proposals in an image, we can obtain B(B?1) number of relational captions, whereas DenseCap returns only B number of captions. This property can be favorable for subsequent algorithms in other downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-TASK TRIPLE-STREAM NETWORKS</head><p>Our relational captioning generates captions as follows. Given an input image, a bounding box detector generates various object proposals, followed by a captioning module that predicts combinatorial captions describing each pair of objects along with POS labels. This pipeline is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, which is composed of a localization module based on the region proposal network (RPN) <ref type="bibr" target="#b54">[54]</ref>, and a triple-stream RNN (LSTM <ref type="bibr" target="#b18">[19]</ref>) module for captioning. In addition, we introduce the relational embeddding module (REM) as an extension, to encourage explicit encoding of Overall architecture of the proposed multi-task triple-stream networks (MTTSNet). Three region features (Union, Subject, Object) come from the same shared branch (Region Proposal Network), and for subject and object features, the first intermediate FC layer share the weights. Relational Embedding Module (REM) is introduced as an extension, which takes into account early dependency between subject and object.</p><p>relational information. Our network supports end-to-end training within a single optimization step that allows joint localization, combination, and description with natural language.</p><p>Specifically, given an image, the RPN generates object proposals. Then, the combination layer takes a pair of proposals and assigns them to the subject and object regions at a time. Also, to take the surrounding context information into account, we utilize the union region of the subject and object regions as side information. These triplet features from the subject, object, and union regions are fed to the triple-stream LSTMs, where each stream takes its own purpose, i.e., subject, object, and union. Given these triplet features, the triple-stream LSTMs collaboratively generate a caption and POS classes of each word. We describe details of these processes in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Region Proposal Networks</head><p>Our network uses fully convolutional layers of VGG-16 [59] 1 up to the final pooling layer (i.e. pool5) for extracting the spatial features via the bilinear region-of-interest (ROI) pooling <ref type="bibr" target="#b24">[25]</ref>. The object proposals are generated by RPN <ref type="bibr" target="#b54">[54]</ref>. It takes the feature tensor from the pool5 layer, and proposes B number of regions of interest after non-maximum suppression (NMS). Each proposed region comes with its confidence score, region feature of shape 512?7?7, and coordinates b=(x, y, w, h) of the bounding box with center (x, y), width w and height h.</p><p>Relational proposals are generated by building pairwise combinations of B number of region proposals, where in turn we get B(B?1) possible region pair combinations. We call this layer as combination layer. A distinctive point of our model with the previous dense captioning methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b73">[73]</ref> is that, while the methods regard each region proposal as an independent target to describe and produce B number of captions, we consider their pairwise B(B?1) number of combinations, which are much denser and explicitly expressible in terms of relationships. Also, we can asymmetrically use each entry of a pair by assigning the roles of the regions, i.e., (subject, object) or vice versa.</p><p>We vectorize the region features, and then apply two fullyconnected (FC) layers to map them into D-dimensional features, where the intermediate dimensions are D u =512 for the union 1. One can improve the performance of our relational method by replacing the backbone network with a deeper one, e.g., ResNet <ref type="bibr" target="#b17">[18]</ref>, which we show later in the experiment section. region and D o =4096 for subject and object regions. Only the first intermediate FC layer for subject and object features shares their weights. We use the rectified linear (ReLU) units <ref type="bibr" target="#b48">[48]</ref> and Dropout <ref type="bibr" target="#b61">[61]</ref> for the FC layers. The subject and object region features are optionally fed to the Relational Embedding Module (REM) which outputs refined features with the same size D=512. The details of the REM is described in Sec. 3.2. In short, the aforementioned process encodes region features into D-dimensional features, which is called region codes.</p><p>Furthermore, we leverage an additional region, the union region b u of (subject, object) motivated by Yang et al. </p><p>where b s b o and b s b o denotes the intersection and union areas of the two boxes respectively. The geometric feature r is encoded into a 64-dimensional geometric vector by passing through an additional FC layer. By concatenating the 64-dimensional geometric vector with the union feature, the shape of this feature is D + 64. Then, the dimension of the union region code is reduced by the following FC layer. This stream of the aforementioned operations is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The three features extracted from the subject, object, and union regions are fed to each LSTM described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relational Captioning Networks</head><p>Our network consists of multiple LSTM modules to generate captions that describe relational information. To this end, we design a new network that explicitly exploits relational cues.</p><p>In the proposed relational region proposal, a distinctive facet is its capability to provide a triplet of region codes corresponding to the subject, object, and union regions, which can be also viewed as the POS of a sentence (subj-pred-obj). The existence of these correspondences between each region in a triplet and POS "horse" <ref type="figure">Fig. 3</ref>. An illustration of the unrolled triple-stream LSTM. Our model consists of two major parts: triple-stream LSTM and a multi-task module. The multi-task module jointly predicts a caption word and its POS class (subj-pred-obj, illustrated as three cells colored according to the POS class), as well as the input vector for the next time step.</p><p>information can lead to the following advantages: 1) input region codes can be adaptively merged depending on their POS roles and be fed to the final word prediction module, and 2) when predicting a word, the POS prior can effectively affect the quality of caption generation by reducing potentially spurious words. To leverage these benefits, we propose the multi-task triple-stream network (MTTSNet). For the first advantage, to derive the POS aware inference, we propose the triple-stream network which consists of three separate LSTMs respectively corresponding to subj-predobj. The outputs of the LSTMs are combined via concatenation. For the second advantage, during word prediction, we jointly infer POS classes of each word. This POS prediction task allows the network to learn the POS prior knowledge for the word prediction.</p><p>Triple-Stream LSTMs. Intuitively, the region codes of the subject and object would be closely related to the respective subject and object related words in a caption, while the union and geometric features may contribute to the predicate. In our relational captioning framework, the LSTM modules need to adaptively take into account input features to generate a caption according to the POS decoding stage. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the proposed triple-stream LSTM module consists of three separate LSTMs, each of which is in charge of the subject, object and union region codes, respectively. From RPN, a triplet of region codes are fed as input to LSTMs, so that a sequence of words (caption) is generated. At each step (word), the triple-stream LSTMs generate three embeddings separately, and a single word is predicted by consolidating the three processed embeddings by the multi-task module (described in the next subsection). The embedding of the predicted word is distributed into all three LSTMs as inputs of the next step and is used to run the next step in a recursive manner. Thus in each step, each entry of the triplet input is used differently, which allows more flexibility than that of a single LSTM as used in traditional captioning models <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b65">[65]</ref>. In other words, the importance of the input features changes at every recursive step according to which POS the word being generated belongs to.</p><p>Multi-task with POS Classification. At each part of the triplestream LSTMs, we obtain three intermediate output features from each LSTM. To predict a word, we aggregate the features from the subject, predicate and object information, via a single FC layer. Also, we add an additional side task, POS prediction, from the same concatenated feature. We call this fusion layer as the multitask module as shown in the right enlarged view of <ref type="figure">Fig. 3</ref>.</p><p>The multi-task module can be viewed as a late fusion approach. An alternative would be early fusion, which consolidates the information in an even earlier step, i.e., the fusion of the three region codes (e.g., concatenation of three codes) followed by a single LSTM model instead of the triple-stream LSTMs. However, we observe that this early fusion approach has lower performance than our late fusion one, which is also consistent with the observation reported by Yang et al. <ref type="bibr" target="#b73">[73]</ref>. Thus, we take the late fusion approach and compare the performance in Sec. 4.</p><p>The POS classification task is leveraged to more effectively train the relational captioning. We impose the POS classification loss during training, so that the networks learn which LSTM they should emphasize more at a word prediction. Thereby, relational captioning is encouraged to generate a sequence of words in subj-pred-obj order, i.e., the order of POS. The POS tag can be easily obtained by a modern natural language processing toolkit, NLTK POS tagger <ref type="bibr" target="#b44">[44]</ref>, which had been established for a long time; thus, it provides a reliable prediction. In our case, we obtain POS (pseudo) ground truth from automatic label augmentation from relationship triplet labels.</p><p>We empirically find that this multi-task learning with POS not only helps the shared representation to be richer, but also guides the word predictions; thus, it helps to improve the captioning performance overall. Since each POS class prediction relies on respective representations from each LSTM, (e.g., predicate class prediction from the pred-LSTM), the gradients from the POS classification are mainly back-propagated through the feature elements representing a class ambiguously within the concatenated feature. Even for the same word output, the gradients from the multitask module may differ by this fact, so that representations across LSTMs can be learned to be further distinctive. Also, the POS prior may make the network suppress spurious word candidates.</p><p>Another potential way to leverage the POS priors would be to add an additional soft attention module to select among the three features instead of concatenating them. We compare this attention approach with our simple concatenation <ref type="bibr" target="#b5">[6]</ref>, where the results show that the performance of the attention approach (44.94 Recall) is lower than our concatenation (45.96 Recall) while using more number of parameters. Thus, we use the simple concatenation.</p><p>Relational Embedding Module. Since our triple-stream network only utilizes the triplet features (subject, object, and union), it alone may lack global understanding of the constituent objects in an entire image, i.e., global context. In this extension, to strengthen the capability of holistic relational understanding across all the objects, we employ the non-local layer <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b68">[68]</ref>, we called the relational embedding module (REM), where we apply the nonlocal layer to each object candidate. This is different from Wang et al. <ref type="bibr" target="#b68">[68]</ref>, where they apply it to the feature map densely while we apply it to ROI pooled features. The REM enhances the relational information across all objects via the attention mechanism.</p><p>Specifically, let X ? R B?Do denote a stack of B number of vectorized region features extracted from the first FC layer after the bilinear ROI pooling. Then, we compute the relational association matrix by:</p><formula xml:id="formula_1">R = softmax(?(XW a )?(XW b ) ) ? R B?B ,<label>(2)</label></formula><p>where ?(?) denotes ReLU and W a , W b ? R Do?512 are learnable weights that map the region features X to each of its own role, (e.g., subject and object) and the softmax operation is applied in row. Then, the relational feature matrix is computed by:</p><formula xml:id="formula_2">A = R?(XW x )W z ? R B?Do ,<label>(3)</label></formula><p>where W x ? R Do?512 and W z ? R Do?512 are again learnable weights. The matrix A encodes aggregated features across all the objects according to the degree of relational association by R, which is similar to the message passing that exchanges the information according to relationships. This relational feature matrix is combined with the original feature X by Z = X + A, so that the holistic relational information is enhanced on top of X. This can be viewed as augmenting richer semantic meanings, e.g., a shirt (X) is augmented to a shirt that someone is in or a shirt on something depending on the surroundings. Also, it is akin to the residual connection, allowing efficient training via the residual learning mechanism <ref type="bibr" target="#b17">[18]</ref>. Different from the non-local approaches <ref type="bibr" target="#b68">[68]</ref>, <ref type="bibr" target="#b69">[69]</ref>, we introduce non-linear activations, ReLU, in Eqs. <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>, motivated by a low-rank bilinear pooling method <ref type="bibr" target="#b31">[32]</ref>. We empirically found this modification leads to noticeable performance improvement. Furthermore, similar to Hu et al. <ref type="bibr" target="#b19">[20]</ref>, we may leverage the box geometric features r in the REM to re-scale the attention. However, in our empirical experiment, this does not help and even lowers the performance than our method that concatenates geometric features to the union feature. Thus, we use the final REM module illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>. Loss functions. The proposed model is trained to minimize the following loss function:</p><formula xml:id="formula_3">L = L cap + ?L P OS + ?L det + ?L box ,<label>(4)</label></formula><p>Model Output of RPN Input of LSTM LSTM POS prediction where L cap , L P OS , L det , and L box denote captioning loss, POS classification loss, detection loss, and bounding box regression loss, respectively. ?, ?, and ? are the balance parameters (we set them to 0.1 for all experiments). The first two terms are for captioning and the next two terms are for the region proposal. L cap and L P OS are cross-entropy losses applied to each word and POS prediction at every time step, respectively. For each time step, L P OS measures a 3-class cross entropy loss. L det is a binary logistic loss for foreground/background regions to distinguish positive and negative object regions <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[25]</ref>, while L box is a smoothed L1 loss <ref type="bibr" target="#b54">[54]</ref>.</p><formula xml:id="formula_4">Direct Union Union region U Single ? Union Object U Single ? Union+Coord. Object U + C Single ? Subj+Obj Object S + O Single ? Subj+Obj+Coord. Object S + O + C Single ? Subj+Obj+Union Object S + O + U Single ? Union (w/MTL) Object U Single Subj+Obj+Coord.(w/MTL) Object S + O + C Single Subj+Obj+Union (w/MTL) Object S + O + U Single Union+Union+Union (w/MTL) Object U + U + U Triple TSNet Object S | O | U + C Triple ? MTTSNet Object S | O | U + C Triple</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we provide the experimental setups, competing methods and performance evaluation of relational captioning with both quantitative and qualitative results, so that we empirically show the benefit and potential of the proposed relational captioning task and the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setups</head><p>Implementation details. We use Torch7 <ref type="bibr" target="#b6">[7]</ref> to implement our model. For the backbone visual feature extraction, we use VGG-16 <ref type="bibr" target="#b59">[59]</ref> and initialize with the weights pre-trained on Ima-geNet <ref type="bibr" target="#b56">[56]</ref>. We pre-train the RPN on the Visual Genome (VG) dense captioning data <ref type="bibr" target="#b34">[35]</ref>. For sequence modeling, we set the dimension of all the LSTM hidden layers to be 512. A training batch contains an image that is resized to have a longer side of 720 pixels. We use Adam optimizer <ref type="bibr" target="#b2">[3]</ref> for training (learning rate lr=10 ?6 , b1=0.9, b2=0.999). For the RPN, we use 12 anchor boxes for generating the anchor positions in each cell of the feature map, and 128 boxes are sampled in each forward pass of training. We use Titan X GPU, and it takes about four days for a model to convergence when training on our relational captioning dataset. We use the setting for the region proposals similar to that of <ref type="bibr" target="#b24">[25]</ref> for fairness. For training, a region is positive if it has at least 0.7 IoU ratio with a corresponding ground truth region, and a region is negative if its IoUs are less than 0.3 with all ground truth regions. For evaluation, after non-maximum suppression (NMS) based on the predicted proposal confidences, 50 confident bounding boxes are selected. We can additionally reduce box pair predictions by discarding the pairs that produce captions with low confidence scores. Caption confidence scores can be computed by sequentially multiplying all of the generated word probabilities. Relational captioning dataset. Since there is no existing dataset for the relational captioning task, we construct a dataset by utilizing VG relationship dataset version 1.2 <ref type="bibr" target="#b34">[35]</ref>   expressions into word level tokens, and we assign the POS class from the triplet association for each word. However, the VG relationship dataset has a limited diversity of the words used. Therefore, na?vely converting such VRD dataset to a captioning dataset is not desirable, in that the captions generated from a trained model on the dataset tends to be too simple (e.g., "building-has-window"). This limited data restricts the expressiveness of the model. To examine the diverse expressions of our relational captioner, we construct our relational captioning dataset to have more natural sentences with richer expressions.</p><p>Through observation, we noticed that labels in the VG relationship dataset lack attributes describing the subject and object, which are perhaps what enriches the expressiveness of sentences the most. We enrich the dataset by leveraging the VG attribute dataset <ref type="bibr" target="#b34">[35]</ref>. The specific procedure of this attribute enrichment is described in Appendix. After this enrichment, we obtain 15,595 different vocabularies for our relational captioning dataset, which was 11,447 different vocabularies before this process.</p><p>We train our model with this dataset, and report its result in this section. In the following subsections, we evaluate in multiple views including a holistic image captioning performance and various analysis such as comparison with scene graph generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relational Dense Captioning: Ablation Study</head><p>Baselines. Since no direct related work for relational captioning exists, we implement several baselines by modifying the most relevant methods, which facilitate our ablation study. All the configurations are summarized in <ref type="table" target="#tab_0">Table 1</ref>    <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b65">[65]</ref> and two dense captioners <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b73">[73]</ref>. To compare with stronger baselines, we modify the image captioners by deploying a stochastic sampling. We annotate the modified versions with stochastic sampling with ?. We annotate (GT ) for the methods that replace RPN with ground truth bounding boxes; thus, those represent proxy upper bounds of performance. (R) indicates ResNet-50 <ref type="bibr" target="#b17">[18]</ref> as a backbone network instead of VGG-16.</p><p>region from each pair is fed to a single LSTM for captioning. Also, we implement two additional variants: Union (w/MTL) additionally predicts the POS classification task, and Union+Coord. appends the geometric feature to the region code of the union. Lastly, to match the number of parameters with our MTTSNet, we additionally introduce the Union+Union+Union baseline with the triple-stream architecture, which only takes the union region as input. ? Subj+Obj and Subj+Obj+Union models use the concatenated region code of (subject, object) and (subject, object, union) respectively and pass them through a single LSTM (an early fusion approach). Also, Subj+Obj+Coord. uses the geometric feature instead of the region code of the union. Moreover, we evaluate the baselines, Subj+Obj+{Union,Coord} again by adding the POS classification (i.e., MTL loss). ? TSNet denotes the proposed triple-stream LSTM model without a branch for the POS classifier. Each stream takes the region codes of (subject, object, union+coord.) separately. MTTSNet (i.e., TSNet+POS) denotes the multi-task triple-stream network with the POS classifier, and MTTSNet+REM denotes the model combined with the REM. Evaluation metrics. Motivated by the evaluation metric suggested for the dense captioning task by Johnson et al. <ref type="bibr" target="#b24">[25]</ref>, we suggest a modified evaluation metric for the relational dense captioning. Firstly, to assess the caption quality, we measure the average METEOR score <ref type="bibr" target="#b10">[11]</ref> for predicted captions (noted as METEOR). Also, we use a mean Average Precision </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>? Person wearing red and black jacket. ? A cloud in blue sky. ? The helmet is black. ? Front wheel of motorcycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>??? The man on a black motorcycle. ??? The man in blue sky. ??? Red motorcycle has a black wheel. ??? The man wearing black helmet. ??? Black wheel on a motorcycle. ??? The head of man. ??? Blue sky has white clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Captioner:</head><p>? A person is playing frisbee in the snow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>? A blue sky with clouds. ? A dog. ? Man in the air. ? A sandy beach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>??? The man on white beach. ??? Green tree has leaves. ??? Black dog on sandy beach. ??? The tree in background. ??? The man on a ground. ??? Green grass on ground. ??? The tree in a large field. ? Image Captioner: ? A group of people riding bikes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>? A man riding a bike. ? Man wearing blue jeans. ? A train on the tracks. ? A building in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>??? The bicycle on street. ??? Black wheel on bicycle. ??? The bicycle has a black wheel. ??? Red bus has a black wheel. ??? The car on street. ??? The man on black motorcycle. ??? Red bus on street.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>? Snowboarder in the air. ? Red jacket on man. ? A snowboard is white. ? Red and white snow board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>??? Green trees in background. ??? The snow on ground.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>? Baseball player swinging a bat. ? A black shirt on a man. ? A red and white baseball field. ? A black and white tennis racket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>??? The man in green grass.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>? Clock on the wall. ? A brick building with a clock. ? White lines on the road ? Woman wearing a red shirt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>??? Brick building has a window. ??? The woman in front of building. ??? The shadow on ground. ??? The woman on road. ??? White clock on building. ??? The woman on black sidewalk. ??? The window on red building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Captioner:</head><p>? A man riding a skateboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>? A man doing a trick on a skateboard. ? A man on a skateboard. ? Skateboard in the air. ? Doorway in front of building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>??? The man wearing black shirt. ??? The people on sidewalk. ??? The people on a black skateboard. ??? White building has a window. ??? The man holding black skateboard. ??? Black window on building. ??? The man on sidewalk. ? <ref type="figure">Fig. 5</ref>. Example captions and region generated by the proposed model on Visual Genome test images. The region detection and caption results are obtained by the proposed model from Visual Genome test images. We compare our result with the image captioner <ref type="bibr" target="#b65">[65]</ref> and the dense captioner <ref type="bibr" target="#b24">[25]</ref> in order to contrast the amount of information and diversity. subject and object bounding boxes greater than the localization threshold, which yields a more challenging metric. For all cases, we use percentage as the unit of metric.</p><p>In addition, we suggest another metric, called "image-level (Img-Lv.) recall." This measures the caption quality at the holistic image level by considering the bag of all captions generated from an image as a single prediction. This metric evaluates the diversity of the produced representations by the model for a given image. Specifically, with the aforementioned language thresholds of METEOR, we measure the recall of the predicted captions over about 20 ground truth captions. Results. <ref type="table" target="#tab_2">Table 2</ref> compares the performance of various methods for the relational dense captioning task on the relational captioning dataset. To compare with a different representation of relationship, we additionally compare with the state-of-the-art scene graph generator, Neural Motifs <ref type="bibr" target="#b80">[80]</ref>. Due to the different output structure, we compare with Neural Motifs trained with the supervision for relationship detection. Similar to the setup in <ref type="bibr" target="#b24">[25]</ref>, we fix the number of region proposals after NMS to 50 for all methods for a fair comparison.</p><p>Within the second row section (2-7th rows) of <ref type="table" target="#tab_2">Table 2</ref>, our TSNet shows the best result suggesting that the triple-stream component alone is a sufficiently strong baseline over the others. On top of TSNet, applying the MTL loss (i.e., MTTSNet) improves overall performance, and especially improves mAP, where the detection accuracy is dominantly improved compared to the other metrics. This shows that triple-stream LSTM is the key module that most leverages the MTL loss across other early  fusion approaches (see the third row section of the table). Also, compared to Union+Union+Union (w/MTL), our MTTSNet shows much higher performance, which validates that the performance improvement by our method is not simply due to the increased number of the model parameters. Moreover, by adding REM to our late fusion method, MTTSNet, we have achieved further improvements in both mAP and Img-Lv. Recall scores (more strongly on Img-Lv. Recall). As another factor, we can see from <ref type="table" target="#tab_2">Table 2</ref> that the relative spatial information (Coord.) and union feature information (Union) improves the results. This is because the union feature itself preserves the spatial information to some extent from the 7 ? 7 grid form of its activation. Also, the relational captioner baselines including our TSNet and MTTSNet perform favorably against Neural Motifs in all metrics. Note that handling free-form language generation which we aim to achieve is more challenging than the simple triplet prediction of scene graph generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Holistic Image Captioning</head><p>We also compare our approach with other image captioning frameworks, Image Captioner (Show&amp;Tell <ref type="bibr" target="#b65">[65]</ref>, SCST <ref type="bibr" target="#b55">[55]</ref>, and RFNet <ref type="bibr" target="#b22">[23]</ref>), and Dense Captioner (DenseCap <ref type="bibr" target="#b24">[25]</ref> and TLSTM <ref type="bibr" target="#b73">[73]</ref>) in a holistic image description perspective. To measure the performance of holistic image-level captioning for dense captioning methods, we use Img-Lv. Recall metric defined in the previous section. We compare them with two relational dense captioning methods, Union and MTTSNet (as well as ??? Standing man wearing blue shirt. ??? The man holding black racket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><p>??? Young man on green court. ??? The fence in background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>??? The elephant has a brown head. ??? Gray elephant in background.</p><p>(c) <ref type="figure">Fig. 7</ref>. Examples of different captions predicted from relational captioning by (a) changing objects, (b) changing subjects, and (c) switching the subject and object. Our model shows different predictions from different subject and object pairs. +REM), denoted as Relational Captioner. For a fair comparison, for Dense and Relational Captioner, we adjust the number of region proposals after NMS to be similar, which is different from the setting in the previous section which fixes the number of proposals before NMS. For fair comparison with the Image Captioner, in addition to the typical selection of words according to maximum probabilities in caption generation, we introduce another baselines using a stochastic sampling (probabilistically selecting a word proportional to the probabilities of words from a model) to allow diverse caption generation from the LSTM. We generate 10 captions from the stochastic variant image captioners in order to match the number of captions between Image Captioner and Dense Captioner. Finally, in order to isolate the performance of the caption generation and the box localization modules, we measure the captioning performance by setting the bounding boxes as the ground truth boxes. We annotate such variant of relational captioners with (GT ). <ref type="table" target="#tab_4">Table 3</ref> compares the image-level recall, METEOR, and additional quantities. #Caption denotes the average number of captions generated from an input image and Caption/Box denotes the average ratio of the number of captions generated and the number of boxes remaining after NMS. Therefore, Caption/Box demonstrates how many captions can be generated given the same number of boxes generated after NMS. By virtue of multiple </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>Sentence based image retrieval performance comparison across different representations. We evaluate ranking using recall at k (R@K, higher is better) and the median rank of the target image (Med, lower is better). The random chance performance is provided for reference. We compare with TLSTM in addition to the baselines (Full Image RNN, Region RNN, DenseCap) suggested in Johnson et al. <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Sentence + Reference Image Retrieved Images &amp; Region-pair</head><p>White cars parked on side of road.</p><p>White plane has a red wing.</p><p>Green trees across blue water.</p><p>White sign near paved road. captions per image from multiple boxes, the Dense Captioner is able to achieve higher performance than all the Image Captioners. While the stochastic sampling methods slightly improve image captioning performance in terms of recall, the performance is still far lower than Dense Captioners or Relational Captioners by a large margin, as the diversity of an image captioner's output is still very limited by its inherent design. Compared with the Dense Captioners, MTTSNet as a Relational Captioner can generate an even larger number of captions, given the same number of boxes. Hence, as a result of learning to generate diverse captions, the MTTSNet achieves higher recall and METEOR. TLSTM <ref type="bibr" target="#b73">[73]</ref> improves the performance of DenseCap <ref type="bibr" target="#b24">[25]</ref> due to a better representational power, but the performance is still lower than that of MTTSNet. Comparing to Union, we can see that it is difficult to obtain better captions than Dense Captioner by only learning to use the union of subject and object boxes, despite having a larger number of captions. Adding REM to our MTTSNet, further improves the performance in both the Recall and the METEOR score. In addition, even when setting the bounding boxes as the ground truth bounding boxes, by virtue of the more powerful language module, MTTSNet (especially MTTSNet+REM) shows favorable performance compared to Union.</p><p>We show prediction examples of our relational captioning model in <ref type="figure">Fig. 5</ref> along with the comparisons against the traditional frameworks, image captioner <ref type="bibr" target="#b65">[65]</ref> and dense captioner <ref type="bibr" target="#b24">[25]</ref>. Our model is able to generate rich and diverse captions for an image, compared to other paradigms. While the dense captioner is able to generate diverse descriptions than an image captioner by virtue of localized regions, our model can generate an even more number of captions from the combination of the bounding boxes. <ref type="figure">Fig. 7</ref> shows caption prediction examples for multiple box pair combinations. Based on the output of the POS predictor, we color the words of the caption as (red, green, blue) for (subjpred-obj) respectively. We note that, while the traditional dense captioning simply takes a single region as input and predicts one dominant description, in our framework, different captions can be obtained from different subject and object pairs. In addition, one can see that the predicted POS is correctly aligned with the words in the generated captions. Although the POS classification is not our target task, for completeness, we measure the accuracy of the MTTSNet POS estimation by comparing it with the ground truth POS, which is 89.7%. The detailed accuracies for subject, predicate, and object are 91.6%, 86.5%, and 90.9%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Scene Graph</head><p>Motivated by scene graph, which is derived from the VRD task, we extend to a new type of a scene graph, which we call "caption graph." <ref type="figure">Fig. 6</ref> shows the caption graphs generated from our MTTSNet as well as the scene graphs from Neural Motifs <ref type="bibr" target="#b80">[80]</ref>. For caption graph, we follow the same procedure with Neural Motifs, but replace the relationship detection network with our MTTSNet. In both methods, we use ground truth bounding boxes for fair comparison.</p><p>By virtue of being free form, our caption graph can have richer expression and information including attributes, whereas </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;person&gt;-&lt;hold&gt;-&lt;phone&gt;</head><p>Sitting man holding small phone. <ref type="figure">Fig. 9</ref>. Qualitative comparison with visual relationship detection model <ref type="bibr" target="#b45">[45]</ref>. The proposed relational captioning model is able to provide more detailed information than the traditional relationship detection model.  the traditional scene graph is limited to a closed set of the subj-pred-obj triplet. For example, in <ref type="figure">Fig. 6-(b,d)</ref>, given the same object "person," our model is able to distinguish the fine-grained category (i.e., man vs boy and man vs woman). In addition, our model can provide more status information about the object (e.g., standing, black), by virtue of the attribute contained in our relational captioning data. Most importantly, the scene graph can contain unnatural relationships (e.g., tree-on-tree in <ref type="figure">Fig. 6-(c)</ref>), because the back-end relationship detection methods, e.g., <ref type="bibr" target="#b80">[80]</ref>, predict object classes independently. In contrast, by predicting the full sentence for every object pair, the relational captioner can assign a more appropriate word with attributes for an object by considering the relations, e.g., "Green leaf on a tree." Lastly, our model is able to assign different words for the same object by considering the context (the man vs baseball player in <ref type="figure">Fig. 6-(d)</ref>), whereas the scene graph generator can only assign one most likely class (man). Thus, our relational captioning framework enables more diverse interpretation of the objects compared to the traditional scene graph generation models, which would be more favorable representation to scene context understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Sentence-based Image and Region-pair Retrieval</head><p>Since our relational captioning framework produces richer image representations than other frameworks, it may have benefits on image and region-pair retrieval by sentence. Our method can directly deal with free-form natural language queries, whereas scene  Also, note that most of the VRD models have the benefit of strong backbones such as ResNet, but our MTTSNet+REM (R) with ResNet-50 surpasses all the other competing methods even with the VRD metrics unfavorable to ours.</p><p>graph or VRD models require additional processing to handle the free-form queries. In this section, we evaluate our method on the retrieval task. Following the same procedure from <ref type="bibr" target="#b24">[25]</ref> but with our relational captioning dataset, we randomly choose 1,000 images from the test set, and from these chosen images, we collect 100 query sentences by sampling four random captions from 25 randomly chosen images. The task is to retrieve the correct image for each query by matching it with the generated captions. Our relational captioning based retrieval is done as follows. For every test image, we generate 100 region proposals from the RPN followed by NMS. To measure the degree of association, i.e., matching score, between a query and a region pair in the image, we compute the probability that the query text may occur from the region pair by multiplying the probability of words over recursive steps. Among all the scores of the region pairs from the image, we take the maximum matching score value as a representative score of matching between the query text and the image. The retrieved images are sorted according to these computed matching scores.</p><p>We compare the retrieval performance with several baselines in <ref type="table">Table 4</ref>. We measure recall at top K, R@K, which is the success ratio across all the queries that, by each given query, its ground-truth image is retrieved within top K ranks. We report K ? {1, 5, 10} cases. We also report the median rank of the correctly retrieved images across all 1000 test images. We follow the same procedure by Johnson et al. of running through random test sets 3 times to report the average results. We add an additional retrieval result with a more competitive dense captioning model, TLSTM <ref type="bibr" target="#b73">[73]</ref>. From the result, our proposed relational captioners show favorable performance against the baselines. This is meaningful because a region pair based method deals with a more difficult input form than that of the single region based approaches. Moreover, MTTSNet+REM consistently shows better retrieval performance compared to MTTSNet. <ref type="figure" target="#fig_13">Fig. 8</ref> shows the qualitative results on the sentence based image and region-pair retrieval. Given a sentence query, we show the retrieved images and their region pairs with the maximum matching score. Image retrieval based on our approach has a distinct advantage in that it retrieves images containing similar contextual relationships despite significant visual differences. More specifically, in the 3rd row of <ref type="figure" target="#fig_13">Fig. 8</ref>, our method can retrieve images with an abstract contextual relationship of "White sign near paved road." The retrieved images are visually diverse but share the same contextual information. Also, the natural language based retrieval from our framework is distinctive compared to traditional relationship detection methods (classification) which cannot handle natural language queries with variable length due to their fixed form input (i.e. subj-pred-obj). For example, in the 1st row, given a query that specifies the color "red," our model is able to retrieve images of a plane with red wings which VRD models are not capable of.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison with VRD Model</head><p>In order to demonstrate the flexibility of our model's output, i.e., natural language based sentences, we qualitatively compare our model with one of the benchmark models of visual relationship detection (VRD) task. We test the VRD benchmark model <ref type="bibr" target="#b45">[45]</ref> and our MTTSNet (and with +REM). The comparison is shown in <ref type="figure">Fig. 9</ref>. While the output of the VRD model is limited to the subj-pred-obj triplet with a smaller number of classes in a closed set, the output of our model has more flexibility and  can contain more contextual information by virtue of being free form. For example, given the same object "person," our model is able to distinguish the fine-grained category, i.e., man and woman. In addition, our model can provide rich information about the object (e.g., smiling, gray) by virtue of leveraging attribute information of our relational captioning data. Thus, our relational captioning framework enables higher level interpretation of the objects compared to the relationship detection framework.</p><p>Since the output of the VRD task has a relatively simple form (i.e., subj-pred-obj triplet) compared to that of our captioning framework (caption with free-form and variable length), a VRD model is easier to train given the same relationship detection dataset. Thus, a direct comparison with a VRD model on the VRD dataset <ref type="bibr" target="#b45">[45]</ref> is unfair for our method. Despite this, we perform quantitative comparisons with VRD models by restricting the output vocabulary of our model such that the words appeared in the VRD dataset without attributes are only used. We use the VRD dataset that contains in total 5000 images with 4000/1000 splits for train/test sets respectively. Similar to the construction process of our relational captioning dataset, we tokenize the form of triplet expression, i.e., subj-pred-obj, to form natural language expressions, and for each word, we assign the POS class from the triplet association. By tokenizing, we obtain 160 vocabularies for the VRD dataset.</p><p>We evaluate on this regime in <ref type="table" target="#tab_8">Tables 5 and 6</ref> with the relational captioning metrics and VRD metrics, respectively. Firstly, <ref type="table" target="#tab_8">Table 5</ref> The man has a white Frisbee.</p><p>The bus has a black wheel. The man wearing blue shirt.</p><p>The man holding white racket. The man wearing black helmet. The light on blue water. <ref type="figure" target="#fig_0">Fig. 11</ref>. Failure cases of our model. The reasons for failure cases are often due to visual ambiguity and illumination. subj-pred-obj are color-coded by red, green, and blue colors according to the output of the POS predictor, respectively.</p><p>shows the comparisons with VRD models <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b74">[74]</ref> on the VRD dataset along with the ablation study. Overall, the ablation study shows similar trends to that of using our relational captioning dataset (c.f ., <ref type="table" target="#tab_2">Table 2</ref>). Our TSNet and MTTSNet (both with and without +REM) show top performance among the relational captioning models, of which difference is with and without POS prediction (w/MTL), respectively. This suggests that, even on the VRD dataset, the triplet-stream component is still a strong baseline over others. Moreover, interestingly, while the POS classification appears to be an easy and basic task, adding the POS classification in the form of multi-task learning consistently helps the caption generation performance by a noticeable margin in our context, as shown in <ref type="table" target="#tab_2">Tables 2 and 5</ref>.</p><p>In the last row, we show the performance of the VRD models by Lu et al. <ref type="bibr" target="#b45">[45]</ref> and Yang et al. <ref type="bibr" target="#b74">[74]</ref> with the relational captioning metrics. Note that these VRD models are designed specifically for triplet classification on the VRD dataset. Thus, in terms of mAP, it has an advantage compared to the results of the other relational captioning baselines. Nonetheless, compared to the VRD model, our relational captioners (especially our MTTSNet+REM) show favorable performance on Img-Lv Recall and METEOR with a notable margin. This suggests that the proposed relational captioning framework is advantageous in generating diverse and semantically natural expressions. On the other hand, VRD models are disadvantageous in these aspects because they use a closed vocabulary set and predict object classes individually without considering the context. <ref type="table" target="#tab_10">Table 6</ref> shows the comparison between our MTTSNet (both with and without +REM) and other VRD models measured on the VRD metrics. Due to the difference of our output type to that of VRD, we use METEOR score thresholds proposed by <ref type="bibr" target="#b24">[25]</ref> as the matching criteria between model outputs and ground truth labels. Among the three VRD tasks (predicate classification, phrase detection and relationship detection) defined in <ref type="bibr" target="#b45">[45]</ref>, we do not measure predicate classification because a simple classification is out of scope for our model, but context understanding. As shown in the table, our model shows favorable or comparable performance to the VRD models despite the fact that they are specifically designed for the VRD task. Also, note that most of the VRD methods take an advantage of strong backbone networks such as ResNet over our MTTSNet+REM that uses VGG-16. According to the table, our method with the ResNet-50 backbone performs better than all the other competing VRD methods. This is worth noting in that, as opposed to VRD, our output label space is more complex than that of VRD due to variable caption length and a much larger number of vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Additional Analysis</head><p>Vocabulary statistics. In addition, we measure the vocabulary statistics and compare those of the frameworks in <ref type="table" target="#tab_12">Table 7</ref>. The types of statistics measured are: 1) an average number of unique words that have been used to describe an image, and 2) an average number of words to describe each box. Specifically, we count the number of unique words in all the predicted sentences and present the average number per image or box. Thus, the metric is proportional to the amount of information we can obtain given an image or a fixed number of boxes. These statistics increase in the order of Image Cap., Scene Graph, Dense Cap., and Relational Cap (both with and without +REM). In conclusion, the proposed relational captioning is favorable in diversity and amount of information (especially when the REM module is added), compared to both of the traditional object-centric scene understanding frameworks, i.e., Dense Cap. and Scene Graph. Importance transition along the triple-LSTMs. Since we have the three state LSTMs to predict a single word, it might be questionable whether each LSTM learns their own semantic roles properly. To see the behavior of each LSTMs, we visualize the weight transition from each LSTM for each time step. For this, given a set of features fed to the triple-stream LSTMs, we compute the L2 norm of the LSTM hidden state vector for each time step as a measure of importance value. These values from the three LSTMs are normalized across time through mean value subtraction. These values can be regarded as information or importance quantities. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the transitions of the representative values across time. As the POS phase changes through subject-predicate-object, the weight of the subject LSTM consistently decreases while that of the object LSTM increases. The predicate LSTM has a relatively consistent intensity between subject and object LSTMs as the POS changes. Thus, LSTMs plausibly disentangle their own roles according to POS. In other words, each word in the captions comes from the corresponding LSTM, e.g., a subject word is generated from the subj-LSTM. Discussion of the failure cases. <ref type="figure" target="#fig_0">Fig. 11</ref> shows failure cases of our relational captioning. The captions generated from our method can be inaccurate for several reasons. One of the important factors is visual ambiguity. Ambiguity may come from visually similar but different objects (first column) or by geometric ambiguity (second column). Lastly, due to illumination, the model may describe the object with a different color (e.g., "blue") (third column). Each of cases requires challenging capabilities, such as geometric reasoning, high resolution spatial representation learning, illumination invariance, etc., which are all fundamental computer vision challenges. we postulate that these problems may be resolved by improving visual feature representation; we leave these failure cases as a future direction. Note that the predicted POS is still correctly aligned with the words in the generated captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduce relational captioning, a new notion which requires a model to localize regions of an image and describe each of the relational region pairs with natural language. To this end, we propose the MTTSNet, which facilitates POS aware relational captioning. In several sibling-tasks, we empirically demonstrate the effectiveness of our framework over scene graph generation and the traditional captioning frameworks. Furthermore, our relational captioning can provide dense, diverse, abundant, high-level and interpretable representations in a caption form, which is a new way to represent imagery. This allows us to take several advantage over the existing tasks of VRD and {image, dense} captionings. Compared to VRD, our relational captioning deals with "openset" (or a much larger set) expression. The VRD task is restricted to subj-pred-obj combinations, of which term represents a fixed number of classes. <ref type="bibr" target="#b1">2</ref> However, the natural language representation we use has free-form with varying lengths, which can represent uncountably many possibilities. In addition, as an object can be referred to with expressive and distinctive attributes, which cannot be done in the VRD task, e.g., <ref type="figure" target="#fig_0">Fig. 12</ref>. A sample of the VG attributes dataset. Each bounding box is labeled with an object name and attributes (Attribute labels for a bounding box can be multiple). a man vs. a boy wearing red hat, our output representation is more general. Finally, as we have shown in our results, our captioning has much higher recall than the other tasks, which can transfer sufficient information to subsequent algorithms and applications. In this regard, our work may open new interesting applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX -ATTRIBUTE ENRICHMENT</head><p>As described in Sec. 4.1, we construct the relational captioning dataset based on the VG relationship dataset, but the dataset lacks attribute information in the captions. To compensate the lack of attributes, we leverage VG attribute dataset <ref type="bibr" target="#b34">[35]</ref>.</p><p>The configuration of the VG attribute dataset is depicted in <ref type="figure" target="#fig_0">Fig. 12</ref>. In the dataset, each object bounding box in an image is associated with "object name" and "attributes" of the object. Note that each object can have multiple attributes at the same time. Since the VG relationship dataset and the attribute dataset share the same image set, while the ground-truth bounding boxes are not shared, to associate the attribute with our captions, we conduct the process to find corresponding bounding boxes between datasets.</p><p>We simply find the attribute that matches the subject/object of the relationship label and assign it to the subject/object caption label. In particular, if an attribute label describes the same subject/object for a relationship label while an associated bounding box overlaps enough, the label is considered to be matched to the subject/object in the relationship label.</p><p>The specific procedure to decide association are as follows: 1) The category words of the subject / object in the relationship label and the object names of the attribute label must match, and the boxes should sufficiently overlap (higher IOU than 0.7), 2) Among the several boxes satisfying this condition, the box with the highest IOU is selected. 3) If a single box is associated with multiple attribute labels, we check the part-of-speech (POS) of candidate attribute labels using the NLTK POS tagger <ref type="bibr" target="#b44">[44]</ref>. The words classified as (NN, VBN, VBG, VBD, JJ) are regarded as appropriate candidates for natural attributes. We filter out the other categories. 4) Among the attribute candidates, the words in the original relationship triplet (i.e. subj-pred-obj) are excluded from the candidates to prevent redundancy. 5) If there are still more than one candidate attributes satisfying all these conditions, we randomly select one among the candidates. 6) If a subject does not have any matched attribute, "the" is added.</p><p>Note that the VG relationship dataset and the VG attribute dataset share the same image set; thus, there exists object-level correspondences. Since we leverage the correspondences, thus, our dataset is likely to follow real distribution of image-description contents in the datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Difference of our proposed relational captioning from existing image understanding frameworks. Compared to traditional frameworks, our work is advantageous in both interaction understanding and highlevel expressive interpretation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall architecture of the proposed multi-task triple-stream networks (MTTSNet). Three region features (Union, Subject, Object) come from the same shared branch (Region Proposal Network), and for subject and object features, the first intermediate FC layer share the weights. Relational Embedding Module (REM) is introduced as an extension, which takes into account early dependency between subject and object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b73">[73]</ref>.Yang  et al. demonstrate that the global context of an image as a sideinformation can improve the captioning performance. Compared to the global context of Yang et al., our union region has more localized information incorporating both subject and object. In addition, to provide relative spatial information, we append geometric features for the subject and object box pair, i.e., (b s , b o ), to the union feature. Given two bounding boxes b s =(x s , y s , w s , h s ) and b o =(x o , y o , w o , h o ), we use the following geometric feature r similar to that of Peyre et al. [51] as r = xo?xs ? wshs , yo?ys ? wshs , woho wshs , ws hs , wo ho , bs bo bs bo ? R 6 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Architecture of the relational embedding module (REM). denotes the matrix multiplication, and the element-wise sum. The softmax operation is applied row-wise. The blue boxes with the FC label denote FC layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(mAP) similar to Johnson et al. which measures both localization and language accuracy. For language accuracy, we measure METEOR score with thresholds {0, 0.05, 0.10.15, 0.2, 0.25}, and we use IOU thresholds {0.2, 0.3, 0.4, 0.5, 0.6} for localization accuracy. The AP values, obtained by all the pairwise combinations of language and localization thresholds, are averaged to get the final mAP score. The major difference of our metric from that of Johnson et al. is that, for the localization AP, we measure for both the subject and object bounding boxes with respective ground truths. In particular, we only consider the samples with IOUs of both the Image Captioner: ? A man riding a motorcycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?</head><label></label><figDesc>Boy in playing baseball. ? A baseball player wearing a helmet. ? Green grass on the field ? Grass on the ground Relational Captioner (Ours): ??? The boy wearing hat. ??? Green grass on a ground. ??? Standing man on a ground. ??? The Head of a man. ??? The man wearing gray shirt. ??? The boy on a large field. ??? The grass on ground.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>?</head><label></label><figDesc>Image Captioner:? A man flying through the air.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>??? White clouds in blue sky. ??? The man on white snow. ??? The man wearing black pants. ??? White snow on top of pole. ??? The man on white surfboard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>?</head><label></label><figDesc>Image Captioner:? A baseball player swinging a bat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>??? The head on man. ??? Standing man on ground. ??? The bat of a baseball player. ??? The man wearing black hat. ??? Large shadow on ground. ??? The people on a baseball court.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>?</head><label></label><figDesc>Image Captioner:? A clock on the side of a road.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>8 .Fig. 6 .</head><label>86</label><figDesc>The man wearing black helmet. 5-6. Sitting woman behind the stand. 1-2. Baseball player wearing helmet 1-4. The man wearing white pants. Scene Graph 7-8. man-wearing-helmet 5-6. man-behind-stand 1-2. man-wearing-helmet 1-4. man-wearing-pant (d) Results of generating "caption graph" from our relational captioniner. In order to compare the diversity of the outputs, we also show the result of the scene graph generator, Neural Motifs<ref type="bibr" target="#b80">[80]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>Sentence based image and region-pair retrieval results on Visual Genome test images. The retrieved results are shown in the ranked order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Smiling woman riding white bike.&lt;laptop&gt;-&lt;on&gt;-&lt;table&gt;Grey laptop on brown desk.&lt;wheel&gt;-&lt;on&gt;-&lt;motorcycle&gt;Black wheel on blue motorcycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 10 .</head><label>10</label><figDesc>Visualization of POS importance transition. Y-axis represents respective representative hidden values of Subject-Predicate-Object LSTMs, and X-axis represents words of each caption in order. subj-pred-obj are color-coded by red, green, and blue colors according to the output of the POS predictor, respectively. Each word in the captions comes from the corresponding LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Comparison of model configurations. '|' and '+' indicate separation and concatenation of input respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>which consists of 85,200 images with 75,456/4,871/4,873 splits for train/validation/test sets respectively. We tokenize the relational mAP (%) Img-Lv. Recall METEOR</figDesc><table><row><cell>Direct Union</cell><cell>-</cell><cell>17.32</cell><cell>11.02</cell></row><row><cell>Union</cell><cell>0.57</cell><cell>25.61</cell><cell>12.28</cell></row><row><cell>Union+Coord.</cell><cell>0.56</cell><cell>27.14</cell><cell>13.71</cell></row><row><cell>Subj+Obj</cell><cell>0.51</cell><cell>28.53</cell><cell>13.32</cell></row><row><cell>Subj+Obj+Coord.</cell><cell>0.57</cell><cell>30.53</cell><cell>14.85</cell></row><row><cell>Subj+Obj+Union</cell><cell>0.59</cell><cell>30.48</cell><cell>15.21</cell></row><row><cell>TSNet (Ours)</cell><cell>0.61</cell><cell>32.36</cell><cell>16.09</cell></row><row><cell>Union (w/MTL)</cell><cell>0.61</cell><cell>26.97</cell><cell>12.75</cell></row><row><cell>Subj+Obj+Coord (w/MTL)</cell><cell>0.63</cell><cell>31.15</cell><cell>15.31</cell></row><row><cell>Subj+Obj+Union (w/MTL)</cell><cell>0.64</cell><cell>31.63</cell><cell>16.63</cell></row><row><cell>Union+Union+Union (w/MTL)</cell><cell>0.58</cell><cell>34.11</cell><cell>14.69</cell></row><row><cell>MTTSNet (Ours)</cell><cell>0.88</cell><cell>34.27</cell><cell>18.73</cell></row><row><cell>MTTSNet (Ours) + REM [68]</cell><cell>1.12</cell><cell>45.96</cell><cell>18.44</cell></row><row><cell>MTTSNet (Ours) + REM (R)</cell><cell>1.48</cell><cell>48.56</cell><cell>19.48</cell></row><row><cell>Neural Motifs [80]</cell><cell>0.25</cell><cell>29.90</cell><cell>15.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Ablation study for the relational dense captioning task on the relational captioning dataset. The second and third row sections (2-7 and 8-12th rows) show the comparison of the baselines with and without the POS classification (w/MTL). In the last row, we show the performance of the state-of-the-art scene graph generator, Neural Motifs<ref type="bibr" target="#b80">[80]</ref>. Union+Union+Union denotes the results of using three LSTMs with only union features as LSTM inputs, (R) indicates ResNet-50<ref type="bibr" target="#b17">[18]</ref> as a backbone network instead of VGG-16.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and described as follows.</figDesc><table><row><cell></cell><cell cols="4">Recall METEOR #Caption Caption/Box</cell></row><row><cell>Image Cap. (Show&amp;Tell) [65]</cell><cell>23.55</cell><cell>8.66</cell><cell>1</cell><cell>N/A</cell></row><row><cell>Image Cap. (Show&amp;Tell) [65]  ?</cell><cell>23.81</cell><cell>9.46</cell><cell>10</cell><cell>N/A</cell></row><row><cell>Image Cap. (SCST) [55]</cell><cell>24.04</cell><cell>14.00</cell><cell>1</cell><cell>N/A</cell></row><row><cell>Image Cap. (SCST) [55]  ?</cell><cell>24.17</cell><cell>13.87</cell><cell>10</cell><cell>N/A</cell></row><row><cell>Image Cap. (RFNet) [23]</cell><cell>24.91</cell><cell>17.78</cell><cell>1</cell><cell>N/A</cell></row><row><cell>Image Cap. (RFNet) [23]  ?</cell><cell>25.26</cell><cell>17.83</cell><cell>10</cell><cell>N/A</cell></row><row><cell>Dense Cap. (DenseCap) [25]</cell><cell>42.63</cell><cell>19.57</cell><cell>9.16</cell><cell>1</cell></row><row><cell>Dense Cap. (TLSTM) [73]</cell><cell>43.15</cell><cell>20.48</cell><cell>9.24</cell><cell>1</cell></row><row><cell>Relational Cap. (Union)</cell><cell>38.88</cell><cell>18.22</cell><cell>85.84</cell><cell>9.18</cell></row><row><cell>Relational Cap. (MTTSNet)</cell><cell>46.78</cell><cell>21.87</cell><cell>89.32</cell><cell>9.36</cell></row><row><cell>Relational Cap. (MTTSNet+REM)</cell><cell>56.52</cell><cell>22.03</cell><cell>80.95</cell><cell>9.24</cell></row><row><cell>Relational Cap. (MTTSNet+REM (R))</cell><cell>59.71</cell><cell>23.27</cell><cell>85.37</cell><cell>9.26</cell></row><row><cell>Relational Cap. (Union) (GT )</cell><cell>41.64</cell><cell>18.90</cell><cell></cell><cell></cell></row><row><cell>Relational Cap. (MTTSNet) (GT )</cell><cell>48.50</cell><cell>21.63</cell><cell>83.44</cell><cell>9.30</cell></row><row><cell cols="2">Relational Cap. (MTTSNet+REM) (GT ) 56.62</cell><cell>22.50</cell><cell></cell><cell></cell></row></table><note>? Direct Union has the same architecture with DenseCap [25], but of which RPN is trained to directly predict union regions. A union region is converted to a 512-dimensional region code, followed by a single LSTM to generate a relational caption.? Union also resembles DenseCap [25] and Direct union, but its RPN predicts individual object regions. The object regions are paired as (subject, object), and then only a union</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Comparisons of the holistic level image captioning. We compare the results of the relational captioners with those of three image captioners</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The roof on yellow train. 5-2. Black wheel on a yellow train. 7-2. The window on a train. 9-2. Off light on yellow train. 2-4. Yellow train on old track. Old man wearing blue hat. 7-3. Red pants on young man. 3-4. Standing boy wearing red hat. 1-2. The man wearing purple hat.</figDesc><table><row><cell>Relational Captioning 1-2. Scene Graph 1-2. building-on-train 5-2. wheel-on-train 7-2. window-on-train 9-2. light-on-train 2-4. train-on-track</cell><cell>Relational Captioning 5-6. Scene Graph 5-6. man-wearing-hat 7-3. pant-on-man 3-4. man-wearing-hat 1-2. man-wearing-hat</cell></row><row><cell>(a)</cell><cell>(b)</cell></row><row><cell>Relational Captioning</cell><cell></cell></row><row><cell>3-4 Green leaf on a tree.</cell><cell></cell></row><row><cell>1-2 White cap on standing man.</cell><cell></cell></row><row><cell>2-6 The man wearing blue pants.</cell><cell></cell></row><row><cell>2-8 Standing man wearing black shirt.</cell><cell></cell></row><row><cell>2-10 The man wearing white hat.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc>Ablation study on the relational dense captioning task with the VRD dataset. Our TSNet and MTTSNet (both with and without +REM) show top performance among the relational captioning models. (R) indicates ResNet-50<ref type="bibr" target="#b17">[18]</ref> as a backbone network instead of VGG-16. In addition, MTTSNet (both with and without +REM) shows favorable performance against the VRD models<ref type="bibr" target="#b45">[45]</ref>,<ref type="bibr" target="#b74">[74]</ref> with a noticeable margin.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 6</head><label>6</label><figDesc>Comparison of our MTTSNet with VRD models on the VRD metrics on the VRD dataset. (R) indicates ResNet-50 [18] as a backbone network instead of VGG-16. Despite the disadvantages for predicting complex captions compared to simple triplets, our MTTSNet and MTTSNet+REM show favorable or comparable performance against the VRD models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 7</head><label>7</label><figDesc>Diversity comparison between image captioning, scene graph generation, dense captioning, and relational captioning frameworks. We measure the number of different words per image (words/img) and the number of words per bounding box (words/box).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. Suppose the number of classes of each term, subj, pred, and obj, is all same as C in the VRD task. Then, the number of all possible combination of VRD is limited to C 3 .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Groupcap: Group-based image captioning with structured relevance and diversity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dealing with missing modalities in the visual question answerdifference prediction task through knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Won</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contrastive learning for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The workshop on statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Kaiming He. Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scene graph generation with external knowledge and image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural module networks for reasoning over text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Explainable neural computation via stack neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent fusion network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tae-Hyun Oh, and In So Kweon. Dense relational captioning: Triple-stream networks for relationship-based captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tae-Hyun Oh, and In So Kweon. Image captioning with very scarce supervised data: Adversarial semi-supervised learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Youngjin Yoon, and In So Kweon. Disjoint multi-task learning between heterogeneous humancentric tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions with action co-occurrence priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Acp++: Action co-occurrence priors for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hierarchical approach for generating descriptive image paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The organization of visually mediated actions in a subject without eye movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><forename type="middle">M</forename><surname>Michael F Land</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><forename type="middle">D</forename><surname>Furneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilchrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocase</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="87" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">VIP-CNN: A visual phrase reasoning convolutional neural network for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Factorizable net: An efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep variation-structured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to assemble neural module tree networks for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Nltk: The natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics</title>
		<meeting>the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Weaklysupervised learning of visual relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive linguistic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attentive relational networks for mapping images to scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengshi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Speaking the same language: Matching machine to human captions by adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to discretely compose reasoning module networks for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Image captioning with compositional neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjiao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Captioning images with diverse objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Exploring context and visual pattern of relationship for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Donghyeon Cho, and In So Kweon. Linknet: Relational embedding for scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Dense captioning with joint inference and visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Shuffle-then-assemble: learning object-agnostic visual relationship features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning to collocate neural modules for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Zoom-net: Mining deep feature interactions for visual relationship recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Visual relationship detection with internal and external linguistic knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Ppr-fcn: Weakly supervised visual relation detection via parallel pairwise r-fcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Relationship proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Towards context-aware interaction recognition for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
