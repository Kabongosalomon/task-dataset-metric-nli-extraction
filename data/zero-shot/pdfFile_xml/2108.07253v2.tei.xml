<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Who&apos;s Waldo? Linking People Across Text and Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><forename type="middle">Yuqing</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Khandelwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
							<email>yoavartzi@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
							<email>snavely@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Averbuch-Elor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Who&apos;s Waldo? Linking People Across Text and Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a task and benchmark dataset for personcentric visual grounding, the problem of linking between people named in a caption and people pictured in an image. In contrast to prior work in visual grounding, which is predominantly object-based, our new task masks out the names of people in captions in order to encourage methods trained on such image-caption pairs to focus on contextual cues, such as the rich interactions between multiple people, rather than learning associations between names and appearances. To facilitate this task, we introduce a new dataset, Who's Waldo, mined automatically from image-caption data on Wikimedia Commons. We propose a Transformer-based method that outperforms several strong baselines on this task, and release our data to the research community to spur work on contextual models that consider both vision and language. Code and data are available at: https://whoswaldo.github.io</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The correspondence between people observed in images and their mentions in text is informed by more than simply their identities and our knowledge of their appearances. Consider the image and caption in <ref type="figure">Figure 1</ref>. We often see such image-caption pairs in newspapers and, as humans, are skilled at recovering associations between the people depicted in images and their references in captions, even if we're unfamiliar with the specific people mentioned. This ability requires complex visual reasoning skills. For the example in <ref type="figure">Figure 1</ref>, we must understand an underlying activity ("passing") and determine who is passing the ball, who is being passed to, and which people in the image are not mentioned at all.</p><p>In this paper, we present a person-centric vision-andlanguage grounding task and benchmark. The general problem of linking between textual descriptions and image regions is known as visual grounding, and is a fundamental * Equal contribution <ref type="figure">Figure 1</ref>. By studying this picture and caption, we can use contextual cues to link between the people referred to in the text and their visual counterparts, even if we are unfamiliar with the specific individuals. This capability requires understanding of a broad set of interactions (e.g. "passing") and expected behaviors (e.g. players pass to their teammates). We propose the task of person-centric visual grounding, where we abstract over identity names (e.g. masking out Sam Schulz and Curtly Hampton with [NAME] tokens) to encourage algorithms to emulate such contextual reasoning. capability in visual semantic tasks with applications including image captioning <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3]</ref>, visual question answering <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref> and instruction following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b6">7]</ref>. Our task and data depart from most existing works along two axes. First, our task abstracts over identity information, instead focusing specifically on the relations and properties specified in images and text. Second, rather than using data annotated by crowd workers, we leverage captions originating from real-life data sources.</p><p>While visual grounding has traditionally centered around localizing objects based on referring expressions, we observe that inferring associations based on expressions in person-centric samples-i.e. people's names-could lead to problematic biases (e.g. with regards to gender). Hence, we formulate the task to use captions that mask out people's names. This allows for an emphasized focus on the context-both in image and text-where the person appears, requiring models to understand complex asymmetric human interactions and expected behaviors. For instance, in the example in <ref type="figure">Figure 1</ref> we might expect a player to pass to someone on their own team.</p><p>To explore this problem, we create Who's Waldo: a collection of nearly 300K images of people paired with textual descriptions and automatically annotated with alignments between mentions of people's names and their corresponding visual regions. Who's Waldo is constructed from the massive public catalog of freely-licensed images and descriptions in Wikimedia Commons. We leverage this unique data source to automatically extract image-text correspondences for over 200K people. We also provide evaluation sets that are validated using Amazon Mechanical Turk and demonstrate that our annotation scheme is highly accurate.</p><p>To link people across text and images, we propose a Transformer-based model, building upon recent work on learning joint contextualized image-text representations. We use similarity measures in the joint embedding space between mentions of people and image regions depicting people to estimate these links. The contextualized Transformerbased representations are particularly suited to handle the masked names, by shifting the reasoning to surrounding contextual cues such as verbs indicating actions and adjectives describing visual qualities. Our results demonstrate that our model effectively distinguishes between different individuals in a wide variety of scenes that capture complex interactions, significantly improving over strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Grounding. The goal of visual grounding is to localize objects in an image given a textual description. Tasks are typically formulated to either recover correspondences between object region proposals and text, or compute attention maps over the whole image. Referring expression comprehension (REC) is a common variant of this problem, where the goal is to identify an image region corresponding to a sentential description (e.g. <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b36">37]</ref>). Sadhu et al. <ref type="bibr" target="#b51">[52]</ref> recently extended this task to a zero-shot setting that also considers expressions with unseen nouns. Qiao et al. <ref type="bibr" target="#b48">[49]</ref> provide a comprehensive survey on REC.</p><p>Still, this line of work has made limited use of descriptions of relationships between objects. The Flickr30K Entities dataset <ref type="bibr" target="#b46">[47]</ref> opened up new avenues for modeling such dependencies by including images, full captions, and ground-truth links between regions and phrases for nearly a hundred object categories. Several methods have since been proposed to visually ground objects from textual descriptions that describe multiple objects <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b37">38]</ref>. A weakly-supervised setting, which assumes that ground truth links between regions and phrases are not available, has also gained attention, with discriminative and contrastive objectives <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b23">24]</ref>, visual and linguistic consistencies <ref type="bibr" target="#b7">[8]</ref> and multilevel aggregation strategies <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b12">13]</ref> used to align the image and language spaces.</p><p>However, most existing tasks in visual grounding permit models to reason over referring expressions directly (allow-ing models to learn priors over different object categories). Our proposed task instead requires models to exclusively reason over context and interactions between objects, as the referring expressions (i.e. names) are masked.</p><p>The creation of most datasets related to visual grounding involves a time-consuming, expensive annotation process that includes both (i) generating referring expressions or full textual descriptions for a given image, and (ii) annotating corresponding regions in the image (e.g. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b67">68]</ref>). We instead construct Who's Waldo through an automatic approach inspired by Conceptual Captions <ref type="bibr" target="#b55">[56]</ref>. While that work uses alt-text image descriptions from HTML (which are noisy and must be aggressively filtered), we use raw descriptions obtained from captions in Wikimedia Commons.</p><p>People-centric Tasks. Person identification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b70">71</ref>] is a task related to the one we propose, and is formulated as a comparison between reference and target images, aiming to determine whether these belong to the same identity. Our work instead focuses on learning a contextual correspondence between image regions and textual captions describing people and their depicted interactions. For ethical reasons (see Ethical Considerations in Section 4), our released dataset does not contain identity information, and thus cannot be easily modified to train such models.</p><p>Another related people-centric task is to select a set of attributes that will generate a description for each person in an image that distinguishes that individual from others in that image <ref type="bibr" target="#b52">[53]</ref>. Finally, Aneja et al. detect out-of-context image and caption pairs, using a dataset collected from news and fact-checking websites. Their data (specifically, the subset of images capturing people) could be used to augment ours.</p><p>Task-agnostic Joint Image-Text Representations. Recent advances have led to a surge of interest in task-agnostic joint visual and textual representations <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref>. Several works, such as LXMERT <ref type="bibr" target="#b58">[59]</ref> and ViLBERT <ref type="bibr" target="#b38">[39]</ref> learn these representations using two-stream transformers <ref type="bibr" target="#b59">[60]</ref> (one per modality). Others, including Vi-sualBERT <ref type="bibr" target="#b33">[34]</ref>, VL-BERT <ref type="bibr" target="#b56">[57]</ref> and UNITER <ref type="bibr" target="#b9">[10]</ref>, use a unified architecture. In our work, we leverage these taskagnostic features to learn to link between the individuals described in the text and their visual counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Person-centric Visual Grounding</head><p>Given an image I with m ? 1 people detections and a corresponding caption x s referring to n ? 1 people (with each person mentioned one or more times), we wish to find a mapping from referred people to visual detections.</p><p>We expect to produce a partial, injective (one-to-one) mapping, since not all referred people will be pictured and no two referred people should map to the same detection. We also find that this mapping is not necessarily surjective (onto), since the image may picture people who are not named in the caption and there could exist detections not mapped to by any referred people.</p><p>In-the-wild captions featuring people often refer to them by name. However, reasoning about visual grounding using actual names of people involves two challenges: the diversity of names creates significant data sparsity, and their surface form (i.e., the text itself) elicits strong biases, e.g., with regard to gender. We therefore abstract over the surface form of the names by replacing each name with the placeholder token [NAME]. This encourages models to focus on the textual context of the names, including adjectives and adverbs that hint at the person's visual appearance and verbs that indicate the action they partake in. In other words, by masking names we seek models that do not memorize what specific people look like, or form stereotypical associations based on specific names, but must instead learn richer contextual cues. As part of our dataset, we provide a mapping from referred people to their respective sets of referring [NAME] tokens.</p><p>While visual grounding has traditionally centered around localization of objects (including unnamed people), we find that visual grounding in the context of named people (which we denote as person-centric) presents additional opportunities. In object-centric visual grounding, referring expressions are not masked, allowing models to also learn by matching images and object classes, rather than entirely from context. Moreover, data for our task (i.e., captioned images of people) is readily available on the web and matches a realistic distribution more closely than object datasets, whose pairs are annotated by workers for the sole purpose of visual grounding tasks.</p><p>Evaluation. Given a mapping produced by an algorithm for an input example, we evaluate by computing accuracy against ground truth links of referred people and detections. This is unlike prior works that extract hundreds of candidate boxes and approximate correct matches using either intersection-over-union ratios or the pointing game, which requires the model to predict a single point per phrase. We also enforce that the people in test images and captions do not appear during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Who's Waldo Dataset</head><p>In this section, we describe Who's Waldo 1 , a new dataset with 270K image-caption pairs, derived from Wikimedia Commons. <ref type="bibr" target="#b1">2</ref> We first describe the process of constructing and annotating this dataset, then present an analysis over dataset statistics. We show samples from our dataset, along with their annotations, in <ref type="figure">Figure 2</ref>.</p><p>Data Collection. Under the broader "People by name" category in Wikimedia Commons there are 407K categories named after people, each with their own hierarchy of subcategories. We refer to this set of people as Wikimedia identities. We identified all sub-categories that are personcentric (e.g. "Barack Obama playing basketball" or "Sally Ride on Challenger in 1983", rather than "John F. Kennedy International Airport") by tokenizing names, matching tokens with regular expressions, and tagging parts of speech. We then downloaded 3.5M images, collated duplicates, and retained references to the Wikimedia identities they originate from. We observe that images originating from an identity are very likely to depict that identity.</p><p>Many images on Wikimedia Commons are also associated with human-provided English captions that describe these images by naming the people present and detailing their settings and interactions. We collected these captions and pre-processed them by pattern matching with regular expressions to remove Wikimedia-specific text structures. We also removed phrases that are variants of "photo by [photographer name]", since photographers are often named in captions but are not pictured in images.</p><p>Detecting People in Images and Captions. To detect bounding boxes for people in images, we used a Switchable Atrous Convolution model with a Cascade R-CNN and ResNet-50 backbone from MMDetection <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b8">9]</ref> trained on COCO <ref type="bibr" target="#b35">[36]</ref>. We subsequently estimated 133 whole-body keypoints using a top-down DarkPose model from MM-Pose <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b10">11]</ref> (trained on COCO <ref type="bibr" target="#b35">[36]</ref> and finetuned on COCO-WholeBody <ref type="bibr" target="#b26">[27]</ref>).</p><p>We applied a pre-trained Punkt sentence tokenizer from NLTK <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b4">5]</ref> to all captions and performed named entity recognition on each sentence using FLAIR <ref type="bibr" target="#b1">[2]</ref> to identify person names. We observe that people can be mentioned more than once in captions and without exact matches (e.g., as "William" and "Bill", or "Barack" and "Obama"). Therefore, we used neural coreference resolution models from Al-lenNLP <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b21">22]</ref> to cluster multiple name entities as individual referred persons.</p><p>Estimating Ground Truth Links. To produce supervision for our task, we automatically generated ground truth links from referred people in captions to detections of people in images. As we will describe, Wikimedia Commons provides reference faces for many referred people. As we can also generate face images for our image detections (via face alignment from estimated pose landmarks), we computed a similarity matrix using FaceNet embeddings <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b53">54]</ref> between reference faces and detected faces. By finding a minimum weight bipartite matching <ref type="bibr" target="#b30">[31]</ref> in this matrix and applying a threshold (set empirically to 0.46), we recovered a partial mapping from referred people to detections.</p><p>We find reference faces for referred people as follows. First, we associate referred people with Wikimedia identities (via the prior coreference resolution step). We also find that many Wikimedia identities have primary images on Wikimedia Commons, which prominently display their faces. We treat these as reference faces of referred people. However, not all referred people have such associations, so our ground truth links are a subset of all links.</p><p>Dataset Size and Splits. The above process yields 271,747 image-caption pairs. <ref type="figure">Figure 3</ref> summarizes the distributions of annotations and identities present in Who's Waldo.</p><p>We split these into 179K training, 6.7K validation, and 6.7K test image-caption pairs. We generate the validation and test splits without overlapping identities from training and by ensuring that examples are challenging and correctly annotated. To do so, we first randomly select 16K identities and produce a validation and test superset from examples containing these identities (observing that additional identities likely appear in these examples as well). We generate the training set from all remaining examples that do not contain any identities in the superset. We then remove all (trivial) examples from the superset with exactly one person detection and one referred person. We manually validate this superset further as described below and divide the resulting examples into validation and test splits.</p><p>Validating Test Images with AMT. While our method approximates ground truth mappings, we want subsets for evaluation that only include correct ground truth links. To that end, we used Amazon Mechanical Turk (AMT) to remove test set examples with incorrect annotations. Given a ground truth link (i.e. identity name and image crop of detected person), we defined the following yes/no AMT task: "Does this [detection crop] contain [identity name]?". For ease of comparison, we also provided workers with a reference image and a link to additional photos for that identity. We assign each ground truth link to two workers. Finally, we select all pairs for which both workers answered "yes".</p><p>We manually inspected 400 responses and-accounting for worker disagreement and error-estimate that our automatic technique was accurate for approximately 95.5% of links in superset examples. However, after removing any examples for which either worker answered "no", we estimate that over 98.5% of links in the retained examples are accurate. Please refer to the supplemental material for additional visualizations over our dataset and generated links.  cal challenges. For example, ImageNet <ref type="bibr" target="#b14">[15]</ref> has been scrutinized based on issues inherited from the "person" category in WordNet <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b64">65]</ref>. Our task and dataset were created with careful attention to ethical questions, which we encountered throughout our work. Access to our dataset will be provided for research purposes only and with restrictions on redistribution. Additionally, as we mask all names in captions, our dataset cannot be easily repurposed for unintended tasks, such as identification of people by name. Due to biases in our data source, we do not consider the data appropriate for developing non-research systems without further processing or augmentation. More details on distribution and intended uses are provided in a supplemental datasheet <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Method</head><p>In this section we present an approach for linking people in text and images. We use a multi-layer Transformer <ref type="bibr" target="#b59">[60]</ref> to learn joint image-text representations such that referred people and their corresponding image regions will be highly similar, while those that do not correspond will be dissimilar. For brevity, we refer to the n names of referred people as names and m image regions of detected people as boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model</head><p>Our method is based on the recent UNITER Transformer model <ref type="bibr" target="#b9">[10]</ref>. As shown in their work, their pretrained model can be leveraged for a wide variety of downstream visionand-language tasks. In this section, we show how UNITER can be modified for our task and fine-tuned on our dataset. An overview of our approach is shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>We extract visual features for each person detection p us-ing a fully-convolutional variant of Faster R-CNN <ref type="bibr" target="#b2">[3]</ref>. Visual features are concatenated with encodings of their spatial coordinates, 3 yielding spatial-visual features f (p). We tokenize words into WordPieces <ref type="bibr" target="#b62">[63]</ref>. In accordance with our task, names are symbolized by [NAME] tokens. For each sub-word w, we extract features g(w) that are composed of a token embedding and position embedding. We feed these spatial-visual and textual features into a Transformer model that uses self-attention layers to learn a contextual representation and captures a more contextspecific representation in upper-hidden layers <ref type="bibr" target="#b17">[18]</ref>. We denote the final hidden layer of spatial-visual features as P k and of textual features as X l , where P k , X l ? R 768 .</p><p>From these contextualized representations, we construct a box-name similarity matrix S (top-right of <ref type="figure" target="#fig_2">Figure 4</ref>). This matrix measures cosine similarities S i,j between the i-th name and the j-th box:</p><formula xml:id="formula_0">S i,j = P T jX i ?P j ? 2 ?X i ? 2 ,<label>(1)</label></formula><p>whereX i is an embedding averaged over all [NAME] tokens for mentions of the i-th referred person in a caption. During inference, for each referred person, we select its corresponding detection as the most similar box in S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Learning</head><p>To train our model, we propose the following loss terms that operate on the similarity matrix S: (1) box-name matching losses defined within and across images and <ref type="formula" target="#formula_1">(2)</ref> an unlinked box classification loss. Box-Name Matching Losses. We define box-name matching losses within images (supervising estimated correspondences with ground truth links) and across images (using a discriminative objective over image-caption pairs).</p><p>We compute the estimated probability for a ground truth link (i, j) over different boxes (p = Softmax(S i,: ) j ) and also over different names (q = Softmax(S :,j ) i ) in its corresponding image-caption pair. We minimize cross-entropy losses over these for all ground truth links L in a batch:</p><formula xml:id="formula_1">L intra = ? 1 |L| l?L log p (l) + log q (l)<label>(2)</label></formula><p>Because we would like to leverage additional images during training (i.e., those without ground truth links), we also compute a matching loss across images containing a single box and name (which are likely to represent the same person). We sample positive and negative box-name pairs. Negative pairs are generated by replacing the box with one from another image (and of a different person). We minimize a binary cross-entropy loss L inter over these pairs.</p><p>Unlinked Box Classification Loss. As not all people depicted in an image are referred to in its caption, we augment S with a constant null nameX ? . We formulate a binary cross-entropy classification loss over similarities between boxes andX ? . We process these similarities S i=?,j through a sigmoid function to obtain normalized values. Boxes linked to names are considered negative matches (i.e., these should yield low similarities withX ? ). We cannot assume all other boxes are positive matches (i.e., should yield high similarities withX ? ) as we are only provided with partial ground truth correspondences from the algorithm in Section 4. Instead, we select unlinked boxes that are (1) insignificant compared to other boxes in the image and (2) blurry. Both are measured using the a detected person's face (computed from whole body landmarks): a face image f is considered insignificant if Area(f ) &lt; 0.6 ? Area(f largest ) and blurry if Var(?(f )) &lt; 50 <ref type="bibr" target="#b44">[45]</ref>, where f largest is the largest face in the image and ? is the Laplace operator. <ref type="figure" target="#fig_3">Figure 5</ref> shows several images from our dataset with unlinked boxes in red. We minimize a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training Data Accuracy binary cross-entropy loss L ? over images containing such positive and negative matches.</p><p>This loss, in addition to providing us a means of directly estimating whether or not a given box is referred to in a caption, also implicitly encourages the contextualized representations of insignificant and blurry faces to be distinguishable from others. As we show in our results, this improves the accuracy of identifying referred people, allowing the model to focus on more relevant boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Evaluation</head><p>We compare our model to other visual grounding methods trained on a variety of datasets. We study four key questions: How well do previous methods for visual grounding perform on our proposed task? To what extent is our model reasoning over complex multimodal signals? What is the impact of our design choices? And, what has our model learned? We also present qualitative results ( <ref type="figure">Figure 6</ref> and supplemental material), which highlight the complexity and unique challenges of our proposed task. Daniel Sch?tz (#20) behind goalkeeper Domenik Schierl .</p><p>Kathryn Hire, an astronaut and Navy reserve component Sailor assigned to the Office of Naval Research, presents items she took to space to Rear Adm. Nevin Carr . <ref type="figure">Figure 6</ref>. Box-name correspondences predicted by our model. We show predicted entities on top of the their associated box (in white). Ground truth links are denoted by matching colors. Please refer to the supplemental material for additional qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Comparison to Prior Work</head><p>We evaluate several recent visual grounding models on the Who's Waldo test set: a weakly-supervised framework by Gupta et al. <ref type="bibr" target="#b23">[24]</ref>, a supervised neural chain conditional random field that captures entity dependencies (SL-CCRF) <ref type="bibr" target="#b37">[38]</ref>, and a supervised network that combines attention from separate modules (MAttNet) <ref type="bibr" target="#b66">[67]</ref>. We also evaluate UNITER <ref type="bibr" target="#b9">[10]</ref>, a pretrained multi-task vision-andlanguage framework, which our model is based on. <ref type="table">Table 1</ref> shows test set accuracies for our approach and for existing methods trained on different datasets. We report 95% binomial proportion confidence intervals (Wilson score intervals) with these accuracies. For existing models, we vary how names are provided during inference because these models are not automatically compatible with our placeholder [NAME] token: (a) unmodified full names, (b) random popular names, or (c) a constant "person" stringe.g., "Harry met Sally" is modified to "person met person".</p><p>We also evaluate several heuristics that illustrate the challenges and biases in our data <ref type="table">(Table 1)</ref>, such as a potential left-to-right bias for named individuals. In particular, we order the names in the caption from left to right, and pair them with detections sorted by (a) decreasing area (Big?Small), (b) left-to-right upper-left coordinates (L?R (All)), or (c) left-to-right upper-left coordinates with only the largest d detections (L?R (Largest)). We set d = max(m, n) for m detections and n names. We also compare to random guessing. We observe that these heuristics yield non-trivial, and even strong, performances. This could be because realistic captions tend to follow a left-toright ordering (especially for posed people-but see <ref type="figure">Figure 6</ref> for counterexamples) and filtering by detection size can remove unreferred people. However, even the strongest heuristic leaves much room for improvement. These heuris-  <ref type="table">Table 2</ref>. Ablation study, evaluating the effect of using different input features, loss terms and the impact using a pretrained model. tics are also useful to frame the performance of pretrained visual grounding models. Supervised models (SL-CCRF and MAttNet) perform similarly to Big?Small, illustrating that these models may be utilizing size-related cuesespecially MAttNet, which only processes names and not full sentences. We show qualitative results for all baselines in the supplemental material. <ref type="table">Table 2</ref> shows ablation results. We train models using only a subset of features by ablating (i) visual features: set instead to a fixed representation, averaged over 1000 random detections; (ii) spatial features: fixed at image center coordinates; (iii) textual embeddings: with all words masked out, retaining only position features and special [NAME] tokens; and (iv) textual and visual embeddings: retaining only spatial features. The impact of each input modality is significant, with performance dropping by 5.5% for (ii) and 12.2% for (iii). While these ablations significantly limit the information available for this task, our model performs much better than random guessing in all cases, suggesting that it learns some data biases. Both (i) and (iii) are capable of learning a left-to-right association. Indeed, their correct matches significantly overlap with those of the "L?R (Largest)" heuristics, by 81.7% for (i) and 82.4% for (iii). Finally, from (iv) we infer that spatial features alone are not enough for learning such similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation study</head><p>We also quantify the importance of each proposed objective. Training without estimated correspondences (i.e., L intra ) yields the most significant drop in performance, resulting in nearly random guessing. This illustrates the importance of supervised data for our task. Ablating the other losses (L inter and L ? ) degrades performance by only 1.7%. The relatively small impact of L inter highlights the importance of having many samples that capture interactions between multiple people, rather than samples with just one detection and referred person.</p><p>We also report the performance obtained by training our full model from scratch, without UNITER's pretrained weights. This leads to a large drop in performance (&gt; 13%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Analysis of results</head><p>We analyze the performance of our model over different test subsets to better understand what the model is learning. We observe that our model is more robust to a larger number of faces compared to L?R (Largest). For instance, in the case of only one referred person in an image, our model retains high performance over an increasing number of faces, while the heuristic drops by almost 20% (from an accuracy of 84.5% for two detected people down to an accuracy of 67.6% for four or more detected people). We further demonstrate this breakdown in the supplemental material. Another subset we consider is an interactive subset of test samples (i.e. those with at least two detections and referred people and a verb in their caption). This potentially more challenging subset constitutes nearly one-third of our test set. Our model's performance drops to 52.1%, while the baseline performance drops to 45.0%.</p><p>We also analyzed whether having multiple mentions of a person's name affects performance. Approximately 3% of referred people in the test set are mentioned multiple times in the caption. For these identities, our model has a modest improvement of 2.1% if provided additional mentions. This illustrates that our model can leverage additional information from co-occurrences in a caption to some extent. Finally, we also analyze the performance of our method over several categories of identity occupations in the supplemental material, as we observe that these correlate well with different situations captured by our dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Limitations</head><p>The complexity of certain interactions, such as in sports games where players compete closely together, poses challenges, not only to our model, but also to the person detector and our method for estimating ground truth links. <ref type="figure" target="#fig_5">Figure 7</ref> (left) demonstrates an example of a basketball game where the bodies of players overlap, thus some are not detected. The example on the right illustrates a failure of our model, where the interaction "blocks" is not correctly interpreted.</p><p>Further, some captions are insufficient to produce meaningful links. For example, in <ref type="figure" target="#fig_5">Figure 7</ref> (center), after replacing the names "Joe Jonas" and "Demi Lovato" with [NAME], it is impossible to tell which performer each corresponds to. Hence our model resorts to a simple left-toright heuristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We present a task, dataset, and method for linking people across images and text. By masking out names of people, we force methods to not memorize the appearance of specific individuals, but to understand contextual cues and interactions between multiple people. Our approach shows encouraging performance on this task, but also indicates that the underlying task is very challenging and, as such, there is ample room for improvement via future methods that leverage our data. In particular, the performance of all methods drops given examples involving actions (as indicated by captions with verbs) and as the number of people referred to in a caption grows, indicating unresolved challenges in scaling to complex scenarios.  <ref type="table">Table 3</ref>. Free licenses for images in our dataset (organized by freedom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Visualizations and Details</head><p>Please refer to the accompanying dataset_ examples.html for samples from our Who's Waldo dataset.</p><p>Our dataset has 215K ground truth links in total (for 193K images). Our dataset originates from over 400K Wikimedia identities and has ground truth links for 93K.</p><p>All images originate from Wikimedia Commons under free licenses. We group the licenses by freedom 4 as in <ref type="table">Table 3</ref>.</p><p>We include a word cloud of the verbs present in our dataset in <ref type="figure">Figure 9</ref>.</p><p>Our dataset contains images for at least 263K male and 70K female Wikimedia identities (these are identities we have labels for). We acknowledge this imbalance in ratio and attribute this to existing biases in our data source. However, our dataset is large enough that one could sample a more balanced subset. Our dataset does present diversity in the occupations of identities, as can be seen in <ref type="figure">Figure 10</ref>.</p><p>We show distributions of image resolutions in <ref type="figure">Figure 12</ref> and sizes of detection boxes (relative to image sizes) in <ref type="figure">Fig-Figure 9</ref>. Visualization of verbs appearing in our dataset's captions. Larger font size correspond to verbs that appear more frequently in the dataset. ure 13. We show a distribution of head poses in our dataset in <ref type="figure">Figure 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Dataset Cleaning</head><p>We filter our data by removing all examples in which there are no people detected in an image or no people referred to in captions. We remove examples with captions that don't contain verbs or words other than names and stop words (i.e. insubstantial captions). We further cleanse this data by removing images taken before 1990 (according to metadata) as we found this was a significant source of noise. We also found the presence of "cropped" versions of images that can be detected directly from file names containing the word "cropped", which usually only picture one person but have captions implying the presence of multiple, and also removed these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Training details</head><p>We download the pretrained UNITER [10] model (UNITER-base). We use the "bert-base-cased" vocabulary from pytorch-transformers and add the [NAME] token. Following their implementation 5 , we define two training tasks that use two non-overlapping subsets of our dataset: <ref type="figure">(1, 1)</ref>, containing images with exactly one referred person and one person box detected in the image, and (m, n), containing all other images (i.e. more than one referred person or more than one box).</p><p>The first task, denoted as Task-1-1, trains on the (1, 1) subset using the L inter objective, with 0.5 probability of negative sampled image-caption pairs. The second task, denoted as Task-M-N, trains on the (m, n) subset using the L intra and L ? objectives. Furthermore, regarding L intra , we note that this loss a sum over two cross-entropy losses, one over different boxes in the image and the other over different names in the caption. Task-1-1 and Task-M-N are trained using a 1 : 2 ratio. Width Height <ref type="figure">Figure 13</ref>. Distribution of detection sizes in our dataset.</p><p>We train 50,000 steps, validating performance over the validation set every 500 steps, with batch size of 1024. The max caption length we consider is 60 tokens, and the number of bounding boxes we consider is between 1 and 100, inclusive. Image-caption pairs not within these boundaries <ref type="bibr">Figure 14</ref>. For each referred person associated with a "primary" image on Wikimedia Commons (right), we compute face dissimilarities between the face in the "primary" image and all detected faces. By finding a minimum weight bipartite matching (over all referred people), we recover a partial matching from referred people to detections (for simplicity, we only show these dissimilarities for a single referred person and for a subset of faces in the image). The estimated link is shown in blue. are filtered out during training. We use a learning rate of 5e ? 5, weight decay of 0.01 and dropout 0.1, consistent with the default UNITER parameters (all other parameters are also set according to their default values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baselines</head><p>Next we provide more details on how we obtain the reported scores on the pretrained models we evaluate on the WikiPeople test set.</p><p>Gupta et al. <ref type="bibr" target="#b23">[24]</ref>. We download their two pretrained models, trained on either COCO <ref type="bibr" target="#b35">[36]</ref> or Flickr30 Entities <ref type="bibr" target="#b46">[47]</ref>, from their official code repository <ref type="bibr" target="#b5">6</ref> . Following their implementation, visual features are extracted using the Bottom-Up Attention model <ref type="bibr" target="#b2">[3]</ref> yielding a 2048-d visual representation. A pretrained BERT <ref type="bibr" target="#b15">[16]</ref> model is used to extract 768-d contextualized word representations. We follow their evaluation protocol and compute a phrase-level attention score for each box by taking the maximum attention score assigned to the box by any of the tokens in the name. The boxes are then ranked according to this phrase level score, with the maximum scoring box selected as the corresponding box. This top-scoring box is compared with the ground-truth box.</p><p>SL-CCRF <ref type="bibr" target="#b36">[37]</ref>. We download the pretrained "Soft-Label Chain CRF Model" from their official code repository 7 , which yields the highest performance among their available models. Following their implementation, visual features are extracted using the Bottom-Up Attention model <ref type="bibr" target="#b2">[3]</ref> yielding a 2048-d visual representation. We use their all default parameters, as follows: 1024-d contextualized word embeddings, the maximum number of mentions is set to 25, and a 5-d spatial feature is concatenated with the visual features. The number of regions proposals are according to the number of detected people boxes. However, as their model also includes a regression bounding box loss, their final predictions aren't entirely aligned with the input bounding boxes. We account for that gap in the evaluation, by considering boxes with IoU ? 0.5.</p><p>MAttNet <ref type="bibr" target="#b66">[67]</ref>. We downloaded a model from the official repository <ref type="bibr" target="#b7">8</ref> that was pretrained on the RefCOCOg dataset <ref type="bibr" target="#b41">[42]</ref>. Following their implementation, visual features are extracted using a modified implementation of Mask R-CNN <ref type="bibr" target="#b24">[25]</ref>, as specified by the authors <ref type="bibr" target="#b66">[67]</ref>. However, we provide our own bounding boxes and compute Faster R-CNN region features <ref type="bibr" target="#b49">[50]</ref> over these, instead of using their proposals. A Language Attention Network with bidirectional LSTMs (as specified by MAttNet <ref type="bibr" target="#b66">[67]</ref>) is used to extract phrase embeddings. We use these modules to predict a detection for each individual referring expression (i.e. a person's name).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Results and Ablations</head><p>We report performance obtained on all three baselines while training on our data in <ref type="table">Table 4</ref>. The low performance obtained on the baselines is not surprising as (1) weakly supervised techniques (such as Gupta et al. <ref type="bibr" target="#b23">[24]</ref>) do not have access to ground truth supervision-in our ablations this similarly results in a significant performance drop; (2) 6 https://github.com/BigRedT/info-ground 7 https://github.com/liujch1998/SoftLabelCCRF 8 https://github.com/lichengunc/MAttNet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head><p>Gupta et al. <ref type="bibr" target="#b23">[24]</ref> 31.78 SL-CCRF <ref type="bibr" target="#b37">[38]</ref> 30.07 MAttNet <ref type="bibr" target="#b66">[67]</ref> 27.53 <ref type="table">Table 4</ref>. Performance obtained on the baselines trained on our data. As further detailed in the text, these baselines cannot be naively adapted for our task. Ours (full) 63.5 61.9 <ref type="table">Table 5</ref>. Ablation study, evaluating the effect of using a bipartite matching algorithm during inference (second column) and using an additional optimal transport loss (second to last row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>phrase grounding techniques (such as MAttNet <ref type="bibr" target="#b66">[67]</ref>) only process the phrase describing the region (which would be masked out in our case); and (3) SL-CCRF also processes the masked out phrases, along with dependencies between string-adjacent phrases (which evidently are not enough on their own for the model to learn meaningful grounding). All results reported in the paper are obtained by selecting, for each referred person, the most similar box according to S. In <ref type="table">Table 5</ref>, we also report performance by performing a minimum weight bipartite matching <ref type="bibr" target="#b30">[31]</ref> over the similarity matrix, thus producing a natural one-to-one mapping. As illustrated in the table, this yields a decrease in performance of approximately 1%. We also train a model with an additional (unsupervised) optimal transport loss, which was proposed for pretraining the UNITER [10] model, as it encourages sparsity, and could potentially improve alignments between words and regions in the image (or names and people's boxes in our case). Results show that adding this loss on top of S does not yield an improvement in performance (and even slightly degrades our full model's performance). This suggests that robust alignments are achieved from the training supervision directly, without need for additional regularization. <ref type="figure" target="#fig_3">Figure 15</ref> illustrates the distribution of samples and performance breakdowns for L?R (Largest) and our model over the numbers of referred people in a caption (n) and people detected in an image (m). We compute average ac- curacies over all relevant test subset images. As illustrated in the figure, the heuristic surpasses our model over only two subsets-(m = 2, n = 1) and (m ? 4, n ? 4), given m detections and n referred people-and performs worse in all other subsets. We find that occupations correlate with different situations-images featuring athletes, for instance, have different properties from those featuring singers. We observe that model performance varies somewhat across different occupation types. For instance, considering only the interactive subset of test samples, accuracy on people with athletic occupations (association football player, basketball player, etc.) is lower than accuracy for politicians or performers (actor, model, musician, etc.), while their distribution in the training set is similar (athletes, politicians, and performers are each captured by 10-13% of the interactive training set). A potential explanation is that interactions within sportsthemed images are broader and more complex than in other categories.</p><p>We also observe that over the full set test, performance over politician samples is significantly lower, and this is also reflected in a lower left-to-right ordering accuracy. A visual analysis reveals that these samples are indeed more challenging, as in many cases the captions mostly mention notable individuals regardless of the visual arrangement of the captured individuals.</p><p>Finally, we experiment with training models using several forms of standard augmentation techniques. Results are reported in <ref type="table">Table 7</ref>. Note that the nature of our dataset and task renders some augmentations more sensible than others.</p><p>In particular, a model trained with random horizontal flipping yields significantly lower performance. This is likely due to the inherent left-to-right ordering in the images and captions, as some captions in our dataset either explicit annotate people with "(left)" and "(right)", or implicitly mention people in the left-to-right order they appear in the image. Other augmentations, such as translating all bounding   The photo shows Kristijan Dobras (SC Wiener Neustadt, blue shirt) and Philipp Huspek (SV Gr?dig, white shirt).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Qualitative Results</head><p>President Putin presenting the banner of the Navy to its Commander-in-Chief Admiral Vladimir Kuroyedov.</p><p>Astronaut Terrence W. Wilcutt , STS-68 pilot, goes over his notes. Checking the notes is Alan M. Rochford, suit expert. <ref type="figure" target="#fig_9">Figure 16</ref>. Additional box-name correspondences predicted by our model. We show predicted entities on top of the their associated box (in white). Ground truth links are denoted by matching colors. 16 SL-CCRF <ref type="bibr" target="#b37">[38]</ref> MAttNet <ref type="bibr" target="#b66">[67]</ref> UNITER <ref type="bibr" target="#b9">[10]</ref> Ours Commandant of the U.S. Marine Corps Gen. James F. Amos , left, participates in a gift exchange with Commandant General of the British Royal Marines Maj. Gen. Ed Davis.</p><p>Caleb Marchbank kicking away from Matt de Boer during the AFL round twelve match between Carlton and Greater Western Sydney on 11 June 2017 at Etihad Stadium.</p><p>Justise Winslow of the Miami Heat defending LeBron James. <ref type="figure" target="#fig_5">Figure 17</ref>. Comparing against supervised visual grounding techniques, SL-CCRF <ref type="bibr" target="#b37">[38]</ref> and MAttNet <ref type="bibr" target="#b66">[67]</ref>, and the pretrained UNITER <ref type="bibr" target="#b9">[10]</ref> model. We show predicted entities on top of the their associated box (in white). Ground truth links are denoted by matching colors. For SL-CCRF <ref type="bibr" target="#b37">[38]</ref>, as their model incorporates a regression loss that modifies the input boxes, we only show the predicted boxes. In both SL-CCRF <ref type="bibr" target="#b37">[38]</ref> and MAttNet <ref type="bibr" target="#b66">[67]</ref>, errors are attributed to selecting the same box for multiple referred people. It should be noted that this is not always the case, and from further visual inspection, in many cases these models are capable of selecting multiple boxes. We can see that the pretrained UNITER model provides unique assignments for all three examples, possibly due to the optimal transport loss they propose to encourage robust word-region alignments. The selected boxes, however, are only accurate in the middle example (and partially accurate in the leftmost example). (Flickr30K Entities) Gupta et al. Justise Winslow of the Miami Heat defending LeBron James. <ref type="figure" target="#fig_6">Figure 18</ref>. Comparing against the weakly-supervised visual grounding technique proposed by Gupta et al. <ref type="bibr" target="#b23">[24]</ref>. We evaluate on both of their pretrained models, trained on Flickr30K Entities <ref type="bibr" target="#b46">[47]</ref> (top row) and COCO <ref type="bibr" target="#b35">[36]</ref> (second row). We show predicted entities on top of the their associated box (in white). Ground truth links are denoted by matching colors. Errors are attributed to either selecting the same box for multiple referred people (e.g. rightmost example), or selecting irrelevant boxes, such as the yellow box in the middle image, top row, or the orange box in the left image, second row.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Left: " Figure 2 .Figure 3 .</head><label>"23</label><figDesc>Justyna Kowalczyk , Kikkan Randall and Ingvild Flugstad ?stberg at the Royal Palace Sprint, part of the FIS World Cup 2012/2013, in Stockholm on March 20, 2013. Kikkan Randall won the sprint cup." Center: " Cheick Diallo blocks Allonzo Trier (#20) in front of Luke Kennard (#5) and Carlon Bragg (#31) in the 2015 McDonald's All-American Boys Game." Right: "At the Gagarin Cosmonaut Training Center in Star City, Russia, Expedition 41/42 backup crew members Scott Kelly of NASA (left), Gennady Padalka of the Russian Federal Space Agency (Roscosmos, center) and Mikhail Kornienko of Roscosmos (right) clasp hands as they pose for pictures in front of a Soyuz simulator at the start of final qualifications." Samples from Who's Waldo, showing detected named entities in bold and entities linked with image regions in unique colors, corresponding to the boxes on the images. Unmatched boxes and entities are colored in black. Mentions per Name Links per Sample Distribution of Samples Caption Length (words) Who's Waldo statistics, including the number of number of mentions (occurrences) for referred people in captions, ground truth box-name links per sample, distribution of samples and caption length (by words).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Ethical Considerations People-centric datasets pose ethi-Caleb Marchbank kicking away from Matt de Boer during the AFL round twelve match between Carlton and Greater Western Sydney on 11 June 2017 at Etihad Stadium in Melbourne, Victoria.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Overview of our approach. Features are extracted from image regions and words and combined with a Transformer to learn similarities between people detected in the image (boxes A-C, colored in unique colors) and names mentioned in the caption (Caleb Marchbank and Matt de Boer in the example above). Correspondences are depicted by matching colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Selecting unlinked boxes. We select small and blurry boxes (colored in red) for our proposed classification loss, encouraging the model to focus on larger (and less blurry) people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>U.S. Air Force Colonel Clay Garrison goes over some final instructions with U.S. Congressman David Valadao prior to take-off from the Fresno Air National Guard Base May 27, 2015.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Left: "Butler's Andrew Smith and Siena's Ryan Rossiter both try to anticipate the rebound, as Butler's Shawn Vanzant closes in from behind." Center: " Joe Jonas and Demi Lovato performing in the Jonas Brothers Live In Concert." Right:" Markus Heikkinen blocks Freddy Guar?n ." Examples our model predicted incorrectly, showing detected named entities in bold and entities linked with image regions in unique colors, corresponding to the boxes on the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Distribution of copyright license groups for images in Who's Waldo. Agreement Copyrighted free use, No restrictions, CC0 Public Domain, Public domain, WTFPL Permissive MIT, BSD, CC BY 1.0, CC BY 2.0, CC BY 2.5, CC BY 3.0, CC BY 4.0, Attribution, OGDL, Licence Ouverte, KOGL Type 1, OGL-C 2.0, OSPL, GODL-India, Beerware Copyleft GPL, GPLv2, GPLv3, LGPL, CC SA 1.0, CC BY-SA 2.0, CC BY-SA 2.5, CC BY-SA 3.0, CC BY-SA 4.0, Nagi BY SA, GFDL 1.1, GFDL 1.2, GFDL 1.3, GFDL, ODbL, OGL, OGL 2, OGL 3, FAL, CeCILL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .Figure 11 .Figure 12 .</head><label>101112</label><figDesc>Distribution of occupations for Wikimedia identities in our dataset.YawPitch Roll Distribution of faces by degree of pose (head orientation) from a random subset of 50,000 detections. Note: yaw is the primary indicator of diversity in pose, as pitch and roll are limited by physical constraints for head rotation.WidthHeight Distribution of image resolutions in our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 15 .</head><label>15</label><figDesc>Accuracy breakdown by number of referred people and detected faces for our model and the L?R (Largest) baseline. The test set sample distribution is illustrated on the left (no images with just a single detection and referred person are included in our evaluation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 16 shows</head><label>16</label><figDesc>additional visualizations of our model's predictions for samples in our test set. Figure 17 and Figure 18 respectively show results obtained with prior supervised and weakly-supervised grounding models. As illustrated in the figures, prior visual grounding works struggle in correctly linking people across images and text for these challenging examples, which cover various interactions between multiple people. Errors can be attributed to selecting a single box for all referred people, or selecting (smaller) boxes that are unreferenced to in the caption. President Bush and Secretary for Housing and Urban Development Martinez, far right, talk with new friends during a break from their house-building efforts at the Waco, Texas, location of Habitat for Humanity's "World Leaders Build" construction drive August 8, 2001. Mohamed Bamba dunks in front of Collin Sexton at the McDonald's All-American Boys Game. Yoann Huget out run Julien Arias to score his second try of the match during Stade toulousain vs Stade fran?ais Paris, March 24th, 2012. President Bush meets with Secretary of Education Rod Paige , left, and Senator Edward Kennedy August 2, 2001, to discuss the education reforms for the country.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Markelle Fultz shoots over Kyle Guy at the McDonald's All-American Boys Game.The photo shows David Alaba (Austria), Gunnar Nielsen (Faroe Islands) Zlatko Junuzovi? (Austria).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>the U.S. Marine Corps Gen. James F. Amos , left, participates in a gift exchange with Commandant General of the British Royal Marines Maj. Gen. Ed Davis. Caleb Marchbank kicking away from Matt de Boer during the AFL round twelve match between Carlton and Greater Western Sydney on 11 June 2017 at Etihad Stadium.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Full Names Gupta et al. [24] COCO 36.9 ? 1.04 Gupta et al. [24] Flickr30K Entities 39.3 ? 1.05 SL-CCRF [38] Flickr30K Entities 43.5 ? 1.06</figDesc><table><row><cell>MAttNet [67]</cell><cell>RefCOCOg</cell><cell>43.6 ? 1.06</cell></row><row><cell>UNITER [10]</cell><cell cols="2">Multiple [36, 30, 44, 56] 36.3 ? 1.03</cell></row><row><cell>Random</cell><cell></cell><cell></cell></row><row><cell cols="2">Gupta et al. [24] COCO</cell><cell>39.3 ? 1.05</cell></row><row><cell cols="2">Gupta et al. [24] Flickr30K Entities</cell><cell>41.1 ? 1.06</cell></row><row><cell cols="2">SL-CCRF [38] Flickr30K Entities</cell><cell>44.1 ? 1.07</cell></row><row><cell>MAttNet [67]</cell><cell>RefCOCOg</cell><cell>44.0 ? 1.07</cell></row><row><cell>UNITER [10]</cell><cell cols="2">Multiple [36, 30, 44, 56] 38.4 ? 1.04</cell></row><row><cell>Constant</cell><cell></cell><cell></cell></row><row><cell cols="2">Gupta et al. [24] COCO</cell><cell>35.6 ? 1.03</cell></row><row><cell cols="2">Gupta et al. [24] Flickr30K Entities</cell><cell>38.2 ? 1.04</cell></row><row><cell cols="2">SL-CCRF [38] Flickr30K Entities</cell><cell>46.4 ? 1.07</cell></row><row><cell>MAttNet [67]</cell><cell>RefCOCOg</cell><cell>24.1 ? 0.92</cell></row><row><cell>UNITER [10]</cell><cell cols="2">Multiple [36, 30, 44, 56] 34.2 ? 1.02</cell></row><row><cell>Random</cell><cell>-</cell><cell>30.9 ? 0.99</cell></row><row><cell>Big?Small</cell><cell>-</cell><cell>48.2 ? 1.07</cell></row><row><cell>L?R (All)</cell><cell>-</cell><cell>38.4 ? 1.04</cell></row><row><cell cols="2">L?R (Largest) -</cell><cell>57.7 ? 1.06</cell></row><row><cell>Ours</cell><cell>Who's Waldo</cell><cell>63.5 ? 1.03</cell></row></table><note>Table 1. Evaluation on the Who's Waldo test set. We compare against prior grounding methods using multiple configurations, varying according to how names are processed. We also compare to several simple baselines, detailed in the text.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Analyzing model performance by identity occupation for the interactive subset and for all data samples. Test accuracy for the strongest baseline and for our model is reported for samples belonging to the occupation categories specified on top. Evaluating the effect of using standard data augmentation techniques during training. boxes within the image or performing random color jittering on the images, yields comparable performance.</figDesc><table><row><cell>Set</cell><cell cols="3">Politicians Athletes Performers</cell></row><row><cell>Interactive</cell><cell></cell><cell></cell><cell></cell></row><row><cell>L?R (Largest)</cell><cell>47.1</cell><cell>43.0</cell><cell>49.8</cell></row><row><cell>Ours</cell><cell>52.5</cell><cell>51.1</cell><cell>54.9</cell></row><row><cell>All</cell><cell></cell><cell></cell><cell></cell></row><row><cell>L?R (Largest)</cell><cell>52.4</cell><cell>70.6</cell><cell>67.4</cell></row><row><cell>Ours</cell><cell>54.8</cell><cell>76.3</cell><cell>71.2</cell></row><row><cell cols="2">Augmentation</cell><cell>Accuracy</cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell>63.5</cell><cell></cell></row><row><cell cols="2">w/ horizontal flips</cell><cell>53.8</cell><cell></cell></row><row><cell cols="2">w/ translations</cell><cell>62.0</cell><cell></cell></row><row><cell cols="2">w/ color jittering</cell><cell>63.0</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Icon created by Stefan Spieler from the Noun Project 2 https://commons.wikimedia.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Following<ref type="bibr" target="#b9">[10]</ref>, these are: [x 1 , y 1 , x 2 , y 2 , w, h, w ? h].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://en.wikipedia.org/wiki/Free_license#By_ freedom</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/ChenRocks/UNITER</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-level multimodal common semantic space for image-phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surabhi</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12476" to="12486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flair: An easy-to-use framework for state-of-the-art nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nltk</surname></persName>
		</author>
		<title level="m">The natural language toolkit. ArXiv, cs.CL/0205028</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person identification using multiple cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Brunelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Falavigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="955" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Touchdown: Natural language navigation and spatial reasoning in visual street environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge aided consistency for weakly supervised phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4042" to="4050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UNITER: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Openmmlab pose estimation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mmpose</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmpose" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Excavating ai: the politics of images in machine learning training sets. Excavating AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Paglen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Align2ground: Weakly supervised phrase grounding guided by image-caption alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karuna</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2601" to="2610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual grounding via accumulated attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7746" to="7755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finding beans in burgers: Deep semanticvisual embedding with localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engilberge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Chevallier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3984" to="3993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00512</idno>
		<title level="m">How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Allennlp: A deep semantic natural language processing platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1803.07640</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Datasheets for datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Briana</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Contrastive learning for weakly supervised phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09920</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="386" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Whole-body human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised multilingual sentence boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Strunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="485" to="525" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards person identification and re-identification with attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1707.07045</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to assemble neural module tree networks for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4673" to="4682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Phrase grounding by soft-label chain conditional random field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5112" to="5122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10437" to="10446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana-Maria</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mapping instructions and visual observations to actions with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1004" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Analysis of focus measure operators for shape-from-focus. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Said</forename><surname>Pertuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domenec</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Garcia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1415" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive image-language cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02334</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Referring expression comprehension: A survey of methods and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Zero-shot grounding of objects from natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Sadhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">It&apos;s not polite to point: Describing people with uncertain attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3089" to="3096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Face recognition using tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sandberg</surname></persName>
		</author>
		<ptr target="https://github.com/davidsandberg/facenet" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Structured matching for phrase localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Azab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriyuki</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="696" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Maf: Multimodal alignment framework for weakly-supervised phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05379</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Weaklysupervised visual grounding of phrases with linguistic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5945" to="5954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klint</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="547" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1307" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7093" to="7102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Weakly supervised phrase localization with multi-scale anchored transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5696" to="5705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Unified vision-language pretraining for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13041" to="13049" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
