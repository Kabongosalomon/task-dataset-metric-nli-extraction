<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULAN: Multitask Universal Lesion Analysis Network for Joint Lesion Detection, Tagging, and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clinical Center</orgName>
								<orgName type="laboratory">Imaging Biomarkers and Computer-Aided Diagnosis Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youbao</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clinical Center</orgName>
								<orgName type="laboratory">Imaging Biomarkers and Computer-Aided Diagnosis Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Center for Biotechnology Information</orgName>
								<orgName type="institution">National Library of Medicine 1</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National Institutes of Health</orgName>
								<address>
									<postCode>20892</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veit</forename><surname>Sandfort</surname></persName>
							<email>veit.sandfort@googlemail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Clinical Center</orgName>
								<orgName type="laboratory">Imaging Biomarkers and Computer-Aided Diagnosis Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clinical Center</orgName>
								<orgName type="laboratory">Imaging Biomarkers and Computer-Aided Diagnosis Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Center for Biotechnology Information</orgName>
								<orgName type="institution">National Library of Medicine 1</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National Institutes of Health</orgName>
								<address>
									<postCode>20892</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clinical Center</orgName>
								<orgName type="laboratory">Imaging Biomarkers and Computer-Aided Diagnosis Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MULAN: Multitask Universal Lesion Analysis Network for Joint Lesion Detection, Tagging, and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When reading medical images such as a computed tomography (CT) scan, radiologists generally search across the image to find lesions, characterize and measure them, and then describe them in the radiological report. To automate this process, we propose a multitask universal lesion analysis network (MULAN) for joint detection, tagging, and segmentation of lesions in a variety of body parts, which greatly extends existing work of single-task lesion analysis on specific body parts. MULAN is based on an improved Mask R-CNN framework with three head branches and a 3D feature fusion strategy. It achieves the state-ofthe-art accuracy in the detection and tagging tasks on the DeepLesion dataset, which contains 32K lesions in the whole body. We also analyze the relationship between the three tasks and show that tag predictions can improve detection accuracy via a score refinement layer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Detection, classification, and measurement of clinically important findings (lesions) in medical images are primary tasks for radiologists <ref type="bibr" target="#b9">[10]</ref>. Generally, they search across the image to find lesions, and then characterize their locations, types, and related attributes to describe them in radiological reports. They may also need to measure the lesions, e.g., according to the RECIST guideline <ref type="bibr" target="#b1">[2]</ref>, for quantitative assessment and tracking. To reduce radiologists' burden and improve accuracy, there have been many efforts in the computer-aided diagnosis area to automate this process. For example, detection, attribute estimation, and malignancy prediction of lung nodules have been extensively studied <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>. Other works include detection and malignancy prediction of breast lesions <ref type="bibr" target="#b8">[9]</ref>, classification of three types of liver lesions <ref type="bibr" target="#b0">[1]</ref>, and segmentation of lymph nodes <ref type="bibr" target="#b11">[12]</ref>. Variants of Faster R-CNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6]</ref> have been used for detection, whereas patchbased dictionaries <ref type="bibr" target="#b0">[1]</ref> or networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> have been studied for classification and segmentation.</p><p>Most existing work on lesion analysis focused on certain body parts (lung, liver, etc.). In practice, a radiologist often needs to analyze various lesions in multiple organs. Our goal is to build such a universal lesion analysis algorithm to mimic radiologists, which to the best of our knowledge is the first work on this problem. To this end, we attempt to integrate the three tasks in one framework. Compared to solving each task separately, the joint framework will be not only more efficient to use, but also more accurate, since different tasks may be correlated and help each other <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>We present the multitask universal lesion analysis network (MULAN) which can detect lesions in CT images, predict multiple tags for each lesion, and segment it as well. This end-to-end framework is based on an improved Mask R-CNN <ref type="bibr" target="#b2">[3]</ref> with three branches: detection, tagging, and segmentation. The tagging (multilabel classification) branch learns from tags mined from radiological reports. We extracted 185 fine-grained and comprehensive tags describing the body part, type, and attributes of the lesions. The relation between the three tasks is analyzed by experiments in this paper. Intuitively, lesion detection can benefit from tagging, because the probability of a region being a lesion is associated with its attribute tags. We propose a score refinement layer in MULAN to explicitly fuse the detection and tagging results and improve the accuracy of both. A 3D feature fusion strategy is developed to leverage the 3D context information to improve detection accuracy.</p><p>MULAN is evaluated on the DeepLesion <ref type="bibr" target="#b16">[17]</ref> dataset, a large-scale and diverse dataset containing measurements and 2D bounding-boxes of over 32K lesions from a variety of body parts on computed tomography (CT) images. It has been adopted to learn models for universal lesion detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref>, measurement <ref type="bibr" target="#b10">[11]</ref>, and classification <ref type="bibr" target="#b15">[16]</ref>. On DeepLesion, MULAN achieves the state-of-the-art accuracy in detection and tagging and performs comparable in segmentation. It outperforms the previous best detection result by 10%. We released the code of MULAN in 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The flowchart of the multitask universal lesion analysis network (MULAN) is displayed in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>. Similar to Mask R-CNN <ref type="bibr" target="#b2">[3]</ref>, MULAN has a backbone network to extract a feature map from the input image, which is then used in the region proposal network (RPN) to predict lesion proposals. Then, an ROIAlign layer <ref type="bibr" target="#b2">[3]</ref> crops a small feature map for each proposal, which is used by three head branches to predict the lesion score, tags, and mask of the proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Backbone with 3D Feature Fusion</head><p>A good backbone network is able to encode useful information of the input image into the feature map. In this study, we adopt the DenseNet-121 <ref type="bibr" target="#b3">[4]</ref> in the backbone with the last dense block and transition layer removed, as we found removing them slightly improved accuracy and speed. Next, we employ the feature pyramid strategy <ref type="bibr" target="#b6">[7]</ref> to add fine-level details into the feature map. This strategy also increases the size of the final feature map, which will benefit the detection and segmentation of small lesions. Different from the original feature pyramid network <ref type="bibr" target="#b6">[7]</ref> which attaches head branches to each level of the pyramid, we attach the head branches only to the finest level <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>. 3D context information is very important when differentiating lesions from non-lesions <ref type="bibr" target="#b14">[15]</ref>. 3D CNNs have been used for lung nodule detection <ref type="bibr" target="#b5">[6]</ref>. However, they are memory-consuming, thus smaller networks need to be used. Universal lesion detection is much more difficult than lung nodule detection, so networks with more channels and layers are potentially desirable. Yan et al. <ref type="bibr" target="#b14">[15]</ref> proposed 3D context enhanced region-based CNN (3DCE) and achieved better detection accuracy than a 3D CNN in the DeepLesion dataset. They first group consecutive axial slices in a CT volume into 3-channel images. The upper and lower images provide 3D context for the central image. A feature map is then extracted for each image with a shared 2D CNN. Lastly, they fuse the feature maps of all images with a convolutional (Conv) layer to produce the 3D-context-enhanced feature map for the central image and predict 2D boxes for the lesions on it.</p><p>The drawback of 3DCE is that the 3D context information is fused only in the last Conv layer, which limits the network's ability to learn more complex 3D features. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (b), we improve 3DCE to relieve this issue. The basic idea is to fuse features of multiple slices in earlier Conv layers. Similar to 3DCE, feature maps (FMs) are fused with a Conv layer (i.e., the 3D fusion layer). Then, the fused central FM is used to replace the original central FM, while the upper and lower FMs are kept unchanged. All FMs are then fed to subsequent Conv layers. Because the new central FM contains 3D context information, sophisticated 3D features can be learned in subsequent layers with nonlinearity. This 3D fusion layer can be inserted between any two layers of the original 2D CNN. In MULAN, one 3D fusion layer is inserted after dense block 2 and another one after the last layer of the feature pyramid. We found fusing 3D context in the beginning of the CNN (before dense block 2) is not good possibly because the CNN has not yet learned good semantic 2D features by then. At the end of the network, only the central feature map is used as the FM of the central image.   The structure and function of the three head branches are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. The detection branch consists of two 2048D fully connected layers (FC) and predicts the lesion score of each proposal, i.e., the probability of the proposal being a lesion. It also conducts bounding-box regression to refine the box <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Head Branches and Score Refinement Layer</head><p>The tagging branch predicts the body part, type, and attributes (intensity, shape, etc.) of the lesion proposal. It applies the same label mining strategy as that in LesaNet <ref type="bibr" target="#b15">[16]</ref>. We first construct the lesion ontology based on the RadLex lexicon. To mine training labels, we tokenize the sentences in the radiological reports of DeepLesion, and then match and filter the tags in the sentences using a text mining module. 185 tags with more than 30 occurrences in DeepLesion are kept. A weighted binary cross-entropy loss is applied on each tag. The hierarchical and mutually exclusive relations between the tags were leveraged in a label expansion strategy and a relational hard example mining loss to improve accuracy <ref type="bibr" target="#b15">[16]</ref>. The score propagation layer and the triplet loss in <ref type="bibr" target="#b15">[16]</ref> are not used. Due to space constraints, we refer readers to the supplementary material (sup. mat.) and <ref type="bibr" target="#b15">[16]</ref> for more implementation details in this branch.</p><p>For the segmentation branch, we follow the method in <ref type="bibr" target="#b12">[13]</ref> and generate pseudo-masks of lesions for training. The DeepLesion dataset does not contain lesions' ground-truth masks. Instead, each lesion has a RECIST measurement <ref type="bibr" target="#b1">[2]</ref>, namely a long axis and a short axis annotated by radiologists. They are utilized to generate four quadrants as the estimation of the real mask <ref type="bibr" target="#b12">[13]</ref>, since most lesions have ellipse-like shapes. We use the Dice loss <ref type="bibr" target="#b13">[14]</ref> as it works well in balancing foreground and background pixels. The predicted mask can be easily used to compute the contour and then the RECIST measurement of the lesion, see <ref type="figure" target="#fig_2">Fig. 2</ref> for an example.</p><p>Intuitively, detection (lesion/non-lesion classification) is closely related to tagging. One way to exploit their synergy is to combine them in one branch to make them share FC features. However, this strategy led to inferior accuracy for both tasks in our experiments probably because detecting a variety of lesions is a hard problem and requires rich features with high nonlinearity, thus a dedicated branch is necessary. In this study, we propose to combine them at the decision level. Specifically, for each lesion proposal, we join its lesion score from the detection branch and the 185 tag scores from the tagging branch as a feature vector, then predict the lesion and tag scores again using a score refinement layer (SRL). Tag predictions can thus support detection explicitly. We also add new features as the input of the layer including the statistics of the proposal (x, y, width, height), the patient's gender, and age. Other relevant features such as medical history and lab results may also be considered. In MULAN, SRL is a simple FC layer as we found more nonlinearity did not improve results possibly due to overfitting. The losses for detection and tagging after this layer are the same as those in the respective branches.</p><p>More implementation details of MULAN are depicted in the sup. mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Discussion</head><p>Implementation: MULAN was implemented in PyTorch based on the maskrcnnbenchmark 2 project. The DenseNet backbone was initialized with an ImageNet pretrained model. The score refinement layer was initialized with an identity matrix so that the scores before and after it were the same when training started. Other layers were randomly initialized. Each mini-batch had 8 samples, where each sample consisted of three 3-channel images for 3D fusion <ref type="figure" target="#fig_0">(Fig. 1</ref>). We used SGD to train MULAN for 8 epochs and set the base learning rate to 0.004, then reduced it by a factor of 10 after the 4th and 6th epochs. It takes MULAN 30ms to predict a sample during inference on a Tesla V100 GPU. Data: The DeepLesion dataset <ref type="bibr" target="#b16">[17]</ref> contains 32,735 lesions and was divided into training (70%), validation (15%), and test (15%) sets at the patient level. When training, we did data augmentation for each image in three ways: random resizing with a ratio of 0.8?1.2; random translation of -8?8 pixels in x and y axes; and 3D augmentation. A lesion in DeepLesion was annotated in one axial slice, but the actual lesion also exists in approximately the same position in several neighboring slices depending on its diameter and the slice interval. Therefore, we can do 3D augmentation by randomly shifting the slice index within half of the lesion's short diameter. Each of these three augmentation methods improved detection accuracy by 0.2?0.4%. Some examples of DeepLesion are presented in Section 1 of the sup. mat.</p><p>Metrics: For detection, we compute the sensitivities at 0.5, 1, 2, and 4 false positives (FPs) per image <ref type="bibr" target="#b14">[15]</ref> and average them, which is similar to the evaluation metric of the LUNA dataset <ref type="bibr" target="#b5">[6]</ref>. For tagging, we use the 500 manually tagged lesions in <ref type="bibr" target="#b15">[16]</ref> for evaluation. The area under the ROC curve (AUC) and F1 score are computed for each tag and then averaged. Since there are no groundtruth (GT) masks in DeepLesion except for RECIST measurements <ref type="bibr" target="#b10">[11]</ref>, we use the average distance from the endpoints of the GT measurement to the predicted contour as a surrogate criterion (see sup. mat. Section 2). The second criterion is the average error of length of the estimated RECIST diameters, which are very useful values for radiologists and clinicians <ref type="bibr" target="#b1">[2]</ref>.</p><p>Qualitative and quantitative results are presented in <ref type="figure" target="#fig_3">Fig. 3</ref> and <ref type="table" target="#tab_1">Table 1</ref>, respectively. Note that in <ref type="table" target="#tab_1">Table 1</ref>, tagging and segmentation accuracy were calculated by predicting tags and masks based on GT bounding-boxes, so that they were under the same setting as previous studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> and independent of the detection accuracy. We will discuss the results of each task below. Detection: <ref type="table" target="#tab_1">Table 1</ref> shows that MULAN significantly surpasses existing work on universal lesion detection by over 10% in average sensitivity. According to the ablation study, 3D fusion and feature pyramid improve detection accuracy the most. If the tagging branch is not added (ablation study (d)), the detection accuracy is 84.79%; When it is added, the accuracy slightly drops to 84.24% (ablation study (f)). However, when the score refinement layer (SRL) is added, we achieve the best detection accuracy of 86.12%. We hypothesize that SRL effectively exploits the correlation between the two tasks and uses the tag predictions to refine the lesion detection score. To verify the impact of SRL, we randomly re-split the training and validation set of DeepLesion five times and found MULAN with SRL always outperformed it without SRL by 0.7 ? 1.1%.  Examples in <ref type="figure" target="#fig_3">Fig. 3</ref> show that MULAN is able to detect true lesions with high confidence score, although there are still FPs when normal tissues have a similar appearance with lesions. We analyzed the detection accuracy by tags and found lung masses/nodules, mediastinal and pelvic lymph nodes, adrenal and liver lesions are among the lesions with the highest sensitivity, while lesions in pancreas, bone, thyroid, and extremity are relatively hard to detect. These conclusions can guide us to collect more training samples with the difficult tags in the future.</p><p>Tagging: MULAN outperforms LesaNet <ref type="bibr" target="#b15">[16]</ref>, a multilabel CNN designed for universal lesion tagging. According to ablation study (c), adding the detection branch improves tagging accuracy. This is probably because detection is hard and requires comprehensive features to be learned in the backbone of MULAN, which are also useful for tagging. <ref type="figure" target="#fig_3">Fig. 3</ref> shows that MULAN is able to predict the body part, type, and attributes of lesions with high accuracy.</p><p>Segmentation: Our predicted RECIST diameters have an average error of 1.97mm compared with the GT diameters. From <ref type="figure" target="#fig_3">Fig. 3</ref>, we can find that MULAN performs relatively well on lesions with clear borders, but struggles on those with indistinct or irregular borders, e.g., the liver mass in <ref type="figure" target="#fig_3">Fig. 3 (c)</ref>. Ablation studies show that feature pyramid is the most crucial strategy. Another interesting finding is that removing the detection branch (ablation study (c)) markedly improves segmentation accuracy. The detection task impairs segmentation, which could be a major reason why the multitask MULAN cannot beat Auto RECIST <ref type="bibr" target="#b10">[11]</ref>, a framework dedicated to lesion measurement. It implies that better segmentation results may be achieved using a single-task CNN.</p><p>More detailed results are shown in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we proposed MULAN, the first multitask universal lesion analysis network which can simultaneously detect, tag, and segment lesions in a variety of body parts. The training data of MULAN can be mined from radiologists' routine annotations and reports with minimum manual effort <ref type="bibr" target="#b16">[17]</ref>. An effective 3D feature fusion strategy was developed. We also analyzed the interaction between the three tasks and discovered that: 1) Tag predictions could improve detection accuracy via a score refinement layer; 2) The detection task improved tagging accuracy but impaired segmentation performance. Universal lesion analysis is a challenging task partially because of the large variance of appearances of the normal and abnormal tissues. Therefore, the 22K training lesions in DeepLesion are still not sufficient for MULAN to learn, which is a main reason for its FPs and FNs. In the future, more training data need to be mined. We also plan to apply or finetune MULAN on other applications of specific lesions. We hope MULAN can be a useful tool for researchers focusing on different types of lesions. brary of Medicine (NLM). It was also supported by NLM of NIH under award number K99LM013001. We thank NVIDIA for GPU card donations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Introduction to the DeepLesion Dataset</head><p>The DeepLesion dataset <ref type="bibr" target="#b16">[17]</ref> was mined from a hospital's picture archiving and communication system (PACS) based on bookmarks, which are markers annotated by radiologists during their routine work to measure significant image findings. It is a large-scale dataset with 32,735 lesions on 32,120 axial slices from 10,594 CT studies of 4,427 unique patients. There are 1 -3 lesions in each axial slice. Different from existing datasets that typically focus on one type of lesion, DeepLesion contains a variety of lesions including those in lungs, livers, kidneys, etc., and enlarged lymph nodes.</p><p>Each lesion in DeepLesion has a RECIST measurement <ref type="bibr" target="#b1">[2]</ref>, which consists of two lines: one measuring the longest diameter of the lesion and the second measuring its longest perpendicular diameter in the axial plane, see <ref type="figure">Fig. 4</ref>. From these two diameters, we can compute a 2D bounding box to train a lesion detection algorithm <ref type="bibr" target="#b16">[17]</ref>, as well as generate a psuedo-mask to train a lesion segmentation algorithm <ref type="bibr" target="#b12">[13]</ref>.</p><p>Besides measuring the lesions, radiologists often describe them in radiological reports and use a hyperlink (shown as "BOOKMARK" in <ref type="figure">Fig. 4</ref>) to link the measurement with the sentence. We can extract tags that describe the lesion in the sentence to train a lesion tagging algorithm <ref type="bibr" target="#b15">[16]</ref>. The predicted tags can provide comprehensive and fine-grained semantic information for the user to understand the lesion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Additional Details in Methods</head><p>Backbone The backbone structure of MULAN is a truncated DenseNet-121 <ref type="bibr" target="#b3">[4]</ref> (87 Conv layers after truncation) with feature pyramid <ref type="bibr" target="#b6">[7]</ref> and 3D feature fusion. The finest level of the feature pyramid corresponds to dense block 1 and has stride 4 <ref type="bibr" target="#b6">[7]</ref>. The channel number after feature pyramid is 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection and Segmentation</head><p>The structures of the region proposal network (RPN), detection branch, and mask branch are similar to those in Mask R-CNN <ref type="bibr" target="#b2">[3]</ref>. Five anchor scales <ref type="bibr" target="#b15">(16,</ref><ref type="bibr">24,</ref><ref type="bibr">32,</ref><ref type="bibr">48,</ref><ref type="bibr">96)</ref> and three anchor ratios (1:2, 1:1, 2:1) are used in RPN. The loss function for detection and segmentation is</p><formula xml:id="formula_0">L det,seg = L RPN,cls + L RPN,box + L det,cls + 10L det,box + L seg,dice ,<label>(1)</label></formula><p>where L RPN,cls and L RPN,box are the classification (lesion vs. non-lesion) and bounding-box regression <ref type="bibr" target="#b2">[3]</ref> losses of RPN; L det,cls and L det,box are those in the detection branch; L seg,dice is the Dice loss <ref type="bibr" target="#b7">[8]</ref> in the segmentation branch.</p><p>Sentence: Within the right middle lobe there is a stable nodule that measures BOOKMARK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tags: Right mid lung, nodule</head><p>Sentence: A large right hepatic mass, incompletely characterized BOOKMARK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tags: Large, liver, liver mass, mass</head><p>Sentence: Low density left adrenal nodule BOOKMARK, likely adenoma. Tags: Hypoattenuation, left adrenal gland, nodule, adenoma <ref type="figure">Fig. 4</ref>. Examples of the CT images, annotations, and reports in DeepLesion <ref type="bibr" target="#b16">[17]</ref>. The red and blue lines in the images are the RECIST measurements. The green boxes are the bounding-boxes. The sentences are extracted from radiological reports according to the bookmarks <ref type="bibr" target="#b15">[16]</ref>. The tags are mined from the sentences and normalized <ref type="bibr" target="#b15">[16]</ref>.</p><p>Tagging The label mining strategy and the loss function of the tagging branch are similar to <ref type="bibr" target="#b15">[16]</ref>, except that the score propagation layer and the triplet loss are not used. Based on the RadLex lexicon <ref type="bibr" target="#b4">[5]</ref>, we run whole-word matching in the sentences to extract the lesion tags and combine all synonyms. Some tags in the sentence are not related to the lesion in the image, so we use a text-mining module <ref type="bibr" target="#b15">[16]</ref> to filter the irrelevant tags. The final 185 tags can be categorized into three classes <ref type="bibr" target="#b15">[16]</ref>: 1. Body parts, which include coarse-level body parts (e.g., chest, abdomen), organs (lung, lymph node), fine-grained organ parts (right lower lobe, pretracheal lymph node), and other body regions (porta hepatis, paraspinal); 2. Types, which include general terms (nodule, mass) and more specific ones (adenoma, liver mass); and 3. Attributes, which describe the intensity, shape, size, etc., of the lesions (hypoattenuation, spiculated, large).</p><p>The tagging branch predicts a score s i,c for each tag c of each proposal i. Because positive labels are sparse for most tags, we adopt a weighted crossentropy (WCE) loss <ref type="bibr" target="#b15">[16]</ref> for each tag as in Eq. 2, where B is the number of true lesions in a minibatch, C is the number of tags; ? i,c = sigmoid(s i,c ); y i,c ? {0, 1} is the ground-truth of lesion i having tag c. The loss weights are ? p c = |P c + N c |/|2P c |, ? n c = |P c + N c |/|2N c |, P c , N c are the number of positive and negative labels of tag c in the training set of DeepLesion, respectively. Similar to the segmentation branch, the tagging branch only considers proposals corresponding to true lesions in the loss function, since we do not know the ground-truth tags of non-lesions, although non-lesions can also have body parts and attributes.</p><formula xml:id="formula_1">L tag,WCE = B i=1 C c=1 (? p c y i,c log ? i,c + ? n c (1 ? y i,c ) log(1 ? ? i,c )) .<label>(2)</label></formula><p>There are hierarchical and mutually exclusive relations between the tags. For example, lung is the parent of left lung (if a lesion is in the left lung, it must be in the lung), while left lung and right lung are exclusive (they cannot both be true for one lesion). These relations can be leveraged to improve tagging accuracy. Tags extracted from reports are often not complete since radiologists typically do not write down all possible characteristics. If a tag is not mentioned in the report, it may still be true. To deal with this label noise problem, first, we use the label expansion strategy <ref type="bibr" target="#b15">[16]</ref> to infer the missing parent tags. If a child tag is mined from the report, all its parents will be set as true. Second, we use the relational hard example mining (RHEM) strategy <ref type="bibr" target="#b15">[16]</ref> to suppress reliable negative tags. If a tag is true, all its exclusive tags must be false, so we can define a new loss term L tag,RHEM to assign higher weights to these exclusive tags.</p><p>Overall The overall loss function of MULAN is</p><formula xml:id="formula_2">L = L det,seg + L tag,WCE + L tag,RHEM + L cls,SRL + L tag,WCE,SRL ,<label>(3)</label></formula><p>where L det,seg is defined in Eq. 1, and L tag,WCE and L tag,RHEM are the losses of the tagging branch. L cls,SRL and L tag,WCE,SRL are the losses of the score refinement layer, which have the same forms as L det,cls in Eq. 1 and L tag,WCE in Eq. 2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Additional Details in Experiments and Results</head><p>Image Preprocessing Method We rescaled the 12-bit CT intensity range to floating-point numbers in [0,255] using a single windowing (-1024-3071 HU) that covers the intensity ranges of the lung, soft tissue, and bone. Every image slice was resized so that each pixel corresponds to 0.8mm. The slice intervals of most CT scans in the dataset are either 1mm or 5mm. We interpolated in the z-axis to make the intervals of all volumes 2mm. The black borders in images were clipped for computation efficiency. We used the official data split of DeepLesion. The input of our experiments are 9-slice sub-volumes in DeepLesion, including the key slice that contains the lesion, 4 slices superior to it, and 4 slices inferior <ref type="bibr" target="#b14">[15]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surrogate Evaluation Criteria for Lesion Segmentation</head><p>There are no ground-truth (GT) masks in DeepLesion. Instead, each lesion has a GT RECIST measurement, so we use two surrogate metrics to evaluate segmentation results, see <ref type="figure" target="#fig_4">Fig. 5</ref>. First, if the predicted mask is accurate, the endpoints of the GT RECIST measurement should be on its contour. Therefore, the average distance from the endpoint of the GT measurement to the contour of the predicted mask is a useful metric (the smaller the better). Second, if the predicted mask is accurate, the lengths of the estimated RECIST measurement (diameters) should be the same with the GT diameters. The average error of lengths is thus another useful metric (the smaller the better). RECIST measurements <ref type="bibr" target="#b10">[11]</ref> can be easily estimated from the predicted mask. We first compute the contour of the predicted mask, then find two points on the contour with the largest distance to form the long axis. Next, we search on the contour to find the short axis that is perpendicular to the long axis and has the largest length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>We show the free-response receiver operating characteristic (FROC) curves in <ref type="figure">Fig. 7</ref>. They correspond to the detection accuracies in <ref type="table" target="#tab_1">Table 1</ref> of the main paper. MULAN outperforms previous methods 3DCE and ULDor. In <ref type="figure">Fig. 6</ref>, the threshold for lesion scores is 0.5. Note that some FPs in the detection results are actually TPs, because there are missing lesion annotations in the test set of DeepLesion. Some examples include the smaller lung mass in <ref type="figure">Fig. 6 (a)</ref> and the two smaller pancreatic lesions in <ref type="figure">Fig. 6 (c)</ref>.</p><p>To turn tag scores into decisions, we calibrated a threshold for each tag that yielded the best F1 on the validation set, and then applied it on the test set. In <ref type="figure">Fig. 6</ref>, MULAN is able to predict the body part, type, and attributes of lesions with high accuracy. Possible reasons of tagging errors include:</p><p>-Some attributes with variable appearances and few training samples have some FPs, such as "benign" and "diffuse" in <ref type="figure">Fig. 6 (d)</ref>; -Adjacent body parts may be confused by the model, such as "right lower lobe" in (a) and "pancreatic head", "pancreatic tail", "lesser sac", and "duodenum" in (c).</p><p>The threshold for mask prediction is 0.5. In <ref type="figure">Fig. 6</ref>, MULAN performs relatively well on lesions with clear borders ((a) and (b)), but struggles on those with indistinct borders ((c) and (d)). For the latter case, GT measurements may sometimes be inaccurate or not consistent (different radiologists have different opinions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results using the Released Tags [16] of DeepLesion</head><p>In this paper, we used an NLP algorithm slightly different from <ref type="bibr" target="#b15">[16]</ref> to mine tags from reports. There are 185 tags in this paper and 171 in <ref type="bibr" target="#b15">[16]</ref>. Since the 171 tags of <ref type="bibr" target="#b15">[16]</ref> have been released in 3 , we also retrained MULAN on these 171 tags so that the results can be compared with others' methods trained on the 171 tags. The results are shown in <ref type="table" target="#tab_4">Table 2</ref> below. The definition of the metrics are the same with those in <ref type="table" target="#tab_1">Table 1</ref> of the main paper. The results are also similar with those in <ref type="table" target="#tab_1">Table 1</ref> of the main paper.</p><p>The detection performance of MULAN at various FPs per image is reported in <ref type="table" target="#tab_5">Table 3</ref>. The 171 tags were used for training, so the results are slightly different from those in <ref type="table" target="#tab_1">Table 1</ref> of the main paper. <ref type="table">Tables 4-6</ref> show the details of the 171 tags and the tag-wise detection and tagging accuracies. The accuracies were computed on the mined tags <ref type="bibr" target="#b15">[16]</ref> of the validation set of DeepLesion. "# Train" and "# Test" are the numbers of positive cases in the training and validation sets, respectively.     <ref type="table">Table 4</ref>. Details of the 171 tags. The detection accuracy (average sensitivity in %), tagging AUC and F1 (%) are also shown. <ref type="table">Table 5</ref>. <ref type="table">Table 4</ref> continued.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Flowchart of MULAN and the 3D feature fusion strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the head branches and the score refinement layer of MULAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of MULAN's lesion detection, tagging, and segmentation results on the test set of DeepLesion. For detection, boxes in green and red are predicted TPs and FPs, respectively. The number above each box is the lesion score (confidence). For tagging, tags in black, red (underlined), and blue (italic) are predicted TPs, FPs, and FNs, respectively. They are ranked by their scores. For segmentation, the green lines are ground-truth RECIST measurements; the orange contours and lines show predicted masks and RECIST measurements, respectively. More visual examples are provided in sup. mat. Section 3. (TP: true positive; FP: false positive; FN: false negative)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of the predicted mask (green contour), estimated RECIST measurement (green segments), and ground-truth RECIST measurement (orange segments with yellow endpoints) of a lesion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Examples of MULAN's lesion detection, tagging, and segmentation results on the test set of DeepLesion. For detection, boxes in green and red are predicted TPs and FPs, respectively. The number above each box is the lesion score (confidence). For tagging, tags in black, red (underlined), and blue (italic) are predicted TPs, FPs, and FNs, respectively. They are ranked according to their scores. For segmentation, the green lines are ground-truth RECIST measurements; the orange contours and lines show predicted masks and RECIST measurements, respectively. (TP: true positive; FP: false positive; FN: false negative) Free-response receiver operating characteristic (FROC) curve of various methods and variations of MULAN on the test set of DeepLesion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Tagging branch Detection branch Segmentation branch</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Prediction results</cell></row><row><cell></cell><cell cols="2">Bounding-box regression</cell><cell></cell></row><row><cell>FC 2048 x 2</cell><cell>Lesion/non-lesion classification</cell><cell>Score</cell><cell>Refined lesion score</cell></row><row><cell></cell><cell></cell><cell>refinement</cell><cell></cell></row><row><cell>FC 1024 x 2</cell><cell>Multilabel tag classification</cell><cell cols="2">layer Additional features: Refined tag scores</cell><cell>Left adrenal gland: 0.998 Adenoma: 0.991</cell></row><row><cell>Conv 256 x 4</cell><cell>pixel classification Lesion/non-lesion</cell><cell cols="2">x, y, w, h; gender, age; etc.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Accuracy comparison and ablation studies on the test set of DeepLesion. Bold results are the best ones. Underlined results in the ablation studies are the worst ones, indicating the ablated strategy is the most important for the criterion.</figDesc><table><row><cell></cell><cell>Detection (%)</cell><cell cols="4">Tagging (%) Segmentation (mm)</cell></row><row><cell></cell><cell cols="2">Avg. sensitivity AUC</cell><cell>F1</cell><cell cols="2">Distance Diam. err.</cell></row><row><cell>ULDor [13]</cell><cell>69.22</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3DCE [15]</cell><cell>75.55</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LesaNet [16] (rerun)</cell><cell>-</cell><cell cols="2">95.12 43.17</cell><cell>-</cell><cell>-</cell></row><row><cell>Auto RECIST [11]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.7088</cell></row><row><cell>MULAN</cell><cell>86.12</cell><cell cols="2">96.01 45.53</cell><cell>1.4138</cell><cell>1.9660</cell></row><row><cell>(a) w/o feature pyramid</cell><cell>79.73</cell><cell cols="2">95.51 43.44</cell><cell>1.6634</cell><cell>2.3780</cell></row><row><cell>(b) w/o 3D fusion</cell><cell>79.57</cell><cell cols="2">95.88 44.28</cell><cell>1.4120</cell><cell>1.9756</cell></row><row><cell>(c) w/o detection branch</cell><cell>-</cell><cell cols="2">95.16 40.03</cell><cell>1.2445</cell><cell>1.7837</cell></row><row><cell>(d) w/o tagging branch</cell><cell>84.79</cell><cell>-</cell><cell>-</cell><cell>1.4230</cell><cell>1.9589</cell></row><row><cell>(e) w/o mask branch</cell><cell>85.21</cell><cell cols="2">95.87 43.76</cell><cell>-</cell><cell>-</cell></row><row><cell>(f) w/o score refine. layer</cell><cell>84.24</cell><cell cols="2">95.65 44.59</cell><cell>1.4260</cell><cell>1.9687</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Accuracy comparison on the test set of DeepLesion using the 171 training tags of<ref type="bibr" target="#b15">[16]</ref>. Note that only LesaNet and MULAN used the tags.</figDesc><table><row><cell></cell><cell>Detection (%)</cell><cell cols="4">Tagging (%) Segmentation (mm)</cell></row><row><cell></cell><cell cols="2">Avg. sensitivity AUC</cell><cell>F1</cell><cell cols="2">Distance Diam. err.</cell></row><row><cell>ULDor [13]</cell><cell>69.22</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3DCE [15]</cell><cell>75.55</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LesaNet [16]</cell><cell>-</cell><cell cols="2">93.98 43.44</cell><cell>-</cell><cell>-</cell></row><row><cell>Auto RECIST [11]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.7088</cell></row><row><cell>MULAN</cell><cell>85.22</cell><cell cols="2">95.12 46.12</cell><cell>1.4354</cell><cell>1.9619</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Sensitivity (%) at various FPs per image on the test set of DeepLesion (the 171 tags were used for training MULAN).</figDesc><table><row><cell>FPs per image</cell><cell>0.5</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>Avg. of [0.5,1,2,4]</cell></row><row><cell>3DCE [15]</cell><cell cols="6">62.48 73.37 80.70 85.65 89.09 91.06</cell><cell>75.55</cell></row><row><cell>ULDor [13]</cell><cell cols="6">52.86 64.80 74.84 84.38 87.17 91.80</cell><cell>69.22</cell></row><row><cell>MULAN</cell><cell cols="6">76.12 83.69 88.76 92.30 94.71 95.64</cell><cell>85.22</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/rsummers11/CADLab/tree/master/MULAN_universal_ lesion_analysis</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookresearch/maskrcnn-benchmark</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/rsummers11/CADLab/tree/master/LesaNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">bodypart  851  82  63  82  28  hilum  bodypart  681  65  69  96  79  chest wall  bodypart  672  69  51  98  72  left lower lobe  bodypart  654  63  70  99  71  right lower lobe  bodypart  647  79  79  98  61  right upper lobe  bodypart  518  72  73  98  65  left upper lung  bodypart  512  79  85  99  72  abdomen lymph node  bodypart  504  44  61  92  43  axilla  bodypart  493  41  62  100  85  mesentery  bodypart  468  47  69  97  52  bone  bodypart  462  52  49  96  65  paraaortic  bodypart  462  40  74  98  46  pelvis lymph node  bodypart  439  67  72  98  72  retroperitoneum lymph node  bodypart  432  31  75</ref>  <ref type="table">bodypart  83  13  71  100  63  extremity  bodypart  79  17  85  97  20  adnexa  bodypart  73  7  43  98  31  paracaval lymph node  bodypart  72  3  83  100  16  airway  bodypart  71  10  45  97  23  aorta  bodypart  71  5  45  92  0  cardiophrenic  bodypart  69  4  75  99  19  rib  bodypart  66  11  14  99  53  diaphragm  bodypart  64  11  59  89  0  pancreatic tail  bodypart  63  3  50  99  9  paraspinal muscle  bodypart  59  4  88  90  13  peripancreatic lymph node  bodypart  57  4  25  98  13  omentum  bodypart  55  7  46  96  5  thigh  bodypart  54  12  90  98  24  psoas muscle  bodypart  54  4  88  89  14  thoracic spine  bodypart  51  9  50  100  44  subpleural  bodypart  51  7  61  97  14  vertebral body  bodypart  50  8  41  100  46  retrocrural lymph node  bodypart  50  4  81  79  50  lumbar  bodypart  48  4  69  99  30  perihilar  bodypart  48  1  100  96  0  pretracheal lymph node  bodypart  47  10  83  97  16  bronchus  bodypart  47  9  56  98  15  small bowel  bodypart  46  8  66  96  7  anterior abdominal wall  bodypart  46  3  83  100  23  pancreatic body  bodypart  45  3  42  100  46  cervix  bodypart  43  0  -0  0  stomach  bodypart  40  4  50  94  8  urinary bladder  bodypart  40  6  63  99  17  lung apex  bodypart  36  5  90  98  9  sacrum  bodypart  33  3  75  100  46  gallbladder  bodypart  33  2  50  100  36  biliary system  bodypart  33  3  42  92  33  pelvic bone  bodypart  32  3  50  95  22  sternum  bodypart  31  3  0  100  26  skin  bodypart  31  8  41  89  21  pericardium  bodypart  29  4  63  98  7  right thyroid lobe  bodypart  28  3  75  100  29  femur  bodypart  25  1  50  97  0  cortex  bodypart  25  3  50  95  4  trachea  bodypart  23  2  25  99  33  ovary  bodypart  22  3  92  98  10  subcutaneous fat  bodypart  21  10  70  98  20  lesser sac  bodypart  15  2  50  99  0  Table 6</ref><p>. <ref type="table">Table 5</ref> continued.</p><p>Tag Class # Train # Test Det. <ref type="table">Tag AUC Tag F1  mass  type  4037  412  77  84  21  nodule  type  3336  403  76  89  53  enlargement  type  996  114  76  76  2  lung nodule  type  752  77  86  94  38  lymphadenopathy  type  739  79  75  87  17  cyst  type  584  83  74  89  35</ref>   <ref type="table">type  53  3  83  98  6  scar  type  48  10  63  80  5  adenoma  type  32  4  94  99  14  implant  type  30  1  0  66  0  expansile  type  29  0  0  0  0  lobular mass  type  24  3  92  81  0  simple cyst  type  24  3  83  98  7  lipoma  type  21  3  0  99  8  hypoattenuation  attribute  1681  188  70  90  40  enhancing  attribute  902  120  66  81  19  large  attribute  558  46  77  83  11  prominent  attribute  396  37  64  91  18  calcified  attribute  375  45  71  75  3  indistinct  attribute  278  18  61  82  11  solid  attribute  267  28  56  87  21  hyperattenuation  attribute  239  30  72  83  21  heterogeneous  attribute  191  19  75  81  20  spiculated  attribute  170  26  70  79  23  sclerotic  attribute  168  20  54  99  58  soft tissue attenuation  attribute  147  12  60  69  5  tiny  attribute  126  21  67  93  11  lobular  attribute  102  18  85  68  3  conglomerate  attribute  98  20  78  84  16  lytic  attribute  90  11  20  99  31  cavitary  attribute  73  26  87  93  26  subcentimeter  attribute  72  11  86  89  7  circumscribed  attribute  70  2  50  63  0  diffuse  attribute  62  9  67  65  4  exophytic  attribute  59  9  72  85  12  oval  attribute  41  4  31  68  9  fat-containing  attribute  37  10  53  74  6  noncalcified  attribute  35  5  95  96  6  nonenhancing  attribute  34  8  72  88  4  lucent  attribute  33  3  75  8  0  thin  attribute  20  7  57  91  13  reticular  attribute  17  6  83  93  4  patchy  attribute  14  3  67  84  0</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved Patch-Based Automated Liver Lesion Classification by Separate Analysis of the Interior and Boundary Regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Safdari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1585" to="1594" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">New response evaluation criteria in solid tumours: Revised RECIST guideline (version 1.1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Eisenhauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Therasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bogaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dancey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arbuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gwyther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dodd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lacombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verweij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Cancer</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="247" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RadLex: a new method for indexing online educational materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiographics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1595" to="1597" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy-or Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">V-Net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting and classifying lesions in mammograms with Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ribli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horv?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pollner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Csabai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning in medical imaging and radiation therapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sahiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pezeshk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Hadjiiski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Drukker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Giger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-Automatic RECIST Labeling on CT Scans with Cascaded Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1806.09507" />
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">CT-realistic data augmentation using generative adversarial network for robust lymph node segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2512004</idno>
		<ptr target="https://doi.org/10.1117/12.2512004" />
		<editor>SPIE. p.</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">109503</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ULDor: A Universal Lesion Detector for CT Scans with Pseudo Masks and Hard Negative Example Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ISBI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Joint learning for pulmonary nodule segmentation, attributes and malignancy prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1109" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D Context Enhanced Region-Based Convolutional Neural Network for End-to-End Lesion Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="511" to="519" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sandfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<title level="m">Holistic and Comprehensive Annotation of Clinically Significant Findings on Diverse CT Images : Learning from Radiology Reports and Label Ontology</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeepLesion: automated mining of largescale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JMI.5.3.036501</idno>
		<ptr target="https://doi.org/10.1117/1.JMI.5.3.036501" />
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
