<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A General Pipeline for 3D Detection of Vehicles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
						</author>
						<title level="a" type="main">A General Pipeline for 3D Detection of Vehicles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autonomous driving requires 3D perception of vehicles and other objects in the in environment. Much of the current methods support 2D vehicle detection. This paper proposes a flexible pipeline to adopt any 2D detection network and fuse it with a 3D point cloud to generate 3D information with minimum changes of the 2D detection networks. To identify the 3D box, an effective model fitting algorithm is developed based on generalised car models and score maps. A two-stage convolutional neural network (CNN) is proposed to refine the detected 3D box. This pipeline is tested on the KITTI dataset using two different 2D detection networks. The 3D detection results based on these two networks are similar, demonstrating the flexibility of the proposed pipeline. The results rank second among the 3D detection algorithms, indicating its competencies in 3D detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Vision-based car detection has been well developed and widely implemented using deep learning technologies. The KITTI <ref type="bibr" target="#b0">[1]</ref> benchmark site reports that the state of the art algorithms are able to achieve ? 90% average precision (AP).</p><p>However, for autonomous vehicles, car detection in 2D images is not sufficient to provide enough information for the vehicle to perform planning and decision making due to the lack of depth data. For a robust and comprehensive perception system in autonomous vehicle, 3D car detection, including car dimensions, locations and orientations in the 3D world, is essential. However the state of the art for 3D car detection algorithms only achieve 62% AP. Gaps still exist as compared to the 2D detection performance and the problem remains as challenging.</p><p>According to the types of input sources, the current algorithms for 3D vehicle detection can be categorised into four major groups, including (1) mono image based, <ref type="bibr" target="#b1">(2)</ref> stereo image, (3) LiDAR (Light Detection and Ranging), and (4) fusion between mono image and Lidar.</p><p>Mono images lack the depth information to recover the 3D location of detected obstacles, therefore assumptions and approximations have to be made. Stereo image based approaches normally involve the construction of depth maps from stereo correspondence matching. The performance of this type of approach depends heavily on the depth map reconstruction and the accuracy drops as distance from the vehicle increases.</p><p>LiDAR, despite its high cost, is able to provide the most direct measurement of object location. But it lacks color information and it is always sparse which poses difficulties in classification. In order to make use of the full capabilities of LiDAR and camera, fusion approaches have been proposed in the literature. To make use of the deep CNN architecture, the point cloud needs to be transformed into other formats. In the process of transformation, information is lost.</p><p>The prior approaches for 3D vehicle detection are not as effective as those for 2D detection. Little attention has been paid to how to transfer the advantages and lessons learnt from 2D detection approaches to 3D detection approaches. Moreover, the field is lacking effective 3D detection approaches that enable the existing 2D approaches to provide 3D information. The state of the art 2D approaches can not be applied to autonomous vehicles which require 3D information.</p><p>In this paper, we propose a flexible 3D vehicle detection pipeline which can make use of any 2D detection network and provide accurate 3D detection results by fusing the 2D network with a 3D point cloud. The general framework structure is illustrated in <ref type="figure">Fig. 1</ref>. The raw image is passed to a 2D detection network which provides 2D boxes around the vehicles in the image plane. Subsequently, a set of 3D points which fall into the 2D bounding box after projection is selected. With this set, a model fitting algorithm detects the 3D location and 3D bounding box of the vehicle. And then another CNN network, which takes the points that fit into the 3D bounding box as input, carries out the final 3D box regression and classification. It requires minimum efforts to modify the existing 2D networks to fit into the pipeline, adding just one additional regression term at the output layer to estimate the vehicle dimensions. The main contributions of the paper are: 1) A general pipeline that enables any 2D detection network to provide accurate 3D detection information. 2) Three generalised car models with score maps, which achieve a more efficient model fitting process. 3) A two-stage CNN that can further improve the detection accuracy.</p><p>This pipeline has been tested using two outstanding 2D networks, PC-CNN <ref type="bibr" target="#b20">[20]</ref> and MS-CNN <ref type="bibr" target="#b21">[21]</ref>. The 3D detection performances based on both networks were evaluated using the KITTI dataset <ref type="bibr" target="#b0">[1]</ref>. We significantly lead the majority of the algorithms in both bird eye detection and 3D detection tasks. We also achieved comparable results to the current state of the art algorithm MV3D <ref type="bibr" target="#b19">[19]</ref> in both tasks.   <ref type="figure">Fig. 1</ref>: General fusion pipeline. All of the point clouds shown are in 3D, but viewed from the top (bird's eye view). The height is encoded by color, with red being the ground. A subset of points is selected based on the 2D detection. Then, a model fitting algorithm based on the generalised car models and score maps is applied to find the car points in the subset and a two-stage refinement CNN is designed to fine tune the detected 3D box and re-assign an objectiveness score to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>This section reviews the works that are related to the proposed pipeline in details. It also highlights the differences between our proposal and the prior works.</p><p>Mono Image Approaches: In [2], a new network was designed to estimate the car dimensions, orientations and probabilities given a detected 2D box from an existing network. Using the criterion the perspective projection of a 3D box should fit tightly with the 2D box in the image, 3D box was recovered by using the estimated information. Similarly in DeepMANTA <ref type="bibr" target="#b2">[3]</ref>, the vehicle orientation and size were estimated from a deep CNN. Additionally, the network also estimated 36 locations of key points on the car in the image coordinates. A 2D/3D shape matching algorithm <ref type="bibr" target="#b3">[4]</ref> was applied to estimate vehicle 3D poses based on these 36 2D part locations.</p><p>Another set of algorithms, e.g. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref>, defined 3D car models with occlusion patterns, carried out detection of the patterns in the 2D image and recovered the 3D occluded structure by reasoning through a MAP (maximum a posteriori) framework.</p><p>These approaches are sensitive to the assumptions made and the parameter estimation accuracy. As shown in the result section, our method outperforms them significantly.</p><p>Stereo Image Approaches: The depth map from stereo correspondence is normally appended to the RGB image as the fourth channel. The RGB-D image is passed to one or more CNNs in order to carry out the detection. In <ref type="bibr" target="#b8">[9]</ref>, Pham et al. proposed a two-stream CNN where the depth channel and RGB channel went through two separate CNN branches and were fused before the fully connected layers.</p><p>Lidar Approaches: The common framework involves three steps: pre-processing (e.g. voxelization), segmentation and classification. A detailed review of LiDAR approaches can be found in <ref type="bibr" target="#b9">[10]</ref>. Wang et al. <ref type="bibr" target="#b10">[11]</ref> proposed a different approach where the point cloud was converted into 3D feature grids and a 3D detection window was slid through the feature grids to identify vehicles. In <ref type="bibr" target="#b11">[12]</ref>, the point cloud was converted into a 2D point map and a CNN was designed to identify the vehicle bounding boxes in the 2D point map. In <ref type="bibr" target="#b12">[13]</ref>, the authors extended the approach of <ref type="bibr" target="#b11">[12]</ref> and applied 3D deep CNN directly on the point cloud. However, this approach is very time consuming and memory intensive due to the 3D convolutions involved. To improve, <ref type="bibr" target="#b13">[14]</ref> proposed a voting mechanism able to perform sparse 3D convolution.</p><p>Fusion Approaches: In <ref type="bibr" target="#b17">[17]</ref>, the sparse point cloud is converted to a dense depth image, which is similar to a stereo one. The RGB-D image was passed through a CNN for detection. In <ref type="bibr" target="#b18">[18]</ref>, the point cloud was converted into a three-channel map HHA which contains horizontal disparity, height above ground and angle in each channel. The resulting six-channel image RGB-HHA was processed by a CNN for detection of vehicles. However these two methods will not be able to output the 3D information directly from the network.</p><p>In oder to address this, MV3D (multi-view 3D) detection network proposed by Chen et al. <ref type="bibr" target="#b19">[19]</ref> included one more type of input generated from the point cloud, the bird's eye view feature input. This input has no projective loss as compared to the depth map, thus 3D proposal boxes can be generated directly. This approach has achieved the current state of the art in 3D vehicle detection. It generates 2D boxes from 3D boxes while ours generate 3D boxes from 2D boxes. And MV3D explores the entire point cloud while ours only focus on a few subsets of the point cloud, which is more efficient and saves computation power.</p><p>2D Detection: The proposed pipeline is flexible in regards to the choice of 2D detection networks. Only a slight change is required on the last fully connected layer of the network so that it is able to estimate the dimensions of the cars. Both <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref> proposed ways to encode the car dimensions to the network. For better accuracy, the 2D detection networks proposed in <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b23">[23]</ref> can be incorporated since they are the leading networks for 2D detection. For faster computation, the approaches presented in <ref type="bibr" target="#b24">[24]</ref> and <ref type="bibr" target="#b25">[25]</ref> can be implemented. In this paper, we implement PC-CNN <ref type="bibr" target="#b20">[20]</ref> and MS-CNN <ref type="bibr" target="#b21">[21]</ref> to demonstrate the flexibility of the pipeline.</p><p>Model Fitting: In <ref type="bibr" target="#b4">[5]</ref>, Xiang et al. proposed 3D voxel patterns (3DVPs) as the 3D car model. 3DVPs encode the occlusion, self-occlusion and truncation information. A boosting detector was designed to identify the 3DVPs in the image, while <ref type="bibr" target="#b5">[6]</ref> implemented a sub-category awareness CNN for 3DVP detection.</p><p>Deformable part-based models (DPM) can be found in <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref> and <ref type="bibr" target="#b29">[29]</ref>. Different classifiers were trained to detect DPM. Fidler et al. extended the DPM to a 3D cuboid model in <ref type="bibr" target="#b30">[30]</ref> in order to allow reasoning in 3D. In <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b2">[3]</ref>, 3D wireframe models were used. Similarly, each wire vertex is encoded with its visibility.</p><p>Due to the various vehicle types, sizes, and occlusion patterns, these prior approaches require a substantial number of models in order to cover all possible cases. In our approach, only three models are used and the occlusion pattern is assigned online when doing model fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TECHNICAL APPROACH</head><p>The input is an image. The first step is to generate 2D bounding boxes for the candidate vehicles. Secondly, these bounding boxes are used to select subsets of the point clouds, using the transformation between the camera and LiDAR. Due to the perspective nature of the camera, the 3D point subset may spread across a much larger area than the vehicle itself as shown in <ref type="figure">Fig.1</ref>. This subset also contains a substantial number of non-vehicle points and points on neighbouring vehicles. All these artefacts add challenges to the 3D box detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Car dimension estimation</head><p>One additional regression layer is needed at the end of the given 2D detection network. This regression method was inspired by <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b1">[2]</ref>. First the average dimensions for all the cars and vans in KITTI dataset is obtained. Let [h,l,w] denote height, length and width of the vehicle. The ground truth regression vector ? * i = (? h , ? l , ? w ) is defined as:</p><formula xml:id="formula_0">? h = log(h * /h) ? l = log(l * /l) ? w = log(w * /w) (1)</formula><p>The dimension regression loss is shown as:</p><formula xml:id="formula_1">L d (i) = ? d C i R(? i ? ? * i )<label>(2)</label></formula><p>where ? d is the weighting factor to balance the losses defined in the original network, e.g. classification loss, 2D box regression loss; C i is 1 if the 2D box is a car and 0 otherwise; R is the smooth L 1 loss function defined in <ref type="bibr" target="#b32">[32]</ref> and ? i is the regression vector from the network. To train the modified network, we can reuse the pre-trained weights from the original network for initialisation. Only a small part of the network needs to be re-trained while the rest can be kept as fixed during training. For example, in MS-CNN, we only re-trained the convolution layer and the fully connected layers after ROI pooling in the detection subnetwork and in PC-CNN, we re-trained the GoogleNet layer, convolution layer and the fully connected layers after the Deconvolution layer in the detection sub-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vehicle model fitting</head><p>We first generate a set of 3D box proposals. For each proposal, the points within the 3D box are compared to the three generalised car models. The proposal with the highest score is selected for the two-stage CNN refinement.</p><p>The 3D box proposals are generated following the principle of RANSAC algorithm (random sample consensus). In each iteration, one point is selected randomly. A second point is randomly selected from points within the cube centred at the first point and with the side length of 1.5l, where l is the car length from the 2D CNN dimension estimation and 1.5 compensates the estimation error. A vertical plane is derived from these two points. Any points with a distance to the plane less than a threshold are considered as inliers to the plane. A maximum 20 points are then randomly selected from the inliers. At each point, a second vertical plane, passing through that point and perpendicular to the first vertical plane, is derived.</p><p>Along the intersection line between these two vertical planes, eight 3D boxes can be generated based on the estimated car width and length. Since the first vertical plane is visible, based on the view direction, four boxes are eliminated. At each of the remaining box locations, a new range is defined by expanding the box by 1.5 times along both w and l directions. The lowest point within the new range can be found and it determines the ground of the 3D box while the roof of the 3D box is set based on the height estimation. In summary, at each iteration, maximum 80 3D box proposals can be generated. Only three generalised car models are used for model fitting. They represent three categories of cars, SUVs, Sedans and Vans. Hatchback cars are considered to be SUVs. We observe that the relative distances/positions between different parts of a car do not vary significantly for different cars from the same category even with different sizes. This invariance indicates that if the cars under the same category are normalised to the same dimension [h, l, w], their shapes and contours will be similar. We verified this and generalised the car models by normalising the cars in the 3D CAD dataset used in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b30">[30]</ref> and <ref type="bibr" target="#b33">[33]</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the side view of the point cloud plots for the three categories. Each plot is an aggregation of the points that are generated from the 3D CAD models, aligned to the same direction and normalised to the same dimension. The SUV/hatchback plot consists of points from 58 CAD models, the sedan plot consists of 65 point sets, and the van plot consists of points from 10 models.</p><p>Each aggregation is then voxelized to a 8 ? 18 ? 10 matrix along the [h l w] direction. Each element in the matrix is assigned different scores based on its position. The elements representing the car shell/surface are assigned a score of 1, indicating that 3D points in the model fitting process fall on the car surface will be counted towards the overall score. The elements inside or outside the car shell are assigned negative scores, and the further away they get from the car shell (either inwards or outwards), the smaller the assigned values. This indicates that no points should be detected from outside or inside the car by LiDAR and the overall score will be penalised for such detections. The elements at the bottom layer of the matrix are assigned a score of 0. Points detected at the bottom layer could be the ground or car's tires, which are difficult to distinguish from each other. They will not be penalised or counted. Self-occlusion can be easily determined from the view direction. This is encoded online when doing the model fitting since view direction changes for different 3D box proposals. Negative scores are assigned to the car surface elements if they are self-occluded. Furthermore, for simplicity, only the four vertical facets are considered for self-occlusion analysis while car roof and bottom are not considered.</p><p>Two slices of the score assignment from the SUV category are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, with the left image depicting the side facet and the right image illustrating the center slice. The car exterior and interior are indicated by orange and blue while the bottom is indicated white. Yellow and green refer to the shell/surface of the car, while green further indicates that those areas might be self-occluded.</p><p>Points within the 3D box proposals will be voxelised into 8 ? 18 ? 10 grids and compared to the three potential vehicle models. Due to the orientation ambiguity, the grids are rotated around their vertical center axis by 180 degree and are then compared to the three models. Out of all the bounding box proposals, the one with the highest score is selected for the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Two-stage refinement CNN</head><p>To further align the detected 3D box to the point cloud, we designed a two-stage CNN. In the literature, 3D CNNs are commonly used to process 3D point clouds, e.g. <ref type="bibr" target="#b11">[12]</ref>. However, these CNNs are extremely slow and memory intensive. In this paper, we found that 2D CNNs are sufficient.</p><p>With the points in a given 3D box, the first CNN outputs a new 3D box. A new set of points can be found within the new 3D box. The second CNN outputs a probability score based on the new set of points to indicate how likely these points represent an actual car.</p><p>However, point sets cannot be input to the CNN directly. We apply normalization and voxelization strategies in order to formalize the points in matrix form in order to fit to the CNN. Furthermore, consistent with 2D image detection cases <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b21">[21]</ref>, a bounding box context is able to provide additional information to improve the detection accuracy. We also include the context of the 3D bounding box as input to the CNN.</p><p>Given a 3D box from the model fitting process, our pipeline expands it along its h, l, w directions by 1.5, 1.5, and 1.6 times respectively to include its context. The points inside this expanded box are normalised and voxelised into a 24 ? 54 ? 32 matrix. The matrix is sparse with ? 0.6% occupied elements on average. As compared to the generalised model, we doubled the resolution of the voxelisation in order to reserve more spatial details and patterns of the distribution of the points. Note that the normalisation is anisometric, it has different scale ratios along different directions.</p><p>The backbones of the CNNs in both stages are based on the VGG-net with configuration D as described in <ref type="bibr" target="#b35">[35]</ref>. After each convolution, an ELU (exponential linear units) layer <ref type="bibr" target="#b36">[36]</ref>, instead of Re(ctified)LU layer, is adopted for a more stable training process. The first stage CNN has two parallel outputs, one for 3D box regression and the other for classification, while the second stage CNN only has one output, classification.</p><formula xml:id="formula_2">? * x c = (X * c ? X c )/L ? * y c = (Y * c ?Y c )/H ? * z c = (Z * c ? Z c )/W ? * x l = (X * l ? X l )/L ? * y l = (Y * l ?Y l )/H ? * z l = (Z * l ? Z l )/W ? * w = log(W * /W )<label>(3)</label></formula><p>The classification loss for both CNNs is So f tMax loss and the 3D box regression loss is SmoothL 1 loss. The ground truth regression vector ? * 3d defined in (3) has seven elements, three for the center of the box, three for the left bottom corner and one for the width. It is just sufficient to recover the 3D bounding box from these seven elements. Due to the anisometric normalisation, a quartic polynomial needs to be solved. Note that across all the inputs, X c/l , Y c/l , Z c/l , L, H, W are all constant as all the 3D boxes are aligned and normalised to the same size.</p><p>Classification has two classes, car and background. A 3D box is classified as positive when the IOU (intersection of union) between its bird's eye view box and the ground truth bird's eye view box is greater than a specific threshold. This threshold is 0.5 for the first stage CNN and 0.7 for the second. 0.7 is consistent with the criteria set by KITTI benchmark. The reason to set a lower threshold for the first stage is to train the network so that it is able to refine the boxes with IoU between 0.5 to 0.7 to a better position where the IoU may be greater than 0.7; otherwise the network will take those boxes as negative and will not be trained to refine them.</p><p>The training of the two networks is carried out independently as they do not share layers. The training batch size is 128, with 50% being positive. Both CNNs are trained for 10K iterations with a constant learning rate of 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT RESULTS AND DISCUSSION</head><p>To verify the flexibility of our approach, the pipeline is tested using PC-CNN <ref type="bibr" target="#b20">[20]</ref> and MS-CNN <ref type="bibr" target="#b21">[21]</ref>. The performance based on both networks is evaluated using the challenging KITTI dataset <ref type="bibr" target="#b0">[1]</ref>, which contains 7481 images for training/validation and 7518 images for testing. The training/validation set has annotated ground truth for 2D  <ref type="bibr" target="#b1">[2]</ref>, which uses different validation set, so its APs are calculated from the 1848 common images with our validation set.</p><p>bounding box in the image plane and 3D bounding box in real world. Following <ref type="bibr" target="#b37">[37]</ref>, we split the training/validation set into training and validation sub-sets. The training sub-set is purely used to train the car dimension regression and twostage CNN while the validation sub-set is for evaluation only. KITTI divides the cars into easy, moderate and hard groups based on their visibilities. We follow this same convention for our evaluation. To further verify the performance of the proposed pipeline, we also tested it using our own autonomous vehicles.</p><p>Metrics: The primary focus of this paper is on 3D detection, we do not evaluate the performance of the pipeline for 2D detection tasks. Following the evaluation metrics proposed in <ref type="bibr" target="#b19">[19]</ref>, we evaluate our proposal based on the Average Precession (AP) for bird's eye view boxes and for 3D boxes. The bird's eye view boxes are generated by projecting the 3D boxes on the same ground plane. The AP is calculated based on the IoU between the output boxes and the ground truth boxes, while in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b2">[3]</ref>, the distance between two boxes are used. We feel that IoU is a more comprehensive index than distance, as it implicitly accounts for not only distance but also alignment and size.</p><p>Bird's Eye View &amp; 3D Box AP: We compare the outputs from our pipeline with other algorithms which can output 3D box information, including Mono3D <ref type="bibr" target="#b38">[38]</ref>, 3DOP <ref type="bibr" target="#b4">[5]</ref> and Deep3DBox <ref type="bibr" target="#b1">[2]</ref> which use image data only, VeloFCN <ref type="bibr" target="#b11">[12]</ref> which uses LiDAR data only and MV3D <ref type="bibr" target="#b19">[19]</ref> which uses fusion.</p><p>The IoU threshold for true positive detection is set at 0.5 and 0.7. The left part of TABLE I shows the results from bird's eye view. In general, the point cloud based approaches all significantly lead the image-based approaches for both IoUs. Within the point cloud based approaches, our pipeline outperforms VeloFCN significantly but underperforms MV3D marginally. When IoU = 0.5, our performance with PC-CNN is about 7% worse on average than MV3D and 5% worse for MS-CNN. When IoU = 0.7, the performances with PC-CNN and MS-CNN are both very close to MV3D except for the performance with PC-CNN for the hard group (7% worse than MV3D).</p><p>The 3D box detection comparisons are listed in the right part of TABLE I. Similarly for both IoU thresholds, our method significantly outperforms all the approaches with the single exception of MV3D. On average, the overall performance is about 10% worse than MV3D for both IoU = 0.5 and 0.7 except that the performance with MS-CNN for moderate group at IoU = 0.5 is only 1.6% less than MV3D.</p><p>We only use point clouds to generate the 3D box and do not take any color information from the image into account. Comparing it to VeloFCN, which also only takes point clouds as inputs, shows the effectiveness of our approach, processing the point cloud as subsets instead of as a whole. Comparing to MV3D, image color information is necessary to further boost the performance of our pipeline. One possible solution is to extract the feature layer which is right before the ROI pooling layer in the 2D detection CNN. Based on the 3D box from the model fitting process, we could find its corresponding 2D box in the image plane and carry out ROI pooling on the extracted feature layer in order to extract the feature vector. Then fuse the feature vector with the one from the refinement CNN to output the final 3D box and its probability.</p><p>Flexibility Anlysis: The comparison between the two approaches using the proposed pipeline verifies the flexibility of our pipeline. PC-CNN and MS-CNN have different network structures. But both approaches achieve comparable AP for the two tasks and IoU thresholds. Furthermore, the two-stage refinement CNN was trained based on the pipeline with PC-CNN and re-used in the pipeline with MS-CNN without any further tuning on the network. This further confirms the flexibility and adaptability of our proposed pipeline.</p><p>Car Dimension Regression Impact: We show the impact from the car dimension regression on the original 2D detection CNN in TABLE II. Similarly, AP is populated for the 2D detection task in image plane. Following KITTI, the IoU threshold is set at 0.7. The left part shows the performance of the original 2D detection CNN while the right part indicates the results after appending the car dimension regression term. The impact is not very significant for both networks, and it even improves the performance marginally for some groups.  With the first CNN, the detection performance is improved significantly in both bird's eye view and 3D box tasks. The improvement is ? 10% and ? 30% respectively. This shows that although only 2D convolution is used and the input 3D matrix is very sparse, the network is still very powerful and effective to locate the 3D box. The improvement from the second CNN is insignificant since it is not designed to regress the 3D box. It is designed to reshuffle the probability of the 3D box from the first CNN.</p><p>Qualitative Results: The first row in <ref type="figure">Fig. 4</ref> shows some of the 3D detection results by applying our pipeline with PC-CNN on KITTI validation dataset. We also tested it using our own dataset collected at Boston USA. The setup of the data collection vehicle is similar to KITTI, with differences in the relative positions between the LiDAR, camera and the car. We applied the pipeline, which is trained based on KITTI training dataset, directly on the Boston data without any fine-tuning of the network weights. The system still works as shown in the second row of <ref type="figure">Fig. 4</ref>. It shows the generalisation capability of the proposed pipeline and indicates its potentials in executing 3D vehicle detection in real situations beyond a pre-designed dataset. Interested readers may refer to the link for video illustrations (https://www.dropbox.com/ s/5hzjvw911xa5mye/kitti_3d.avi?dl=0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper we propose a flexible 3D vehicle detection pipeline which is able to adopt the advantages of any 2D detection networks in order to provide 3D information. The effort to adapt the 2D networks to the pipeline is minimal. One additional regression term is needed at the network output to estimate vehicle dimensions. The pipeline also takes advantage of point clouds in 3D measurements. An effective model fitting algorithm based on generalised car models and score maps is proposed to fit the 3D bounding boxes from the point cloud. Finally a two-stage CNN is developed to fine tune the 3D box. The outstanding results based on two different 2D networks indicate the flexibility of the pipeline and its capability in 3D vehicle detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Generalised car models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Score map (scores are indicated at bottom.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Average Precision benchmark for bird's eye view and 3D box based on KITTI validation set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Bird's Eye View</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3D Box</cell></row><row><cell>Algorithm</cell><cell></cell><cell>IoU = 0.5</cell><cell></cell><cell></cell><cell>IoU = 0.7</cell><cell></cell><cell></cell><cell>IoU = 0.5</cell><cell></cell><cell></cell><cell>IoU = 0.7</cell></row><row><cell></cell><cell cols="3">Easy Moderate Hard</cell><cell cols="3">Easy Moderate Hard</cell><cell cols="3">Easy Moderate Hard</cell><cell cols="2">Easy Moderate Hard</cell></row><row><cell>*  Mono3D [38]</cell><cell>30.50</cell><cell>22.39</cell><cell cols="2">19.16 5.22</cell><cell>5.19</cell><cell>4.13</cell><cell>25.19</cell><cell>18.20</cell><cell cols="2">15.52 2.53</cell><cell>2.31</cell><cell>2.31</cell></row><row><cell>*  3DOP [5]</cell><cell>55.04</cell><cell>41.25</cell><cell cols="2">34.55 12.63</cell><cell>9.49</cell><cell>7.59</cell><cell>46.04</cell><cell>34.63</cell><cell cols="2">30.09 6.55</cell><cell>5.07</cell><cell>4.10</cell></row><row><cell>*  *  Deep3DBox [2]</cell><cell>29.96</cell><cell>24.91</cell><cell cols="2">19.46 9.01</cell><cell>7.94</cell><cell>6.57</cell><cell>24.76</cell><cell>21.95</cell><cell cols="2">16.87 5.40</cell><cell>5.66</cell><cell>3.97</cell></row><row><cell>*  VeloFCN [12]</cell><cell>79.68</cell><cell>63.82</cell><cell cols="2">62.80 40.14</cell><cell>32.08</cell><cell cols="2">30.47 67.92</cell><cell>57.57</cell><cell cols="2">52.56 15.20</cell><cell>13.66</cell><cell>15.98</cell></row><row><cell>*  MV3D [19]</cell><cell>96.34</cell><cell>89.39</cell><cell cols="2">88.67 86.55</cell><cell>78.10</cell><cell cols="2">76.67 96.02</cell><cell>89.05</cell><cell cols="2">88.38 71.29</cell><cell>62.68</cell><cell>56.56</cell></row><row><cell cols="2">Ours (MS-CNN [21]) 90.36</cell><cell>88.46</cell><cell cols="2">84.75 82.17</cell><cell>77.15</cell><cell cols="2">74.42 87.16</cell><cell>87.38</cell><cell cols="2">79.40 55.82</cell><cell>55.26</cell><cell>51.89</cell></row><row><cell cols="2">Ours (PC-CNN [20]) 88.31</cell><cell>83.74</cell><cell cols="2">79.62 83.61</cell><cell>77.36</cell><cell cols="2">69.61 87.69</cell><cell>79.92</cell><cell cols="2">78.65 57.63</cell><cell>51.74</cell><cell>51.39</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* sources from [19]* * sources from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Impact on the original 2D detection CNN from appending the car dimension regression term. To analyse the effectiveness of the steps involved in the 3D box generation, the AP is calculated after each step (model fitting, first stage CNN and second stage CNN) for both bird's eye view and 3D box tasks. For this study, the IoU threshold is set to 0.5. Since the results based on MS-CNN and PC-CNN are quite comparable, only PC-CNN results are presentedin TABLE III.The results from the model fitting are not as good as the final oens, but they are better than all the image based algorithms in TABLE I and comparable to VeloFCN. This indicates that the model fitting algorithm can work properly.</figDesc><table><row><cell cols="7">Fig. 4: Qualitative result illustration on KITTI data (top row) and Boston data (bottom row). Blue boxes are the 3D detection</cell></row><row><cell>results.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ablation Study:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2D Detection</cell><cell cols="3">Original Easy Moderate Hard</cell><cell cols="3">With Dimension Regression Easy Moderate Hard</cell></row><row><cell>MS-CNN</cell><cell>91.64</cell><cell>89.95</cell><cell cols="2">79.55 93.98</cell><cell>89.92</cell><cell>79.69</cell></row><row><cell>PC-CNN</cell><cell>94.62</cell><cell>89.60</cell><cell cols="2">79.97 90.22</cell><cell>89.03</cell><cell>81.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Ablation study based on KITTI validation set. Numbers indicate AP with IoU threshold at 0.5.</figDesc><table><row><cell>Step</cell><cell cols="3">Bird's Eye View Easy Moderate Hard</cell><cell cols="3">3D Box Easy Moderate Hard</cell></row><row><cell cols="2">Model fitting 77.71</cell><cell>73.27</cell><cell cols="2">70.06 56.32</cell><cell>51.33</cell><cell>47.40</cell></row><row><cell>First CNN</cell><cell>88.16</cell><cell>83.60</cell><cell cols="2">79.65 87.51</cell><cell>79.76</cell><cell>78.81</cell></row><row><cell cols="2">Second CNN 88.31</cell><cell>83.74</cell><cell cols="2">79.62 87.69</cell><cell>79.92</cell><cell>78.65</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Xinxin Du is with the Singapore-MIT Alliance for Research and Technology (SMART) Centre, Singapore xinxin@smart.mit.edu 2 Marcelo H. Ang Jr. is with the National University of Singapore, Singapore mpeangh@nus.edu.sg 3 Sertac Karaman and Daniela Rus are with the Massachusetts Institute of Technology, Cambridge, MA, USA sertac@mit.edu rus@csail.mit.edu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This research was supported by the National Research Foundation, Prime Minister's Office, Singapore, under its CREATE programme, Singapore-MIT Alliance for Research and Technology (SMART) Future Urban Mobility (FM) IRG. We are grateful for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00496</idno>
		<title level="m">3d bounding box estimation using deep learning and geometry</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuli?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o (n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1903" to="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Subcategory-aware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="924" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are cars just 3d boxes?-jointly estimating the 3d shape of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3678" to="3685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explicit occlusion modeling for 3d object class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3326" to="3333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust object proposals re-ranking for object detection in autonomous driving using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perception, planning, control, and coordination for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meghjani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machines</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07916</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08069</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1355" to="1361" />
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation of dense range information in complex urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Schoenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2033" to="2038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crf based road detection with multi-sensor fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="192" to="198" />
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for robust rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fusing lidar and images for pedestrian detection using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2198" to="2205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Car detection for autonomous vehicle: Lidar and vision fusion approach through deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H A</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A unified multiscale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2129" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Teaching 3d geometry to deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3362" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6" to="7" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Supervised learning and evaluation of kitti&apos;s cars detector with dpm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Yebes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?zaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="768" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-view and 3d deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2232" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d object detection and viewpoint estimation with a deformable 3d cuboid model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detailed 3d representations for object recognition and modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2608" to="2623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Beat the mturkers: Automatic image labeling from weak 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3198" to="3205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
