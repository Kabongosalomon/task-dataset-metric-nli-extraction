<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open Source Automatic Speech Recognition for German</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Milde</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>K?hn</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">group, FB Informatik</orgName>
								<orgName type="institution">Universit?t Hamburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology 1 and Natural language Systems</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open Source Automatic Speech Recognition for German</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High quality Automatic Speech Recognition (ASR) is a prerequisite for speech-based applications and research. While state-of-the-art ASR software is freely available, the language dependent acoustic models are lacking for languages other than English, due to the limited amount of freely available training data. We train acoustic models for German with Kaldi on two datasets, which are both distributed under a Creative Commons license. The resulting model is freely redistributable, lowering the cost of entry for German ASR. The models are trained on a total of 412 hours of German read speech data and we achieve a relative word error reduction of 26% by adding data from the Spoken Wikipedia Corpus to the previously best freely available German acoustic model recipe and dataset. Our best model achieves a word error rate of 14.38 on the Tuda-De test set. Due to the large amount of speakers and the diversity of topics included in the training data, our model is robust against speaker variation and topic shift.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past years a lot of progress has been made to make Automatic Speech Recognition (ASR) more robust and practicable, mainly due to incoporating (deep) neural networks as central part of the recognition pipeline. There has also been a shift towards making the underlying recognition software more accessible. With the introduction of the Kaldi toolkit <ref type="bibr" target="#b0">[1]</ref>, a state-of-the-art open source toolkit for speaker-independent large vocabulary ASR became available for researchers and developers alike. Over the past years, it has evolved into a very popular open source ASR toolkit, either pushing state-of-the-art acoustic models or following their performance closely.</p><p>For English, open source resources to train Kaldi acoustic models as well as language models and a phoneme lexicon exist, in sufficient quality and quantity: TED-LIUM <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and Librispeech (1000h) <ref type="bibr" target="#b3">[4]</ref> allow large-scale training of speech recognizers, with word error rates in the low single digits in their respective domains (6.5% WER for presentation speech, 3.2% WER for clean read speech <ref type="bibr" target="#b4">[5]</ref>). These resources exist alongside proprietary resources, such as: TIMIT <ref type="bibr" target="#b5">[6]</ref>, Switchboard <ref type="bibr" target="#b6">[7]</ref> and the Fisher corpus <ref type="bibr" target="#b7">[8]</ref>. The latter two also enable low word error rates (WERs) on more difficult spontaneous conversational telephone speech test sets (e.g. for Switchboard 5.5% WER in <ref type="bibr" target="#b8">[9]</ref>, within close range of human performance).</p><p>However, models trained from open source and freely available resources allow personal, academic and commercial use cases without licensing issues, lowering the barrier of entry. Having access to a locally running speech recognition software (or a private server instance) solves privacy issues of speech APIs from cloud providers. English speech recognition models for Kaldi are available as pretrained packages or freely available training recipes and these models are used in the wild for down-stream NLP applications, e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. We would like to establish the In the remainder of the paper we discuss the freely available data resources for German and our recognition results. One of our data resources is automatically aligned data from the Spoken Wikipedia project. This is a very interesting resource, as new speech data is consistently added to the project by volunteers (see the growth rate in <ref type="figure">Figure 2</ref>) and the training process can be extended form time to time with new data. Our final model can deal with different microphone and unknown speakers in an open vocabulary setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Sets</head><p>In the following, we briefly describe the data resources that we used to train our models. Also, in <ref type="table">Table 1</ref> we give an overview of the amount of available training data. All of the following resources are freely available and are published with permissive Creative Commons open source licenses (i.e. free for all, commercial use allowed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spoken Wikipedia Corpus</head><p>The Spoken Wikipedia 1 is a project run by volunteers to read and record Wikipedia articles. The audio files produced are linked to the Wikipedia articles, with semi-structured metadata attached. Sub-projects exist for many languages, but English, German, and Dutch are the largest ones, by a large margin. The Spoken Wikipedia Corpora <ref type="bibr" target="#b11">[12]</ref> (SWC) are a collection of time-aligned spoken Wikipedia articles for Dutch, English and German using a fully automated pipeline to download, normalize and align the data. Crucially, the exact correspondences to original articles is preserved in the alignment data. For German, both an alignment of normalized words as well as a phone-based alignment exists. We use word-based alignments.</p><p>Being based on found data, the alignments are not perfect: Parts of the articles are not aligned at all, e.g. because of incorrect normalization or pronunciation that deviates from the expectation. For material such as tables or formulas it is unknown how they will be read (or if they are read at all) and they are therefore excluded from the alignment process.</p><p>Being recorded by volunteers reading complete articles, the data fits very well how a user naturally speaks, arguably better than a controlled recording in a lab. The vocabulary is quite large due to the encyclopedic nature of the articles. Topics of the articles are diverse and range from obscure technical articles like "Brainfuck" (an esoteric programming language) to a description of "Isar" (a river).</p><p>To train Kaldi on the Spoken Wikipedia, we adapted the pre-existing pipeline from the Spoken Wikipedia Corpora which bootstraps the Sphinx speech recognizer <ref type="bibr" target="#b12">[13]</ref> used for the SWC alignment by iteratively training new models on the data of the previous alignment. As a speech recognizer can not be trained on partly aligned long audio (some recordings last several hours), the SWC pipeline contains a snippet extractor which searches for continuously aligned data of appropriate length. The snippet extractor generates training segments along Voice Activity Detection (VAD) boundaries and discards utterances that are too short (smaller than 0.6 seconds), have more than 20% of unaligned data, more than two consecutive unaligned words, an unaligned word at the beginning or end or pauses longer than 1.5 seconds. We call this approach conservative pruning, as it tries to minimize errors in the training data.</p><p>In a second setting, we deviate from the pre-existing snippet extraction and extract all segments defined by VAD boundaries for which at least 65% of the words are aligned. There are no other restrictions, e. g. the start and end words do not need to be aligned. We call this approach minimal pruning, as it keeps much more training material than the conservative approach. The German Spoken Wikipedia has 363 speakers who committed 349h of audio to the project, of which 249h could be successfully aligned. Of these, 141h of audio is extracted with conservative pruning and 285h with minimal pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tuda-De</head><p>In <ref type="bibr" target="#b13">[14]</ref>, an Open Source corpus of German utterances was described and publicly released, with a focus on distant speech recognition. Sentences were sourced from different text genres: Wikipedia, parliament speeches, simple command and control prompts. Volunteers, mostly students, read the sentences into four different microphones, placed at a distance of one meter from the speaker. One of these microphones was a Microsoft Kinect. The corpus contains data from the beamformed signal of the Kinect, as well as mixed down single channel raw data from the microphone array (due to driver restrictions the raw multi channel data could not be recorded). Yamaha PSG-01S, a simple USB table microphone and a Samson C01U, a studio microphone, were also used to record audio simultaneously. A further simultaneous recording was made with a built-in laptop microphone (Realtek), at a different position in the room and next to a very noisy fan. For nearly every utterance the corpus contains five sound files, apart from a few where driver hiccups resulted in fewer recordings. Four of these streams are fairly clean and comprehensible, the recordings from the Realtek microphone next to a noisy fan are very difficult to understand, even for humans. Female speakers make up about 30% of the data and most speakers are between 18 and 30 years old. We use version 2 of the corpus. <ref type="bibr" target="#b14">[15]</ref> is an open source Text-To-Speech (TTS) system. It also contains a manually created phoneme dictionary resource for German, containing 26,231 words and their phoneme transcriptions in a dialect of extended SAM-PA BAS <ref type="bibr" target="#b15">[16]</ref>. We use Sequitur <ref type="bibr" target="#b16">[17]</ref> to train a graphemeto-phoneme (G2P) model, to be able to add automatically generated entries for out-of-vocabulary (OOV) words to the lexicon as needed. For the Tuda-De corpus the final lexicon size is 28,131 words; this includes all words from the MARY lexicon and automatically generated entries for all OOV words in the train set. When we combine the Tuda-De transcriptions with the SWC transcriptions, more OOV lexicon entries need to be automatically generated and the final lexicon size grows to 126,794 words using the conservatively pruned SWC data, respectively 182,784 words with minimally pruned SWC data. To measure the effects of an even larger vocabulary size, we also computed the 300,000 most frequent words in the German Wikipedia (April 2018) and generated additional phonetic entries. We merged the vocabulary with the previous lexicon and obtained a larger lexicon containing 350,029 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lexicon</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARY-TTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Language Models</head><p>We used the same text sources as in <ref type="bibr" target="#b13">[14]</ref> and trained similar baseline language models. In particular, we trained 3-gram and 4-gram language models with Kneser-Ney smoothing <ref type="bibr" target="#b17">[18]</ref> and different vocabulary sizes on approximately 8 million German sentences. The sentences are selected from similar sources as the spoken data (Wikipedia, Parliament  and some crawled sentences). Also, they are already filtered, so that sentences from the development and test sets of the Tuda-De corpus are not included in LM training texts. All sentences were normalized using the frontend of the MARY TTS software <ref type="bibr" target="#b14">[15]</ref>, similarly to the normalization process of the SWC corpus. We also use the newly released Kaldi-RNNLM <ref type="bibr" target="#b18">[19]</ref> to train a recurrent neural network based LM on the same text sources. We use the same parameters as in the Switchboard LSTM 1e example: two stacked LSTM layers with a cell width of 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Evaluation</head><p>We use Gaussian Mixture Model (GMM) -Hidden Markov Models (HMM) and Time-Delayed Neural Networks (TDNNs) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> as acoustic models following the chainrecipe (s5_r2) of the TED-LIUM corpus <ref type="bibr" target="#b1">[2]</ref> example in Kaldi. The TDNNs have a width of 1024 neurons. For GMM-HMM models, we adapted the Kaldi egs for the Switchboard corpus (swbd s5c, model tri4). As input to the TDNN we also use online i-vectors (helping with speaker adaptation, c.f. <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>). As the TDNN-HMM chain models are sequence discriminatively trained on the utterances, they are more prone to overfitting and do not cope well with incorrect transcriptions <ref type="bibr" target="#b24">[25]</ref>. Since the SWC transcriptions are aligned from found data, we expect that some of the transcriptions could be problematic, particularly when we apply only minimal pruning to SWC. We follow the recipe used in the Kaldi TED-LIUM TDNN example and clean the training data by decoding it and removing utterances which do not match their supposed transcriptions. While analyzing the cleaned utterances, we also noted that some of the Tuda-De utterances are wrongly annotated, mostly because of hiccups in the recording software <ref type="bibr" target="#b25">[26]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">OOV Rate</head><p>Due to the type of text used in our data sets, the number of unseen words in the test set is quite high, with an OOV rate of 14% for the lexicon with 28k entries, 8% using 126k words and 3.2% using 350k words. The OOV rate poses a lower bound on achievable WER and also explains the large influence of vocabulary size on observed WERs.</p><p>The largest problem the ASR model faced during evaluation was compounding. As German is a very productive language, compounds unknown to the language model are quite frequent, even though the acoustic model is clearly able to recover the information needed. Because the language model tends to create more tokens than in the original text when trying to recognize compounds not in the lexicon, each of such errors is counted as at least two errors: a substitution and an insertion error. For example, the word "nachzumachen" is recognized as "nach zu machen", resulting in three recognition errors: two insertions and a substitution. Overall, about a quarter of the errors (2.6k of the 11.5k with the 350k vocabulary model) are part of a sequence of insertions followed by a substitution, indicative for an error as just described. We manually checked a sample of these errors and could verify that indeed most of them are compounds recognized as multiple word such as "Umweltvereinbarung", "Fu?pfad", or "zweitausendzw?lf".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Differences Between Microphones</head><p>The dev and test set of Tuda-De is recorded using multiple microphones. In <ref type="table" target="#tab_4">Table 3</ref> we calculated WER individually on the dev and test sets per microphone. The differences in recognition accuracy are surprisingly small for Kinect-RAW, Samson and Yamaha recordings. The usual range of WER for TDNN-HMM models we observed for these microphones is between 15.03% and 15.77%. However, the beamformed WER result for the Kinect is significantly higher than decoding the raw (mixed down to one channel) data. The beamforming algorithm of the Microsoft Kinect is closed source, but a few observations are very noticable in the recorded signal. There is a very audible "tin can effect" in the audio signals, probably from a noise suppression algorithm. The beam also seems to get misdirected after pauses, too, c.f. Section 3 in <ref type="bibr" target="#b25">[26]</ref>. The recordings were made with automatic gain control, in some of the utterances the beginning is difficult to understand as a result.</p><p>An exception to the otherwise good results are also WERs from the Realtek microphone. It produced heavily distorted recordings due to a nearby laptop fan, making these recordings very challenging to decode. The data from this microphone is however not officially part of the dev and test set (it is also not included in <ref type="table" target="#tab_2">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Conversational Speech</head><p>In the Verbmobil project <ref type="bibr">(1993)</ref><ref type="bibr">(1994)</ref><ref type="bibr">(1995)</ref><ref type="bibr">(1996)</ref><ref type="bibr">(1997)</ref><ref type="bibr">(1998)</ref><ref type="bibr">(1999)</ref><ref type="bibr">(2000)</ref>, the goal was to establish whether translation of spontaneous speech into other languages is possible <ref type="bibr" target="#b26">[27]</ref>. Conversational speech data was recorded for German, English and Japanese, in the limited domain of scheduling appointments. We used the dev and test data of the first revision of the German subset of the Verbmobil corpus (VM1). Since our acoustic models are trained exclusively on read speech, it provides a good test set showing how well our models cope with a more challenging conversational and spontaneous speaking style.</p><p>In <ref type="table" target="#tab_6">Table 4</ref> we show results for decoding VM1 utterances with our acoustic models. We decode with two different vocabularies and FSTs, a general purpose vocabulary (as also used for the results in <ref type="table" target="#tab_2">Table 2</ref>) and a domain specific vocabulary, using the lexicon words of the VM1 corpus (6851 words). For the latter we recomputed our LM with the reduced vocabulary. We do not use the manual lexicon entries of the VM1 corpus and instead use the same lexicon we use in the general purpose case, reducing it and generating  automatic OOV phoneme lexicon entries as needed.</p><p>The domain specific WER score with limited vocabulary is usually found in the literature for the Verbmobil corpus. A newer reference score for a DNN-HMM trained with Kaldi is 12.5% WER in <ref type="bibr" target="#b27">[28]</ref>. Our score of 20.04% WER is probably due not using the optimized and manually generated lexicon as well as due to a mismatch in the training data for the acoustic model (read speech vs. conversational speech). The model in <ref type="bibr" target="#b27">[28]</ref> is exclusively trained on in-domain audio data, while we excluded any proprietary VM1 speech training data and only used our freely available open source speech recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Outlook</head><p>We have introduced a freely available ASR model for German which improves upon the previously best one by a large margin, both due to improvements in algorithms and a significant increase of freely available data. The free acoustic model fosters replicable research, and lowers the cost of entry for (non-cloud based) ASR, as the model can be readily downloaded 2 . In light of the recent privacy debate on data handling, especially in the EU, freely available acoustic models for German have obvious advantages over cloud based or closed source models. Our models can be run locally and user-recorded speech data does not have to be transferred to a 3rd party cloud provider, where privacy concerns will arise.</p><p>Our evaluation shows that the model performs well on new speakers, different microphones with around 14.4% WER for rescored TDNN-HMM models. The size of the general purporse vocabulary has a large effect on WERsa large part of the remaining recognition errors are due to vocabulary problems and the underlying language model. We expect a subword unit or decompounding approach to work better than a fixed word approach for German read speech <ref type="bibr" target="#b28">[29]</ref>. A remaining challenge is conversational speech, but reasonable performance can be achieved with a domain specific vocabulary. On the other hand, as more and more articles are spoken and recorded by volunteers for the Spoken Wikipedia project, we also expect benefits for our acoustic models through the use of the additional data.</p><p>The recipes we built for German can also be adapted to other languages. A good candidate, due to data be readily available in the Spoken Wikipedia corpus, is Dutch. The availability of Dutch data outside the Spoken Wikipedia corpus is even more limited than of German data -there are currently only ten hours available on Voxforge 3 , while up to 200 hours can potentially be used for model training from the SWC corpus.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Distribution of speaker contribution and amount of aligned material for the SWC corpus. Growth of the English, German and Dutch Spoken Wikipedia resources over time. English and German both grow with about 33h of additional audio per year.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>WER results on the Tuda-De dev and test sets. The scores are for decoding combined data from Kinect (Beam and RAW), Samson and Yamaha microphones.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>resulting in (completely) wrongly assigned utterance transcriptions. The cleanup removes about 1.6% of the Tuda-De data and 6.9% of the combined Tuda-De and conservatively pruned SWC data (268.5h ? 250h). With minimally pruned SWC data, 8.8% of the combined training data is removed from 412 hours, resulting in 375 hours of cleaned training data.We use the dev and test set from the Tuda-De corpus to measure word error rates (WER). The experiments in<ref type="bibr" target="#b13">[14]</ref> defined a closed vocabulary task with no out-of-vocabulary</figDesc><table><row><cell>Model</cell><cell>Microphone</cell><cell cols="2">WER</cell></row><row><cell></cell><cell></cell><cell>dev</cell><cell>test</cell></row><row><cell cols="4">TDNN-HMM Kinect-RAW 13.82 15.03</cell></row><row><cell></cell><cell>Samson</cell><cell cols="2">14.19 15.18</cell></row><row><cell></cell><cell>Yamaha</cell><cell cols="2">14.84 15.77</cell></row><row><cell></cell><cell cols="3">Kinect-Beam 19.12 20.86</cell></row><row><cell></cell><cell>Realtek</cell><cell cols="2">66.46 63.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>WER results of models trained on combined data (Tuda-De and SWC) for the different microphones in the Tuda-De dev and train sets. All WER results above are with the lexicon of 350,029 words and without RNNLM rescoring.</figDesc><table><row><cell>(OOV) words, as OOV words in test and dev were added</cell></row><row><cell>to the lexicon. This makes WER rates somewhat lower in</cell></row><row><cell>comparison, but a bit unrealistic. In Table 2 we show results</cell></row><row><cell>for a more realistic open domain setting, where the dev and</cell></row><row><cell>test vocabulary is not known a priori. Using only a 28,131</cell></row><row><cell>word vocabulary yields very high WER for GMM-HMM</cell></row><row><cell>and TDNN-HMM models alike, because of a high OOV</cell></row><row><cell>rate. Extending the vocabulary to 126,794 words reduces</cell></row><row><cell>both GMM-HMM and TDNN-HMM WER by about 20%</cell></row><row><cell>relative. Adding SWC data to the Tuda-De utterances im-</cell></row><row><cell>proves these TDNN-HMM results significantly, even when</cell></row><row><cell>we use the same vocabulary size. Using a minimal pruning</cell></row><row><cell>strategy with the SWC data and subsequently relying more</cell></row><row><cell>on Kaldi's cleaning scripts gives slightly better results: 26%</cell></row><row><cell>relative reduction vs. 23.3 % relative reduction. Finally, we</cell></row><row><cell>achieve our best WERs when we use a significantly larger</cell></row><row><cell>vocabulary and a better LM. Our test score with an open</cell></row><row><cell>domain vocabulary of 350,029 words is 16.49 WER and</cell></row><row><cell>can be further improved by using lattice rescoring with an</cell></row><row><cell>LSTM LM to 14.38 WER. This is a significant improve-</cell></row><row><cell>ment over the 20.5 WER (without OOVs) reported in [14].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>HMM General purp. (~350k) 46.42 50.56 TDNN-HMM General purp. (~350k) 33.69 38.23 GMM-HMM Domain specific (~7k) 27.18 29.12 TDNN-HMM Domain specific (~7k) 18.17 20.04</figDesc><table><row><cell>Model</cell><cell>Vocabulary</cell><cell cols="2">WER</cell></row><row><cell></cell><cell></cell><cell>dev</cell><cell>test</cell></row><row><cell>GMM-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>WER results on the Verbmobil (VM1) dev and test data, without RNNLM rescoring.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://en.wikipedia.org/wiki/Wikipedia: WikiProject_Spoken_Wikipedia</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Training scripts and model files are currently available at: https://github.com/uhh-lt/kaldi-tuda-de/ 3 http://www.voxforge.org/nl/Downloads</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Del?glise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Est?ve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC</title>
		<meeting>LREC<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3935" to="3939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">TED-LIUM 3: twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Est?ve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04699</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The CAPIO 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandrashekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00059</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NASA STI/Recon technical report n</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SWITCH-BOARD: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Fisher corpus: a resource for the next generations of speech-to-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="69" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-L</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="132" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A context-aware speech recognition and understanding system for air traffic control domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oualil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szasz?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Helmke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU<address><addrLine>Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="404" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ambient search: A document retrieval system for speech streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Milde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radomski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?hlh?user</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING 2016</title>
		<meeting>COLING 2016<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2082" to="2091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The spoken Wikipedia corpus collection: Harvesting, alignment and an application to hyperlistening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?hn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Design of the CMU Sphinx-4 decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouvea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurospeech</title>
		<meeting>Eurospeech<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1181" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open source german distant speech recognition: Corpus and acoustic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radeck-Arneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Milde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouv?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radomski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?hlh?user</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Text, Speech, and Dialogue (TSD)</title>
		<meeting>Text, Speech, and Dialogue (TSD)<address><addrLine>Pilsen, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="480" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The german text-to-speech synthesis system MARY: A tool for research, development and teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schr?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trouvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bavarian Archive for Speech Signals</title>
		<ptr target="http://www.bas.uni-muenchen.de/forschung/Bas/BasSAMPA" />
		<imprint/>
	</monogr>
	<note>Extended sam-pa</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint-sequence models for graphemeto-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="434" to="451" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Detroit, MI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural network language modeling with letter-based features and importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Alberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in speech recognition</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="393" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speaker adaptation of neural network acoustic models using ivectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU<address><addrLine>Olomouc, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="55" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving dnn speaker independence with i-vector inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="225" to="229" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speaker adaptive training of deep neural network acoustic models using i-vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1938" to="1949" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Purely sequence-trained neural networks for ASR based on latticefree MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>San Fransisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An open source corpus and recording software for distant speech recognition with the microsoft kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schnelle-Walka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radeck-Arneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radomski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ITG</title>
		<meeting>ITG<address><addrLine>Erlangen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Verbmobil: foundations of speech-to-speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wahlster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin/Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comparing open-source speech recognition toolkits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gaida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Petrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Proba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malatawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suendermann-Oeft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>DHBW Stuttgart</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved subword modeling for WFST-based speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Virpioja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kurimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2551" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
