<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GAUDI: A Neural Architect for Immersive 3D Scene Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengsheng</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Talbott</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyuan</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Goh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ulbricht</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Dehghan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Susskind</surname></persName>
						</author>
						<title level="a" type="main">GAUDI: A Neural Architect for Immersive 3D Scene Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce GAUDI, a generative model capable of capturing the distribution of complex and realistic 3D scenes that can be rendered immersively from a moving camera. We tackle this challenging problem with a scalable yet powerful approach, where we first optimize a latent representation that disentangles radiance fields and camera poses. This latent representation is then used to learn a generative model that enables both unconditional and conditional generation of 3D scenes. Our model generalizes previous works that focus on single objects by removing the assumption that the camera pose distribution can be shared across samples. We show that GAUDI obtains state-of-the-art performance in the unconditional generative setting across multiple datasets and allows for conditional generation of 3D scenes given conditioning variables like sparse image observations or text that describes the scene. * denotes equal contribution. Corresponding Unconditional z ? p(z) sample 3D scene and poses</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GAUDI</head><p>Our goal is to learn a generative model given an empirical distribution of trajectories over 3D scenes. Let X = {x i?{0,...,n} } denote a collection of examples defining an empirical distribution, where each example x i is a trajectory. Every trajectory x i is defined as a variable length sequence of corresponding RGB, depth images and 6DOF camera poses (see <ref type="figure">Fig. 3</ref>).</p><p>We decompose the task of learning a generative model in two stages. First, we obtain a latent representation z = [z scene , z pose ] for each example x ? X that represents the scene radiance field and pose in separate disentangled vectors. Second, given a set of latents Z = {z i?{0,...,n} } we learn the distribution p(Z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Optimizing latent representations for radiance fields and camera poses</head><p>We now turn to the task of finding a latent representation z ? Z for each example x ? X (i.e. for each trajectory in the empirical distribution). To obtain this latent representation we take an encoder-less view and interpret z's as free parameters to be found via an optimization problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>. To map latents z to trajectories x, we design a network architecture (i.e. a decoder) that disentangles camera poses and radiance field parameterization. Our decoder architecture is composed of 3 networks (shown in <ref type="figure">Fig. 2</ref>):</p><p>? The camera pose decoder network c (parameterized by ? c ), is responsible for predicting camera posesT s ? SE(3) at the normalized temporal position s ? [?1, 1] in the trajectory, conditioned on z pose which represents the camera poses for the whole trajectory. To ensure that the output of c is a valid camera pose (e.g. an element of SE(3)), we output a 3D vector representing a normalized quaternion q s for the orientation and a 3D translation vector t s .</p><p>3 Volume Rendering &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 0 6 X c x P g Z V Y Y z g d N S L 9 W Y U D a m Q + x a K m m E 2 s / m h 0 7 J m V U G J</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In order for learning systems to be able to understand and create 3D spaces, progress in generative models for 3D is sorely needed. The quote "The creation continues incessantly through the media of humans." is often attributed to Antoni Gaud?, who we pay homage to with our method's name. We are interested in generative models that can capture the distribution of 3D scenes and then render views from scenes sampled from the learned distribution. Extensions of such generative models to conditional inference problems could have tremendous impact in a wide range of tasks in machine learning and computer vision. For example, one could sample plausible scene completions that are consistent with an image observation, or a text description (see <ref type="figure" target="#fig_0">Fig. 1</ref> for 3D scenes sampled from GAUDI). In addition, such models would be of great practical use in model-based reinforcement learning and planning <ref type="bibr" target="#b11">[12]</ref>, SLAM [39], or 3D content creation.</p><p>Recent works on generative modeling for 3D objects or scenes <ref type="bibr" target="#b53">[56,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7</ref>] employ a Generative Adversarial Network (GAN) where the generator explicitly encodes radiance fields -a parametric function that takes as input the coordinates of a point in 3D space and camera pose, and outputs a density scalar and RGB value for that 3D point. Images can be rendered from the radiance field generated by the model by passing the queried 3D points through the volume rendering equation to project onto any 2D camera view. While compelling on small or simple 3D datasets (e.g. single objects or a small number of indoor scenes), GANs suffer from training pathologies including mode collapse <ref type="bibr" target="#b51">[54,</ref><ref type="bibr" target="#b58">61]</ref> and are difficult to train on data for which a canonical coordinate system does not exist, as is the case for 3D scenes <ref type="bibr" target="#b54">[57]</ref>. In addition, one key difference between modeling distributions of 3D objects vs. scenes is that when modeling objects it is often assumed that camera poses are sampled from a distribution that is shared across objects (i.e. typically over SO <ref type="formula" target="#formula_4">(3)</ref>), which is not true for scenes. This is because the distribution of valid camera poses depends on each particular scene independently (based on the structure and location of walls and other objects). In addition, for scenes this distribution can encompass all poses over the SE(3) group. This fact becomes more clear when we think about camera poses as a trajectory through the scene(cf. <ref type="figure" target="#fig_1">Fig. 3(b)</ref>).</p><p>In GAUDI, we map each trajectory (i.e. a sequence of posed images from a 3D scene) into a latent representation that encodes a radiance field (e.g. the 3D scene) and camera path in a completely disentangled way. We find these latent representations by interpreting them as free parameters and formulating an optimization problem where the latent representation for each trajectory is optimized via a reconstruction objective. This simple training process is scalable to thousands of trajectories.</p><p>Interpreting the latent representation of each trajectory as a free parameter also makes it simple to handle a large and variable number of views for each trajectory rather than requiring a sophisticated encoder architecture to pool across a large number of views. After optimizing latent representations for an observed empirical distribution of trajectories, we learn a generative model over the set of latent representations. In the unconditional case, the model can sample radiance fields entirely from the prior distribution learned by the model, allowing it to synthesize scenes by interpolating within the latent space. In the conditional case, conditional variables available to the model at training time (e.g. images, text prompts, etc.) can be used to generate radiance fields consistent with those variables. Our contributions can be summarized as:</p><p>? We scale 3D scene generation to thousands of indoor scenes containing hundreds of thousands of images, without suffering from mode collapse or canonical orientation issues during training.</p><p>? We introduce a novel denoising optimization objective to find latent representations that jointly model a radiance field and the camera poses in a disentangled manner.</p><p>? Our approach obtains state-of-the-art generation performance across multiple datasets.</p><p>? Our approach allows for various generative setups: unconditional generation as well as conditional on images or text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In recent years the field has witnessed outstanding progress in generative modeling for the 2D image domain, with most approaches focusing either on adversarial <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> or auto-regressive models <ref type="bibr" target="#b61">[64,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b8">9]</ref>. More recently, score matching based approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b55">58]</ref> have gained popularity. In particular, Denoising Diffusion Probabilistic Models (DDPMs) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b60">63]</ref> have emerged as strong contenders to both adversarial and auto-regressive approaches. In DDPMs, the goal is to learn a step-by-step inversion of a fixed diffusion Markov Chain that gradually transforms an empirical data distribution to a fixed posterior, which typically takes the form of an isotropic Gaussian distribution. In parallel, the last couple of years have seen a revolution in how 3D data is represented within neural networks. By representing a 3D scene as a radiance field, NeRF <ref type="bibr" target="#b28">[29]</ref> introduces an approach to optimize the weights of a MLP to represent the radiance of 3D points that fall inside the field-of-view of a given set of posed RGB images. Given the radiance for a set of 3D points that lie on a ray shot from a given camera pose, NeRF <ref type="bibr" target="#b28">[29]</ref> uses volumetric rendering to compute the color for the corresponding pixel and optimizes the MLP weights via a reconstruction loss in image space.</p><p>A few attempts have also been made at incorporating a radiance field representation within generative models. Most approaches have focused on the problem of single objects with known canonical orientations like faces or Shapenet objects with shared camera pose distributions across samples in a dataset <ref type="bibr" target="#b53">[56,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b67">70,</ref><ref type="bibr" target="#b40">43]</ref>. Extending these approaches from single objects to completely unconstrained 3D scenes is an unsolved problem. One paper worth mentioning in this space is GSN <ref type="bibr" target="#b6">[7]</ref>, which breaks the radiance field into a grid of local radiance fields that collectively represent a scene. While this decomposition of radiance fields endows the model with high representational capacity, GSN still suffers from the standard training pathologies of GANs, like mode collapse <ref type="bibr" target="#b58">[61]</ref>, which are exacerbated by the fact that unconstrained 3D scenes do not have a canonical orientation. As we show in our experiments (cf. Sect. 4), these issues become prominent as the training set size increases, impacting the capacity of the generative model to capture complex distributions. Separately, a line of recent approaches have also studied the problem of learning generative models of scenes without employing radiance fields <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b62">65,</ref><ref type="bibr" target="#b44">47]</ref>. These works assume that the model has access to room layouts and a database of object CAD models during training, simplifying the problem of scene generation to a selection of objects from the database and pose predictions for each object.</p><p>Finally, approaches that learn to predict a target view given a single (or multiple) source view and relative pose transformation have been recently proposed <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b66">69,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref>. The pure reconstruction objective employed by these approaches forces them to learn a deterministic conditional function that maps a source image and a relative camera transformation to a target image. The first is that this scene completion problem is ill-posed (e.g. given a single source view of a scene there are multiple target completions that are equally likely). Attempts at modeling the problem in a probabilistic manner have been proposed <ref type="bibr" target="#b46">[49,</ref><ref type="bibr" target="#b42">45]</ref>. However, these approaches suffer from inconsistency in predicted scenes because they do not explicitly model a 3D consistent representation like a radiance field.  <ref type="figure" target="#fig_3">Figure 2</ref>: Architecture of the decoder model that disentangles camera poses from 3D geometry and appearance of the scene. Our decoder is composed by 3 submodules. A decoder d that takes as input a latent code representing the scene z scene and produces a factorized representation of 3D space via a tri-plane latent encoding W. A radiance field network f that takes as input points p ? R 3 and is conditioned on W to predict a density ? and a signal a to be rendered via volumetric rendering (Eq. 1). Finally, we decode the camera poses through a network c that takes as input a normalized temporal position s ? [?1, 1] and is conditioned on z pose which represents camera poses for the whole trajectory x to predict the camera poseT s ? SE(3).</p><p>? The scene decoder network d (parameterized by ? d ), is responsible for predicting a conditioning variable for the radiance field network f . This network takes as input a latent code that represents the scene z scene and predicts an axis-aligned tri-plane representation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4]</ref> W ? R 3?S?S?F . Which correspond to 3 feature maps [W xy , W xz , W yz ] of spatial dimension S ? S and F channels, one for each axis aligned plane: xy, xz and yz.</p><p>? The radiance field decoder network f (parameterized by ? f ), is tasked with reconstructing image level targets using the volumetric rendering equation in Eq. 1. The input to f is p ? R 3 and the tri-</p><formula xml:id="formula_0">plane representation W = [W xy , W xz , W yz ]. Given a 3D point p = [i, j, k]</formula><p>for which radiance is to be predicted, we orthogonally project p into each plane in W and perform bi-linear sampling. We concatenate the 3 bi-linearly sampled vectors into</p><formula xml:id="formula_1">w xyz = [W xy (i, j), W xz (j, k), W yz (i, k)] ? R 3F</formula><p>, which is used to condition the radiance field function f . We implement f as a MLP that outputs a density value ? and a signal a. To predict the value v of a pixel, the volumetric rendering equation is used (cf. Eq. 1) where a 3D point is expressed as ray direction r (corresponding with the pixel location) at particular depth u.</p><formula xml:id="formula_2">v(r, W) = u f un T r(u)? (r(u), w xyz ) a (r(u), d, w xyz ) du T r(u) = exp ? u un ?(r(u), w xyz )du .<label>(1)</label></formula><p>We formulate a denoising reconstruction objective to jointly optimize for ? d , ? c , ? f and {z} i={0,...,n} , shown in Eq. 2. Note that while latents z are optimized for each example x independently, the parameters of the networks ? d , ? c , ? f are amortized across all examples x ? X. As opposed to previous auto-decoding approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>, each latent z is perturbed during training with additive noise that is proportional to the empirical standard deviation across all latents, z = z+?N (0, std(Z)), inducing a contractive representation <ref type="bibr" target="#b43">[46]</ref>. In this setting, ? controls the trade-off between the entropy of the distribution z ? Z and the reconstruction term, with ? = 0 the distribution of z's becomes a collection of indicator functions, whereas non-trivial structure in latent space arises for ? &gt; 0. We use a small ? &gt; 0 value to enforce a latent space in which interpolated samples (or samples that contain small deviations from the empirical distribution, as the ones that one might get from sampling a subsequent generative model) are included in the support of the decoder.</p><formula xml:id="formula_3">min ? d ,? f ,?c,Z E x?X L scene (x im s , z scene , T s ) + ?L pose (T s , z pose , s)<label>(2)</label></formula><p>We optimize parameters ? d , ? f , ? c and latents z ? Z with two different losses. The first loss function L scene measures the reconstruction between the radiance field encoded in z scene and the images in the trajectory x im s (where s denotes the normalized temporal position of the frame in the trajectory), given ground-truth camera poses T s required for rendering. We use an l 2 loss for RGB and l 1 for depth <ref type="bibr" target="#b0">1</ref> . The second loss function L pose measures the camera pose reconstruction error between the posesT s encoded in z pose and the ground-truth poses. We employ an l 2 loss on translation and l 1 loss for the normalized quaternion part of the camera pose. Although theoretically normalized quaternions are not necessarily unique (e.g. q and ?q) we do not observe any issues empirically during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prior Learning</head><p>Given a set of latents z ? Z resulting from minimizing the objective in Eq. 2, our goal is to learn a generative model p(Z) that captures their distribution (i.e. after minimizing the objective in Eq. 2 we interpret z ? Z as examples from an empirical distribution in latent space). In order to model p(Z) we employ a Denoising Diffusion Probabilistic Model (DDPM) <ref type="bibr" target="#b14">[15]</ref>, a recent score-matching <ref type="bibr" target="#b15">[16]</ref> based model that learns to reverse a diffusion Markov Chain with a large but finite number of timesteps. In DDPMs <ref type="bibr" target="#b14">[15]</ref> it is shown that this reverse process is equivalent to learning a sequence of denoising auto-encoders with tied weights. The supervised denoising objective in DDPMs makes learning p(Z) simple and scalable. This allows us to learn a powerful generative model that enables both unconditional and conditional generation of 3D scenes. For training our prior p ?p (Z) we take the objective function in <ref type="bibr" target="#b14">[15]</ref> defined in Eq. 3. In Eq. 3 t denotes the timestep, ? N (0, I) is the noise and? t is a noise magnitude parameter with a fixed scheduling. Finally, ?p denotes the denoising model.</p><formula xml:id="formula_4">min ?p E t,z?Z, ?N (0,I) ? ?p ( ?? t z + ? 1 ?? t , t) 2<label>(3)</label></formula><p>At inference time, we sample z ? p ?p (Z) by following the inference process in DDPMs. We start by sampling z T ? N (0, I) and iteratively apply ?p to gradually denoise z T , thus reversing the diffusion Markov Chain to obtain z 0 . We then feed z 0 as input to the decoder architecture (cf. <ref type="figure" target="#fig_3">Fig. 2</ref>) and reconstruct a radiance field and a camera path.</p><p>If the goal is to learn a conditional distribution of the latents p(Z|Y ), given paired data {z ? Z, y ? Y }, the denoising model ? is augmented with a conditioning variable y, resulting in ?p (z, t, y), implementation details about how the conditioning variable is used in the denoising architecture can be found in the appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we show the applicability of GAUDI to multiple problems. First, we evaluate reconstruction quality and performance of the reconstruction stage. Then, we evaluate the performance of our model in generative tasks including unconditional and conditional inference, in which radiance fields are generated from conditioning variables corresponding to images or text prompts. Full experimental settings and details can be found in the appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We report results on 4 datasets: Vizdoom <ref type="bibr" target="#b20">[21]</ref>, Replica <ref type="bibr" target="#b57">[60]</ref>, VLN-CE <ref type="bibr" target="#b22">[23]</ref> and ARKit Scenes <ref type="bibr" target="#b0">[1]</ref>, which vary in number of scenes and complexity (see <ref type="figure" target="#fig_1">Fig. 3</ref> and Tab. 1).</p><p>Vizdoom <ref type="bibr" target="#b20">[21]</ref>: Vizdoom is a synthetic simulated environment with simple texture and geometry. We use the data provided by <ref type="bibr" target="#b6">[7]</ref> to train our model. It is the simplest dataset in terms of number of scenes and trajectories, as well as texture, serving as a test bed to examine GAUDI in the simplest setting.</p><p>Replica <ref type="bibr" target="#b57">[60]</ref>: Replica is a dataset comprised of 18 realistic scenes from which trajectories are rendered via Habitat <ref type="bibr" target="#b52">[55]</ref>. We used the data provided by <ref type="bibr" target="#b6">[7]</ref> to train our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VLN-CE [23]</head><p>: VLN-CE is a dataset originally designed for vision and language navigation in continuous environments. This dataset is composed of 3.6K trajectories of an agent navigating between two points in a 3D scene from the 3D dataset <ref type="bibr" target="#b5">[6]</ref>. We render observations via Habitat <ref type="bibr" target="#b52">[55]</ref>.</p><p>Notably, this dataset contains also textual descriptions of the trajectories taken by an agent. In Sect. 4.5 we train GAUDI in a conditional manner to generate 3D scenes given a description. <ref type="bibr" target="#b0">1</ref> We obtain depth predictions by aggregating densities across a ray as in <ref type="bibr" target="#b28">[29]</ref> 5 &lt; l a t e x i t s h a 1 _ b a s e   <ref type="figure" target="#fig_1">Figure 3</ref>: (a) Examples of the 4 datasets we use in this paper (from left to right): Vizdoom <ref type="bibr" target="#b20">[21]</ref>, Replica <ref type="bibr" target="#b57">[60]</ref>, VLN-CE <ref type="bibr" target="#b22">[23]</ref>, ARKitScenes <ref type="bibr" target="#b0">[1]</ref>. (b) Top-down views of 2 different camera paths in VLN-CE <ref type="bibr" target="#b22">[23]</ref>. Blue and red dots represent start-end positions and the camera path is highlighted in blue.</p><formula xml:id="formula_5">6 4 = " o 2 h v L Y V s B n 1 L j h L m Q o N U Q G S R 9 w I = " &gt; A A A B 8 X i c b V D L S g N B E J z 1 G e M r 6 t H L Y B D i J e x K U I 9 B L x 4 j m A c m S 5 i d d J I h s 7 P L T K 8 Y l v y F F w + K e P V v v P k 3 T p I 9 a G J B Q 1 H V T X d X E E t h 0 H W / n Z X V t f W N z d x W f n t n d 2 + / c H D Y M F G i O d R 5 J C P d C p g B K R T U U a C E V q y B h Y G E Z j C 6 m f r N R 9 B G R O o e x z H 4 I R s o 0 R e c o Z U e O g h P m J b Y 2 a R b K L p l d w a 6 T L y M F E m G W r f w 1 e l F P A l B I Z f M m L b n x u i n T K P g E i b 5 T m I g Z n z E B t C 2 V L E Q j J / O L p 7 Q U 6 v 0 a D /</formula><formula xml:id="formula_6">L T K 8 Y l v y F F w + K e P V v v P k 3 T p I 9 a G J B Q 1 H V T X d X E E t h 0 H W / n Z X V t f W N z d x W f n t n d 2 + / c H D Y M F G i O d R 5 J C P d C p g B K R T U U a C E V q y B h Y G E Z j C 6 m f r N R 9 B G R O o e x z H 4 I R s o 0 R e c o Z U e O g h P m J a C s 0 m 3 U H T L 7 g x 0 m X g Z K Z I M t W 7 h q 9 O L e B K C Q i 6 Z M W 3 P j d F P m U b B J U z y n c R A z P i I D a B t q W I h G D + d X T y h p 1 b p 0 X 6 k b S m k M / X 3 R M p C Y 8 Z h Y D t D h k O z 6 E 3 F / 7 x 2 g v 0 r P x U q T h A U n y / q J 5 J i R K f v 0 5 7 Q w F G O L W F c C 3 s r 5 U O m G U c b U t 6 G 4 C 2 + v E w</formula><p>ARKitScenes <ref type="bibr" target="#b0">[1]</ref>: ARKitScenes is a dataset of scans of indoor spaces. This dataset contains more than 5K scans of about 1.6K different indoor spaces. As opposed to the previous datasets where RGB, depth and camera poses are obtained via rendering in a simulation (i.e. either Vizdoom <ref type="bibr" target="#b20">[21]</ref> or Habitat <ref type="bibr" target="#b52">[55]</ref>), ARKitScenes provides raw RGB and depth of the scans and camera poses estimated using ARKit SLAM. In addition, whereas trajectories from the previous datasets are point-to-point, as typically done in navigation, the camera trajectories for ARKitScenes resembles a natural scan a of full indoor space. In our experiments we use a subset of 1K scans from ARKitScenes to train our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reconstruction</head><p>We first validate the hypothesis that the optimization problem described in Eq. 2 can find latent codes z that are able reconstruct the trajectories in the empirical distribution in a satisfactory way. In Tab. 1 we report reconstruction performance of our model across all datasets. <ref type="figure">Fig. 4</ref> shows reconstructions of random trajectories for each dataset. For all our experiments we set the dimension of z scene and z pose to 2048 and ? = 0.1 unless otherwise stated. During training, we normalize camera poses for each trajectory so that the middle frame in a trajectory becomes the origin of the coordinate system. See appendix E for ablation experiments. <ref type="figure">Figure 4</ref>: Qualitative reconstruction results of random trajectories on different datasets (one for each column): Vizdoom <ref type="bibr" target="#b20">[21]</ref>, Replica <ref type="bibr" target="#b57">[60]</ref>, VLN-CE <ref type="bibr" target="#b22">[23]</ref>and ARKitScenes <ref type="bibr" target="#b0">[1]</ref>. For each pair of images the left is ground-truth and right is reconstruction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interpolation</head><p>In addition, to evaluate the structure of the latent representation obtained from minimizing the optimization problem in Eq. 2, we show interpolation results between pairs of latents (z i , z j ) in <ref type="figure">Fig. 5</ref>. To render images while interpolating the scene we place a fixed camera at the origin of the 6 coordinate system. We observe a smooth transition of scenes in both geometry (walls, ceilings) and texture (stairs, carpets). More visualizations are included in the appendix E.1.  <ref type="figure">Figure 5</ref>: Interpolation of 3D scenes in latent space (e.g. interpolating the encoded radiance field) for the VLN-CE dataset <ref type="bibr" target="#b22">[23]</ref>. Each row corresponds to a different interpolation path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Unconditional generative modeling</head><p>Given latent representations z ? Z that can reconstruct samples x ? X with high accuracy as shown in Sect. 4.2, we now evaluate the capacity of the prior p ?p (Z) to capture the empirical distribution x ? X by learning the distribution of latents z i ? Z. To do so we sample z ? p ?p (Z) by following the inference process in DDPMs, and then feed z through the decoder network, which results in trajectories of RGB images that are then used for evaluation. We compare our approach with the following baselines: GRAF <ref type="bibr" target="#b53">[56]</ref>, ?-GAN <ref type="bibr" target="#b4">[5]</ref> and GSN <ref type="bibr" target="#b6">[7]</ref>. We sample 5k images from predicted and target distributions for each model and dataset and report both FID <ref type="bibr" target="#b13">[14]</ref> and SwAV-FID <ref type="bibr" target="#b30">[31]</ref> scores. We report quantitative results in Tab. 2, where we can see that GAUDI obtains state-of-the-art performance across all datasets and metrics. We attribute this performance improvement to the fact that GAUDI learns disentangled yet corresponding latents for radiance fields and camera poses, which is key when modeling scenes (see ablations in the appendix E). We note that to obtain these great empirical results GAUDI needs to simultaneously find latents with high reconstruction fidelity while also efficiently learning their distribution.</p><p>VizDoom <ref type="bibr" target="#b20">[21]</ref> Replica <ref type="bibr" target="#b57">[60]</ref> VLN-CE <ref type="bibr" target="#b22">[23]</ref> ARKitScenes <ref type="bibr" target="#b0">[1]</ref> FID  <ref type="table">Table 2</ref>: Generative performance of state-of-the-art approaches for generative modelling of radiance fields on 4 scene datasets: Vizdoom <ref type="bibr" target="#b20">[21]</ref>, Replica <ref type="bibr" target="#b57">[60]</ref>, VLN-CE <ref type="bibr" target="#b22">[23]</ref> and ARKitScenes <ref type="bibr" target="#b0">[1]</ref>, according to FID <ref type="bibr" target="#b13">[14]</ref> and SwAV-FID <ref type="bibr" target="#b30">[31]</ref> metrics.</p><formula xml:id="formula_7">? SwAV-FID ? FID ? SwAV-FID ? FID ? SwAV-FID ? FID ? SwAV-FID</formula><p>In <ref type="figure">Fig. 6</ref> we show samples from the unconditional distribution learnt by GAUDI for different datasets. We observe that GAUDI is able to generate diverse and realistic 3D scenes from the empirical distribution which can be rendered from the sampled camera poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Conditional Generative Modeling</head><p>In addition to modeling the distribution p(Z), with GAUDI we can also tackle conditional generative problems p(Z|Y ), where a conditioning variable y ? Y is given to modulate p(Z). For all conditioning variables y we assume the existence of paired data {z, y} to train the conditional model <ref type="bibr" target="#b39">[42,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">41]</ref>. In this section we show both quantitative and qualitative results for conditional inference problems. The first conditioning variable we consider are textual descriptions of trajectories. Second, we consider a conditional model where randomly sampled RGB images in a trajectory act as conditioning variables. Finally, we use a categorical variable that indicates the 3D environment (i.e. <ref type="figure">Figure 6</ref>: Different scenes sampled from unconditional GAUDI (one sample per row) and rendered from their corresponding sampled camera poses (one dataset per column): Vizdoom <ref type="bibr" target="#b20">[21]</ref>, Replica <ref type="bibr" target="#b57">[60]</ref>, VLN-CE <ref type="bibr" target="#b22">[23]</ref>and ARKitScenes <ref type="bibr" target="#b0">[1]</ref>.   <ref type="bibr" target="#b22">[23]</ref> dataset. GAUDI is able to produce high-quality scene renderings with low FID and SwAV-FID scores. In the right table we show the difference in average per-environment FID score between the conditional and unconditional models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Text Conditioning</head><p>We tackle the challenging task of training a text conditional model for 3D scene generation. We use the navigation text descriptions provided in VLN-CE <ref type="bibr" target="#b22">[23]</ref> to condition our model. These text descriptions contain high level information about the scene as well as the navigation path (i.e. "Walk out of the bedroom and into the living room", "Exit the room through the swinging doors and then enter the bedroom"). We employ a pre-trained RoBERTa-base <ref type="bibr" target="#b25">[26]</ref> text encoder and use its intermediate representation to condition the diffusion model. <ref type="figure">Fig. 7</ref> shows qualitative results of GAUDI for this task. To the best of our knowledge, this is the first model that allows for conditional 3D scene generation from text in an amortized manner (i.e. without distilling CLIP <ref type="bibr" target="#b37">[40]</ref> through a costly optimization problem <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>).</p><p>Go through the hallway Go up the stairs Walk into the kitchen <ref type="figure">Figure 7</ref>: Text conditional 3D scene generation using GAUDI (one sample per row). Our model is able to capture the conditional distributions of scenes by generating multiple plausible scenes and camera paths that match the given text prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Image Conditioning</head><p>We now analyze whether GAUDI is able to pick up information from the RGB images to predict a distribution over Z. In this experiment we randomly pick images in a trajectory x ? X and use it as a conditioning variable y. For this experiment we use trajectories in the VLN-CE dataset <ref type="bibr" target="#b22">[23]</ref>. During each training iteration we sample a random image for each trajectory x and use it as a conditioning variable. We employ a pre-trained ResNet-18 <ref type="bibr" target="#b12">[13]</ref> as an image encoder. During inference, the resulting conditional GAUDI model is able to sample radiance fields where the given image is observed from a stochastic viewpoint. In <ref type="figure">Fig. 8</ref> we show samples from the model conditioned on different RGB images. <ref type="figure">Figure 8</ref>: Image conditional 3D scene generation using GAUDI (one sample per row). Given a conditioned image (top row), our model is able to sample scenes where the same or contextually similar view is observed from a stochastic viewpoint.</p><p>Environment ID: 1 Environment ID: 2 Environment ID: 3 <ref type="figure">Figure 9</ref>: Samples from the GAUDI model conditioned on a categorical variable denoting the indoor scene (one sample per row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Categorical Conditioning</head><p>Finally, we analyze how GAUDI performs when conditioned on a categorical variable that indicates the underlying 3D indoor environment in which each trajectory was recorded. We perform experiments in the VLN-CE <ref type="bibr" target="#b22">[23]</ref> dataset, where we employ a trainable embedding layer to learn a representation for categorical variables indicating each environment. We compare the per-environment FID score of conditional model with its unconditional counterpart. This per-enviroment FID score is computed only on real images of the same indoor environment that the model is conditioned on. Our hypothesis is that if the model efficiently captures the information in the conditioning variable it should capture the environment specific distribution better than its unconditional counterpart trained on the same data. In Tab. 3 the last column shows difference (e.g. the ?) on the average per-environment FID score between the conditional and unconditional model on VLN-CE dataset. We observe that the conditional model consistently obtains a better FID score than the unconditional model across all indoor environments, resulting in a sharp reduction of average FID and SwAV-FID scores. In addition, in <ref type="figure">Fig. 9</ref> we show samples from the model conditioned on a given categorical variable. <ref type="bibr" target="#b8">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced GAUDI, a generative model that captures distributions of complex and realistic 3D scenes. GAUDI uses a scalable two-stage approach which first involves learning a latent representation that disentangles radiance fields and camera poses. The distribution of disentangled latent representations is then modeled with a powerful prior. Our model obtains state-of-the-art performance when compared with recent baselines across multiple 3D datasets and metrics. GAUDI can be used both for conditional and unconditional problems, and enabling new tasks like generating 3D scenes from text descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Limitations, Future Work and Societal Impact</head><p>Although GAUDI represents a step forward in generative models for 3D scenes, we would like to clearly discuss the limitations. One current limitation of our model is the fact that inference is not real-time. The reason for this is two fold: (i) sampling from the DDPM prior is slow even if it is amortized for the whole 3D scene. Techniques for improving inference efficiency in DDPMs have been recently proposed <ref type="bibr" target="#b56">[59,</ref><ref type="bibr" target="#b63">66,</ref><ref type="bibr" target="#b64">67]</ref> and can complement GAUDI. (ii) Rendering from a radiance field is not as efficient as rendering other 3D structures like meshes. Recent work have also tackled this problem <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b65">68,</ref><ref type="bibr" target="#b41">44]</ref> and could be applied to our approach. In addition, many of the latest image generative models <ref type="bibr" target="#b38">[41,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">52]</ref> use multiple stages of up-sampling through diffusion models to render high-res images. These up-sample stages could be directly applied to GAUDI. In addition, one could considering studying efficient encoders to replace the optimization process to find latents. While attempts have been made at using transformers <ref type="bibr" target="#b50">[53]</ref> for short trajectories (5-10 frames) it is unclear how to scale to thousands of images per trajectory like the ones in <ref type="bibr" target="#b0">[1]</ref>. Finally, the main limitation for a model like GAUDI to exhibit improved generation and generalization abilities is the lack of massive-scale and open-domain 3D datasets. In particular ones with other associated modalities like textual descriptions.</p><p>When considering societal impact of generative models a few aspects that need attention are the use generative models for creating disingenuous data, e.g. "DeepFakes" <ref type="bibr" target="#b29">[30]</ref>, training data leakage and privacy <ref type="bibr" target="#b59">[62]</ref>, and amplification of the biases present in training data <ref type="bibr" target="#b17">[18]</ref>. One specific ethical consideration that applies to GAUDI is the impact that a model which can easily create immersive 3D scenes can have on future generations and their detachment of reality <ref type="bibr" target="#b2">[3]</ref>. For an in-depth review of ethical considerations in generative modeling we refer the reader to <ref type="bibr" target="#b48">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Settings and Details</head><p>In this section we describe details about data and model hyper-parameters. For all experiments our latents zscene and zpose have 2048 dimensions. In the first stage, when latents are optimized via Eq. 2, zscene gets reshaped to a 8 ? 8 ? 32 feature map before feeding it to the scene decoder network. In the second stage, when training the DDPM prior we reshape zscene and zpose to 8 ? 8 ? 64 latent and leverage the power of a UNet <ref type="bibr" target="#b47">[50]</ref> denoising architecture.</p><p>For each dataset, trajectories have different length, physical scale, as well as near and far planes for rendering, which we adjust accordingly in our model.</p><p>Vizdoom <ref type="bibr" target="#b20">[21]</ref>: In Vizdoom, trajectories contains 600 steps on average. In each step the camera is allowed to move forward 0.5 game units or rotate left or right by 30 degrees. We set the unit length of an element in the tri-plane representation as 0.05 game units (meaning each latent code wxyz represents a volume of space of 0.05 cubic game units). The near plane is at 0.0 game units and the far plane at 800 game units. We use the data and splits provided by <ref type="bibr" target="#b6">[7]</ref>.</p><p>Replica <ref type="bibr" target="#b57">[60]</ref>: In Replica, all trajectories contain 100 steps. In each step, the camera can either rotate left or right by 25 degrees or move forward 15 centimeters. We set the unit length of an element in the tri-plane representation as 25 centimeters (meaning each latent code wxyz represents a volume of space of 0.25 cubic centimeters). The near plane is at 0.0 meters and the far plane at 6 meters. We use the data and splits provided by <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VLN-CE [23]</head><p>: in VLN-CE trajectories contain a variable number of steps between 30 and 150, approximately. In each step, the camera can either rotate left or right by 25 degrees or move forward 15 centimeters. We set the unit length of an element in the tri-plane representation as 50 centimeters. The near plane is at 0.0 meters and the far plane at 12 meters. We use the data and training splits provided by <ref type="bibr" target="#b22">[23]</ref>.</p><p>ARKitScenes <ref type="bibr" target="#b0">[1]</ref>: in ARKitScenes trajectories contain a number of steps around 1000 on average. In these trajectories the camera is able to move continuously in any direction and orientation. We set the unit length of an element in the tri-plane representation as 20 centimeters. The near plane is at 0.0 meters and the far plane at 8 meters. We use the 3DOD split of data provided by <ref type="bibr" target="#b0">[1]</ref> C Decoder Architecture Design and Details</p><p>In this section we describe the decoder model in <ref type="figure" target="#fig_3">Fig. 2</ref> in the main paper. The decoder network is composed of 3 modules: scene decoder, camera pose decoder and radiance field decoder.</p><p>? The scene decoder network follows the architecture of the VQGAN decoder <ref type="bibr" target="#b8">[9]</ref>, parameterized with convolutional architecture that contains a self-attention layers at the end of each block. The output ? The camera pose decoder is implemented as an MLP with 4 conditional batch normalization (CBN) blocks with residual connections and hidden size of 256, as in <ref type="bibr" target="#b26">[27]</ref>. The conditional batch normalization parameters are predicted from zpose. We apply positional encoding to the inputs the camera pose encoder (s ? [?1, 1]). <ref type="figure" target="#fig_0">Fig. 10(a)</ref> shows the architecture of the camera pose decoder module.</p><p>? The radiance field decoder is implemented as an MLP with 8 linear layers with hidden dimension of 512 and LeakyReLU activations. We apply positional encoding to the inputs the radiance field decoder (p ? R 3 ) and concatenate the conditioning variable wxyz to the output of every other layer in the MLP starting from the input layer (e.g. layers 0, 2, 4, and 6). To improve efficiency, we render a small resolution feature map of 512 channels (two times smaller than the output resolution) instead of an RGB image and use a UNet <ref type="bibr" target="#b47">[50]</ref> with additional deconvolution layers to predict the final image <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>. <ref type="figure" target="#fig_0">Fig. 10(b)</ref> shows the architecture of the radiance field decoder module.</p><p>For training we initialize all latents z = 0 and train them jointly with the parameters of the 3 modules. We use the Adam optimizer and a learning rate of 0.001 for latents and 0.0001 for model parameters. We train our model on 8 A100 NVIDIA GPUs for 2-7 days (depending on dataset size), with a batch size of 16 trajectories where we randomly sample 2 images per trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Prior Architecture Design and Details</head><p>We employ a Denoising Diffusion Probabilistic Model (DDPM) <ref type="bibr" target="#b14">[15]</ref> to learn the distribution p(Z). Specifically, we adopt the UNet architecture from <ref type="bibr" target="#b32">[33]</ref> to denoise the latent at each timestep. During training, we sample t ? {1, ..., T } uniformly and take the gradient descent step on ?p from Eq. 3. Different from <ref type="bibr" target="#b32">[33]</ref>, we keep the original DDPM training scheme with fixed time-dependent covariance matrix and linear noise schedule. During inference, we start from sampling latent from zero-mean unit-variance Gaussian distribution and perform the denoising step iteratively. To accelerate the sampling efficiency, we leverage DDIM <ref type="bibr" target="#b56">[59]</ref> to denoise only 50 steps by modeling the deterministic non-Markovian diffusion processes.</p><p>For conditional generative modelling tasks, the conditioning mechanism should be general to support conditioning inputs from diverse modalities (i.e. text, image, categorical class, etc.). To fulfill this requirement, we first project the conditional inputs into an embedding representations c via a modality-specific encoder. For text conditioning, we employ a pre-trained RoBERTa-base <ref type="bibr" target="#b25">[26]</ref>. For image conditioning, we employ a ResNet-18 <ref type="bibr" target="#b12">[13]</ref> pre-trained on ImageNet. For categorical conditioning, we employ a trainable per-environment embedding layer. We freeze the encoders for text and image inputs to avoid over-fitting issues. We borrow the cross attention module from LDM <ref type="bibr" target="#b45">[48]</ref> to fuse the conditioning representation c with the intermediate activations at multiple levels in the UNet <ref type="bibr" target="#b47">[50]</ref>. The cross-attention module implements an attention mechanism with key and value generated from c while the query generated from the intermediate activations in the UNet architecture (we refer readers to <ref type="bibr" target="#b45">[48]</ref> for more details).</p><p>For training the DDPM prior, we use the Adam optimizer and learning rate of 4.0e ?06 . We train our model on 1 A100 NVIDIA GPU for 1-3 days for unconditional prior learning and 3-5 days for conditional prior learning experiments (depending on dataset size), with a batch size of 256 and 32 respectively. For the hyper-parameters of the DDPM model, we set the number diffusion steps to 1000, noise schedule as linearly decreasing from 0.0195 to 0.0015, base channel size to 224, attention resolutions at <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref>, and number of attention heads to 8. <ref type="table">Table 4</ref>: Ablation experiment for the critical parameters of the optimization process described in Eq. 2</p><p>In addition, we also provide ablation experiments for the second stage of our model where we learn the prior p(Z). In particular, we ablate critical factors of our model: the importance of learning corresponding scene and pose latents, the width of the denoising network in the DDPM prior, and the noise scale parameter ?. In Tab. 5 we show results for each factor. In particular, in the first two rows of Tab. 5 we show the result of training the prior while breaking the correspondence of z = [zpose, zscene]. We break this correspondence by forming random pairs of z = [zpose, zscene] after optimizing the latent representations, and then training the prior on these random pairs. We observe that training the prior to render scenes from a random pose latent impacts both the FID and SwAV-FID scores substantially, which provides support for our claim that the distribution of valid camera poses depends on the scene. In addition, we can see how the width of the denoising model affects performance. By increasing the number of channels, the DDPM prior is able to better capture the distribution of latents. Finally, we also show how different noise scales ? impact the capacity of the generative model to capture the distribution of scenes. All results Tab 5 are performed on the full VLN-CE dataset <ref type="bibr" target="#b22">[23]</ref>. <ref type="bibr">VLN</ref>  <ref type="table">Table 5</ref>: Ablation study for different design choice of GAUDI.</p><p>In Tab. 6 we report ablation results for modulation on the denoising architecture in the DDPM prior. We compare cross-attention style conditioning as in LDM <ref type="bibr" target="#b45">[48]</ref> with FiLM style conditioning <ref type="bibr">[38]</ref>. For FiLM style conditioning, we take the mean of the conditioning representation c across spatial dimension and project it into the same space as denoising timestep embedding. After that, we take the sum of the conditioning and timestep embedding and predict the scaling and shift factors of the affine transformation applied to the UNet intermediate activations. We compare the performance of the two conditioning mechanisms in Tab. 6. We observe that the cross-attention style conditioning performs better than the FiLM style across all our conditional generative modeling experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Additional Visualizations</head><p>In this section we provide additional visualizations for both figures in this appendix and videos that can be found attached in the supplementary material. In <ref type="figure" target="#fig_0">Fig. 11</ref>   <ref type="table">Table 6</ref>: Ablation study for conditioning mechanism of GAUDI.</p><p>pairs of latents obtained for VLN-CE dataset <ref type="bibr" target="#b22">[23]</ref>, where each row represents a interpolation path between a random pair of latents (i.e. rightmost and leftmost columns). We can see how the model tends to produce smoothly changing interpolation paths which align similar scene content. In addition we refer readers to the folder ./interpolations in which videos of interpolations can be found where for each interpolated scene we immersively navigate it by moving the camera forwards and rotating left and right.</p><p>In addition, we provide more visualization of samples from the unconditional GAUDI model in <ref type="figure" target="#fig_0">Fig. 12</ref> for VLN-CE <ref type="bibr" target="#b22">[23]</ref>, <ref type="figure" target="#fig_0">Fig. 13</ref> for ARKitScenes <ref type="bibr" target="#b0">[1]</ref> and <ref type="figure" target="#fig_0">Fig. 14 for</ref> Replica <ref type="bibr" target="#b57">[60]</ref>. In all these figures, each row represents a sample from the prior that is rendered from its corresponding sampled camera path. We note how these qualitative results reinforce the fidelity and variability of the distribution captured by GAUDI, which is also reflected in the quantitative results in Tab. 2 of the main paper. In addition, the folder ./uncond_samples contains videos of more samples from the unconditional GAUDI model for all datasets.</p><p>Finally, the folder ./cond_samples contains a video showing samples from GAUDI conditioned on different modalities like text, images or categorical variables. These visualizations corresponds to the results in Sect. 4.5 of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F License</head><p>Due to licensing issues we cannot release the VLN-CE <ref type="bibr" target="#b22">[23]</ref> raw trajectory data and we refer the reader to https://github.com/jacobkrantz/VLN-CE and to the license of the Matterport3D data http://kaldir. vc.in.tum.de/matterport/MP_TOS.pdf. <ref type="figure" target="#fig_0">Figure 11</ref>: Additional interpolation of 3D scenes in latent space for the VLN-CE dataset <ref type="bibr" target="#b22">[23]</ref>. Each row corresponds to a different interpolation path between random pairs of latent representations (z i , z j ). 18 <ref type="figure" target="#fig_0">Figure 12</ref>: Additional visualizations of scenes sampled from unconditional GAUDI for VLN-CE dataset <ref type="bibr" target="#b22">[23]</ref>. Each row to a scene rendered from camera poses sampled from the prior. <ref type="figure" target="#fig_0">Figure 13</ref>: Additional visualizations of scenes sampled from unconditional GAUDI for ARKitScenes dataset <ref type="bibr" target="#b0">[1]</ref>. Each row corresponds to a scene rendered from camera poses sampled from the prior. <ref type="figure" target="#fig_0">Figure 14</ref>: Additional visualizations of scenes sampled from unconditional GAUDI for Replica dataset <ref type="bibr" target="#b57">[60]</ref>. Each row corresponds to a scene rendered from camera poses sampled from the prior.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>zFigure 1 :</head><label>1</label><figDesc>? p(z | "go through hallway") z ? p(z | ) GAUDI allows to model both conditional and unconditional distributions over complex 3D scenes. Sampled scenes and poses from (left) the unconditional distribution, and (right) a distribution conditioned on an image observation or a text prompt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>p 2 R 3 &lt;</head><label>3</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " E y d 6 3 N o W c X z H E C F Q X t V g 1 N b m 8 e 4 = " &gt; A A A B / 3 i c b V D L S s N A F J 3 U V 6 2 v q u D G T b A I r k o i R V 0 W 3 b i s Y B / Q l j K Z 3 r R D J 5 M w c y P W m I W / 4 s a F I m 7 9 D X f + j Z O 2 C 2 0 9 M H A 4 5 1 7 u m e N F g m t 0 n G 8 r t 7 S 8 s r q W X y 9 s b G 5 t 7 x R 3 9 x o 6 j B W D O g t F q F o e 1 S C 4 h D p y F N C K F N D A E 9 D 0 R l e Z 3 7 w D p X k o b 3 E c Q T e g A 8 l 9 z i g a q V c 8 6 A Q U h 5 6 f P K S 9 D s I 9 J l G o I e 0 V S 0 7 Z m c B e J O 6 M l M g M t V 7 x q 9 M P W R y A R C a o 1 m 3 X i b C b U I W c C U g L n V h D R N m I D q B t q K Q B 6 G 4 y y Z / a x 0 b p 2 3 6 o z J N o T 9 T f G w k N t B 4 H n p n M 0 u p 5 L x P / 8 9 o x + h f d h M s o R p B s e s i P h Y 2 h n Z V h 9 7 k C h m J s C G W K m 6 w 2 G 1 J F G Z r K C q Y E d / 7 L i 6 R x W n b P y p W b S q l 6 O a s j T w 7 J E T k h L j k n V X J N a q R O G H k k z + S V v F l P 1 o v 1 b n 1 M R 3 P W b G e f / I H 1 + Q N i V J b / &lt; / l a t e x i t &gt; z pose &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 2 Q 7 6 / 2 r s h Y r T + G f R n a 5 4 t 8 / V o Y = " &gt; A A A B + X i c b V D L S s N A F L 3 x W e s r 6 t L N Y B F c l a Q U d V l w 4 7 K C f U A b w m Q 6 a Y d O H s x M q j H k T 9 y 4 U M S t f + L O v 3 H S Z q G t B w Y O 5 9 z L P X O 8 m D O p L O v b W F v f 2 N z a r u x U d / f 2 D w 7 N o + O u j B J B a I d E P B J 9 D 0 v K W U g 7 i i l O + 7 G g O P A 4 7 X n T m 8 L v z a i Q L A r v V R p T J 8 D j k P m M Y K U l 1 z S H A V Y T z 8 8 e c j d 7 T J 9 y 1 6 x Z d W s O t E r s k t S g R N s 1 v 4 a j i C Q B D R X h W M q B b c X K y b B Q j H C a V 4 e J p D E m U z y m A 0 1 D H F D p Z P P k O T r X y g j 5 k d A v V G i u / t 7 I c C B l G n h 6 s s g p l 7 1 C / M 8 b J M q / d j I W x o m i I V k c 8 h O O V I S K G t C I C U o U T z X B R D C d F Z E J F p g o X V Z V l 2 A v f 3 m V d B t 1 + 7 L e v G v W W o 2 y j g q c w h l c g A 1 X 0 I J b a E M H C M z g G V 7 h z c i M F + P d + F i M r h n l z g n 8 g f H 5 A 5 3 z l E M = &lt; / l a t e x i t &gt; w xyz &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n U 9 d x t 1 j d C 9 r B 9 6 D 2 a E N D e N 3 x w s = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 E y y C q 5 J I U Z d F N y 4 r 2 A e 0 I U y m k 3 b o Z B J m J m I M + R I 3 L h R x 6 6 e 4 8 2 + c t F l o 6 4 G B w z n 3 c s 8 c P 2 Z U K t v + N i p r 6 x u b W 9 X t 2 s 7 u 3 n 7 d P D j s y S g R m H R x x C I x 8 J E k j H L S V V Q x M o g F Q a H P S N + f 3 R R + / 4 E I S S N + r 9 K Y u C G a c B p Q j J S W P L M + C p G a + k H W z 7 3 s M c 0 9 s 2 E 3 7 T m s V e K U p A E l O p 7 5 N R p H O A k J V 5 g h K Y e O H S s 3 Q 0 J R z E h e G y W S x A j P 0 I Q M N e U o J N L N 5 s F z 6 1 Q r Y y u I h H 5 c W X P 1 9 0 a G Q i n T 0 N e T R U y 5 7 B X i f 9 4 w U c G V m 1 E e J 4 p w v D g U J M x S k V W 0 Y I 2 p I F i x V B O E B d V Z L T x F A m G l u 6 r p E p z l L 6 + S 3 n n T u W i 2 7 l q N 9 n V Z R x W O 4 Q T O w I F L a M M t d K A L G B J 4 h l d 4 M 5 6 M F + P d + F i M V o x y 5 w j + w P j 8 A Y z r k 6 8 = &lt; / l a t e x i t &gt; Wxy &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 9 v l V Y Q i 9 f J g N T I W l P 4 l j 5 h L 8 a w = " &gt; A A A C K X i c b V D L S s N A F J 3 4 r P U V d e l m s A g u p C R S 1 I 1 Q d O O y g n 1 A G s J k O m m H T h 7 M T K R p y O + 4 8 V f c K C j q 1 h 9 x 0 g a s r Q c G z p x z L / f e 4 0 a M C m k Y n 9 r S 8 s r q 2 n p p o 7 y 5 t b 2 z q + / t t 0 Q Y c 0 y a O G Q h 7 7 h I E E Y D 0 p R U M t K J O E G + y 0 j b H d 7 k f v u B c E H D 4 F 4 m E b F 9 1 A + o R z G S S n L 0 e t d H c u B 6 a T u D V 9 D 6 / T n p K M l O 4 a y Q j O e E 0 T i z H b 1 i V I 0 J 4 C I x C 1 I B B R q O / t r t h T j 2 S S A x Q 0 J Y p h F J O 0 V c U s x I V u 7 G g k Q I D 1 G f W I o G y C f C T i e X Z v B Y K T 3 o h V y 9 Q M K J O t u R I l + I x H d V Z b 6 n m P d y 8 T / P i q V 3 a a c 0 i G J J A j w d 5 M U M y h D m s c E e 5 Q R L l i i C M K d q V 4 g H i C M s V b h l F Y I 5 f / I i a Z 1 V z f N q 7 a 5 W q V 8 X c Z T A I T g C J 8 A E F 6 A O b k E D N A E G j + A Z v I F 3 7 U l 7 0 T 6 0 r 2 n p k l b 0 H I A / 0 L 5 / A O p H q F A = &lt; / l a t e x i t &gt; W = [W xy , W yz , W xz ] &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U S W V d L 0 K / k j k Y X o f r I 1 1 W h / I 3 k o = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y U R U Z d F N y 4 r 2 A e 0 I U y m k 3 b o Z B J m J m I b + i V u X C j i 1 k 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b Z X W 1 j c 2 t 8 r b l Z 3 d v f 2 q f X D Y V n E q C W 2 R m M e y G 2 B F O R O 0 p Z n m t J t I i q O A 0 0 4 w v s 3 9 z i O V i s X i Q U 8 S 6 k V 4 K F j I C N Z G 8 u 1 q P 8 J 6 F I R Z Z + Z n T 9 O Z b 9 e c u j M H W i V u Q W p Q o O n b X / 1 B T N K I C k 0 4 V q r n O o n 2 M i w 1 I 5 z O K v 1 U 0 Q S T M R 7 S n q E C R 1 R 5 2 T z 4 D J 0 a Z Y D C W J o n N J q r v z c y H C k 1 i Q I z m c d U y 1 4 u / u f 1 U h 1 e e x k T S a q p I I t D Y c q R j l H e A h o w S Y n m E 0 M w k c x k R W S E J S b a d F U x J b j L X 1 4 l 7 f O 6 e 1 m / u L + o N W 6 K O s p w D C d w B i 5 c Q Q P u o A k t I J D C M 7 z C m z W 1 X q x 3 6 2 M x W r K K n S P 4 A + v z B 4 5 w k 7 A = &lt; / l a t e x i t &gt; Wxz &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r W 4 Y X c W 3 C / N V V J y O Z c + y W W s W Y L A = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 E y y C q 5 J I U Z d F N y 4 r 2 A e 0 I U y m k 3 b o Z B J m J k I a 8 i V u X C j i 1 k 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 f s y o V L b 9 b V Q 2 N r e 2 d 6 q 7 t b 3 9 g 8 O 6 e X T c k 1 E i M O n i i E V i 4 C N J G O W k q 6 h i Z B A L g k K f k b 4 / u y v 8 / h M R k k b 8 U a U x c U M 0 4 T S g G C k t e W Z 9 F C I 1 9 Y O s n 3 t Z O s 8 9 s 2 E 3 7 Q W s d e K U p A E l O p 7 5 N R p H O A k J V 5 g h K Y e O H S s 3 Q 0 J R z E h e G y W S x A j P 0 I Q M N e U o J N L N F s F z 6 1 w r Y y u I h H 5 c W Q v 1 9 0 a G Q i n T 0 N e T R U y 5 6 h X i f 9 4 w U c G N m 1 E e J 4 p w v D w U J M x S k V W 0 Y I 2 p I F i x V B O E B d V Z L T x F A m G l u 6 r p E p z V L 6 + T 3 m X T u W q 2 H l q N 9 m 1 Z R x V O 4 Q w u w I F r a M M 9 d K A L G B J 4 h l d 4 M + b G i / F u f C x H K 0 a 5 c w J / Y H z + A I / 2 k 7 E = &lt; / l a t e x i t &gt; Wyz &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S n Y Q S 6 1 M 9 Q i h K l q m d y E 6 x Q 3 C + L w = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y N U Y 9 E L x 4 h k U e E D Z k d e m F k d n Y z M 2 t C C F / g x Y P G e P W T v P k 3 D r A H B S v p p F L V n e 6 u I B F c G 9 f 9 d n J r 6 x u b W / n t w s 7 u 3 v 5 B 8 f C o q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R 7 c x v P a H S P J b 3 Z p y g H 9 G B 5 C F n 1 F i p / t A r l t y y O w d Z J V 5 G S p C h 1 i t + d f s x S y O U h g m q d c d z E + N P q D K c C Z w W u q n G h L I R H W D H U k k j 1 P 5 k f u i U n F m l T 8 J Y 2 Z K G z N X f E x M a a T 2 O A t s Z U T P U y 9 5 M / M / r p C a 8 9 i d c J q l B y R a L w l Q Q E 5 P Z 1 6 T P F T I j x p Z Q p r i 9 l b A h V Z Q Z m 0 3 B h u A t v 7 x K m h d l 7 7 J c q V d K 1 Z s s j j y c w C m c g w d X U I U 7 q E E D G C A 8 w y u 8 O Y / O i / P u f C x a c 0 4 2 c w x / 4 H z + A L u v j O c = &lt; / l a t e x i t &gt; Z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r l 8 U g L e 6 + Y P + X O s J k 2 K E g u g J I W I = " &gt; A A A C F 3 i c b V D L S s N A F J 3 U V 6 2 v q E s 3 w S K 4 C o m I u h G K b l x W 6 A v a E C b T S T t 0 8 n D m R i g x f + H G X 3 H j Q h G 3 u v N v n L Q p a O u B g X P O v Z e 5 9 3 g x Z x I s 6 1 s r L S 2 v r K 6 V 1 y s b m 1 v b O / r u X k t G i S C 0 S S I e i Y 6 H J e U s p E 1 g w G k n F h Q H H q d t b 3 S d 1 9 v 3 V E g W h Q 0 Y x 9 Q J 8 C B k P i M Y l O X q Z m + I I e 0 F G I a e n z a y z J W X 3 Z m 8 U + p h J k A J x 9 W r l m l N Y C w S u y B V V K D u 6 l + 9 f k S S g I Z A O J a y a 1 s x O C k W w A i n W a W X S B p j M s I D 2 l U 0 x A G V T j q 5 K z O O l N M 3 / E i o F 4 I x c X 9 P p D i Q c h x 4 q j N f U s 7 X c v O / W j c B / 8 J J W R g n Q E M y / c h P u A G R k Y d k 9 J m g B P h Y E U w E U 7 s a Z I g F J q C i r K g Q 7 P m T F 0 n r x L T P z N P b 0 2 r t q o i j j A 7 Q I T p G N j p H N X S D 6 q i J C H p E z + g V v W l P 2 o v 2 r n 1 M W 0 t a M b O P / k D 7 / A G t B q G D &lt; / l a t e x i t &gt;T s = [q s |t s ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>S t h T S m f p 7 I m W h M e M w s J 0 h w 6 F Z 9 K b i f 1 4 7 w f 6 V n w o V J w i K z x f 1 E 0 k x o t P 3 a U 9 o 4 C j H l j C u h b 2 V 8 i H T j K M N K W 9 D 8 B Z f X i a N 8 7 J 3 U a 7 c V Y r V 6 y y O H D k m J 6 R E P H J J q u S W 1 E i d c K L I M 3 k l b 4 5 x X p x 3 5 2 P e u u J k M 0 f k D 5 z P H 1 G q k L I = &lt; / l a t e x i t &gt; (a) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + U t 9 s q v j A W v 9 B 9 y d c K b r s g V f H v w = " &gt; A A A B 8 X i c b V D L S g N B E J z 1 G e M r 6 t H L Y B D i J e x K U I 9 B L x 4 j m A c m S 5 i d d J I h s 7 P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>a 5 2</head><label>2</label><figDesc>X v o l y 5 q x S r 1 1 k c O X J M T k i J e O S S V M k t q Z E 6 4 U S R Z / J K 3 h z j v D j v z s e 8 d c X J Z o 7 I H z i f P 1 M w k L M = &lt; / l a t e x i t &gt; (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>&lt; l a t e x i t s h a 1 _</head><label>1</label><figDesc>b a s e 6 4 = " M a z d H G m t 0 n i 1 F 4 N W K h F 8 / 6 B 9 j M g = " &gt; A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a k q M u i G 5 c V 7 A M 6 Q 8 m k m T Y 0 k x m S j F C H / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n S A T X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w q y t o 0 F r H q B U Q z w S V r G 2 4 E 6 y W K k S g Q r B t M b n O / + 8 i U 5 r F 8 M N O E + R E Z S R 5 y S o y V P C 8 i Z h y E 2 d N s w A f V m l N 3 5 s C r x C 1 I D Q q 0 B t U v b x j T N G L S U E G 0 7 r t O Y v y M K M O p Y L O K l 2 q W E D o h I 9 a 3 V J K I a T + b Z 5 7 h M 6 s M c R g r + 6 T B c / X 3 R k Y i r a d R Y C f z j H r Z y 8 X / v H 5 q w m s / 4 z J J D Z N 0 c S h M B T Y x z g v A Q 6 4 Y N W J q C a G K 2 6 y Y j o k i 1 N i a K r Y E d / n L q 6 R z U X c v 6 4 3 7 R q 1 5 U 9 R R h h M 4 h X N w 4 Q q a c A c t a A O F B J 7 h F d 5 Q i l 7 Q O / p Y j J Z Q s X M M f 4 A + f w C F P J I D &lt; / l a t e x i t &gt; z i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 6 f l h Q N v R H X d o M A Y U 5 T Q v v W V x e Q = " &gt; A A A B 8 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y I q M u i G 5 c V 7 A M 6 Q 8 m k m T Y 2 k w x J R q h D f 8 O N C 0 X c + j P u / B s z 7 S y 0 9 U D g c M 6 9 3 J M T J p x p 4 7 r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h + 0 t U w V o S 0 i u V T d E G v K m a A t w w y n 3 U R R H I e c d s L x T e 5 3 H q n S T I p 7 M 0 l o E O O h Y B E j 2 F j J 9 2 N s R m G U P U 3 7 D / 1 q z a 2 7 M 6 B l 4 h W k B g W a / e q X P 5 A k j a k w h G O t e 5 6 b m C D D y j D C 6 b T i p 5 o m m I z x k P Y s F T i m O s h m m a f o x C o D F E l l n z B o p v 7 e y H C s 9 S Q O 7 W S e U S 9 6 u f i f 1 0 t N d B V k T C S p o Y L M D 0 U p R 0 a i v A A 0 Y I o S w y e W Y K K Y z Y r I C C t M j K 2 p Y k v w F r + 8 T N p n d e + i f n 5 3 X m t c F 3 W U 4 Q i O 4 R Q 8 u I Q G 3 E I T W k A g g W d 4 h T c n d V 6 c d + d j P l p y i p 1 D + A P n 8 w e G w J I E &lt; / l a t e x i t &gt; z j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " r l 8 U g L e 6 + Y P + X O s J k 2 K E g u g J I W I = " &gt; A A A C F 3 i c b V D L S s N A F J 3 U V 6 2 v q E s 3 w S K 4 C o m I u h G K b l x W 6 A v a E C b T S T t 0 8 n D m R i g x f + H G X 3 H j Q h G 3 u v N v n L Q p a O u B g X P O v Z e 5 9 3 g x Z x I s 6 1 s r L S 2 v r K 6 V 1 y s b m 1 v b O / r u X k t G i S C 0 S S I e i Y 6 H J e U s p E 1 g w G k n F h Q H H q d t b 3 S d 1 9 v 3 V E g W h Q 0 Y x 9 Q J 8 C B k P i M Y l O X q Z m + I I e 0 F G I a e n z a y z J W X 3 Z m 8 U + p h J k A J x 9 W r l m l N Y C w S u y B V V K D u 6 l + 9 f k S S g I Z A O J a y a 1 s x O C k W w A i n W a W X S B p j M s I D 2 l U 0 x A G V T j q 5 K z O O l N M 3 / E i o F 4 I x c X 9 P p D i Q c h x 4 q j N f U s 7 X c v O / W j c B / 8 J J W R g n Q E M y / c h P u A G R k Y d k 9 J m g B P h Y E U w E U 7 s a Z I g F J q C i r K g Q 7 P m T F 0 n r x L T P z N P b 0 2 r t q o i j j A 7 Q I T p G N j p H N X S D 6 q i J C H p E z + g V v W l P 2 o v 2 r n 1 M W 0 t a M b O P / k D 7 / A G t B q G D &lt; / l a t e x i t &gt;T s = [q s |t s ] &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E y d 6 3 N o W c X z H E C F Q X t V g 1 N b m 8 e 4 = " &gt; A A A B / 3 i c b V D L S s N A F J 3 U V 6 2 v q u D G T b A I r k o i R V 0 W 3 b i s Y B / Q l j K Z 3 r R D J 5 M w c y P W m I W / 4 s a F I m 7 9 D X f + j Z O 2 C 2 0 9 M H A 4 5 1 7 u m e N F g m t 0 n G 8 r t 7 S 8 s r q W X y 9 s b G 5 t 7 x R 3 9 x o 6 j B W D O g t F q F o e 1 S C 4 h D p y F N C K F N D A E 9 D 0 R l e Z 3 7 w D p X k o b 3 E c Q T e g A 8 l 9 z i g a q V c 8 6 A Q U h 5 6 f P K S 9 D s I 9 J l G o I e 0 V S 0 7 Z m c B e J O 6 M l M g M t V 7 x q 9 M P W R y A R C a o 1 m 3 X i b C b U I W c C U g L n V h D R N m I D q B t q K Q B 6 G 4 y y Z / a x 0 b p 2 3 6 o z J N o T 9 T f G w k N t B 4 H n p n M 0 u p 5 L x P / 8 9 o x + h f d h M s o R p B s e s i P h Y 2 h n Z V h 9 7 k C h m J s C G W K m 6 w 2 G 1 J F G Z r K C q Y E d / 7 L i 6 R x W n b P y p W b S q l 6 O a s j T w 7 J E T k h L j k n V X J N a q R O G H k k z + S V v F l P 1 o v 1 b n 1 M R 3 P W b G e f / I H 1 + Q N i V J b / &lt; / l a t e x i t &gt; z pose &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z g J J T U J k l O p D Q O W M 8 P 8 9 P P K 7 F t 4 = " &gt; A A A B 9 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 0 L I r R T 0 W v X i s Y D 9 g u 5 R s m m 1 D s 8 m a Z A t l 6 e / w 4 k E R r / 4 Y b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z w o Q z b V z 3 2 1 l Z X V v f 2 C x s F b d 3 d v f 2 S w e H T S 1 T R W i D S C 5 V O 8 S a c i Z o w z D D a T t R F M c h p 6 1 w e D f 1 W y O q N J P i 0 Y w T G s S 4 L 1 j E C D Z W C j T q M I H 8 C + 8 c e U G 3 V H Y r 7 g x o m X g 5 K U O O e r f 0 1 e l J k s Z U G M K x 1 r 7 n J i b I s D K M c D o p d l J N E 0 y G u E 9 9 S w W O q Q 6 y 2 d E T d G q V H o q k s i U M m q m / J z I c a z 2 O Q 9 s Z Y z P Q i 9 5 U / M / z U x P d B B k T S W q o I P N F U c q R k W i a A O o x R Y n h Y 0 s w U c z e i s g A K 0 y M z a l o Q / A W X 1 4 m z c u K d 1 W p P l T L t d s 8 j g I c w w m c g Q f X U I N 7 q E M D C D z B M 7 z C m z N y X p x 3 5 2 P e u u L k M 0 f w B 8 7 n D 0 l h k H 4 = &lt; / l a t e x i t &gt; s 2 [ 1, 1] &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 9 J + J u C 5 F x M n v U s R 7 p w s l v r J L d I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 8 t 2 F p o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g T j 2 5 n / 8 I R K 8 1 j e m 0 m C f k S H k o e c U W O l J u u X K 2 7 V n Y O s E i 8 n F c j R 6 J e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S v q h 6 l 9 V a s 1 a p 3 + R x F O E E T u E c P L i C O t x B A 1 r A A O E Z X u H N e X R e n H f n Y 9 F a c P K Z Y / g D 5 / M H y V O M 8 A = = &lt; / l a t e x i t &gt; c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c t a j A + E w 7 9 A F F 9 g q 5 h D F o r I 4 a I g = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 b u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U C P v l i l t 1 5 y C r x M t J B X L U + + W v 3 i B m a Y T S M E G 1 7 n p u Y v y M K s O Z w G m p l 2 p M K B v T I X Y t l T R C 7 W f z Q 6 f k z C o D E s b K l j R k r v 6 e y G i k 9 S Q K b G d E z U g v e z P x P 6 + b m v D G z 7 h M U o O S L R a F q S A m J r O v y Y A r Z E Z M L K F M c X s r Y S O q K D M 2 m 5 I N w V t + e Z W 0 L q r e V d V r X F Z q t 3 k c R T i B U z g H D 6 6 h B v d Q h y Y w Q H i G V 3 h z H p0 X 5 9 3 5 W L Q W n H z m G P 7 A + f w B z O m M 8 A = = &lt; / l a t e x i t &gt; f &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b z c J Z N x u v y e r P n n i j w i d i n E z i g o = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 4 W e t X V P D i Z b E I H q Q k I u q x 6 M V j B f s B T S i b 7 a Z d u p u E 3 Y l Y Y g 7 + F S 8 e F P H q 3 / D m v 3 H b 5 q C t D w Y e 7 8 0 w M y 9 I B N f g O N / W w u L S 8 s p q a a 2 8 v r G 5 t W 3 v 7 D Z 1 n C r K G j Q W s W o H R D P B I 9 Y A D o K 1 E 8 W I D A R r B c P r s d + 6 Z 0 r z O L q D U c J 8 S f o R D z k l Y K S u v e 9 l n u Z 9 S U 6 w B + w B g j A j u Z d 3 7 Y p T d S b A 8 8 Q t S A U V q H f t L 6 8 X 0 1 S y C K g g W n d c J w E / I w o 4 F S w v e 6 l m C a F D 0 m c d Q y M i m f a z y f 0 5 P j J K D 4 e x M h U B n q i / J z I i t R 7 J w H R K A g M 9 6 4 3 F / 7 x O C u G l n / E o S Y F F d L o o T A W G G I / D w D 2 u G A U x M o R Q x c 2 t m A 6 I I h R M Z G U T g j v 7 8 j x p n l b d 8 6 p 7 e 1 a p X R V x l N A B O k T H y E U X q I Z u U B 0 1 E E W P 6 B m 9 o j f r y X q x 3 q 2 P a e u C V c z s o T + w P n 8 A L + 6 W O A = = &lt; / l a t e x i t &gt; { , a} &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k B 6 O h S J t 9 N P M y J M A T v w 7 e s 7 j H Z 0 = " &gt; A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V Z J S 1 G X B j c s K 9 g F t C J P p p B 0 6 e T B z U x t D / s S N C 0 X c + i f u / B u n b R b a e u D C 4 Z x 7 u f c e L x Z c g W V 9 G 6 W N z a 3 t n f J u Z W / / 4 P D I P D 7 p q C i R l L V p J C L Z 8 4 h i g o e s D R w E 6 8 W S k c A T r O t N b u d + d 8 q k 4 l H 4 A G n M n I C M Q u 5 z S k B L r m k O g M 3 A 8 7 P H 3 M 1 m 6 V P u m l W r Z i 2 A 1 4 l d k C o q 0 H L N r 8 E w o k n A Q q C C K N W 3 r R i c j E j g V L C 8 M k g U i w m d k B H r a x q S g C k n W 1 y e 4 w u t D L E f S V 0 h 4 I X 6 e y I j g V J p 4 O n O g M B Y r X p z 8 T + v n 4 B / 4 2 Q 8 j B N g I V 0 u 8 h O B I c L z G P C Q S 0 Z B p J o Q K r m + F d M x k Y S C D q u i Q 7 B X X 1 4 n n X r N v q o 1 7 h v V Z r 2 I o 4 z O 0 D m 6 R D a 6 R k 1 0 h 1 q o j S i a o m f 0 i t 6 M z H g x 3 o 2 P Z W v J K G Z O 0 R 8 Y n z / I K Z R e &lt; / l a t e x i t &gt; w xyz &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l c R J a 1 Z S a s k F U F S M 9 o V w 6 U I P C O Q = " &gt; A A A C B H i c b V C 7 T s M w F L 3 h W c q r w N j F o k J i q h K o g L G C h b E g + p C a U D m u 0 1 p 1 n M h 2 k K o o A w u / w s I A Q q x 8 B B t / g 9 t m g J Y j W T o + 5 1 7 d e 4 8 f c 6 a 0 b X 9 b S 8 s r q 2 v r h Y 3 i 5 t b 2 z m 5 p b 7 + l o k Q S 2 i Q R j 2 T H x 4 p y J m h T M 8 1 p J 5 Y U h z 6 n b X 9 0 N f H b D 1 Q q F o k 7 P Y 6 p F + K B Y A E j W B u p V y q 7 I d Z D P 0 j j D L l M o N n X T 2 + z + 9 N e q W J X 7 S n Q I n F y U o E c j V 7 p y + 1 H J A m p 0 I R j p b q O H W s v x V I z w m l W d B N F Y 0 x G e E C 7 h g o c U u W l 0 y M y d G S U P g o i a Z 7 Q a K r + 7 k h x q N Q 4 9 E 3 l Z E c 1 7 0 3 E / 7 x u o o M L L 2 U i T j Q V Z D Y o S D j S E Z o k g v p M U q L 5 2 B B M J D O 7 I j L E E h N t c i u a E J z 5 k x d J 6 6 T q n F V r N 7 V K / T K P o w B l O I R j c O A c 6 n A N D W g C g U d 4 h l d 4 s 5 6 s F + v d + p i V L l l 5 z w H 8 g f X 5 A 5 Y G m B A = &lt; / l a t e x i t &gt; p 2 R 3 (a) Architecture of the camera pose decoder network. (b) Architecture of the radiance field network. of the scene decoder is a feature map of shape 64 ? 64 ? 768. To obtain the tri-plane representation W = [Wxy, Wxz, Wyz] we split the channel dimension of the output feature map in 3 chunks of equal size 64 ? 64 ? 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Reconstruction results of the optimization process described in Eq. 2. The first column shows the number of scenes (#sc), trajectories (#tr) and images (#im) per dataset. Due to the large number of images on VLN-CE<ref type="bibr" target="#b22">[23]</ref> and ARKitScenes<ref type="bibr" target="#b0">[1]</ref> datasets we sample 10 random images per trajectory to compute the reconstruction metrics.</figDesc><table><row><cell></cell><cell>#sc-#tr-#im</cell><cell>l 1 ?</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>Rot Err. ?</cell><cell>Trans. Err ?</cell></row><row><cell>Vizdoom [21]</cell><cell>1-32-1k</cell><cell>0.004</cell><cell>44.42</cell><cell>0.98</cell><cell>0.01</cell><cell>1.26</cell></row><row><cell>Replica [60]</cell><cell>18-100-1k</cell><cell>0.006</cell><cell>38.86</cell><cell>0.99</cell><cell>0.03</cell><cell>0.01</cell></row><row><cell>VLN-CE [23]</cell><cell>90-3.6k-600k</cell><cell>0.031</cell><cell>25.17</cell><cell>0.73</cell><cell>0.30</cell><cell>0.02</cell></row><row><cell>ARKitScenes [1]</cell><cell>300-1k-600k</cell><cell>0.039</cell><cell>24.51</cell><cell>0.76</cell><cell>0.16</cell><cell>0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>?</head><label></label><figDesc>GRAF [56] 47.50 ? 2.13 5.44 ? 0.43 65.37 ? 1.64 5.76 ? 0.14 90.43 ? 4.83 8.65 ? 0.27 87.06 ? 9.99 13.44 ? 0.26 ?-GAN[5] 143.55 ? 4.81 15.26 ? 0.15 166.55 ? 3.61 13.17 ? 0.20 151.26 ? 4.19674 14.07 ? 0.56 134.80 ? 10.60 15.58 ? 0.13</figDesc><table /><note>GSN [7] 37.21 ? 1.17 4.56 ? 0.19 41.75 ? 1.33 4.14 ? 0.02 43.32 ? 8.86 6.19 ? 0.49 79.54 ? 2.60 10.21 ? 0.15 GAUDI 33.70 ? 1.27 3.24 ? 0.12 18.75 ? 0.63 1.76 ? 0.05 18.52 ? 0.11 3.63 ? 0.65 37.35 ? 0.38 4.14 ? 0.03</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The resolutions are 64 ? 64, 64 ? 64, 128 ? 128 and 64 ? 64 respectively.the particular indoor space) from which each trajectory was obtained (i.e. a one-hot vector). Tab. 3 shows quantitative results for the different conditional inference problems.</figDesc><table><row><cell cols="2">Text Conditioning</cell><cell cols="2">Image Conditioning</cell><cell cols="2">Categorical Conditioning</cell><cell cols="2">Avg. ? Per-Environment</cell></row><row><cell>FID ?</cell><cell>SwAV-FID ?</cell><cell>FID ?</cell><cell>SwAV-FID ?</cell><cell>FID ?</cell><cell>SwAV-FID ?</cell><cell>FID ?</cell><cell>SwAV-FID ?</cell></row><row><cell>18.50</cell><cell>3.75</cell><cell>19.51</cell><cell>3.93</cell><cell>18.74</cell><cell>3.61</cell><cell>?50.79</cell><cell>?4.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Quantitative results of Conditional Generative Modeling on VLN-CE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>we provide additional interpolations between random 16 Text Conditioning Image Conditioning Categorical Conditioning FID ? SwAV-FID ? FID ? SwAV-FID ? FID ? SwAV-FID ?</figDesc><table><row><cell>FiLM Module [38] 20.99</cell><cell>4.11</cell><cell>21.01</cell><cell>4.21</cell><cell>18.75</cell><cell>3.63</cell></row><row><cell>Cross Attention [48] 18.50</cell><cell>3.75</cell><cell>19.51</cell><cell>3.93</cell><cell>18.74</cell><cell>3.61</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">[38] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. [39] Benjamin Planche, Xuejian Rong, Ziyan Wu, Srikrishna Karanam, Harald Kosch, YingLi Tian, Jan Ernst, and Andreas Hutter. Incremental scene synthesis. In Advances in Neural Information Processing Systems, pages 1668-1678, 2019.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Ablation Study</head> <ref type="bibr" target="#b14">15</ref> <p>We now provide additional ablations studies for the critical components in GAUDI. First, we analyze how the dimensionality of the latent code z d and the magnitude of ? affect the optimization problem defined in Eq. 2. Tab. 4 shows reconstruction metrics for both RGB images and camera poses for a subset of 100 trajectories in the VLN-CE dataset <ref type="bibr" target="#b22">[23]</ref>. We observe a clear trend where increasing the magnitude of ? makes it harder to find latent codes with high reconstruction accuracy. This drop in accuracy is expected since ? controls the amount of noise in latent codes during training. Finally, we observe that reconstruction performance starts to degrade when the latent code dimensionality grows past 2048. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Feigin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gebauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kurz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Dimry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Joffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arik</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Round 1</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05776</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Ethical implications of emerging mixed reality technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Egliston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><forename type="middle">Z</forename><surname>Eric R Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxiao</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07945</idno>
		<title level="m">Efficient geometry-aware 3d generative adversarial networks</title>
		<meeting><address><addrLine>Leonidas Guibas, Jonathan Tremblay, Sameh Khamis</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Eric R Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pi-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00926</idno>
		<title level="m">Periodic implicit generative adversarial networks for 3d-aware image synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06158</idno>
		<title level="m">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unconstrained scene generation with locally conditioned radiance fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Susskind</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Equivariant neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2761" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08985</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and explicit neural view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengsheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ulbricht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="3791" to="3800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10122</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">World models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Zero-shot text-guided object generation with dream fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01455</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Imperfect imaganation: Implications of gans exacerbating biases on facial data augmentation and snapchat selfie lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niharika</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Olmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sailik</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lydia</forename><surname>Manikonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subbarao</forename><surname>Kambhampati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09528</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vizdoom: A doom-based ai research platform for visual reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Kempka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Runc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Toczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Ja?kowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computational Intelligence and Games (CIG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nerf-vae: A geometry aware 3d scene generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Adam R Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pol</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sona</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">Jimenez</forename><surname>Mokr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5742" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond the nav-graph: Vision and language navigation in continuous environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video autoencoder: self-supervised disentanglement of static 3d structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9730" to="9740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyaw Zaw</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11571</idno>
		<title level="m">Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Bar-On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>Hanocka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03221</idno>
		<title level="m">Text-driven neural stylization for meshes</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nerf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08934</idno>
		<title level="m">Representing scenes as neural radiance fields for view synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The creation and detection of deepfakes: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisroel</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenke</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On self-supervised image representations for GAN evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giraffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12100</idno>
		<title level="m">Representing scenes as compositional generative neural feature fields</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Atiss: Autoregressive transformers for indoor scene synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Despoina</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlan</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Shugrina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04618</idno>
		<title level="m">Convolutional occupancy networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Aaron Van den Oord, and Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rebain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09996</idno>
		<title level="m">Lolnerf: Learn from one look</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14335" to="14345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Look outside the room: Synthesizing a consistent long-term 3d scene video from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanchi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Higher order contractive auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="645" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast and flexible indoor scene synthesis via deep convolutional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6182" to="6190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10752</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Geometry-free view synthesis: Transformers and no 3d priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Ethics and creativity in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Petrini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03111</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<editor>Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi</editor>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noha</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhani</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tagliasacchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS&apos;</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2234" to="2242" />
			<date type="published" when="2016" />
			<publisher>Curran Associates Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Habitat: A platform for embodied ai research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9339" to="9347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02442</idno>
		<title level="m">Generative radiance fields for 3d-aware image synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Improved adversarial systems for 3d object generation and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Denoising diffusion implicit models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shobhit</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05797</idno>
		<title level="m">The replica dataset: A digital replica of indoor spaces</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting and mode collapse in gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Tung</forename><surname>Hoang Thanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">This face does not exist... but it might be yours! identity leakage in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Tinsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Czajka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1320" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Score-based generative modeling in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11287" to="11302" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Neural discrete representation learning. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sceneformer: Indoor scene generation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandan</forename><surname>Yeshwanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="106" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning fast samplers for diffusion models by differentiating through sample quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learning to efficiently sample from diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03802</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05131</idno>
		<title level="m">Plenoxels: Radiance fields without neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vickie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02190</idno>
		<title level="m">Neural radiance fields from one or few images</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Cips-3d: A 3d-aware generator of gans based on conditionally-independent pixel synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09788</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
