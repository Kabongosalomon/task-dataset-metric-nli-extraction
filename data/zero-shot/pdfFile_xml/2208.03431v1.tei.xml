<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IVT: An End-to-End Instance-guided Video Transformer for 3D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Qiu</surname></persName>
							<email>qiuzhongwei@xs.ustb.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiansheng</forename><surname>Yang</surname></persName>
							<email>yangqiansheng@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
							<email>wangjian33@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Qiu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiansheng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology Beijing</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IVT: An End-to-End Instance-guided Video Transformer for 3D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<note>ACM Reference Format:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS ? Computing methodologies ? Object recognition KEYWORDS Video Transformer, Human Pose Estimation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video 3D human pose estimation aims to localize the 3D coordinates of human joints from videos. Recent transformer-based approaches focus on capturing the spatiotemporal information from sequential 2D poses, which cannot model the contextual depth feature effectively since the visual depth features are lost in the step of 2D pose estimation. In this paper, we simplify the paradigm into an end-toend framework, Instance-guided Video Transformer (IVT), which enables learning spatiotemporal contextual depth information from visual features effectively and predicts 3D poses directly from video frames. In particular, we firstly formulate video frames as a series of instance-guided tokens and each token is in charge of predicting the 3D pose of a human instance. These tokens contain body structure information since they are extracted by the guidance of joint offsets from the human center to the corresponding body joints. Then, these tokens are sent into IVT for learning spatiotemporal contextual depth. In addition, we propose a cross-scale instanceguided attention mechanism to handle the variational scales among multiple persons. Finally, the 3D poses of each person are decoded from instance-guided tokens by coordinate regression. Experiments on three widely-used 3D pose estimation benchmarks show that the proposed IVT achieves state-of-the-art performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Comparison of (a) The pipeline of 2D-to-3D video pose lifting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">51]</ref>, (b) Recurrent structure based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref> for video 3D pose estimation, (c) Our end-to-end Instance-guided Video Transformer (IVT). IVT is a singlestage framework while others are two-stage methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>3D human pose estimation aims to localize the 3D joints of person(s) from monocular images or videos. As a fundamental computer vision task, it has a lot of applications, including action recognition <ref type="bibr" target="#b26">[27]</ref>, human-robot interaction detection <ref type="bibr" target="#b20">[21]</ref>, and virtual reality <ref type="bibr" target="#b31">[32]</ref>, etc. Unfortunately, estimating 3D human poses from monocular 2D images or videos is very challenging because of the lack of depth information.</p><p>To tackle this problem, some image-based approaches learn depth information from image feature by depth map supervision <ref type="bibr" target="#b39">[40]</ref> or 3D heatmap supervision <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37]</ref>, while other image-based approaches <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49]</ref> firstly estimate 2D pose from image, and then lift 2D pose to 3D pose. However, the depth information implied in a single image is still limited. Compare to a single image, video can provide more motion cues which are quite helpful for the inference of depth. Thus, video-based approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51]</ref> are developed rapidly in recent years. And we also focus on the video-based method in this paper.</p><p>By the benefit of the amazing performance of modern 2D pose estimators, many video-based approaches follow the 2D-to-3D lifting paradigm as <ref type="figure">Figure 1 (a)</ref>, and exploit spatial and temporal modeling methods to improve the performance of 3D pose estimation. <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b12">[13]</ref> apply convolution neural network (CNN) and recurrent neural network (RNN) respectively to model temporal dependency among the sequential 2D poses. But limited by the formulation of CNN and RNN, they are not good at capturing the long-range dependency in both spatial and temporal dimensions. To alleviate this situation, some works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">42]</ref> introduce graph neural network (GNN) to exploit spatial-temporal information between keypoints, which can capture both short-term and long-term dependency by setting an appropriate adjacent matrix. Besides GNN, other works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b50">51]</ref> use transformer to get more representative features from pose sequence, and also improve the performance of video 3D pose estimation significantly.</p><p>Despite the significant progress achieved by the above GNN or transformer-based methods, they still did not break out of the 2Dto-3D lifting paradigm. This paradigm only considers the structure of the 2D pose for depth estimation, while ignoring the contextual depth information contained in the semantic feature of video frames, since the semantic feature has been dropped at the 2D pose prediction stage. But we suppose that the context depth information embedded in the semantic feature is more effective than the 2D pose structure for 3D pose estimation.</p><p>As shown in <ref type="figure">Figure 1</ref> (b), some video-based approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38]</ref> with the recurrent neural network, firstly conduct human detection and then predict 3D pose directly from the cropped video patches, which can be regarded as exploiting contextual depth information from the semantic feature of video frames to some extent. But they apply the recurrent neural network to exchange features between different video frames for only temporal modeling, which is not effective compared with GNN or transformer-based spatial-temporal modeling methods mentioned above. Besides, the inputs of cropped patches bring a new problem of keypoints feature alignment.</p><p>To handle the above problems, we simplify the video 3D pose estimation into an end-to-end transformer-based framework as shown in <ref type="figure">Figure 1</ref> (c), which aims to make full usage of the spatial-temporal depth feature and predicts 3D pose directly from video frames. In order to capture the effective contextual depth information and reduce the computational burden introduced by conducting self-attention on the dense semantic feature of video frames, we propose an Instance-guided Video Transformer to model the spatial-temporal depth information by the guidance of human instance.</p><p>Firstly, Instance-guided Video Transformer (IVT) introduces a series of instance-guided visual tokens and each token is capable for predicting 3D pose of an instance. These tokens are contructed by aggregating features from related spatial points by the guidance of a set of learned 2D offsets from human center to the corresponding human joints. This mechanism enables each token can capture the whole body information. For a query token, the attention is computed on both spatial and temporal dimensions to exchange the context depth information. Furthermore, we propose a cross-scale instance-guided attention mechanism to handle the variational scales among multiple persons. As a result, IVT enables effective spatial-temporal depth feature exchange and brings significant improvement to video 3D pose estimation.</p><p>In summary, IVT is a simple and unified framework that is suitable for both single-person and multi-person video 3D pose estimation tasks. And to the best of our knowledge, for multi-person video 3D pose estimation, IVT is the first end-to-end method that leverages transformer to directly capture multi-person depth information in the video. Our contributions can be summarized as follows:</p><p>? We propose a novel end-to-end transformer-based framework for both single-person and multi-person video 3D pose estimation, called instance-guided video transformer (IVT). ? We design a novel instance-guided attention mechanism to enable effective spatial-temporal depth information learning in videos. ? IVT achieves new state-of-the-art results on three widelyused video 3D pose estimation benchmarks, Human3.6M, 3DPW, and CMU Panoptic datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Image-based 3D Pose Estimation</head><p>The image-based multi-person 3D pose estimation methods can be mainly divided into two kinds of paradigms: top-down <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref> and bottom-up approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. The top-down paradigm follows a pipeline of conducting human detection firstly and performing single-person 3D pose estimation later. For the single-person cases, they predict 3D poses by learning 3D heatmaps <ref type="bibr" target="#b29">[30]</ref>, or estimating 2D poses by 2D pose estimator <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> and lifting 2D poses to 3D poses <ref type="bibr" target="#b48">[49]</ref>. Typically, PoseNet <ref type="bibr" target="#b29">[30]</ref> predicts the root depths of each person at the stage of human detection, then estimates the 3D coordinates from 3D heatmaps. The bottom-up paradigm <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref> follows a pipeline of firstly estimating the 3D coordinates for each human joint in an image and then assigning them to different human instances. For example, MubyNet <ref type="bibr" target="#b47">[48]</ref> estimates keypoints and limb core at the same time and then integrates limb score to group keypoints into different persons. HMOR <ref type="bibr" target="#b39">[40]</ref> propose hierarchical multi-person ordinal relations as an additional loss to help depth learning. However, the image-based approaches are not good at handling occlusion cases as the video-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video-based 3D Pose Estimation</head><p>Video-based multi-person 3D pose estimation aims to capture temporal information for 3D pose estimation. The ways of extracting temporal information can be divided into two categories: based on image visual features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref> and based on 2D poses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>The methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref> based on image visual features usually crop the human features according to human bounding boxes, and then use 3D convolution or recurrent neural network to extract the temporal information from these cropped sequences features. Sun et al. <ref type="bibr" target="#b37">[38]</ref> propose a skeleton-disentangling framework to separate 3D human pose and shape estimation into spatial and temporal dimensions. TCMR <ref type="bibr" target="#b6">[7]</ref> uses ResNet to extract visual features from video frames, then captures temporal information on these deep features by the recurrent neural network. However, these methods essentially conduct single-person video 3D pose estimation, which brings a new problem of feature alignment due to crop images.</p><p>The methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b50">51]</ref> based on 2D coordinates usually estimate a sequence of 2D poses at first, then lift 2D coordinates sequence to 3D pose by a temporal lifting network. Typically, Cai et al. <ref type="bibr" target="#b2">[3]</ref> exploit spatial-temporal relationships for 3D pose estimation via Graph Convolutional Networks (GCN). Zheng et al. <ref type="bibr" target="#b50">[51]</ref> propose PoseFormer, a spatial-temporal transformer network to capture the spatial-temporal information among human joints. However, these methods cannot capture truly depth features from visual images since the depth feature is lost in the stage of 2D human pose estimation. Besides, these methods disassemble the multiperson video task into a single-person video task. Thus, the depth information between different persons can not be captured. In this paper, we tackle the problems and build an end-to-end multi-person video 3D pose estimation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformer in Human Pose Estimation</head><p>Recently, the transformer-based approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51]</ref> have been proposed to improve the long-term modeling capabilities of sequence for human pose estimation. TransPose <ref type="bibr" target="#b43">[44]</ref> and TF-Pose <ref type="bibr" target="#b28">[29]</ref> formulate human joints as visual tokens and capture the relationship between human joints by self-attention. METRO <ref type="bibr" target="#b19">[20]</ref> and PRTR <ref type="bibr" target="#b19">[20]</ref> exploit the end-to-end transformer-based pose estimation network. PoseFormer <ref type="bibr" target="#b50">[51]</ref> explores the spatial-temporal attention mechanism for 3D pose estimation. However, the Pose-Former didn't study the attention on real depth features from images since it lifts 3D poses from a sequence of 2D poses. Moreover, the existing transformer-based pose estimation methods are designed for single-person pose estimation, which limits their applications.</p><p>In this paper, we study an end-to-end multi-person 3D pose estimation framework and explore to extract the relationship between multi-person joints in both spatial and temporal dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we elaborate on the detail of the proposed Instanceguided Video Transformer (IVT). The framework of IVT is shown in <ref type="figure">Figure 2</ref>. Given a sequence of video frames = { | ? [1, ]}, IVT firstly extracts deep features by a backbone network, which are used to learn instance 2D offsets (from body center to keypoints) and temporal feature motion (trajectory). Then, for each frame, the deep features are organized as visual tokens with the guidance of instance 2D offsets, called instance-guided tokens. And each instance-guided token is in charge of predicting the 3D pose of its corresponding instance by the assistant of the aggregated whole body information. This process is denoted as Instance-Guided Tokenization (IGT). After conducting IGT, instance-guided tokens are sent into a video transformer to capture context depth information in both spatial and temporal dimensions. Finally, the outputted visual tokens are further used to decode 3D poses for persons detected from a human center heatmap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation of Transformer Module</head><p>First of all, we review the formulation of the basic transformer module. Given query matrix Q, key matrix K, value matrix V, where the first dimension of them is sample dimension while the second is feature dimension, a typical attention formulation (Q, K, V) can be expressed as:</p><formula xml:id="formula_0">(Q, K, V) = ( Q ? K ? ) ? V (1)</formula><p>where is the length of feature within these matrixs. If we split Q, K, V into ? heads via feature axis and conduct attention for each head, denoted as</p><formula xml:id="formula_1">{Q 1 , ..., Q ? }, {K 1 , ..., K ? }, {V 1 , ..., V ? }, the</formula><p>Multi-Head Attention (Q, K, V) can be formulated as:</p><formula xml:id="formula_2">(Q, K, V) = ( (? 1 , ..., ? ? )) ? = (Q , K , V ), ? [1, ?]<label>(2)</label></formula><p>where P is linear projection function, means concatenating matrixs along feature axis. Specially, when Q, K, V are derived from a same input matrix X, we can get Multi-Head Self-Attention (?) further, which is denoted as:</p><formula xml:id="formula_3">(X) = (Q, K, V) Q = P (X), K = P (X), V = P (X)<label>(3)</label></formula><p>where P , P , P are linear projection functions for generating Q, K, V, respectively. Then, two basic transformer modules used in this paper can be formulated as:</p><formula xml:id="formula_4">X = ( (X )) (4) X = ( (Q, K, V))<label>(5)</label></formula><p>and they are served for self-attention and cross-attention respectively. While, in the above equations, (?) represents feed forward network, which consists of two linear layers. For simple expression, the layer norm and shortcut path are ignored here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance-guided Tokenization</head><p>In this section, we introduce the process of generating instanceguided tokens for each video frame , named Instance-Guided Tokenization (IGT).</p><p>We firstly extract deep feature for by a backbone network (?), and the shape of is ? ? . Then we split into feature blocks, denote as , which has a shape of ? , where = ? ? and represents the block size. To extract visual tokens, traditional vision transformers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> take each block as a token and capture the spatial and temporal relationships among these block tokens. Then, tokens are generated from features , denoted as . The shape of is ? , where = is feature dim of tokens. However, this tokenization is not fine-grained to capture the context information from the human body to predict depth for 3D human pose estimation. Here, we introduce the instance-guided tokenization (IGT) approach. Different from traditional tokenization, IGT considers the features from whole body as well as the relationship between human joints when extracting visual tokens, which enables each token to encode the body structure of its corresponding human instance.</p><p>For the th block in , denoted as [ ], the process of instanceguided tokenization includes two steps. Firstly, it gathers features from corresponding blocks of [ ] with the guidance of instance 2D offsets. As shown in <ref type="figure">Figure 2</ref>, instance 2D offsets are the joint offsets from body center to joints, and these offsets are preserved in an offset map 2 , which is predicted by several convolutional layers based on deep feature . 2 contains the whole body information of each instance and indicates the feature locations of relative keypoints. The gathered blocks are concatenated into one feature vector, denoted as [ ], and its length is ? . Secondly, a multi-head self-attention module is used to encode [ ] and generate instance-guided token [ ]. This enables feature exchange between different joints of a human instance and makes the token feature be aware of body structure, which improves the   <ref type="figure">Figure 2</ref>: The overview of Instance-guided Video Transformer (IVT), which includes instance-guided tokenization (IGT), instance-guided spatial attention (ISA), and instance-guided temporal attention (ITA). Given video frames of height and width , deep features are extracted by embedding network (?), and are further used to estimate trajectory motions by network (?). The keypoints features are extracted from deep features according to the trajectory and instance 2D offsets and to further learn instance-aware tokens of shape ? by IGT. For a query token [ ] at the ? block in ? frame, ISA is computed in each frame and ITA is computed among temporal frames to capture depth information. Each token in the final layer of IVT outputs the 3D coordinates of a person. MHA, LN, and FFN denote multi-head attention, layer norm, and feed-forward network, respectively. ? means element-wise addition. represents the joints number. N represents token numbers in each frame.</p><formula xml:id="formula_5">T I 1 T I ? ? ? ? ? ? MHA LN FFN ? N ... Q K set V set ...</formula><formula xml:id="formula_6">? 1 T ? ? ISA T ? 1 ISA T ? ? b J C ? i i i [ ] q T i ? [ ] k T i ? [ ] k t i ? ... 1 [ ] k i ? 1 [ ] v i ? ... [ ] v t i ? [ ] v T i ? ( , 1) T T O ? ( 1, 2) T T O ? ? 2 T I ? 2 T ? ? 2 ISA T ? ?</formula><p>robustness of instance-guided token for predicting whole body 3D pose. Concretely, this self-attention can be formulated as:</p><formula xml:id="formula_7">= ? ( [ ], ( , )) = ( ( )) [ ] = ? ( , ? )<label>(6)</label></formula><p>where and are multi-head self-attention module and feed forward module respectively. ? ( , ? ) means reshaping the input data into target ? . As a result, each generated instance-guided token [ ] can encode the global context from a human instance.</p><p>Once the token feature for each video frame is extracted, we pass all the token features = { | ? [1, ]} of video frames to the instance-guided video transformer for 3D pose estimation, which will be elaborated in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Instance-guided Video Transformer</head><p>Instance-guided video transformer (IVT) takes the instance-guided tokens as inputs and conducts spatial-temporal attention to capture context depth information in both spatial and temporal dimensions. It can be divided into two sequential attention stages, Instance-guided Spatial Attention(ISA) and Instance-guided Temporal Attention(ITA). ISA computes the correlation between all tokens within one frame, which can gather the context depth features from other human instances or objects. Based on the output of ISA, ITA calculates attention among a group of corresponding tokens in the temporal dimension, which can aggregate depth information of the same instance from different video frames. Since the inputs are whole images, the human instances in images suffer variational scales. To tackle this problem, we propose a cross-scale attention mechanism for IVT. It enables IVT to be more robust to handle the different scales of human instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.1</head><p>Instance-guided Spatial Attention. Instance-guided Spatial Attention (ISA) conducts spatial self-attention within one frame. Here, for the th frame, it is tokenized as instance-guided tokens . The tokens are sent into ISA and output , which can be formulated as:</p><formula xml:id="formula_8">= ( ) = ( ( ))<label>(7)</label></formula><p>where and are multi-head self-attention and feedforward network, respectively.</p><p>(?) means instance-guided spatial attention. Due to the instance-guided tokens, ISA can capture more fine-grained keypoints relationships between the same person and different human instances at the same time. We compute ISA on each video frame to obtain a sequence of token maps denoted as</p><formula xml:id="formula_9">= { | ? [1, ]}.</formula><p>3.3.2 Instance-guided Temporal Attention. Temporal information is important for 3D human pose estimation, especially in handling occlusion problems. To capture global depth information from temporal features, we introduce instance-guided temporal attention (ITA) here, which computes the cross-attention on instance-guided tokens from different video frames. The query, key, and value for ITA are generated from the token maps outputted from ISA. For a query token [ ] at the th block in th frame, ITA is computed on the same block places among different frames, and we denote the query, key and value for updating this token as Q [ ], K [ ] and V [ ] respectively, which  </p><p>where P , P , and P are linear projection layers for generating the query, key, and value, respectively. means concatenating matrixes along sample dimension. ? [1, ] represents the block index. It is worth to note that, before conducting temporal attention, all the token maps from the video sequence are aligned with optical flow, which is calculated between each pair of adjacent frames in advance. Therefore, the ITA can be computed on the tokens from different frames with the same block index . As shown in <ref type="figure">Figure 2</ref>, the green line represents the corresponding relationship between adjacent frames. Then, for the th token at th frame, the instance-guided temporal attention(ITA) is computed as: </p><formula xml:id="formula_11">[ ] = ( [ ]) = ( (Q [ ], K [ ], V [ ]))<label>(9)</label></formula><formula xml:id="formula_12">= ( ( )) + , ? [1, ]<label>(10)</label></formula><p>3.3.3 Cross-scale Attention. Scale information also matters in depth estimation because the relative depth of different subjects is correlated with their scales. Therefore, to improve the accuracy of depth estimation, we design a cross-scale attention mechanism for the instance-guided video transformer. This cross-scale mechanism encourages the video transformer to calculate attention between blocks with different scales, which can make the relative depth information from other persons be aggregated better. Concretely, we split the deep feature map with different block sizes at the instance-guided tokenization stage. In this paper, we apply three block sizes ? {2, 4, 8}. Then, we pass all the tokens from three scales into cross-scale instance-guided spatial attention (CISA), as shown in <ref type="figure" target="#fig_4">Figure 3</ref>. Due to the feature dimension of tokens from different scales being different, we add projection layers before the cross-scale attention for aligning the feature dimension. Meanwhile, back-projection layers are added after cross-scale attention for restoring the feature dimension of those tokens to their original state.</p><p>After cross-scale instance-guided spatial attention for each frame, three sequences of token maps with different scales are outputted. Then these three sequences are passed into ITA to perform temporal attention individually. Finally, token maps from different scales are added up into one token map frame by frame. For clearity, we name the above operation as multi-scale instance-guided temporal attention (MITA).</p><p>In a nutshell, combined with CISA and MITA, the whole process of IVT in Equation 10 can be recapped as</p><formula xml:id="formula_13">= ( ( )) + , ? [1, ]<label>(11)</label></formula><p>where (?) denotes cross-scale instance-guided spatial attention and (?) denotes multi-scale instance-guided temporal attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>For th frame in video, the outputted token map are used to learn root keypoints heatmaps ? of size ? and 3D offset of size ? ? by a convolutional layer. For each point in ? with high confidence, the corresponding 3D offsets at the place in are extracted to decode a whole 3D pose of size ? 3 for a person, represents joints number. Finally, NMS is used to remove the superfluous 3D poses. The decoding process is same as previous work <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>During training, we use 1 loss for 3D offsets regression and 2 loss for heatmap learning. Meanwhile, the instance 2D offsets 2 are learned with the supervision of ground-truth instance offset? 2 . The loss function L is</p><formula xml:id="formula_14">L = 1 ( ,^) + 1 ( 2 ,^2 ) + 2 ( ? ,^?),<label>(12)</label></formula><p>where^and^? means the ground-truth of and ? , respectively. represents a loss weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we elaborate the experiment results of IVT. We firstly introduce the implemental details of IVT, and then report results and compare with SOTA methods on three widely-used datasets: Human3.6M, 3DPW, and CMU Panoptic. All ablation studies are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implemental Details</head><p>We use HRNet-32 <ref type="bibr" target="#b40">[41]</ref> pre-trained on 2D pose estimation dataset COCO <ref type="bibr" target="#b23">[24]</ref> as the backbone network of IVT and SPyNet <ref type="bibr" target="#b35">[36]</ref> as the motion estimation network between different frames. In our experiments, IVT is stacked with 3 layers, and it is trained on 8 V100 GPUs with a batch size of 4 sequences/GPU, while the sequence length is 5 frames and the input size is 512 ? 512. The total training epochs is 50. Adam optimizer is adopted and the initial learning rate is 5e-4, which decreases 10? at 30 and 40 epochs. The loss weight equals 10 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets and Metrics</head><p>4.2.1 Human3.6M dataset. Human3.6 <ref type="bibr" target="#b14">[15]</ref> is the largest indoor benchmark for single-person 3D pose estimation, which includes 7 subjects that performing 15 actions. Following the previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>, we use two protocols for evaluation. For Protocol 1, IVT is trained on the subjects S1, S5, S6, S7, and S8, and tested on the subjects S9 and S11 by using Mean-Per-Joint-Position-Error (MPJPE), which measures the Euclidean distances between the ground truth joints and the predicted joints. For Protocol 2, subjects S1, S5, S7, S8, and S9 are used for training, and S11 is used for testing by PA-MPJPE. It calculates the Euclidean distance between predicted and ground-truth 3D joint coordinates after root joint alignment and further rigid alignment by Procrustes analysis <ref type="bibr" target="#b11">[12]</ref>. We also give a fine-grained analysis of videos from the Hu-man3.6M dataset in <ref type="figure" target="#fig_7">Figure 4</ref>. As shown in <ref type="figure" target="#fig_7">Figure 4</ref> (a), given the video inputs, GAST-Net <ref type="bibr" target="#b24">[25]</ref> and PoseFormer <ref type="bibr" target="#b50">[51]</ref> fail on these <ref type="table">Table 2</ref>: Quantitative comparison with state-of-the-art methods on multi-person 3D human pose estimation dataset (3DPW) in PA-MPJPE. Frame denotes the number of input frames used in each method. * denotes transformer-based methods. Lower is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Frames PA-MPJPE ? Doersch et al. <ref type="bibr" target="#b8">[9]</ref> NeurIPS' <ref type="bibr">19 31</ref> 74.4 Kanazawa et al. <ref type="bibr" target="#b16">[17]</ref> CVPR'19 10 72.6 Cheng et al. <ref type="bibr" target="#b5">[6]</ref> AAAI'20 &gt;90 71.8 Sun et al. <ref type="bibr" target="#b37">[38]</ref> ICCV' <ref type="bibr">19 45</ref> 69.5 Kolotouros et al. <ref type="bibr" target="#b18">[19]</ref> ICCV'19 1 59.2 Kocabas et al. <ref type="bibr" target="#b17">[18]</ref> CVPR'20 16 57.6 Luo et al. <ref type="bibr" target="#b27">[28]</ref> ACCV'20 90 54.7 Cheng et al. <ref type="bibr" target="#b4">[5]</ref> CVPR'21 &gt;90 62.9 Choi et al. <ref type="bibr" target="#b6">[7]</ref> CVPR'21 <ref type="bibr" target="#b15">16</ref> 52.7 Ours (IVT)*   <ref type="table">Table 2</ref>. In the multi-person 3DPW dataset, IVT outperforms previous videobased methods and achieves 46.0mm in PA-MPJPE. Compared with METRO <ref type="bibr" target="#b22">[23]</ref>, the relative gain is 6% in PA-MPJPE. These results verify the effectiveness and generalization ability of the proposed IVT since 3DPW is an in-the-wild dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Results on CMU Panoptic</head><p>Dataset. The comparisons with SOTA methods on CMU Panoptic dataset are shown in <ref type="table" target="#tab_3">Table 3</ref>. It shows that, IVT obtains 48.4mm in MPJPE and achieves a new state-of-the-art result with a relative gain of 10% compared with the the DAS <ref type="bibr" target="#b42">[43]</ref>. The results on CMU Panoptic dataset further demonstrate the effectiveness of the proposed IVT framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we verify the effectiveness of the proposed ISA, ITA, and cross-scale attention mechanism in instance-guided video transformer (IVT). Based on IVT, we also study the influence of frame numbers on video transformer. Then, we compare the parameters and computational costs of different types of IVT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Effectiveness of proposed attention mechanisms.</head><p>We conduct the ablation study on CMU Panoptic dataset to verify the effectiveness of proposed modules in the instance-guided video transformer.</p><p>First of all, to verify the different types of attention mechanisms, we build an end-to-end multi-person 3D pose estimation baseline   These results show that the proposed ISA, ITA, and cross-scale attention mechanism are useful for 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.2</head><p>The ablation study on frame number. To explore the influence of frame numbers for IVT, we conduct the ablation study of frame numbers based on IVT with ITA. As shown in <ref type="table" target="#tab_5">Table 5</ref>, the frames number means the used frames, but the truth receptive field on time is ? 5 since the sampling rate of the video is 5 frames. For example, = 5 means we used 5 frames but the interval is 25 from the first frame to the last frame. As shown in <ref type="table" target="#tab_5">Table 5</ref>, given more frames from 1 to 5, the performance of IVT improves to 49.5mm from 50.8mm. But with the frame number increasing to 9, the performance of IVT decreases to 50.00mm. The result shows that long-term frames could damage the performance of IVT since the long-term motion is hard to estimate.  <ref type="table" target="#tab_4">Table 4</ref>. Compared with the baseline with spatial attention, instance-guided attention (ISA) obtains a relative gain of 2.3% by adding 1.92MB parameters, while the flops of ISA decrease since the instance-aware tokens are based on image blocks. Even for the IVT with CISA and ITA, the increasing parameters and the computational costs are acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we propose a novel end-to-end instance-guided video transformer (IVT) for video 3D human pose estimation to capture global depth context. To capture the depth context in both spatial and temporal dimensions, we propose instance-guided spatial attention (ISA) and instance-guided temporal attention (ITA) mechanisms. To further tackle the variational human scales in video, we propose cross-scale attention for IVT. Combined with ISA, ITA, and cross-scale attention, IVT outperforms state-of-the-art methods on three widely-used 3D human pose estimation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENT</head><p>This work was supported by the Scientific and Technological Innovation of Shunde Graduate School of University of Science and Technology Beijing (No. BK20AE004 and No.BK19CE017).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The illustration of the cross-scale instance-guided attention for IVT. 1 , 2 , and 3 represents three different scales for cross-scale attention. IGT means instance-guided tokenization. The projection layer is a Linear layer to project visual tokens from different scales into the same size. Backprojection is the inverse operation of the projection layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5 46. 0</head><label>0</label><figDesc>hard cases with complex postures or occlusions, but our IVT performs well on these cases since the captured temporal depth context. The red circle denotes the wrong pose predicted by GAST-Net and PoseFormer.As shown inFigure 4(b) and (c), the MPJPE of IVT is lower than GAST-Net and PoseFormer, and the depth error of IVT is lower than GAST-Net and PoseFormer. It shows that the improvements of IVT mainly benefit from better depth prediction. Combined withFigure 4(a), (b), and (c), we can found that GAST-Net, PoseFormer, and IVT have similar results on frame 235. The MPJPE and depth error show the similar results at frame 235 since the human poses in this period are clear.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 3 . 2</head><label>32</label><figDesc>Results on 3DPW Dataset. The comparisons with state-ofthe-art methods on the 3DPW dataset are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison between IVT and the SOTA video methods (GAST-Net [25] and PoseFormer [51]) on video from Human3.6M dataset. (a) visualization results (Red circle denotes the wrong prediction). (b) The curve of MPJPE-Frame shows the MPJPE on each frame. (c) The curve of Depth Error-Frame shows the MPJPE in depth dimension on each frame. The depth prediction by IVT is better than GAST-Net and PoseFormer. Best viewed in color.4.4.3 Parameters and computational costs. The comparisons of different attention mechanisms on parameters and computational costs are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison with state-of-the-art methods on Human3.6M under Protocol 1 (MPJPE) and Protocol 2 (PA-MPJPE). denotes the number of input frames used in each method, and * represents a Transformer-based model. Bold indicates the best and underline indicates the second best. Dir. Disc. Eat. Greet Phone Photo Pose Purch. Sit SitD. Somke Wait WalkD. Walk WalkT. Avg. Dabral et al. [8] (f=243) ECCV'18 44.8 50.4 44.7 49.0 52.9 61.4 43.5 45.5 63.1 87.3 51.7 48.5 52.2 37.6 41.9 52.1 Cai et al. [3] ( = 7) ICCV'19 44.6 47.4 45.6 48.8 50.8 59.0 47.2 43.9 57.9 61.9 49.7 46.6 51.3 37.1 39.4 48.8 Pavllo et al. [33] ( = 243) CVPR'19 45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8 Lin et al. [22] ( = 50) BMVC'19 42.5 44.8 42.6 44.2 48.5 57.1 52.6 41.4 56.5 64.5 47.4 43.0 48.1 33.0 35.1 46.6 Yeh et al. [46] ( = 243) NeurIPS'19 44.8 46.1 43.3 46.4 49.0 55.2 44.6 44.0 58.3 62.7 47.1 43.9 48.6 32.7 33.3 46.7 Liu et al. [26] ( = 243) CVPR'20 41.8 44.8 41.1 44.9 47.4 54.1 43.4 42.2 56.2 63.6 45.3 43.5 45.3 31.3 32.2 45.1 Zeng et al. [49] ( = 243) ECCV'20 46.6 47.1 43.9 41.6 45.8 49.6 46.5 40.0 53.4 61.1 46.1 42.6 43.1 31.5 32.6 44.8 Wang et al. Dir. Disc. Eat. Greet Phone Photo Pose Purch. Sit SitD. Somke Wait WalkD. Walk WalkT. Avg. Hossain et al. [13] ( = 243) ECCV'18 35.7 39.3 44.6 43.0 47.2 54.0 38.3 37.5 51.6 61.3 46.5 41.4 47.3 34.2 39.4 44.1 Chen et al. [4] ( = 81) TCSVT'21 33.1 35.3 33.4 35.9 36.1 41.7 32.8 33.3 42.6 49.4 37.0 32.7 36.5 25.5 27.9 35.6 Liu et al. [26] ( = 243) * ICRA'21 32.7 36.2 33.4 36.5 36.0 41.5 33.6 33.1 44.1 46.8 36.7 33.1 35.8 24.2 24.8 35.2 Zheng et al. [51] ( = 81) * ICCV'21 32.5 34.8 32.6 34.6 35.3 39.5 32.1 32.0 42.8 48.5 34.8 32.4 35.3 24.5 26.0 34.6 Ours (IVT) ( = 5) * 27.0 24.8 32.2 30.1 27.8 32.1 22.3 28.7 30.7 24.4 32.7 37.8 21.9 31.1 24.7 28.5based on CMU Panoptic dataset. Meanwhile, some visualization results on Human3.6M are given for presenting the superiority of IVT in an intuitionistic way.</figDesc><table><row><cell>Protocol 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[42] ( = 96)</cell><cell cols="17">ECCV'20 41.3 43.9 44.0 42.2 48.0 57.1 42.2 43.2 57.3 61.3 47.0 43.5 47.0 32.6 31.8 45.6</cell></row><row><cell>Chen et al. [4] ( = 81)</cell><cell cols="17">TCSVT'21 42.1 43.8 41.0 43.8 46.1 53.5 42.4 43.1 53.9 60.5 45.7 42.1 46.2 32.2 33.8 44.6</cell></row><row><cell>Lin et al. [23] ( = 1)  *</cell><cell>CVPR'21</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.0</cell></row><row><cell>Liu et al. [26] ( = 243)  *</cell><cell cols="17">ICRA'21 43.3 46.1 40.9 44.6 46.6 54.0 44.1 42.9 55.3 57.9 45.8 43.4 47.3 30.4 30.3 44.9</cell></row><row><cell>Zheng et al. [51] ( = 81)  *</cell><cell cols="17">ICCV'21 41.5 44.8 39.8 42.5 46.5 51.6 42.1 42.0 53.3 60.7 45.5 43.3 46.1 31.8 32.2 44.3</cell></row><row><cell>Ours (IVT) ( = 5)  *</cell><cell></cell><cell cols="16">36.5 40.1 38.4 40.7 42.6 42.8 30.1 43.4 46.1 58.0 40.2 37.1 40.8 32.1 33.5 40.2</cell></row><row><cell>Protocol 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cai et al. [3] ( = 7)</cell><cell cols="17">ICCV'19 35.7 37.8 36.9 40.7 39.6 45.2 37.4 34.5 46.9 50.1 40.5 36.1 41.0 29.6 32.3 39.0</cell></row><row><cell>Lin et al. [22] ( = 50)</cell><cell cols="17">BMVC'19 32.5 35.3 34.3 36.2 37.8 43.0 33.0 32.2 45.7 51.8 38.4 32.8 37.5 25.8 28.9 36.8</cell></row><row><cell>Pavllo et al. [33] ( = 243)</cell><cell cols="17">CVPR'19 34.1 36.1 34.4 37.2 36.4 42.2 34.4 33.6 45.0 52.5 37.4 33.8 37.8 25.6 27.3 36.5</cell></row><row><cell>Liu et al. [26] ( = 243)</cell><cell cols="17">CVPR'20 32.3 35.2 33.3 35.8 35.9 41.5 33.2 32.7 44.6 50.9 37.0 32.5 37.0 25.2 27.2 35.6</cell></row><row><cell>Wang et al. [42] ( = 96)</cell><cell cols="17">ECCV'20 32.9 35.2 35.6 34.4 36.4 42.7 31.2 32.5 45.6 50.2 37.3 32.8 36.3 26.0 23.9 35.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Results on Human3.6M Dataset. The comparisons with stateof-the-art methods on the Human3.6M dataset are shown inTable 1. Our IVT with = 5 achieves new state-of-the-art results with an MPJPE of 41.3mm and a PA-MPJPE of 28.5mm in Protocol 1 and Protocol 2, respectively. The relative gains are 6.8% and 17.6%, respectively. The results demonstrate the effectiveness of the proposed IVT. Compared with other transformer-based methods<ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51]</ref>, IVT outperforms them. Even the PoseFormer<ref type="bibr" target="#b50">[51]</ref> is based on a frame number 81, IVT with = 5 obtains better results since the PoseFormer loses the visual depth feature in the process of temporal modeling.</figDesc><table><row><cell>4.2.2 3DPW dataset. 3DPW [39] is a multi-person outdoor 3D pose</cell></row><row><cell>estimation dataset, which contains 22K images for training and 35K</cell></row><row><cell>images for testing. Following the previous works [7, 18, 23], we</cell></row><row><cell>train IVT on the training set and evaluate IVT on the testing set in</cell></row><row><cell>PA-MPJPE.</cell></row><row><cell>4.2.3 CMU Panoptic dataset. CMU Panoptic [16] is a larger-scale</cell></row><row><cell>multi-person dataset, captured by multiple cameras. Following the</cell></row><row><cell>settings of previous works [40, 50], we use 160K images from differ-</cell></row><row><cell>ent videos as the training set and the videos from two cameras (16,</cell></row><row><cell>30) as the testing set. For comparison, MPJPE is used for evaluation.</cell></row><row><cell>4.3 Comparison with SOTA Methods</cell></row><row><cell>4.3.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with SOTA methods on multi-person 3D human pose estimation dataset (CMU Panoptic) in MPJPE. ? means an extra refining network is used. Lower is better.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>MPJPE (mm) ?</cell></row><row><cell>SFB [47]</cell><cell>CVPR'18</cell><cell>153.4</cell></row><row><cell>PoseNet [30]</cell><cell>ICCV'19</cell><cell>87.6</cell></row><row><cell cols="2">MubyNet [48] NeurIPS'18</cell><cell>78.1</cell></row><row><cell>SMAP [50]</cell><cell>ECCV'20</cell><cell>73.1</cell></row><row><cell>LoCO [11]</cell><cell>CVPR'20</cell><cell>69.0</cell></row><row><cell>SMAP  ? [50]</cell><cell>ECCV'20</cell><cell>61.8</cell></row><row><cell>DAS [43]</cell><cell>CVPR'22</cell><cell>53.8</cell></row><row><cell>Ours(IVT)</cell><cell></cell><cell>48.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of IVT on CMU Panoptic. SA means traditional spatial attention. ISA means using instanceguided spatial attention in IVT. ITA means using instanceguided temporal attention in IVT. CISA represents crossscale ISA. MITA means multi-scale ITA. Flops are computed on two input images with a size of 512 ? 512.</figDesc><table><row><cell cols="2">Methods Feature</cell><cell cols="3">Params (M) Flops (T) MPJPE (mm)?</cell><cell>?</cell></row><row><cell cols="2">Baseline + SA</cell><cell>32.96</cell><cell>0.127</cell><cell>52.0</cell><cell>-</cell></row><row><cell>IVT</cell><cell>+ ISA</cell><cell>34.88</cell><cell>0.123</cell><cell>50.8</cell><cell>? 2.3%</cell></row><row><cell>IVT</cell><cell>+ ISA + ITA</cell><cell>35.81</cell><cell>0.126</cell><cell>49.5</cell><cell>? 2.6%</cell></row><row><cell>IVT</cell><cell>+ CISA + MITA</cell><cell>40.85</cell><cell>0.134</cell><cell>48.4</cell><cell>? 2.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of IVT (ISA-ITA) on frame numbers on CMU Panoptic dataset. Note that each video is sampled with a sampling rate of 5 frames. Thus, the temporal receptive field is = ? 5, where means the used frame number.Table 4, SA achieves 52.0mm in MPJPE. Combined with the instance-guided attention, IVT with ISA obtains 50.8mm in MPJPE and achieves a relative gain of 2.3%. Compared with only using ISA, the ITA obtains 49.5mm in MPJPE and achieves a relative gain of 2.6%. Besides, the cross-scale attention mechanism brings a relative gain of 2.3% and achieves 48.4mm in MPJPE.</figDesc><table><row><cell>Frames</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell></row><row><cell>MPJPE (mm)</cell><cell>51.8</cell><cell>50.3</cell><cell>49.5</cell><cell>49.5</cell><cell>50.0</cell></row><row><cell cols="6">with traditional simple spatial attention as [2, 10], noted as SA.</cell></row><row><cell>As shown in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Is Space-Time Attention All You Need for Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="813" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anatomy-aware 3d human pose estimation with bone-based pose decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="198" to="209" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular 3D multiperson pose estimation by integrating top-down and bottom-up networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7649" to="7659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">2020. 3d human pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10631" to="10638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond static features for temporally consistent 3d human pose and shape from a video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1964" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compressed volumetric heatmaps for multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Alletto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7204" to="7213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized procrustes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="33" to="51" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unifying multimodal transformer for bi-directional image and text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM22</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1138" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="190" to="204" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pose recognition with cascade transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1944" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10166" to="10175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Trajectory space factorization for deep video-based 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A graph attention spatio-temporal convolutional network for 3D human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifei</forename><surname>Zhu</surname></persName>
		</author>
		<editor>ICRA. IEEE</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3374" to="3380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sen-ching Cheung, and Vijayan Asari. 2020. Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2020. 3d human motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weian</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15320</idno>
		<title level="m">Tfpose: Direct human pose estimation with transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Camera distanceaware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6951" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">UNOC: Understanding occlusion for embodied presence in virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Parger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengcheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">David</forename><surname>Twigg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Steinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning recurrent structure-guided attention network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME. IEEE</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="418" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dgcn: Dynamic graph convolutional network for efficient multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11924" to="11931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11179" to="11188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hmor: Hierarchical multi-person ordinal relations for monocular multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="242" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="764" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transpose: Keypoint localization via transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11802" to="11812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chirality nets for human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Alin-Ionut Popa, and Cristian Sminchisescu</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Smap: Single-shot multi-person absolute 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="550" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Chen Chen, and Zhengming Ding. 2021. 3d human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="11656" to="11665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
