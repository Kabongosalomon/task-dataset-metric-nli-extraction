<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universal Lesion Detection by Learning from Multiple Heterogeneously Labeled Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">PAII Inc</orgName>
								<address>
									<postCode>20817</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinzheng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">PAII Inc</orgName>
								<address>
									<postCode>20817</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">PAII Inc</orgName>
								<address>
									<postCode>20817</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakai</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">PAII Inc</orgName>
								<address>
									<postCode>20817</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">PAII Inc</orgName>
								<address>
									<postCode>20817</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">PAII Inc</orgName>
								<address>
									<postCode>20817</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Universal Lesion Detection by Learning from Multiple Heterogeneously Labeled Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Universal lesion detection</term>
					<term>Incomplete labels</term>
					<term>Heterogeneously labeled datasets</term>
					<term>Multi-task learning</term>
					<term>Embedding matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lesion detection is an important problem within medical imaging analysis. Most previous work focuses on detecting and segmenting a specialized category of lesions (e.g., lung nodules). However, in clinical practice, radiologists are responsible for finding all possible types of anomalies. The task of universal lesion detection (ULD) was proposed to address this challenge by detecting a large variety of lesions from the whole body. There are multiple heterogeneously labeled datasets with varying label completeness: DeepLesion, the largest dataset of 32,735 annotated lesions of various types, but with even more missing annotation instances; and several fully-labeled single-type lesion datasets, such as LUNA for lung nodules and LiTS for liver tumors. In this work, we propose a novel framework to leverage all these datasets together to improve the performance of ULD. First, we learn a multi-head multi-task lesion detector using all datasets and generate lesion proposals on DeepLesion. Second, missing annotations in DeepLesion are retrieved by a new method of embedding matching that exploits clinical prior knowledge. Last, we discover suspicious but unannotated lesions using knowledge transfer from single-type lesion detectors. In this way, reliable positive and negative regions are obtained from partially-labeled and unlabeled images, which are effectively utilized to train ULD. To assess the clinically realistic protocol of 3D volumetric ULD, we fully annotated 1071 CT sub-volumes in DeepLesion. Our method outperforms the current state-of-the-art approach by 29% in the metric of average sensitivity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At the core of oncology imaging for diagnosis of potential cancers, radiologists are responsible to find and report all possible abnormal findings (e.g., tumors, lymph nodes, and other lesions). It is not only time-consuming to scan through a 3D medical image, human readers may also miss some abnormal findings. This spurs research on automated lesion detection to decrease reading time and improve accuracy <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">32]</ref>. Existing work commonly focus on lesions of specific types and organs. For example, lung nodules <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref>, liver tumors <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3]</ref>, and lymph nodes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">34]</ref> have been extensively studied. However, in clinical scenarios, a CT scan may contain multiple types of lesions in different organs.</p><p>For instance, metastasis (e.g., lung cancer) can spread to regional lymph nodes and other body parts (e.g., liver, bone, adrenal, etc.). To help radiologists find all of them, a universal lesion detection (ULD) algorithm, which can identify a variety of lesions in the whole body, is ideal. Designing a model for each organ / lesion type is inefficient and less scalable, significantly increasing inference time and model size. For rare lesion types with fewer training data, single-type models have higher risks of overfitting. More importantly, given the wide range of lesion types, a group of single-type models will still miss some infrequent types. Hence, a ULD system that covers all kinds of lesions is of great clinical value, approaching to address radiologists' daily workflows and real needs.</p><p>To learn an effective ULD system, a comprehensive and diverse dataset of lesion images is required. The conventional data curation paradigm demands experienced radiologists to relabel all lesions, thus is difficult to acquire. Most manually-labeled lesion datasets <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref> are relatively small (?1K lesions) and contain specific single lesion types. To tackle this problem, the DeepLesion dataset <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b45">45]</ref> was collected by mining lesions directly from the picture archiving and communication system (PACS), which stores the RECIST <ref type="bibr" target="#b10">[11]</ref> markers already annotated by radiologists during their daily work. DeepLesion includes over 32K lesions on various body parts in computed tomography (CT) scans. The ULD task accuracy has been constantly improving <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44]</ref> upon the release of dataset <ref type="bibr" target="#b45">[45]</ref>. Along with its large scale and ease of collection, DeepLesion also has a limitation: not all lesions in every image were annotated. This is because radiologists generally mark only representative lesions in each scan <ref type="bibr" target="#b10">[11]</ref> in their routine work. This missing annotation or incomplete label problem can also be found in other object detection datasets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">40]</ref>, which will cause incorrect training signals (some negative proposals are actually positive), resulting in lower detection accuracy. In medical images, the appearance of lesions and non-lesions can be quite similar, making it difficult to mine missing annotations and ensure that they are true lesions.</p><p>Several public, fully-labeled, and single-type lesion datasets <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref> exist and provide annotations of specific lesion types. For example, the LiTS dataset <ref type="bibr" target="#b3">[4]</ref> contains primary and secondary liver tumors in 201 CT scans. While DeepLesion <ref type="bibr" target="#b45">[45]</ref> is large, universal, but partially-labeled, these datasets <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref> are small, specialized, but fully-labeled. This heterogeneity of dataset labels poses challenges in leveraging multi-source datasets in ULD.</p><p>In this paper, we propose to alleviate the missing annotation problem and leverage multiple datasets to improve ULD accuracy. Our framework is shown in <ref type="figure">Fig. 1</ref>. First, we design a lesion detector with several head branches to focus on lesions of different organs. It is trained on multiple lesion datasets in a multi-task fashion, which can handle the heterogeneous label problem. Given a test image, it can predict several groups of lesion proposals matching the semantics of each dataset in <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref>. It is named "multi-expert lesion detector" (MELD) since each group of proposal is predicted by an "expert" learned from one dataset. Then, we employ MELD on the partially-labeled and unlabeled slices in DeepLesion to generate lesion proposals. The key components of our framework are two novel algorithms to mine missing positive and reliable negative regions. In the missing annotation matching (MAM) algorithm, a lesion embedding <ref type="bibr" target="#b43">[43]</ref> is extracted for each proposal which describes its body part, type, and attributes. By comparing the embedding of each proposal and each annotated lesion within each patient's multiple CT scans, we can find unannotated lesions that are similar to existing annotations. In the negative region mining (NRM) algorithm, we obtain suspicious lesions from the proposals of the single-type experts in MELD. Because these proposals can be noisy, we do not treat them as lesions, but consider the rest part of the image as reliable negative region without lesions. These positive and negative regions are then used to finetune MELD. We employ three single-type datasets in our framework, namely LUNA (LUng Nodule Analysis) <ref type="bibr" target="#b33">[33]</ref>, LiTS (Liver Tumor Segmentation Benchmark) <ref type="bibr" target="#b3">[4]</ref>, and NIH-LN (NIH Lymph Node) <ref type="bibr" target="#b0">[1]</ref>. Notice that it is not our goal to achieve new state-of-the-art results on these specialized datasets. <ref type="figure" target="#fig_0">Fig. 2</ref> exhibits exemplar lesions from the four datasets. In our experiments, 27K missing annotations and 150K suspicious lesions were found in 233K partially-labeled and unlabeled slices. For evaluation, we manually annotated all lesions in 1K sub-volumes in DeepLesion as the test set 1 . The original test set in DeepLesion was annotated on selected key slices. It is different from clinical practice where 3D volumetric data are used. Besides, the key-slice test set was not fully annotated, leading to inaccurate performance evaluation. In the fullylabeled volumetric test set, our method outperforms the current state-of-the-art method on DeepLesion by 29% (average sensitivity from 32.4% to 41.8%). , and NIH-LN <ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref>. DeepLesion has an overlap with the single-type datasets (first row 1-3), but it also includes many clinically significant lesion types that are not covered by other datasets (first row 4-9), demonstrating the value a ULD system.</p><p>The main contributions of this paper are fourfold. 1) The heterogeneous dataset fusion problem in lesion detection are tackled for the first time via our simple yet effective MELD network; 2) We propose two novel methods, i.e. missing annotation matching (MAM) and negative region mining (NRM), to alleviate the missing annotation problem, enabling us to leverage partially-labeled and unlabeled images in training successfully; 3) MELD and NRM can transfer meaningful knowledge from single-type datasets to universal lesion detection models; and 4) ULD accuracy on DeepLesion <ref type="bibr" target="#b45">[45]</ref> is significantly improved upon previous state-of-the-art work <ref type="bibr" target="#b44">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Universal lesion detection: ULD has been improved using 3D context <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b39">39]</ref>, attention mechanism <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">39]</ref>, multi-task learning <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b38">38]</ref>, and hard negative mining <ref type="bibr" target="#b37">[37]</ref>. 3D context information in neighboring slices is important for detection, as lesions may be less distinguishable in just one 2D axial slice. Volumetric attention <ref type="bibr" target="#b39">[39]</ref> exploited 3D information with multi-slice image inputs and a 2.5D network and obtained top results on the LiTS dataset. In <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">39]</ref>, attention mechanisms were applied to emphasize important regions and channels in feature maps. MVP-Net <ref type="bibr" target="#b18">[19]</ref> learned to encode position (body part) information in an attention module. The multi-task universal lesion analysis network (MULAN) <ref type="bibr" target="#b44">[44]</ref> achieved the state-of-the-art accuracy on DeepLesion with a 3D feature fusion strategy and the Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> architecture. It jointly learned lesion detection, segmentation, and tagging with a proposed score refinement layer to improve detection with 171 lesion tags. However, it did not handle the missing annotations and the variation of lesions in different organs, neither did it leverage multiple datasets. ULDor <ref type="bibr" target="#b37">[37]</ref> mined hard negative proposals with a trained detector to retrain the model, but the mined negatives may actually contain positives because of missing annotations. Inspired by <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b39">39]</ref>, we build MELD based on 2.5D Mask R-CNN.</p><p>Learning with incomplete labels: In detection, knowledge distillation <ref type="bibr" target="#b13">[14]</ref> can help to find missing annotations. The basic idea is to use the prediction of one model to train another. Predictions from multiple transformations of unlabeled data were merged to generate new training annotations in <ref type="bibr" target="#b29">[29]</ref>. Dong et al. <ref type="bibr" target="#b8">[9]</ref> progressively generated pseudo-boxes from old models to train new ones. Prior knowledge can also help to infer missing annotations. Wu et al. <ref type="bibr" target="#b40">[40]</ref> argued that a proposal with a small overlap with an existing box is less likely to be a missing annotation. Niitani et al. <ref type="bibr" target="#b26">[27]</ref> introduced part-aware sampling that assumes an object (car) must contain its parts (tire). Jin et al. <ref type="bibr" target="#b15">[16]</ref> mined hard negative and positive proposals from unlabeled videos based on the prior that object proposals should be continuous across frames. In our framework, we leverage embedding matching and knowledge from multiple specialized datasets to find missing annotations and reliable negative regions.</p><p>Multi-task and multi-dataset learning: Our problem is related to multitask learning <ref type="bibr" target="#b27">[28]</ref> where different tasks are learned jointly, which has been proved beneficial in medical imaging <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">44]</ref>. Because of the difficulty in data annotation of medical images, it is sometimes required to learn from multiple datasets labeled by different institutes using varying criterion <ref type="bibr" target="#b36">[36]</ref>. Zhou et al. <ref type="bibr" target="#b47">[47]</ref> and Dmitriev et al. <ref type="bibr" target="#b7">[8]</ref> studied how to learn multi-organ segmentation from singleorgan datasets, incorporating priors on organ sizes and dataset-conditioned features, respectively. Cohen et al. <ref type="bibr" target="#b5">[6]</ref> observed that the same class label had different distribution (concept shift) between multiple chest X-ray datasets and simply training with all datasets is not optimal. The relation and corporation of multiple datasets for lesion detection has not been inspected. The domain-attentive universal detector <ref type="bibr" target="#b38">[38]</ref> used a domain attention module to learn DeepLesion as well as 10 other object detection datasets. Yet, it did not exploit the semantic relation between datasets. Our framework leverages the synergy of lesion datasets both to learn better features and to use their semantic overlaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>As shown in <ref type="figure">Fig. 1</ref>, we first train a multi-expert lesion detector (MELD) to generate proposals, and then perform missing annotation matching (MAM) and negative region mining (NRM) to find missing positive and reliable negative regions to finetune MELD. Knowledge from single-type datasets is transferred to the universal detector in two ways. First, single-type datasets are jointly trained in MELD to help it learn better feature representation for ULD; Second, in NRM, the single-type experts of MELD help to mine suspicious lesions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Expert Lesion Detector (MELD) for Heterogeneously Labeled Datasets</head><p>We propose MELD based on an improved Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> architecture. Its overall framework is displayed in <ref type="figure">Fig. 3</ref>. The input of the network is 9 consecutive axial CT slices and the outputs are the detected 2D lesion proposals in the central slice and their segmentation masks. In lesion detection, 3D context in neighboring slices is important, so we use a 2.5D DenseNet backbone plus the feature pyramid network (FPN) <ref type="bibr" target="#b20">[21]</ref> similar to <ref type="bibr" target="#b44">[44]</ref>. From the backbone feature, a   <ref type="figure">Fig. 3</ref>. Framework of the multi-expert lesion detector (MELD). MELD jointly learns multiple lesion datasets in a multi-task fashion. Each dataset d has its own RPN, detection scores, and bounding-box regression head. MELD also has several classification heads, each focusing on lesions in different organs.</p><p>region proposal network (RPN) <ref type="bibr" target="#b30">[30]</ref> predicts initial lesion proposals and forwards them to the classification, bounding-box regression, and mask heads <ref type="bibr" target="#b12">[13]</ref>.</p><p>In ULD, one challenge is that lesions in different organs have very distinct appearances, while lesions and non-lesions in the same organ can look similar. We propose a divide-and-conquer strategy to alleviate this problem. Existing ULD algorithms treat all kinds of lesions as one class and use a binary classifier to distinguish them from non-lesions. In contrast, MELD has multiple classification heads focusing on lesions of different organs. Lymph node, lung, and liver are the most common organs in DeepLesion <ref type="bibr" target="#b43">[43]</ref>, so we build three heads for these three organs and another whole-body head which learns all lesions. When training, every proposal goes through all heads to obtain detection scores s 0 , . . . , s 3 and cross-entropy losses L 0 , . . . , L 3 . At the same time, we learn a gating head to predict organ weights w i ? [0, 1], i = 0, . . . , 3, representing how much the proposal belongs to organ i. w 0 should be always 1 as it corresponds to the whole-body head. The overall loss is L = ? 3 i=0 w i L i . When testing, the detection scores are fused by the organ weights and then normalized, i.e.,</p><formula xml:id="formula_1">s = ? 3 i=0 w i s i / ? 3 i=0 w i .<label>(1)</label></formula><p>To train the gating head, we take advantage of the lesion annotation network (LesaNet) <ref type="bibr" target="#b43">[43]</ref>. LesaNet was trained on labels mined from radiological reports of DeepLesion. We use it to predict the organ of lesions in DeepLesion, then adopt the predicted scores (0 ? 1) as soft targets to supervise the gating head. This organ stratification strategy allows each classification head to learn organ-specific parameters to model the subtle difference between lesions and non-lesions of the organ. We find it improves the ULD accuracy. Next, we further extend the above network to jointly learn multiple datasets. In our problem, the datasets are heterogeneously labeled. The definition of lesion in different datasets is overlapping but not identical. Single-type datasets lack annotations of other types. For instance, enlarged lymph nodes often exist but were not annotated in LUNA and LiTS. In addition, since the datasets'  <ref type="figure">Fig. 4</ref>. Examples of matched lesions in DeepLesion. In each sub-plot, the lesion on the left is an existing annotation; the right one is a matched missing annotation in another study/series/slice of the same patient. Their embedding distance is also shown.</p><p>patient population and collection criteria are all different, there exists a concept shift <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref>. For example, the distribution of liver tumors in DeepLesion and LiTS may slightly vary. This issue was also found in multiple Chest X-ray datasets <ref type="bibr" target="#b5">[6]</ref>. Therefore, combining them is not straightforward and it is better to treat different datasets as different learning tasks.</p><p>In MELD, we make the datasets share the same network backbone and fully connected layers in the classification heads. Each head splits in the last layer and outputs 4 detection scores to match each dataset's semantics. When a training sample comes from dataset d, only the d'th detection score in each classification head will be learned. Additionally, each dataset has its own RPN and boundingbox regression layer <ref type="bibr" target="#b38">[38]</ref>, see <ref type="figure">Fig. 3</ref>. Wang et al. <ref type="bibr" target="#b38">[38]</ref> introduced a domain attention module to learn features from different image domains. Our datasets are from the same image domain (CT scans), so it is feasible to simply make them share features. Experiments show that this multi-task strategy improves the accuracy on all datasets. Given a test image, MELD can efficiently predict several groups of lesion proposals matching the semantics of each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Missing Annotation Matching (MAM)</head><p>In clinical practice, each patient generally undergo multiple CT scans (studies) at different time points to monitor their disease progress <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">45]</ref>. Each study typically contains multiple image volumes (series) that are scanned at the same time point but differ in reconstruction filters, contrast phases, etc. One lesion instance exists across multiple studies and series, but radiologists often do not mark them all in their daily work <ref type="bibr" target="#b46">[46]</ref>. Besides, a large lesion spans in multiple slices in a volume, but radiologists generally mark it only on the slice where it has the largest cross-sectional size <ref type="bibr" target="#b10">[11]</ref>, known as the key slice. These clinical prior knowledge gives us a chance to find those missing annotations that belong to the same lesion instance with existing annotations but were not marked by radiologists.</p><p>First, we train MELD using the existing annotations on key slices in the training set of DeepLesion. Then, we apply the network on all slices in the training set. After sampling a slice every 5mm, we obtained 1,429K proposals from 233K partially-labeled and unlabeled slices, a large extension compared to the 22K key slices. The next step is to establish correspondence between the proposals and existing annotations. We leverage the lesion embedding generated by LesaNet <ref type="bibr" target="#b43">[43]</ref>, which encodes the body part, type, and attributes of lesions and have proved its efficacy in lesion retrieval. The distance of two embeddings should be small if they are from the same lesion instance. Hence, within each patient, we compute the L2 distance between every annotation and every proposal and keep those pairs whose distance is smaller than a threshold ?. <ref type="figure">Fig. 4</ref> illustrates three pairs of matched lesions. We found the mined lesions are mostly the same instances with existing ones, but sometimes they are actually different instances with similar semantic attributes, e.g., two liver metastatic tumors. Note that the mined lesions have similar but not identical appearance with existing ones, since they have different time point, reconstruction kernel, slice position, etc., see <ref type="figure">Fig. 4</ref>. Therefore, the mined ones can still provide valuable new information when they are used in training. They are only used in training the classification heads but not the bounding-box regression head, since they are detected proposals and the box may be inaccurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Negative Region Mining (NRM)</head><p>Besides positive samples, negative samples are also important when training a detector. They are sampled from the background region of training images. If the background region contains missing annotations, the algorithm will learn from wrong supervision signals and degrade in accuracy <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">40]</ref>. In our problem, the MAM strategy cannot find all missing annotations because some of them do not belong to the same instance with any existing annotation. One idea is to treat all universal proposals of MELD as missing annotations. However, there may actually be many false positives (FPs) in the proposals.</p><p>Our solution is to explore the semantic overlap between datasets and seek help from the single-type datasets. Recall that MELD is an ensemble of four dataset experts, namely the DeepLesion expert and three single-type experts: LUNA, LiTS, and NIH-LN. For each slice in the training set of DeepLesion, it can output four groups of proposals. Compared to the universal proposals from the DeepLesion expert, the single-type proposals generally have fewer FPs in their specialties. This is because each single-type expert only needs to learn to detect a single lesion type, which is a much simpler task. Also, their training datasets are fully-labeled. For each proposal from the three single-type experts, if its detection score is higher than a threshold ? and it does not overlap with existing or mined annotations, we regard the proposal as a suspicious lesion. Then, we can either treat the suspicious lesions as positive samples or ignore them (do not sample them as either positive or negative) during finetuning. It is found that ignoring them achieved better accuracy, which prevents the FPs in these suspicious proposals from polluting the positive sample set. We also note that the suspicious lesions only include lung nodules, liver tumors, and LNs due to the single-type datasets used. Adding more single-type datasets will help to mine more suspicious lesions.</p><p>Apart from the existing annotations, the mined missing annotations, and the suspicious lesions, the rest part of an image in DeepLesion is treated as reliable negative region, as depicted in <ref type="figure">Fig. 1</ref>. Previous ULD algorithms <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">44]</ref> were all limited to the 22K labeled training slices. It will bias the algorithms toward lesion-rich body parts and cause many FPs in under-represented body parts. With MAM and NRM, we can exploit the massive unlabeled slices and improve performance on the whole body. We anticipate the proposed methods to also be useful in other large-scale but partially-labeled datasets such as OpenImage <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>, where the missing annotations may be mined using embedding-based matching and with the help of other specialized object datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>To date, DeepLesion <ref type="bibr" target="#b45">[45]</ref> is the largest dataset for universal lesion detection, containing 32,735 lesions annotated on 32,120 axial CT slices from 10,594 studies of 4,427 patients. It was mined from the National Institutes of Health Clinical Center based on marks annotated by radiologists during their routine work to measure significant image findings <ref type="bibr" target="#b10">[11]</ref>. Thus, it closely reflects clinical needs. The LUNA (LUng Nodule Analysis) dataset <ref type="bibr" target="#b33">[33]</ref> consists of 1,186 lung nodules annotated in 888 CT scans. LiTS (LIver Tumor Segmentation Benchmark) <ref type="bibr" target="#b3">[4]</ref> includes 201 CT scans with 0 to 75 liver tumors annotated per scan. We used 131 scans of them with released annotations. NIH-Lymph Node (NIH-LN) <ref type="bibr" target="#b0">[1]</ref> contains 388 mediastinal LNs in 90 CT scans and 595 abdominal LNs in 86 scans. Without loss of generality, we chose these three single-type datasets for joint learning with DeepLesion. Single-type datasets of other organs can be added in the future. More dataset details will be described in the supplementary material.</p><p>For DeepLesion, we used the official data split which has 70%, 15%, 15% for training, validation, and test, respectively. The official test set includes only key slices and may contain missing annotations, which will bias the accuracy. We invited a board-certified radiologist to further fully annotate 1071 sub-volumes chosen from the test set of DeepLesion using the same RECIST criterion <ref type="bibr" target="#b10">[11]</ref> as in DeepLesion. We call the official test set "key-slice test set" and the new one "volumetric test set". In the latter set, there are 1,642 original annotations and 2,023 manually added ones. For LUNA, LiTS, and NIH-LN, we randomly used 80% of each dataset for joint training with DeepLesion, and left 20% for validation. Image preprocessing and data augmentation steps are the same with <ref type="bibr" target="#b44">[44]</ref>, which will be described in detail in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>The proposed framework was implemented in PyTorch based on the maskrcnnbenchmark project <ref type="bibr" target="#b25">[26]</ref>. The backbone of MELD is DenseNet-121 <ref type="bibr" target="#b14">[15]</ref> initialized with an ImageNet pretrained model. The gating head has two FC-512 (fullyconnected layers with 512 neurons), one FC-3 (3 organs), and a sigmoid function.</p><p>A classification head consists of two FC-1024 followed by an FC-4 (4 datasets). We use lung, liver, and LN as organ experts since they are the most common organs in DeepLesion <ref type="bibr" target="#b43">[43]</ref>. These layers were randomly initialized. Each minibatch had 4 samples, where each sample consisted of 9 axial CT slices for 3D feature fusion <ref type="bibr" target="#b44">[44]</ref>. We used Rectified Adam (RAdam) <ref type="bibr" target="#b23">[24]</ref> to train MELD for 8 epochs and set the base learning rate to 0.0001, then reduced it by a factor of 10 after the 4th and 6th epochs. For single-type datasets, we used all slices that contain lesions and the same number of randomly sampled negative slices (without lesions) to train in each epoch. It took MELD 35ms to process a slice during inference on a Quadro RTX 6000 GPU.</p><p>For MAM, we empirically set the distance threshold ? = 0.15. 27K missing annotations were mined from the training set of DeepLesion, in addition to the 23K existing annotations. We randomly checked 100 of them and found 90% are true lesions. For NRM, we set the detection score threshold ? = 0.5. An average of 0.45 suspicious lesions were detected per slice. We then finetuned MELD from an intermediate checkpoint in the 4th epoch with RAdam for 4 epochs using the same learning rate schedule (10 ?5 to 10 ?6 ). In each finetuning epoch, we kept the original 22K key slices and randomly selected 10K unlabeled slices to add into the training set. MAM and NRM were used to mine missing annotations and reliable negative region in these 32K slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Metrics</head><p>The free-response receiver operating characteristic (FROC) curve is the standard metric in lesion detection <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b34">34]</ref>. Sensitivities at different number of FPs per image are calculated to show the recall at different precision levels. We evaluate the ULD accuracy on the fully-annotated volumetric test set of DeepLesion. Following the LUNA challenge <ref type="bibr" target="#b33">[33]</ref>, sensitivities at 1/8, 1/4, 1/2, 1, 2, 4, 8 FPs per sub-volume are computed. Note that our 2.5D framework outputs 2D detections per slice, while this metric is for 3D detections. Thus, we designed a simple heuristic approach to stack 2D boxes to 3D ones if the intersection over union (IoU) of two 2D boxes in consecutive slices is greater than 0.5. If any 2D crosssection of a stacked 3D box has an IoU &gt; 0.5 with a 2D ground-truth box, the 3D box is counted as a TP. To compare with prior work, we also calculated the sensitivities at 0.5, 1, 2, and 4 FPs per key slice on the key-slice test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>In this section, we evaluate the effectiveness of our proposed algorithms: multiexpert lesion detector (MELD), missing annotation matching (MAM), and negative region mining (NRM). Our baseline method is the previous state of the art on DeepLesion, MULAN <ref type="bibr" target="#b44">[44]</ref>. In <ref type="table">Table 1</ref>, we can find that MELD outperformed the baseline by 1.7% in average sensitivity at different FP levels. Adding MAM and NRM both significantly boosted the accuracy. This means that the missing annotations play a critical role in the detector's performance. MAM added matched lesions to the positive sample set to make the algorithm learn more about the appearance of different lesions. NRM removed suspicious lesions from the negative sample set to reduce its noise, so that the algorithm can learn the appearance of normal tissues better. Finally, MELD with both MAM and NRM achieved the best result, a relative improvement of 29% compared to the baseline. We also explored to add different single-type datasets to mine suspicious lesions in NRM. <ref type="table">Table 2</ref> listed the detection accuracy of lesions in different organs. We can see that adding a dataset is generally beneficial for lesions in the corresponding organ, confirming the effectiveness of our algorithm to transfer knowledge from single-type datasets. The influence of different parameter values is studied in <ref type="figure" target="#fig_1">Fig. 5</ref>. In MAM, if the distance threshold ? is too small, fewer missing annotations will be matched, providing less new information; If it is too large, the matched missing annotations may be noisy. Sub-plot (b) shows that adding unlabeled training images is helpful. With MAM and NRM, the accuracy was already improved on the original training set with no added slices (from MELD's 34.1% in <ref type="table">Table 1</ref> to 39.4%). With more unlabeled slices added, MAM and NRM can find positive and negative samples that bring new information, especially for under-represented body parts in the original training set. The accuracy reached the best when the number of added slices is about half of the size of the original training set.</p><p>The joint training strategy in MELD can improve the baseline not only on DeepLesion, but also on single-type datasets, especially when the number of training samples is small. Note that it is not our goal to compare with best al-   <ref type="figure">Fig. 6</ref>. Comparison of the baseline <ref type="bibr" target="#b44">[44]</ref> and MELD with different proportions of training data in the single-type datasets. On LUNA, we report the average sensitivity at 1/8 ? 8 FPs per volume <ref type="bibr" target="#b33">[33]</ref>. On LiTS and NIH-LN which have ground-truth masks, we report the Dice score.</p><p>gorithms specially designed for each single-type dataset. We combined DeepLesion with a proportion of training volumes from all single-type datasets to train MELD. For comparison, we trained the baseline <ref type="bibr" target="#b44">[44]</ref> with one single-type dataset each time of the same training size. Evaluation was made on the validation set (20% of each dataset). <ref type="figure">Fig. 6</ref> shows that MELD always outperformed the baseline on the three single-type datasets. MELD's superiority is more evident when the number of training data is getting smaller. This is because DeepLesion contains lesions in a variety of organs, so it can help the single-type datasets learn effective features in the network backbone and organ heads. It is especially useful in medical image analysis where training data is often limited. It also indicates that the network has the capacity to learn different lesion types in multiple datasets at the same time. Among the three single-type datasets, lung nodules have relatively distinct appearance <ref type="figure" target="#fig_0">(Fig. 2)</ref>, thus are easier to learn. Besides, LUNA has the more training data, so the superiority of MELD is smaller. Some liver tumors have clear separation with normal tissues, while others can be subtle, making it a harder task. Lymph nodes exist throughout the body and are sometimes hard to be discriminated from the surrounding vessels, muscles, and other organs, leading to the lowest accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with Other Methods</head><p>Previous works were evaluated on the partially-labeled key-slice test set. We use the same criterion and compare with other methods in <ref type="table">Table 3</ref>. MELD outperformed the previous state-of-the-art method, MULAN, either without or with the extra training information of 171 lesion tags <ref type="bibr" target="#b44">[44]</ref>. MAM and NRM further boosted the accuracy and demonstrated that the mined missing annotations and reliable negative regions are helpful. Different strategies to combine multiple lesion datasets are compared in Table 4. The baseline <ref type="bibr" target="#b44">[44]</ref> is trained using a single dataset (DeepLesion). A straightforward way to incorporate the single-type datasets is to directly concatenate them with DeepLesion and treat them as one task. Another method is to only sample positive regions from them to avoid the influence of unannotated lesions of other types. These two methods combine multiple datasets in the data level and slightly improved the baseline. Because the datasets have heterogeneous labels, a better solution is to use multi-task learning and treat each dataset as a task and let them learn shared features. We proposed negative region mining to leverage single-type datasets in a novel way: using them to mine suspicious lesions in a universal dataset, which is similar to knowledge distillation <ref type="bibr" target="#b13">[14]</ref>. We also find that treating the mined lesions as ignore is better than regarding them as true lesions, possibly because they contain some noise and concept shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Results</head><p>Qualitative comparison of the baseline <ref type="bibr" target="#b44">[44]</ref> and the proposed method is shown in <ref type="figure" target="#fig_3">Fig. 7</ref>. The baseline mistakenly detected vessels in the lung and liver, diaphragm, and bowels because of their similar appearance with lesions. These FPs have been reduced notably by our MELD+MAM+NRM. From <ref type="table">Table 1</ref> we can find that the baseline achieved 50% recall at 4 FPs per sub-volume while the proposed method achieved it at 2 FPs. On the other hand, the scores of TPs also increased in our method. This is because MAM added more positive samples and NRM excluded many missing annotations when finetuning MELD. Without the wrong negative training signals, true lesions can be learned more confidently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a framework to mine positive and negative regions from a partially-labeled dataset for lesion detection. A multi-expert lesion detector with organ stratification was proposed to generate lesion proposals. Medical knowledge was leveraged to find missing annotations with an embedding matching approach. Multiple single-type datasets were utilized to mine suspicious lesions and generate reliable negative regions, so as to transfer their knowledge to the universal lesion detection model. As a result, our framework provides a powerful means to exploit multi-source, heterogeneously and imperfectly labeled data, significantly pushing forward universal lesion detection performance. The four lesion datasets used in our work are summarized in <ref type="table" target="#tab_6">Table 5</ref>. <ref type="figure" target="#fig_4">Fig. 8</ref> shows the statistics of major organs of lesions in DeepLesion <ref type="bibr" target="#b45">[45]</ref>. Based on the lesion tags provided by <ref type="bibr" target="#b43">[43]</ref>, we analyzed 17,705 lesions with body part tags. Lymph node (LN), lung, and liver are the most common organs, which are covered by the organ heads in our multi-expert lesion detector (MELD) and our chosen single-type lesion datasets <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref>. Note that it is easy to extend our proposed MELD and negative region mining (NRM) to more organs heads and more lesion datasets (e.g., tumors in kidney <ref type="bibr" target="#b1">[2]</ref>, pancreas, colon <ref type="bibr" target="#b35">[35]</ref>, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">More Details on Datasets</head><p>To observe the distribution of the four datasets, we calculated the 256D lesion embeddings from LesaNet <ref type="bibr" target="#b43">[43]</ref> and visualize them using t-SNE <ref type="bibr" target="#b24">[25]</ref>. From <ref type="figure">Fig. 9</ref>, we can find the single-type datasets lie within subspaces of DeepLesion. NIH-LN is more scattered as lymph nodes exist throughout the body and have diverse contextual appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">More Details on Preprocessing</head><p>The image preprocessing and data augmentation steps in our experiments are described in this section. We tried to build a unified lesion detection/segmentation framework for various datasets, and used the same workflow for all input images. First, we normalized the orientations of x, y, and z-axes of all datasets to the same direction. Then, we rescaled the 12-bit CT intensity range to floating-point numbers in [0,255] using a single windowing (-1024-3071 HU) that covers the intensity ranges of the lung, soft tissue, and bone. Every axial slice was resized so that each pixel corresponds to 0.8mm. We interpolated in the z-axis to make the slice intervals of all volumes to be 2mm.   DeepLesion LUNA LiTS NIH-Lymph Node <ref type="figure">Fig. 9</ref>. Scatter map of embeddings <ref type="bibr" target="#b43">[43]</ref> of lesions in DeepLesion <ref type="bibr" target="#b45">[45]</ref>, LUNA <ref type="bibr" target="#b33">[33]</ref>, LiTS <ref type="bibr" target="#b3">[4]</ref>, and NIH-LN <ref type="bibr" target="#b0">[1]</ref> computed by t-SNE.</p><p>clipped for computation efficiency. When training, we did data augmentation by randomly resizing each slice with a ratio of 0.8?1.2 and randomly shifting the image and annotation by -8?8 pixels in x and y axes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Exemplar lesions from DeepLesion (first row), LUNA (second row 1-3), LiTS (4-6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>of unlabeled slices added (% of the original training set) Parameter study of the proposed algorithms. Average sensitivity at FP=0.125?8 per sub-volume on DeepLesion is shown. In (b), the x-axis is the ratio between the number of added slices and the original training size (22K key slices).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison of the baseline [44] (top row) and our proposed method (bottom row) on the volumetric test set of DeepLesion. Green and red boxes indicate TPs and FPs, respectively. Numbers above boxes are detection scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Statistics of major organs of lesions in DeepLesion<ref type="bibr" target="#b43">[43]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, ? 3 = 1.0, 0.1, 0.0, 0.9</figDesc><table><row><cell>Training/test image from dataset d</cell><cell>Feature map</cell><cell>Detection scores</cell><cell>Losses</cell><cell cols="3">Training phase</cell></row><row><cell>2.5D</cell><cell></cell><cell></cell><cell></cell><cell cols="2">= ?</cell><cell>,</cell></row><row><cell>Feature extractor</cell><cell></cell><cell>?</cell><cell></cell><cell>=</cell><cell>?</cell><cell>?</cell><cell>,</cell></row><row><cell></cell><cell>RoIAlign</cell><cell></cell><cell></cell><cell cols="3">Inference</cell></row><row><cell></cell><cell>RPN d Proposals</cell><cell></cell><cell></cell><cell cols="2">phase</cell></row><row><cell></cell><cell></cell><cell cols="2">Shared mask head</cell><cell></cell><cell></cell></row></table><note>0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Results with different components of the proposed framework . Sensitivity (%) at different FPs per sub-volume on the volumetric test set of DeepLesion is shown. Organ-stratified results with different single-type datasets used. In each row, we use certain datasets in NRM, then compute the detection accuracy of lesions in different organs. Bold results indicate the best accuracy for each organ (column). Average sensitivity at FP=0.125?8 per sub-volume on DeepLesion is shown.</figDesc><table><row><cell>Method</cell><cell cols="2">FP@0.125 0.25 0.5</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>Average</cell></row><row><cell>Baseline [44]</cell><cell>7.6</cell><cell cols="5">12.6 20.7 30.6 42.1 51.8 61.2</cell><cell>32.4</cell></row><row><cell>MELD</cell><cell>7.7</cell><cell cols="5">13.5 21.3 32.3 43.7 54.8 65.2</cell><cell>34.1</cell></row><row><cell>MELD+MAM</cell><cell>12.9</cell><cell cols="5">20.8 29.2 39.0 49.6 58.7 67.0</cell><cell>39.6</cell></row><row><cell>MELD+NRM</cell><cell>13.5</cell><cell cols="5">20.2 29.5 38.8 49.4 58.5 67.3</cell><cell>39.6</cell></row><row><cell>MELD+MAM+NRM</cell><cell>16.0</cell><cell cols="5">22.8 32.0 41.7 51.3 60.3 68.3</cell><cell>41.8</cell></row><row><cell cols="2">Single-type dataset</cell><cell cols="5">Lung Liver Lymph node Overall</cell></row><row><cell cols="2">LUNA (lung nodules)</cell><cell>35.5 31.7</cell><cell></cell><cell>31.7</cell><cell>39.4</cell><cell></cell></row><row><cell cols="2">LiTS (liver tumors)</cell><cell>34.6 39.1</cell><cell></cell><cell>33.8</cell><cell>40.1</cell><cell></cell></row><row><cell cols="3">NIH-LN (lymph nodes) 34.0 31.8</cell><cell></cell><cell>33.0</cell><cell>39.6</cell><cell></cell></row><row><cell>All</cell><cell></cell><cell>35.1 38.9</cell><cell></cell><cell>35.3</cell><cell cols="2">41.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparison with previous studies. Sensitivity (%) at different FPs per slice on the key-slice test set of DeepLesion is shown. Different strategies to combine multiple datasets. Average sensitivity (%) at FP=0.125?8 per sub-volume on the volumetric test set of DeepLesion is shown.</figDesc><table><row><cell>Method</cell><cell>FP@0.5</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>Average</cell></row><row><cell>ULDor [37]</cell><cell>52.9</cell><cell cols="3">64.8 74.8 84.4</cell><cell>69.2</cell></row><row><cell>Domain-attentive universal detector [38]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.3</cell><cell>-</cell></row><row><cell>Volumetric attention [39]</cell><cell>69.1</cell><cell cols="3">77.9 83.8 87.5</cell><cell>79.6</cell></row><row><cell>MVP-Net [19]</cell><cell>73.8</cell><cell cols="3">81.8 87.6 91.3</cell><cell>83.6</cell></row><row><cell>MULAN (without tags) [44]</cell><cell>76.1</cell><cell cols="3">82.5 87.5 90.9</cell><cell>84.3</cell></row><row><cell>MULAN (with 171 tags) [44]</cell><cell>76.1</cell><cell cols="3">83.7 88.8 92.3</cell><cell>85.2</cell></row><row><cell>MELD (proposed)</cell><cell>77.8</cell><cell cols="3">84.8 89.0 91.8</cell><cell>85.9</cell></row><row><cell>MELD+MAM+NRM (proposed)</cell><cell>78.6</cell><cell cols="3">85.5 89.6 92.5</cell><cell>86.6</cell></row><row><cell>Method</cell><cell></cell><cell cols="3">Average sensitivity</cell><cell></cell></row><row><cell>Single dataset (Baseline [44])</cell><cell></cell><cell></cell><cell>32.4</cell><cell></cell><cell></cell></row><row><cell>Dataset concatenation</cell><cell></cell><cell></cell><cell>34.5</cell><cell></cell><cell></cell></row><row><cell cols="2">Dataset concatenation (positive only)</cell><cell></cell><cell>33.1</cell><cell></cell><cell></cell></row><row><cell>Multi-task learning</cell><cell></cell><cell></cell><cell>35.1</cell><cell></cell><cell></cell></row><row><cell cols="2">Proposed (suspicious lesions as positive)</cell><cell></cell><cell>40.7</cell><cell></cell><cell></cell></row><row><cell cols="2">Proposed (suspicious lesions as ignore)</cell><cell></cell><cell>41.8</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Statistics of the four lesion datasets used in our work.</figDesc><table><row><cell>Name</cell><cell></cell><cell>Lesion types</cell><cell>Organs</cell><cell>#</cell><cell>3D</cell><cell># 2D</cell><cell>#</cell><cell>Fully-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Volumes</cell><cell>Slices</cell><cell>Lesions</cell><cell>annotated?</cell></row><row><cell cols="2">DeepLesion [45]</cell><cell>Various</cell><cell>Whole</cell><cell>10,594</cell><cell></cell><cell>928K</cell><cell>32,735</cell><cell>No</cell></row><row><cell></cell><cell></cell><cell></cell><cell>body</cell><cell>sub-</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">volumes</cell><cell></cell><cell></cell></row><row><cell cols="2">LUNA (LUng Nod-</cell><cell>Lung nodule</cell><cell>Lung</cell><cell>888</cell><cell></cell><cell>226K</cell><cell>1,186</cell><cell>Yes</cell></row><row><cell cols="2">ule Analysis) [33]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LiTS (LIver Tu-</cell><cell>Liver tumor</cell><cell>Liver</cell><cell>130</cell><cell></cell><cell>85K</cell><cell>908</cell><cell>Yes</cell></row><row><cell cols="2">mor Segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Benchmark) [4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NIH-LN</cell><cell>(NIH-</cell><cell>Mediastinal</cell><cell>Lymph</cell><cell>176</cell><cell></cell><cell>134K</cell><cell>983</cell><cell>Yes</cell></row><row><cell cols="2">Lymph Node) [1]</cell><cell>and abdominal</cell><cell>node</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>lymph nodes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The black borders in images were</figDesc><table><row><cell>lymph node -29.78 % lung -22.01 % liver -11.51 % kidney -5.72 % bone -2.90 % adrenal gland -2.41 % pancreas -2.41 % spleen -1.31 % others -21.95 %</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">These annotations will be made publicly available. We were unable to annotate full volumes as images in DeepLesion were released in sub-volumes containing 7?220 consecutive slices.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://wiki.cancerimagingarchive.net/display/Public/CT+Lymph+Nodes" />
		<title level="m">CT Lymph Nodes dataset -The Cancer Imaging Archive</title>
		<imprint>
			<publisher>TCIA) Public Access</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="https://kits19.grand-challenge.org/" />
		<title level="m">KiTS19 Challenge Homepage</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully convolutional network for liver segmentation and lesions detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-319-46976-8{_}9</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-319-46976-8{_}9" />
	</analytic>
	<monogr>
		<title level="j">Int. Work. Deep Learn. Med. Image Anal</title>
		<imprint>
			<biblScope unit="page" from="77" to="85" />
			<date type="published" when="2016-10" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Liver Tumor Segmentation Benchmark (LiTS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vorontsov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1901.04056" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic segmentation and detection of mediastinal lymph nodes and anatomical structures in CT data for lung cancer staging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>J?rgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Leira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lang?</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-019-01948-8</idno>
		<ptr target="https://doi.org/10.1007/s11548-019-01948-8" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the limits of cross-domain generalization in automated X-ray prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hashir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bertrand</surname></persName>
		</author>
		<ptr target="https://github.com/ieee8023/xray-generalization" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Accurate Pulmonary Nodule Detection in Computed Tomography Images Using Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.04303" />
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="559" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Multi-Class Segmentations From Single-Class Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dmitriev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cvpr. pp</title>
		<imprint>
			<biblScope unit="page" from="9501" to="9511" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Few-Example Object Detection with Model Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2844853</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2844853" />
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1641" to="1654" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilevel Contextual 3-D CNNs for False Positive Reduction in Pulmonary Nodule Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1558" to="1567" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">New response evaluation criteria in solid tumours: Revised RECIST guideline (version 1.1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Eisenhauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Therasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bogaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dancey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arbuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gwyther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dodd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lacombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verweij</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ejca.2008.10.026</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ejca.2008.10.026" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Cancer</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="247" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Universal multi-modal deep network for classification and segmentation of medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negahdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/8363710/" />
	</analytic>
	<monogr>
		<title level="j">In: ISBI. vol</title>
		<imprint>
			<biblScope unit="page" from="872" to="876" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1503.02531" />
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised hard example mining from videos for improved object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1808.04285" />
	</analytic>
	<monogr>
		<title level="m">ECCV. vol. 11217 LNCS</title>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="316" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1811.00982" />
	</analytic>
	<monogr>
		<title level="j">Tech. rep</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1709.07330" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">MVP-Net: Multi-view FPN with Position-aware Attention for Deep Universal Lesion Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.04247" />
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy-or Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.08324" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>S?nchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mediastinal lymph node detection and station mapping on chest CT using spatial priors and random forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4362" to="4374" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using Tree-Based Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/vandermaaten14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sampling Techniques for Large-Scale Object Detection from Sparsely Annotated Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1811.10862" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TKDE.2009.191</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2009.191" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1712.04440" />
		<title level="m">Data Distillation: Towards Omni-Supervised Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2577031</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2577031" />
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1170" to="1181" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning in medical imaging and radiation therapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sahiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pezeshk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Hadjiiski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Drukker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Giger</surname></persName>
		</author>
		<idno type="DOI">http:/doi.wiley.com/10.1002/mp.13264</idno>
		<ptr target="http://doi.wiley.com/10.1002/mp.13264" />
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The LUNA16 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Traverso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bel</forename><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2017.06.015</idno>
		<ptr target="http://dx.doi.org/10.1016/j.media.2017.06.015" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mollura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A large annotated medical image dataset for the development and evaluation of segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopp-Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09063</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Embracing Imperfect Datasets: A Review of Deep Learning Solutions for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jeyaseelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.10454" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">ULDor: A Universal Lesion Detector for CT Scans with Pseudo Masks and Hard Negative Example Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1901.06359.pdfhttp://arxiv.org/abs/1901.06359" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>ISBI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Towards Universal Object Detection by Domain Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.04402" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Volumetric Attention for 3D Medical Image Segmentation and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-030-32226-7{_}20</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-030-32226-7{_}20" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Soft Sampling for Robust Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1806.06986" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Less is more: Simultaneous view classification and landmark detection for abdominal ultrasound images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grbic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1805.10376.pdf" />
	</analytic>
	<monogr>
		<title level="m">MICCAI. vol. 11071 LNCS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="711" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3D context enhanced region-based convolutional neural network for end-to-end lesion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-030-00928-1{_}58</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-030-00928-1{_}58" />
	</analytic>
	<monogr>
		<title level="m">MICCAI. vol. 11070 LNCS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sandfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<title level="m">Holistic and Comprehensive Annotation of Clinically Significant Findings on Diverse CT Images : Learning from Radiology Reports and Label Ontology</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">MULAN : Multitask Universal Lesion Analysis Network for Joint Lesion Detection , Tagging , and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sandfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DeepLesion: automated mining of largescale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JMI.5.3.036501</idno>
		<ptr target="https://doi.org/10.1117/1.JMI.5.3.036501" />
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep Lesion Graphs in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-scale Lesion Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.10535" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Prior-aware Neural Network for Partially-Supervised Multi-Organ Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.06346" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">DeepEM: Deep 3D ConvNets With EM For Weakly Supervised Pulmonary Nodule Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Vang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="812" to="820" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zlocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.02283" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
